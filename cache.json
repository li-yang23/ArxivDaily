{"2025-02-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.02749v3","updated":"2025-02-11T18:59:47Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07780v1","updated":"2025-02-11T18:59:35Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08446v2","updated":"2025-02-11T18:59:26Z","published":"2024-06-12T17:37:09Z","title":"OLMES: A Standard for Language Model Evaluations","summary":"  Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.\n","authors":["Yuling Gu","Oyvind Tafjord","Bailey Kuehl","Dany Haddad","Jesse Dodge","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.08446v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.01130v3","updated":"2025-02-11T18:55:04Z","published":"2024-07-01T09:51:48Z","title":"Cross-Lingual Transfer Learning for Speech Translation","summary":"  There has been increasing interest in building multilingual foundation models\nfor NLP and speech research. This paper examines how to expand the speech\ntranslation capability of these models with restricted data. Whisper, a speech\nfoundation model with strong performance on speech recognition and English\ntranslation, is used as the example model. Using speech-to-speech retrieval to\nanalyse the audio representations generated by the encoder, we show that\nutterances from different languages are mapped to a shared semantic space. This\nshared embedding space can then be leveraged for zero-shot cross-lingual\ntransfer in speech translation. By fine-tuning the Whisper decoder with only\nEnglish-to-Chinese speech translation data, improved performance for\ntranslation to Chinese can be obtained for multiple languages, in addition to\nEnglish. Furthermore, for languages related to those seen in training it is\npossible to perform speech translation, despite the model never seeing the\nlanguage in training, or being able to perform transcription.\n","authors":["Rao Ma","Mengjie Qian","Yassir Fathullah","Siyuan Tang","Mark Gales","Kate Knill"],"pdf_url":"https://arxiv.org/pdf/2407.01130v3.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2406.08598v3","updated":"2025-02-11T18:42:44Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07755v1","updated":"2025-02-11T18:32:24Z","published":"2025-02-11T18:32:24Z","title":"An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa\n  and Dynamic Contextual Positional Gating","summary":"  This paper presents a novel Natural Language Processing (NLP) framework for\nenhancing medical diagnosis through the integration of advanced techniques in\ndata augmentation, feature extraction, and classification. The proposed\napproach employs back-translation to generate diverse paraphrased datasets,\nimproving robustness and mitigating overfitting in classification tasks.\nLeveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with\nDynamic Contextual Positional Gating (DCPG), the model captures fine-grained\ncontextual and positional relationships, dynamically adjusting the influence of\npositional information based on semantic context to produce high-quality text\nembeddings. For classification, an Attention-Based Feedforward Neural Network\n(ABFNN) is utilized, effectively focusing on the most relevant features to\nimprove decision-making accuracy. Applied to the classification of symptoms,\nclinical notes, and other medical texts, this architecture demonstrates its\nability to address the complexities of medical data. The combination of data\naugmentation, contextual embedding generation, and advanced classification\nmechanisms offers a robust and accurate diagnostic tool, with potential\napplications in automated medical diagnosis and clinical decision support. This\nmethod demonstrates the effectiveness of the proposed NLP framework for medical\ndiagnosis, achieving remarkable results with an accuracy of 99.78%, recall of\n99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only\nunderscore the model's robust performance in classifying medical texts with\nexceptional precision and reliability but also highlight its superiority over\nexisting methods, making it a highly promising tool for automated diagnostic\nsystems.\n","authors":["Mohammad Ali Labbaf Khaniki","Sahabeh Saadati","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2502.07755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07747v1","updated":"2025-02-11T18:14:44Z","published":"2025-02-11T18:14:44Z","title":"WHODUNIT: Evaluation benchmark for culprit detection in mystery stories","summary":"  We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.\n","authors":["Kshitij Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04463v2","updated":"2025-02-11T18:06:02Z","published":"2025-02-06T19:18:16Z","title":"Training Language Models to Reason Efficiently","summary":"  Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.\n","authors":["Daman Arora","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.04463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v2","updated":"2025-02-11T17:40:41Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v2.pdf","comment":"Accepted to ACM Computing Surveys; 55 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07717v1","updated":"2025-02-11T17:18:47Z","published":"2025-02-11T17:18:47Z","title":"Making Language Models Robust Against Negation","summary":"  Negation has been a long-standing challenge for language models. Previous\nstudies have shown that they struggle with negation in many natural language\nunderstanding tasks. In this work, we propose a self-supervised method to make\nlanguage models more robust against negation. We introduce a novel task, Next\nSentence Polarity Prediction (NSPP), and a variation of the Next Sentence\nPrediction (NSP) task. We show that BERT and RoBERTa further pre-trained on our\ntasks outperform the off-the-shelf versions on nine negation-related\nbenchmarks. Most notably, our pre-training tasks yield between 1.8% and 9.1%\nimprovement on CondaQA, a large question-answering corpus requiring reasoning\nover negation.\n","authors":["MohammadHossein Rezaei","Eduardo Blanco"],"pdf_url":"https://arxiv.org/pdf/2502.07717v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.06663v4","updated":"2025-02-11T16:57:29Z","published":"2024-08-13T06:28:43Z","title":"Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","summary":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","authors":["Kaiser Sun","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2408.06663v4.pdf","comment":"Updated Draft"},{"id":"http://arxiv.org/abs/2406.12009v3","updated":"2025-02-11T16:49:17Z","published":"2024-06-17T18:25:02Z","title":"FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure","summary":"  Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets.\n","authors":["Ziyue Xu","Peilin Zhou","Xinyu Shi","Jiageng Wu","Yikang Jiang","Dading Chong","Bin Ke","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2406.12009v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13921v3","updated":"2025-02-11T16:48:15Z","published":"2025-01-23T18:59:02Z","title":"The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities","summary":"  Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource.\n","authors":["MediaTek Research"," :","Chan-Jan Hsu","Chia-Sheng Liu","Meng-Hsi Chen","Muxi Chen","Po-Chun Hsu","Yi-Chang Chen","Da-Shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2501.13921v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00507v2","updated":"2025-02-11T16:39:55Z","published":"2025-02-01T17:55:58Z","title":"A statistically consistent measure of Semantic Variability using\n  Language Models","summary":"  To address the issue of variability in the output generated by a language\nmodel, we present a measure of semantic variability that is statistically\nconsistent under mild assumptions. This measure, denoted as semantic spectral\nentropy, is a easy to implement algorithm that requires just off the shelf\nlanguage models. We put very few restrictions on the language models and we\nhave shown in a clear simulation studies that such method can generate accurate\nmetric despite randomness that arise from the language models.\n","authors":["Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07687v1","updated":"2025-02-11T16:38:16Z","published":"2025-02-11T16:38:16Z","title":"Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition","summary":"  We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited.\n","authors":["Imry Ziv","Nur Lan","Emmanuel Chemla","Roni Katzir"],"pdf_url":"https://arxiv.org/pdf/2502.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12645v3","updated":"2025-02-11T16:36:32Z","published":"2024-06-18T14:13:13Z","title":"Evaluating Evidence Attribution in Generated Fact Checking Explanations","summary":"  Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.\n","authors":["Rui Xing","Timothy Baldwin","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2406.12645v3.pdf","comment":"Accepted to NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.07683v1","updated":"2025-02-11T16:35:04Z","published":"2025-02-11T16:35:04Z","title":"exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem","summary":"  The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.\n","authors":["Sajad Ebrahimi","Sara Salamat","Negar Arabzadeh","Mahdi Bashari","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07677v1","updated":"2025-02-11T16:27:28Z","published":"2025-02-11T16:27:28Z","title":"Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach","summary":"  Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing\n","authors":["Param Kulkarni","Yingchi Liu","Hao-Ming Fu","Shaohua Yang","Isuru Gunasekara","Matt Peloquin","Noah Spitzer-Williams","Xiaotian Zhou","Xiaozhong Liu","Zhengping Ji","Yasser Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2502.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2402.11355v5","updated":"2025-02-11T16:03:35Z","published":"2024-02-17T18:12:02Z","title":"A Practical Method for Generating String Counterfactuals","summary":"  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n","authors":["Matan Avitan","Ryan Cotterell","Yoav Goldberg","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2402.11355v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.05670v2","updated":"2025-02-11T16:02:57Z","published":"2025-02-08T19:13:40Z","title":"Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences","summary":"  Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.\n","authors":["Ada Defne Tur","Gaurav Kamath","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2502.05670v2.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.07663v1","updated":"2025-02-11T15:56:22Z","published":"2025-02-11T15:56:22Z","title":"Human Decision-making is Susceptible to AI-driven Manipulation","summary":"  Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.\n","authors":["Sahand Sabour","June M. Liu","Siyang Liu","Chris Z. Yao","Shiyao Cui","Xuanming Zhang","Wen Zhang","Yaru Cao","Advait Bhat","Jian Guan","Wei Wu","Rada Mihalcea","Tim Althoff","Tatia M. C. Lee","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07663v1.pdf","comment":"Work in progress. Code and data will be made available via\n  https://github.com/Sahandfer/Manipulation-Susceptibility"},{"id":"http://arxiv.org/abs/2502.06556v2","updated":"2025-02-11T15:48:42Z","published":"2025-02-10T15:24:30Z","title":"ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms","summary":"  Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.\n","authors":["Yibo Wang","Congying Xia","Wenting Zhao","Jiangshu Du","Chunyu Miao","Zhongfen Deng","Philip S. Yu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2502.06556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05878v2","updated":"2025-02-11T15:45:52Z","published":"2025-02-09T12:26:05Z","title":"Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models","summary":"  Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field.\n","authors":["Mengxi Xiao","Zihao Jiang","Lingfei Qian","Zhengyu Chen","Yueru He","Yijing Xu","Yuecheng Jiang","Dong Li","Ruey-Ling Weng","Min Peng","Jimin Huang","Sophia Ananiadou","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2502.05878v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.03736v3","updated":"2025-02-11T15:42:19Z","published":"2024-06-06T04:22:11Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data","summary":"  Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.\n","authors":["Jingyang Ou","Shen Nie","Kaiwen Xue","Fengqi Zhu","Jiacheng Sun","Zhenguo Li","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.03736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01192v3","updated":"2025-02-11T15:36:41Z","published":"2024-11-02T09:39:49Z","title":"Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and\n  Cross-Cultural Embedding Models and Benchmarks","summary":"  We introduce {\\bf Swan}, a family of embedding models centred around the\nArabic language, addressing both small-scale and large-scale use cases. Swan\nincludes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on\nArMistral, a pretrained Arabic large language model. To evaluate these models,\nwe propose ArabicMTEB, a comprehensive benchmark suite that assesses\ncross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text\nembedding performance, covering eight diverse tasks and spanning 94 datasets.\nSwan-Large achieves state-of-the-art results, outperforming\nMultilingual-E5-large in most Arabic tasks, while the Swan-Small consistently\nsurpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan\nmodels are both dialectally and culturally aware, excelling across various\nArabic domains while offering significant monetary efficiency. This work\nsignificantly advances the field of Arabic language modelling and provides\nvaluable resources for future research and applications in Arabic natural\nlanguage processing. Our models and benchmark are available at our GitHub page:\n\\href{https://github.com/UBC-NLP/swan}{https://github.com/UBC-NLP/swan}\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Abdellah El Mekki","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2411.01192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07642v1","updated":"2025-02-11T15:33:17Z","published":"2025-02-11T15:33:17Z","title":"FoQA: A Faroese Question-Answering Dataset","summary":"  We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.\n","authors":["Annika Simonsen","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.07642v1.pdf","comment":"Camera-ready version for RESOURCEFUL workshop, 2025"},{"id":"http://arxiv.org/abs/2502.07637v1","updated":"2025-02-11T15:25:10Z","published":"2025-02-11T15:25:10Z","title":"BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish","summary":"  In this study, we introduce the process for creating BiaSWE, an\nexpert-annotated dataset tailored for misogyny detection in the Swedish\nlanguage. To address the cultural and linguistic specificity of misogyny in\nSwedish, we collaborated with experts from the social sciences and humanities.\nOur interdisciplinary team developed a rigorous annotation process,\nincorporating both domain knowledge and language expertise, to capture the\nnuances of misogyny in a Swedish context. This methodology ensures that the\ndataset is not only culturally relevant but also aligned with broader efforts\nin bias detection for low-resource languages. The dataset, along with the\nannotation guidelines, is publicly available for further research.\n","authors":["Ktriin Kukk","Danila Petrelli","Judit Casademont","Eric J. W. Orlowski","Micha Dzieliski","Maria Jacobson"],"pdf_url":"https://arxiv.org/pdf/2502.07637v1.pdf","comment":"To appear at NoDaLiDa 2025"},{"id":"http://arxiv.org/abs/2502.07629v1","updated":"2025-02-11T15:17:00Z","published":"2025-02-11T15:17:00Z","title":"Exploring Mobile Touch Interaction with Large Language Models","summary":"  Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices.\n","authors":["Tim Zindulka","Jannek Sekowski","Florian Lehmann","Daniel Buschek"],"pdf_url":"https://arxiv.org/pdf/2502.07629v1.pdf","comment":"21 pages, 16 figures, 3 tables, ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.07623v1","updated":"2025-02-11T15:10:23Z","published":"2025-02-11T15:10:23Z","title":"Lexical categories of stem-forming roots in Mapudngun verb forms","summary":"  After developing a computational system for morphological analysis of the\nMapuche language, and evaluating it with texts from various authors and styles,\nit became necessary to verify the linguistic assumptions of the source used as\nthe basis for implementing this tool.\n  In the present work, the primary focus is on the lexical category\nclassification of Mapud\\\"ungun roots recognised as verbal in the source\nutilised for the development of the morphological analysis system.\n  The results of this lexical category revision directly benefit the\ncomputational analyser, as they are implemented as soon as they are verified.\nAdditionally, it is hoped that these results will help clarify some\nuncertainties about lexical categories in the Mapuche language.\n  This work addresses a preliminary task to identify the valency of true verbal\nroots, the results of which will be presented in a subsequent work that\ncomplements this article.\n","authors":["Andrs Chanda"],"pdf_url":"https://arxiv.org/pdf/2502.07623v1.pdf","comment":"22 pages, 2 large tables, 2 sample tables"},{"id":"http://arxiv.org/abs/2502.07616v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Tractable Transformers for Flexible Conditional Generation","summary":"  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n","authors":["Anji Liu","Xuejie Liu","Dayuan Zhao","Mathias Niepert","Yitao Liang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2502.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14317v2","updated":"2025-02-11T14:51:08Z","published":"2024-08-26T14:45:03Z","title":"Claim Verification in the Age of Large Language Models: A Survey","summary":"  The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.\n","authors":["Alphaeus Dmonte","Roland Oruche","Marcos Zampieri","Prasad Calyam","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2408.14317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07601v1","updated":"2025-02-11T14:50:43Z","published":"2025-02-11T14:50:43Z","title":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models","summary":"  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/\n","authors":["Jiacong Xu","Shao-Yuan Lo","Bardia Safaei","Vishal M. Patel","Isht Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2502.07601v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07599v1","updated":"2025-02-11T14:49:44Z","published":"2025-02-11T14:49:44Z","title":"DPO-Shift: Shifting the Distribution of Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) and its variants have become\nincreasingly popular for aligning language models with human preferences. These\nmethods aim to teach models to better distinguish between chosen (or preferred)\nand rejected (or dispreferred) responses. However, prior research has\nidentified that the probability of chosen responses often decreases during\ntraining, and this phenomenon is known as likelihood displacement. To tackle\nthis challenge, in this work we introduce \\method to controllably shift the\ndistribution of the chosen probability. Then, we show that \\method exhibits a\nfundamental trade-off between improving the chosen probability and sacrificing\nthe reward margin, as supported by both theoretical analysis and experimental\nvalidation. Furthermore, we demonstrate the superiority of \\method over DPO on\ndownstream tasks such as MT-Bench and a designed win rate experiment. We\nbelieve this study shows that the likelihood displacement issue of DPO can be\neffectively mitigated with a simple, theoretically grounded solution. Our code\nis available at https://github.com/Meaquadddd/DPO-Shift.\n","authors":["Xiliang Yang","Feng Jiang","Qianen Zhang","Lei Zhao","Xiao Li"],"pdf_url":"https://arxiv.org/pdf/2502.07599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07586v1","updated":"2025-02-11T14:34:05Z","published":"2025-02-11T14:34:05Z","title":"We Can't Understand AI Using our Existing Vocabulary","summary":"  This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.\n","authors":["John Hewitt","Robert Geirhos","Been Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07586v1.pdf","comment":"Position paper"},{"id":"http://arxiv.org/abs/2502.04964v2","updated":"2025-02-11T14:32:15Z","published":"2025-02-07T14:30:12Z","title":"CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs","summary":"  Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.\n","authors":["Roman Vashurin","Maiya Goloburda","Preslav Nakov","Artem Shelmanov","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2502.04964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v1","updated":"2025-02-11T14:23:13Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v3","updated":"2025-02-11T14:18:17Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Samples for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07575v1","updated":"2025-02-11T14:17:29Z","published":"2025-02-11T14:17:29Z","title":"Towards Efficient and Multifaceted Computer-assisted Pronunciation\n  Training Leveraging Hierarchical Selective State Space Model and Decoupled\n  Cross-entropy Loss","summary":"  Prior efforts in building computer-assisted pronunciation training (CAPT)\nsystems often treat automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD) as separate fronts: the former\naims to provide multiple pronunciation aspect scores across diverse linguistic\nlevels, while the latter focuses instead on pinpointing the precise phonetic\npronunciation errors made by non-native language learners. However, it is\ngenerally expected that a full-fledged CAPT system should perform both\nfunctionalities simultaneously and efficiently. In response to this surging\ndemand, we in this work first propose HMamba, a novel CAPT approach that\nseamlessly integrates APA and MDD tasks in parallel. In addition, we introduce\na novel loss function, decoupled cross-entropy loss (deXent), specifically\ntailored for MDD to facilitate better-supervised learning for detecting\nmispronounced phones, thereby enhancing overall performance. A comprehensive\nset of empirical results on the speechocean762 benchmark dataset demonstrates\nthe effectiveness of our approach on APA. Notably, our proposed approach also\nyields a considerable improvement in MDD performance over a strong baseline,\nachieving an F1-score of 63.85%. Our codes are made available at\nhttps://github.com/Fuann/hmamba\n","authors":["Fu-An Chao","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07575v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2502.07555v1","updated":"2025-02-11T13:48:10Z","published":"2025-02-11T13:48:10Z","title":"O1 Embedder: Let Retrievers Think Before Action","summary":"  The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.\n","authors":["Ruin Yan","Zheng Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2502.07555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07552v1","updated":"2025-02-11T13:41:06Z","published":"2025-02-11T13:41:06Z","title":"Unsupervised Translation of Emergent Communication","summary":"  Emergent Communication (EC) provides a unique window into the language\nsystems that emerge autonomously when agents are trained to jointly achieve\nshared goals. However, it is difficult to interpret EC and evaluate its\nrelationship with natural languages (NL). This study employs unsupervised\nneural machine translation (UNMT) techniques to decipher ECs formed during\nreferential games with varying task complexities, influenced by the semantic\ndiversity of the environment. Our findings demonstrate UNMT's potential to\ntranslate EC, illustrating that task complexity characterized by semantic\ndiversity enhances EC translatability, while higher task complexity with\nconstrained semantic variability exhibits pragmatic EC, which, although\nchallenging to interpret, remains suitable for translation. This research marks\nthe first attempt, to our knowledge, to translate EC without the aid of\nparallel data.\n","authors":["Ido Levy","Orr Paradise","Boaz Carmeli","Ron Meir","Shafi Goldwasser","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2502.07552v1.pdf","comment":"19 pages (including appendix and bibliography), Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07544v1","updated":"2025-02-11T13:30:41Z","published":"2025-02-11T13:30:41Z","title":"Grammar Control in Dialogue Response Generation for Language Learning\n  Chatbots","summary":"  Chatbots based on large language models offer cheap conversation practice\nopportunities for language learners. However, they are hard to control for\nlinguistic forms that correspond to learners' current needs, such as grammar.\nWe control grammar in chatbot conversation practice by grounding a dialogue\nresponse generation model in a pedagogical repository of grammar skills. We\nalso explore how this control helps learners to produce specific grammar. We\ncomprehensively evaluate prompting, fine-tuning, and decoding strategies for\ngrammar-controlled dialogue response generation. Strategically decoding Llama3\noutperforms GPT-3.5 when tolerating minor response quality losses. Our\nsimulation predicts grammar-controlled responses to support grammar acquisition\nadapted to learner proficiency. Existing language learning chatbots and\nresearch on second language acquisition benefit from these affordances. Code\navailable on GitHub.\n","authors":["Dominik Glandorf","Peng Cui","Detmar Meurers","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2502.07544v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07541v1","updated":"2025-02-11T13:28:56Z","published":"2025-02-11T13:28:56Z","title":"Corporate Greenwashing Detection in Text - a Survey","summary":"  Greenwashing is an effort to mislead the public about the environmental\nimpact of an entity, such as a state or company. We provide a comprehensive\nsurvey of the scientific literature addressing natural language processing\nmethods to identify potentially misleading climate-related corporate\ncommunications, indicative of greenwashing. We break the detection of\ngreenwashing into intermediate tasks, and review the state-of-the-art\napproaches for each of them. We discuss datasets, methods, and results, as well\nas limitations and open challenges. We also provide an overview of how far the\nfield has come as a whole, and point out future research directions.\n","authors":["Tom Calamai","Oana Balalau","Tho Le Guenedal","Fabian M. Suchanek"],"pdf_url":"https://arxiv.org/pdf/2502.07541v1.pdf","comment":"35 pages, 1 figure, 21 pages (appendix), working paper"},{"id":"http://arxiv.org/abs/2502.00641v2","updated":"2025-02-11T13:12:16Z","published":"2025-02-02T03:07:45Z","title":"Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance","summary":"  The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.\n","authors":["Borui Xu","Yao Chen","Zeyi Wen","Weiguo Liu","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2502.00641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13381v2","updated":"2025-02-11T12:44:39Z","published":"2025-01-23T04:50:03Z","title":"Do as We Do, Not as You Think: the Conformity of Large Language Models","summary":"  Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.\n","authors":["Zhiyuan Weng","Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.13381v2.pdf","comment":"ICLR 2025 (Oral). Code: https://github.com/Zhiyuan-Weng/BenchForm"},{"id":"http://arxiv.org/abs/2412.19018v4","updated":"2025-02-11T12:39:22Z","published":"2024-12-26T01:56:42Z","title":"Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability","summary":"  Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods.\n","authors":["Ruixi Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.19018v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00233v2","updated":"2025-02-11T12:33:13Z","published":"2024-12-31T02:53:27Z","title":"Zero-Shot Strategies for Length-Controllable Summarization","summary":"  Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications.\n","authors":["Fabian Retkowski","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2501.00233v2.pdf","comment":"Accepted to NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2412.14872v2","updated":"2025-02-11T12:25:11Z","published":"2024-12-19T14:11:15Z","title":"Theoretical Proof that Generated Text in the Corpus Leads to the\n  Collapse of Auto-regressive Language Models","summary":"  Auto-regressive language models (LMs) have been widely used to generate text\non the World Wide Web. The generated text is often collected into the training\ncorpus of the next generations of LMs. Previous work experimentally found that\nLMs collapse when trained on recursively generated text. This paper presents\ntheoretical proof that once a corpus (such as the World Wide Web) begins to\nincorporate generated text, and the training text of each LM is sampled from\nthis corpus, then no matter how small the amount of text generated by each LM\nthat enters the corpus is, after a sufficient amount of time, LM collapse is\nbound to occur. Our proof is validated by a series of experiments showing that\nthe collapsed LMs perform no better than an untrained LM with randomly\ninitialized parameters. By proving the existence of LM collapse, we express our\nconcerns about the current situation in which an increasing amount of generated\ntext may be used in LM training. The source code is available in the online\ndata warehouse: https://github.com/wanglc02/generated-data\n","authors":["Lecheng Wang","Xianjie Shi","Ge Li","Jia Li","Xuanming Zhang","Yihong Dong","Wenpin Jiao","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2412.14872v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.00696v3","updated":"2025-02-11T12:21:13Z","published":"2024-09-01T11:24:54Z","title":"Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation","summary":"  Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.\n","authors":["Jasper Dekoninck","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2409.00696v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04411v2","updated":"2025-02-11T12:09:51Z","published":"2025-02-06T11:26:30Z","title":"Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing","summary":"  Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.\n","authors":["Kunfeng Lai","Zhenheng Tang","Xinglin Pan","Peijie Dong","Xiang Liu","Haolan Chen","Li Shen","Bo Li","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.04411v2.pdf","comment":"work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors"},{"id":"http://arxiv.org/abs/2411.04585v2","updated":"2025-02-11T12:00:39Z","published":"2024-11-07T10:11:38Z","title":"The State and Fate of Summarization Datasets: A Survey","summary":"  Automatic summarization has consistently attracted attention due to its\nversatility and wide application in various downstream tasks. Despite its\npopularity, we find that annotation efforts have largely been disjointed, and\nhave lacked common terminology. Consequently, it is challenging to discover\nexisting resources or identify coherent research directions. To address this,\nwe survey a large body of work spanning 133 datasets in over 100 languages,\ncreating a novel ontology covering sample properties, collection methods and\ndistribution. With this ontology we make key observations, including the lack\nin accessible high-quality datasets for low-resource languages, and the field's\nover-reliance on the news domain and on automatically collected distant\nsupervision. Finally, we make available a web interface that allows users to\ninteract and explore our ontology and dataset collection, as well as a template\nfor a summarization data card, which can be used to streamline future research\ninto a more coherent body of work.\n","authors":["Noam Dahan","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2411.04585v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07490v1","updated":"2025-02-11T11:49:03Z","published":"2025-02-11T11:49:03Z","title":"Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More","summary":"  Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.\n","authors":["Xialie Zhuang","Zhikai Jia","Jianjin Li","Zhenyu Zhang","Li Shen","Zheng Cao","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07490v1.pdf","comment":"15 pages,7 figures"},{"id":"http://arxiv.org/abs/2502.07487v1","updated":"2025-02-11T11:46:38Z","published":"2025-02-11T11:46:38Z","title":"Multi-Agent Collaboration for Multilingual Code Instruction Tuning","summary":"  Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap.\n","authors":["Jian Yang","Wei Zhang","Jiaxi Yang","Yibo Miao","Shanghaoran Quan","Zhenhe Wu","Qiyao Peng","Liqun Yang","Tianyu Liu","Zeyu Cui","Binyuan Hui","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.07487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07459v1","updated":"2025-02-11T11:07:44Z","published":"2025-02-11T11:07:44Z","title":"PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian","summary":"  Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul\n","authors":["Erfan Moosavi Monazzah","Vahid Rahimzadeh","Yadollah Yaghoobzadeh","Azadeh Shakery","Mohammad Taher Pilehvar"],"pdf_url":"https://arxiv.org/pdf/2502.07459v1.pdf","comment":"Accepted at NAACL 2025 Main Conference, the dataset is available on\n  HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)"},{"id":"http://arxiv.org/abs/2406.07222v2","updated":"2025-02-11T11:02:10Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v2.pdf","comment":"New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2502.07455v1","updated":"2025-02-11T10:57:12Z","published":"2025-02-11T10:57:12Z","title":"RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation","summary":"  Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.\n","authors":["Viacheslav Vasilev","Julia Agafonova","Nikolai Gerasimenko","Alexander Kapitanov","Polina Mikhailova","Evelina Mironova","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2502.07455v1.pdf","comment":"Accepted for NAACL 2025 Findings, GitHub:\n  https://github.com/ai-forever/RusCode"},{"id":"http://arxiv.org/abs/2502.07445v1","updated":"2025-02-11T10:43:36Z","published":"2025-02-11T10:43:36Z","title":"Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon","summary":"  Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.\n","authors":["Nurit Cohen-Inger","Yehonatan Elisha","Bracha Shapira","Lior Rokach","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07442v1","updated":"2025-02-11T10:37:01Z","published":"2025-02-11T10:37:01Z","title":"Hierarchical Document Parsing via Large Margin Feature Matching and\n  Heuristics","summary":"  We present our solution to the AAAI-25 VRD-IU challenge, achieving first\nplace in the competition. Our approach integrates large margin loss for\nimproved feature discrimination and employs heuristic rules to refine\nhierarchical relationships. By combining a deep learning-based matching\nstrategy with greedy algorithms, we achieve a significant boost in accuracy\nwhile maintaining computational efficiency. Our method attains an accuracy of\n0.98904 on the private leaderboard, demonstrating its effectiveness in document\nstructure parsing. Source codes are publicly available at\nhttps://github.com/ffyyytt/VRUID-AAAI-DAKiet\n","authors":["Duong Anh Kiet"],"pdf_url":"https://arxiv.org/pdf/2502.07442v1.pdf","comment":"DocUI@AAAI-25, 2 pages, technical report"},{"id":"http://arxiv.org/abs/2410.18850v2","updated":"2025-02-11T10:36:13Z","published":"2024-10-24T15:32:52Z","title":"kNN For Whisper And Its Effect On Bias And Speaker Adaptation","summary":"  Speech recognition performance varies by language, domain, and speaker\ncharacteristics such as accent, but fine-tuning a model on any of these\ncategories may lead to catastrophic forgetting. Token-level $k$ nearest\nneighbor search ($k$NN), first proposed for neural sequence decoders for\nnatural language generation (NLG) and machine translation (MT), is a\nnon-parametric method that instead adapts using inference-time search in an\nexternal datastore, without training the underlying model. We show that\nWhisper, a transformer end-to-end speech model, benefits from $k$NN. We\ninvestigate the differences between the speech and text setups. We discuss\nimplications for speaker adaptation, and analyze improvements by gender,\naccent, and age.\n","authors":["Maya K. Nachesa","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.18850v2.pdf","comment":"Accepted to Findings of NAACL 2025. 7 pages incl. appendix, 2\n  figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.07424v1","updated":"2025-02-11T10:10:26Z","published":"2025-02-11T10:10:26Z","title":"RomanLens: Latent Romanization and its role in Multilinguality in LLMs","summary":"  Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? For\nnon-Latin script languages, we investigate the role of romanization - the\nrepresentation of non-Latin scripts using Latin characters - as a bridge in\nmultilingual processing. Using mechanistic interpretability techniques, we\nanalyze next-token generation and find that intermediate layers frequently\nrepresent target words in romanized form before transitioning to native script,\na phenomenon we term Latent Romanization. Further, through activation patching\nexperiments, we demonstrate that LLMs encode semantic concepts similarly across\nnative and romanized scripts, suggesting a shared underlying representation.\nAdditionally in translation towards non Latin languages, our findings reveal\nthat when the target language is in romanized form, its representations emerge\nearlier in the model's layers compared to native script. These insights\ncontribute to a deeper understanding of multilingual representation in LLMs and\nhighlight the implicit role of romanization in facilitating language transfer.\nOur work provides new directions for potentially improving multilingual\nlanguage modeling and interpretability.\n","authors":["Alan Saji","Jaavid Aktar Husain","Thanmay Jayakumar","Raj Dabre","Anoop Kunchukuttan","Mitesh M. Khapra","Ratish Puduppully"],"pdf_url":"https://arxiv.org/pdf/2502.07424v1.pdf","comment":"18 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.02577v2","updated":"2025-02-11T10:06:14Z","published":"2025-02-04T18:53:42Z","title":"A comparison of translation performance between DeepL and Supertext","summary":"  As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.\n","authors":["Alex Flckiger","Chantal Amrhein","Tim Graf","Frdric Odermatt","Martin Pmsl","Philippe Schlpfer","Florian Schottmann","Samuel Lubli"],"pdf_url":"https://arxiv.org/pdf/2502.02577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00560v2","updated":"2025-02-11T10:02:55Z","published":"2024-12-31T17:46:51Z","title":"Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference","summary":"  Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.\n","authors":["Mingqi Gao","Yixin Liu","Xinyu Hu","Xiaojun Wan","Jonathan Bragg","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2501.00560v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07418v1","updated":"2025-02-11T09:54:39Z","published":"2025-02-11T09:54:39Z","title":"Entity Linking using LLMs for Automated Product Carbon Footprint\n  Estimation","summary":"  Growing concerns about climate change and sustainability are driving\nmanufacturers to take significant steps toward reducing their carbon\nfootprints. For these manufacturers, a first step towards this goal is to\nidentify the environmental impact of the individual components of their\nproducts. We propose a system leveraging large language models (LLMs) to\nautomatically map components from manufacturer Bills of Materials (BOMs) to\nLife Cycle Assessment (LCA) database entries by using LLMs to expand on\navailable component information. Our approach reduces the need for manual data\nprocessing, paving the way for more accessible sustainability practices.\n","authors":["Steffen Castle","Julian Moreno Schneider","Leonhard Hennig","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2502.07418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07391v1","updated":"2025-02-11T09:19:46Z","published":"2025-02-11T09:19:46Z","title":"Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation\n  Generation","summary":"  Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g.,\nentity, event, or person) in an inherent way. Multimodal Sarcasm Explanation\n(MuSE) aims at revealing the intended irony in a sarcastic post using a natural\nlanguage explanation. Though important, existing systems overlooked the\nsignificance of the target of sarcasm in generating explanations. In this\npaper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn\nmodel, aka. TURBO. We design a novel shared-fusion mechanism to leverage the\ninter-modality relationships between an image and its caption. TURBO assumes\nthe target of the sarcasm and guides the multimodal shared fusion mechanism in\nlearning intricacies of the intended irony for explanations. We evaluate our\nproposed TURBO model on the MORE+ dataset. Comparison against multiple\nbaselines and state-of-the-art models signifies the performance improvement of\nTURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and\none-shot settings for our task and observe that LLM-generated explanation,\nthough remarkable, often fails to capture the critical nuances of the sarcasm.\nFurthermore, we supplement our study with extensive human evaluation on TURBO's\ngenerated explanations and find them out to be comparatively better than other\nsystems.\n","authors":["Palaash Goel","Dushyant Singh Chauhan","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2502.07391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07386v1","updated":"2025-02-11T09:12:05Z","published":"2025-02-11T09:12:05Z","title":"Parametric type design in the era of variable and color fonts","summary":"  Parametric fonts are programatically defined fonts with variable parameters,\npioneered by Donald Kunth with his MetaFont technology in the 1980s. While\nDonald Knuth's ideas in MetaFont and subsequently in MetaPost are often seen as\nlegacy techniques from the pre-graphical user interface (GUI) era of type\ndesign, recent trends like variable fonts suggest a resurgence of certain\nprinciples. This paper explores a modern type design process built on\nparametric design principles, specifically using MetaPost. The author created\ntwo variable fonts with this method and released them under a free, open-source\nlicense. The paper details the methodology, workflow, and insights gained from\nthis process.\n","authors":["Santhosh Thottingal"],"pdf_url":"https://arxiv.org/pdf/2502.07386v1.pdf","comment":"Conference: Grapholinguistics in the 21st century - From graphemes to\n  knowledge"},{"id":"http://arxiv.org/abs/2502.07373v1","updated":"2025-02-11T08:48:46Z","published":"2025-02-11T08:48:46Z","title":"EvoFlow: Evolving Diverse Agentic Workflows On The Fly","summary":"  The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels.\n","authors":["Guibin Zhang","Kaijie Chen","Guancheng Wan","Heng Chang","Hong Cheng","Kun Wang","Shuyue Hu","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2502.07373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07365v1","updated":"2025-02-11T08:37:16Z","published":"2025-02-11T08:37:16Z","title":"LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation","summary":"  Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines.\n","authors":["Zican Dong","Junyi Li","Jinhao Jiang","Mingyu Xu","Wayne Xin Zhao","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07352v1","updated":"2025-02-11T08:23:56Z","published":"2025-02-11T08:23:56Z","title":"Bridging the Evaluation Gap: Leveraging Large Language Models for Topic\n  Model Evaluation","summary":"  This study presents a framework for automated evaluation of dynamically\nevolving topic taxonomies in scientific literature using Large Language Models\n(LLMs). In digital library systems, topic modeling plays a crucial role in\nefficiently organizing and retrieving scholarly content, guiding researchers\nthrough complex knowledge landscapes. As research domains proliferate and\nshift, traditional human centric and static evaluation methods struggle to\nmaintain relevance. The proposed approach harnesses LLMs to measure key quality\ndimensions, such as coherence, repetitiveness, diversity, and topic-document\nalignment, without heavy reliance on expert annotators or narrow statistical\nmetrics. Tailored prompts guide LLM assessments, ensuring consistent and\ninterpretable evaluations across various datasets and modeling techniques.\nExperiments on benchmark corpora demonstrate the method's robustness,\nscalability, and adaptability, underscoring its value as a more holistic and\ndynamic alternative to conventional evaluation strategies.\n","authors":["Zhiyin Tan","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2502.07352v1.pdf","comment":"accepted by IRCDL 2025"},{"id":"http://arxiv.org/abs/2406.12109v2","updated":"2025-02-11T08:22:35Z","published":"2024-06-17T21:37:09Z","title":"Can LLMs Learn Macroeconomic Narratives from Social Media?","summary":"  This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.\n","authors":["Almog Gueta","Amir Feder","Zorik Gekhman","Ariel Goldstein","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2406.12109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07346v1","updated":"2025-02-11T08:17:19Z","published":"2025-02-11T08:17:19Z","title":"BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models","summary":"  Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.\n","authors":["Xu Huang","Wenhao Zhu","Hanxu Hu","Conghui He","Lei Li","Shujian Huang","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.07346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07340v1","updated":"2025-02-11T08:05:56Z","published":"2025-02-11T08:05:56Z","title":"Aligning Large Language Models to Follow Instructions and Hallucinate\n  Less via Effective Data Filtering","summary":"  Training LLMs on data that contains unfamiliar knowledge during the\ninstruction tuning stage can make LLMs overconfident and encourage\nhallucinations. To address this challenge, we introduce a novel framework,\nNOVA, which identifies high-quality data that aligns well with the LLM's\nlearned knowledge to reduce hallucinations. NOVA includes Internal Consistency\nProbing (ICP) and Semantic Equivalence Identification (SEI) to measure how\nfamiliar the LLM is with instruction data. Specifically, ICP evaluates the\nLLM's understanding of the given instruction by calculating the tailored\nconsistency among multiple self-generated responses. SEI further assesses the\nfamiliarity of the LLM with the target response by comparing it to the\ngenerated responses, using the proposed semantic clustering and well-designed\nvoting strategy. Finally, we introduce an expert-aligned reward model,\nconsidering characteristics beyond just familiarity to enhance data quality. By\nconsidering data quality and avoiding unfamiliar data, we can utilize the\nselected data to effectively align LLMs to follow instructions and hallucinate\nless. Extensive experiments and analysis show that NOVA significantly reduces\nhallucinations and allows LLMs to maintain a strong ability to follow\ninstructions.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Cheng Gao","Yuzhuo Bai","Zhitong Wang","Kaikai An","Kangyang Luo","Chen Qian","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.07340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v1","updated":"2025-02-11T07:46:29Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models (Camera Ready)","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v1.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.07322v1","updated":"2025-02-11T07:42:09Z","published":"2025-02-11T07:42:09Z","title":"MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs","summary":"  As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.\n","authors":["Zilu Dong","Xiangqing Shen","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2502.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20163v2","updated":"2025-02-11T07:35:58Z","published":"2024-10-26T12:34:07Z","title":"UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers","summary":"  Existing information retrieval (IR) models often assume a homogeneous\nstructure for knowledge sources and user queries, limiting their applicability\nin real-world settings where retrieval is inherently heterogeneous and diverse.\nIn this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous\nknowledge retriever that (1) builds a unified retrieval space for heterogeneous\nknowledge and (2) follows diverse user instructions to retrieve knowledge of\nspecified types. UniHGKR consists of three principal stages: heterogeneous\nself-supervised pretraining, text-anchored embedding alignment, and\ninstruction-aware retriever fine-tuning, enabling it to generalize across\nvaried retrieval contexts. This framework is highly scalable, with a BERT-based\nversion and a UniHGKR-7B version trained on large language models. Also, we\nintroduce CompMix-IR, the first native heterogeneous knowledge retrieval\nbenchmark. It includes two retrieval scenarios with various instructions, over\n9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering\nfour different types of data. Extensive experiments show that UniHGKR\nconsistently outperforms state-of-the-art methods on CompMix-IR, achieving up\nto 6.36% and 54.23% relative improvements in two scenarios, respectively.\nFinally, by equipping our retriever for open-domain heterogeneous QA systems,\nwe achieve a new state-of-the-art result on the popular ConvMix task, with an\nabsolute improvement of up to 5.90 points.\n","authors":["Dehai Min","Zhiyang Xu","Guilin Qi","Lifu Huang","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2410.20163v2.pdf","comment":"NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2408.00662v2","updated":"2025-02-11T07:32:30Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass. We\nrelease the source codes of MultiEA at: https://github.com/kepsail/MultiEA.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang","Jiangtao Cui","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2408.00662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07316v1","updated":"2025-02-11T07:26:50Z","published":"2025-02-11T07:26:50Z","title":"CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction","summary":"  Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.\n","authors":["Junlong Li","Daya Guo","Dejian Yang","Runxin Xu","Yu Wu","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2502.07316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05343v2","updated":"2025-02-11T07:17:37Z","published":"2024-10-07T07:19:50Z","title":"EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos\n  Referring to Procedural Texts","summary":"  Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.\n","authors":["Yuto Haneji","Taichi Nishimura","Hirotaka Kameko","Keisuke Shirai","Tomoya Yoshida","Keiya Kajimura","Koki Yamamoto","Taiyu Cui","Tomohiro Nishimoto","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2410.05343v2.pdf","comment":"Main 6 pages, supplementary 13 pages"},{"id":"http://arxiv.org/abs/2502.07306v1","updated":"2025-02-11T07:09:37Z","published":"2025-02-11T07:09:37Z","title":"TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language\n  Navigation","summary":"  In this work, we propose a modular approach for the Vision-Language\nNavigation (VLN) task by decomposing the problem into four sub-modules that use\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)\nin a zero-shot setting. Given navigation instruction in natural language, we\nfirst prompt LLM to extract the landmarks and the order in which they are\nvisited. Assuming the known model of the environment, we retrieve the top-k\nlocations of the last landmark and generate $k$ path hypotheses from the\nstarting location to the last landmark using the shortest path algorithm on the\ntopological map of the environment. Each path hypothesis is represented by a\nsequence of panoramas. We then use dynamic programming to compute the alignment\nscore between the sequence of panoramas and the sequence of landmark names,\nwhich match scores obtained from VLM. Finally, we compute the nDTW metric\nbetween the hypothesis that yields the highest alignment score to evaluate the\npath fidelity. We demonstrate superior performance compared to other approaches\nthat use joint semantic maps like VLMaps \\cite{vlmaps} on the complex\nR2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of\nvisual grounding on navigation performance.\n","authors":["Navid Rajabi","Jana Kosecka"],"pdf_url":"https://arxiv.org/pdf/2502.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10704v2","updated":"2025-02-11T07:05:58Z","published":"2024-12-14T06:24:55Z","title":"VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal\n  Retrieval-Augmented Generation","summary":"  Understanding information from a collection of multiple documents,\nparticularly those with visually rich elements, is important for\ndocument-grounded question answering. This paper introduces VisDoMBench, the\nfirst comprehensive benchmark designed to evaluate QA systems in multi-document\nsettings with rich multimodal content, including tables, charts, and\npresentation slides. We propose VisDoMRAG, a novel multimodal Retrieval\nAugmented Generation (RAG) approach that simultaneously utilizes visual and\ntextual RAG, combining robust visual retrieval capabilities with sophisticated\nlinguistic reasoning. VisDoMRAG employs a multi-step reasoning process\nencompassing evidence curation and chain-of-thought reasoning for concurrent\ntextual and visual RAG pipelines. A key novelty of VisDoMRAG is its\nconsistency-constrained modality fusion mechanism, which aligns the reasoning\nprocesses across modalities at inference time to produce a coherent final\nanswer. This leads to enhanced accuracy in scenarios where critical information\nis distributed across modalities and improved answer verifiability through\nimplicit context attribution. Through extensive experiments involving\nopen-source and proprietary large language models, we benchmark\nstate-of-the-art document QA methods on VisDoMBench. Extensive results show\nthat VisDoMRAG outperforms unimodal and long-context LLM baselines for\nend-to-end multimodal document QA by 12-20%.\n","authors":["Manan Suri","Puneet Mathur","Franck Dernoncourt","Kanika Goswami","Ryan A. Rossi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2412.10704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11507v3","updated":"2025-02-11T07:03:51Z","published":"2024-10-15T11:20:42Z","title":"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs","summary":"  While various vertical domain large language models (LLMs) have been\ndeveloped, automatically evaluating their performance across different domains\nremains a critical challenge. Current benchmark-based methods often rely on\nstatic and costly datasets, are misaligned with practical user needs, and lack\nflexibility across domains. To address these limitations, we revisit the\nevaluation process and introduce two key concepts: Benchmark+, which extends\nthe traditional question-answer benchmark into a more flexible\n``strategy-criterion'' format; and Assessment+, which enhances the interaction\nprocess, enabling deeper exploration and supporting analysis from broader\nperspectives. We propose TestAgent, an agent-based evaluation framework that\nimplements these concepts using retrieval-augmented generation and\nreinforcement learning. TestAgent enables automatic dynamic benchmark\ngeneration and in-depth assessment across diverse vertical domain scenarios.\nExperiments on tasks ranging from constructing multiple vertical domain\nevaluations to converting static benchmarks into dynamic forms demonstrate the\neffectiveness of TestAgent. This work offers an interesting perspective on\nautomatic evaluation for LLMs and highlights a pathway for dynamic and\ndomain-adaptive assessments.\n","authors":["Wanying Wang","Zeyu Ma","Pengfei Liu","Mingang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16751v3","updated":"2025-02-11T07:00:28Z","published":"2024-02-26T17:16:28Z","title":"Value Preferences Estimation and Disambiguation in Hybrid Participatory\n  Systems","summary":"  Understanding citizens' values in participatory systems is crucial for\ncitizen-centric policy-making. We envision a hybrid participatory system where\nparticipants make choices and provide motivations for those choices, and AI\nagents estimate their value preferences by interacting with them. We focus on\nsituations where a conflict is detected between participants' choices and\nmotivations, and propose methods for estimating value preferences while\naddressing detected inconsistencies by interacting with the participants. We\noperationalize the philosophical stance that \"valuing is deliberatively\nconsequential.\" That is, if a participant's choice is based on a deliberation\nof value preferences, the value preferences can be observed in the motivation\nthe participant provides for the choice. Thus, we propose and compare value\npreferences estimation methods that prioritize the values estimated from\nmotivations over the values estimated from choices alone. Then, we introduce a\ndisambiguation strategy that combines Natural Language Processing and Active\nLearning to address the detected inconsistencies between choices and\nmotivations. We evaluate the proposed methods on a dataset of a large-scale\nsurvey on energy transition. The results show that explicitly addressing\ninconsistencies between choices and motivations improves the estimation of an\nindividual's value preferences. The disambiguation strategy does not show\nsubstantial improvements when compared to similar baselines--however, we\ndiscuss how the novelty of the approach can open new research avenues and\npropose improvements to address the current limitations.\n","authors":["Enrico Liscio","Luciano C. Siebert","Catholijn M. Jonker","Pradeep K. Murukannaiah"],"pdf_url":"https://arxiv.org/pdf/2402.16751v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07299v1","updated":"2025-02-11T06:53:59Z","published":"2025-02-11T06:53:59Z","title":"Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification","summary":"  The interactions between DNA, RNA, and proteins are fundamental to biological\nprocesses, as illustrated by the central dogma of molecular biology. While\nmodern biological pre-trained models have achieved great success in analyzing\nthese macromolecules individually, their interconnected nature remains\nunder-explored. In this paper, we follow the guidance of the central dogma to\nredesign both the data and model pipeline and offer a comprehensive framework,\nLife-Code, that spans different biological functions. As for data flow, we\npropose a unified pipeline to integrate multi-omics data by\nreverse-transcribing RNA and reverse-translating amino acids into\nnucleotide-based sequences. As for the model, we design a codon tokenizer and a\nhybrid long-sequence architecture to encode the interactions of both coding and\nnon-coding regions with masked modeling pre-training. To model the translation\nand folding process with coding sequences, Life-Code learns protein structures\nof the corresponding amino acids by knowledge distillation from off-the-shelf\nprotein language models. Such designs enable Life-Code to capture complex\ninteractions within genetic sequences, providing a more comprehensive\nunderstanding of multi-omics with the central dogma. Extensive Experiments show\nthat Life-Code achieves state-of-the-art performance on various tasks across\nthree omics, highlighting its potential for advancing multi-omics analysis and\ninterpretation.\n","authors":["Zicheng Liu","Siyuan Li","Zhiyuan Chen","Lei Xin","Fang Wu","Chang Yu","Qirong Yang","Yucheng Guo","Yujie Yang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.07299v1.pdf","comment":"12 pages main text with 6 pages Appendix"},{"id":"http://arxiv.org/abs/2501.07890v2","updated":"2025-02-11T06:47:01Z","published":"2025-01-14T06:59:51Z","title":"GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via\n  Introducing Self-Rethinking Mechanism","summary":"  Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple\nsmaller expert models as opposed to a single large network. However, these\nexperts typically operate independently, leaving a question open about whether\ninterconnecting these models could enhance the performance of MoE networks. In\nresponse, we introduce GRAPHMOE, a novel method aimed at augmenting the\ncognitive depth of language models via a self-rethinking mechanism constructed\non Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to\nsimulate iterative thinking steps, thereby facilitating the flow of information\namong expert nodes. We implement the GRAPHMOE architecture using Low-Rank\nAdaptation techniques (LoRA) and conduct extensive experiments on various\nbenchmark datasets. The experimental results reveal that GRAPHMOE outperforms\nother LoRA based models, achieving state-of-the-art (SOTA) performance.\nAdditionally, this study explores a novel recurrent routing strategy that may\ninspire further advancements in enhancing the reasoning capabilities of\nlanguage models.\n","authors":["Chen Tang","Bo Lv","Zifan Zheng","Bohao Yang","Kun Zhao","Ning Liao","Xiaoxing Wang","Feiyu Xiong","Zhiyu Li","Nayu Liu","Jingchi Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.07890v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.19846v7","updated":"2025-02-11T06:22:30Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v7.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2408.09366v2","updated":"2025-02-11T06:15:47Z","published":"2024-08-18T05:41:36Z","title":"Improving and Assessing the Fidelity of Large Language Models Alignment\n  to Online Communities","summary":"  Large language models (LLMs) have shown promise in representing individuals\nand communities, offering new ways to study complex social dynamics. However,\neffectively aligning LLMs with specific human groups and systematically\nassessing the fidelity of the alignment remains a challenge. This paper\npresents a robust framework for aligning LLMs with online communities via\ninstruction-tuning and comprehensively evaluating alignment across various\naspects of language, including authenticity, emotional tone, toxicity, and\nharm. We demonstrate the utility of our approach by applying it to online\ncommunities centered on dieting and body image. We administer an eating\ndisorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and\nsuccessfully differentiate communities with varying levels of eating disorder\nrisk. Our results highlight the potential of LLMs in automated moderation and\nbroader applications in public health and social science research.\n","authors":["Minh Duc Chu","Zihao He","Rebecca Dorn","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2408.09366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02904v2","updated":"2025-02-11T06:14:31Z","published":"2025-02-05T05:57:37Z","title":"ScholaWrite: A Dataset of End-to-End Scholarly Writing Process","summary":"  Writing is a cognitively demanding task involving continuous decision-making,\nheavy use of working memory, and frequent switching between multiple\nactivities. Scholarly writing is particularly complex as it requires authors to\ncoordinate many pieces of multiform knowledge. To fully understand writers'\ncognitive thought process, one should fully decode the end-to-end writing data\n(from individual ideas to final manuscript) and understand their complex\ncognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset,\nthe first-of-its-kind keystroke logs of an end-to-end scholarly writing process\nfor complete manuscripts, with thorough annotations of cognitive writing\nintentions behind each keystroke. Our dataset includes LaTeX-based keystroke\ndata from five preprints with nearly 62K total text changes and annotations\nacross 4 months of paper writing. ScholaWrite shows promising usability and\napplications (e.g., iterative self-writing) for the future development of AI\nwriting assistants for academic research, which necessitate complex methods\nbeyond LLM prompting. Our experiments clearly demonstrated the importance of\ncollection of end-to-end writing data, rather than the final manuscript, for\nthe development of future writing assistants to support the cognitive thinking\nprocess of scientists. Our de-identified dataset, demo, and code repository are\navailable on our project page.\n","authors":["Linghe Wang","Minhwa Lee","Ross Volkov","Luan Tuyen Chau","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2502.02904v2.pdf","comment":"Equal contribution: Linghe Wang, Minhwa Lee | project page:\n  https://minnesotanlp.github.io/scholawrite/"},{"id":"http://arxiv.org/abs/2502.07286v1","updated":"2025-02-11T06:06:25Z","published":"2025-02-11T06:06:25Z","title":"Small Language Model Makes an Effective Long Text Extractor","summary":"  Named Entity Recognition (NER) is a fundamental problem in natural language\nprocessing (NLP). However, the task of extracting longer entity spans (e.g.,\nawards) from extended texts (e.g., homepages) is barely explored. Current NER\nmethods predominantly fall into two categories: span-based methods and\ngeneration-based methods. Span-based methods require the enumeration of all\npossible token-pair spans, followed by classification on each span, resulting\nin substantial redundant computations and excessive GPU memory usage. In\ncontrast, generation-based methods involve prompting or fine-tuning large\nlanguage models (LLMs) to adapt to downstream NER tasks. However, these methods\nstruggle with the accurate generation of longer spans and often incur\nsignificant time costs for effective fine-tuning. To address these challenges,\nthis paper introduces a lightweight span-based NER method called SeNER, which\nincorporates a bidirectional arrow attention mechanism coupled with\nLogN-Scaling on the [CLS] token to embed long texts effectively, and comprises\na novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to\nreduce redundant candidate token-pair spans significantly and model\ninteractions between token-pair spans simultaneously. Extensive experiments\ndemonstrate that our method achieves state-of-the-art extraction accuracy on\nthree long NER datasets and is capable of extracting entities from long texts\nin a GPU-memory-friendly manner. Code:\nhttps://github.com/THUDM/scholar-profiling/tree/main/sener\n","authors":["Yelin Chen","Fanjin Zhang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2502.07286v1.pdf","comment":"AAAI'25, 9 pages, 1 appendix pages"},{"id":"http://arxiv.org/abs/2502.06101v2","updated":"2025-02-11T05:53:00Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v2.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"},{"id":"http://arxiv.org/abs/2410.11287v2","updated":"2025-02-11T05:41:41Z","published":"2024-10-15T05:10:34Z","title":"Process Reward Model with Q-Value Rankings","summary":"  Process Reward Modeling (PRM) is critical for complex reasoning and\ndecision-making tasks where the accuracy of intermediate steps significantly\ninfluences the overall outcome. Existing PRM approaches, primarily framed as\nclassification problems, employ cross-entropy loss to independently evaluate\neach step's correctness. This method can lead to suboptimal reward distribution\nand does not adequately address the interdependencies among steps. To address\nthese limitations, we introduce the Process Q-value Model (PQM), a novel\nframework that redefines PRM in the context of a Markov Decision Process. PQM\noptimizes Q-value rankings based on a novel comparative loss function,\nenhancing the model's ability to capture the intricate dynamics among\nsequential decisions. This approach provides a more granular and theoretically\ngrounded methodology for process rewards. Our extensive empirical evaluations\nacross various sampling policies, language model backbones, and multi-step\nreasoning benchmarks show that PQM outperforms classification-based PRMs. The\neffectiveness of the comparative loss function is highlighted in our\ncomprehensive ablation studies, confirming PQM's practical efficacy and\ntheoretical advantage.\n","authors":["Wendi Li","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.11287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07272v1","updated":"2025-02-11T05:39:49Z","published":"2025-02-11T05:39:49Z","title":"GENERator: A Long-Context Generative Genomic Foundation Model","summary":"  Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of promoter sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions.\n","authors":["Wei Wu","Qiuyi Li","Mingyang Li","Kun Fu","Fuli Feng","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04674v2","updated":"2025-02-11T05:36:24Z","published":"2025-02-07T05:39:55Z","title":"AdParaphrase: Paraphrase Dataset for Analyzing Linguistic Features\n  toward Generating Attractive Ad Texts","summary":"  Effective linguistic choices that attract potential customers play crucial\nroles in advertising success. This study aims to explore the linguistic\nfeatures of ad texts that influence human preferences. Although the creation of\nattractive ad texts is an active area of research, progress in understanding\nthe specific linguistic features that affect attractiveness is hindered by\nseveral obstacles. First, human preferences are complex and influenced by\nmultiple factors, including their content, such as brand names, and their\nlinguistic styles, making analysis challenging. Second, publicly available ad\ntext datasets that include human preferences are lacking, such as ad\nperformance metrics and human feedback, which reflect people's interests. To\naddress these problems, we present AdParaphrase, a paraphrase dataset that\ncontains human preferences for pairs of ad texts that are semantically\nequivalent but differ in terms of wording and style. This dataset allows for\npreference analysis that focuses on the differences in linguistic features. Our\nanalysis revealed that ad texts preferred by human judges have higher fluency,\nlonger length, more nouns, and use of bracket symbols. Furthermore, we\ndemonstrate that an ad text-generation model that considers these findings\nsignificantly improves the attractiveness of a given text. The dataset is\npublicly available at: https://github.com/CyberAgentAILab/AdParaphrase.\n","authors":["Soichiro Murakami","Peinan Zhang","Hidetaka Kamigaito","Hiroya Takamura","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2502.04674v2.pdf","comment":"Accepted to NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2502.08020v1","updated":"2025-02-11T23:40:53Z","published":"2025-02-11T23:40:53Z","title":"Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding","summary":"  Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications\n","authors":["Ziyao Wang","Muneeza Azmart","Ang Li","Raya Horesh","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2502.08020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08009v1","updated":"2025-02-11T23:09:50Z","published":"2025-02-11T23:09:50Z","title":"The Geometry of Prompting: Unveiling Distinct Mechanisms of Task\n  Adaptation in Language Models","summary":"  Decoder-only language models have the ability to dynamically switch between\nvarious computational tasks based on input prompts. Despite many successful\napplications of prompting, there is very limited understanding of the internal\nmechanism behind such flexibility. In this work, we investigate how different\nprompting methods affect the geometry of representations in these models.\nEmploying a framework grounded in statistical physics, we reveal that various\nprompting techniques, while achieving similar performance, operate through\ndistinct representational mechanisms for task adaptation. Our analysis\nhighlights the critical role of input distribution samples and label semantics\nin few-shot in-context learning. We also demonstrate evidence of synergistic\nand interfering interactions between different tasks on the representational\nlevel. Our work contributes to the theoretical understanding of large language\nmodels and lays the groundwork for developing more effective,\nrepresentation-aware prompting strategies.\n","authors":["Artem Kirsanov","Chi-Ning Chou","Kyunghyun Cho","SueYeon Chung"],"pdf_url":"https://arxiv.org/pdf/2502.08009v1.pdf","comment":"To appear in NAACL Findings 2025"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2406.08446v2","updated":"2025-02-11T18:59:26Z","published":"2024-06-12T17:37:09Z","title":"OLMES: A Standard for Language Model Evaluations","summary":"  Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.\n","authors":["Yuling Gu","Oyvind Tafjord","Bailey Kuehl","Dany Haddad","Jesse Dodge","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.08446v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.06808v2","updated":"2025-02-11T18:52:51Z","published":"2024-11-26T00:06:47Z","title":"Effect of Adaptive Communication Support on LLM-powered Human-Robot\n  Collaboration","summary":"  Effective human-robot collaboration requires robot to adopt their roles and\nlevels of support based on human needs, task requirements, and complexity.\nTraditional human-robot teaming often relies on a pre-determined robot\ncommunication scheme, restricting teamwork adaptability in complex tasks.\nLeveraging strong communication capabilities of Large Language Models (LLMs),\nwe propose a Human-Robot Teaming Framework with Multi-Modal Language feedback\n(HRT-ML), a framework designed to enhance human-robot interaction by adjusting\nthe frequency and content of language-based feedback. HRT-ML framework includes\ntwo core modules: a Coordinator for high-level, low-frequency strategic\nguidance, and a Manager for subtask-specific, high-frequency instructions,\nenabling passive and active interactions with human teammates. To assess the\nimpact of language feedback in collaborative scenarios, we conducted\nexperiments in an enhanced Overcooked environment with varying levels of task\ncomplexity (easy, medium, hard) and feedback frequency (inactive, passive,\nactive, superactive). Our results show that as task complexity increases\nrelative to human capabilities, human teammates exhibited a stronger preference\ntowards robotic agents that can offer frequent, proactive support. However,\nwhen task complexities exceed the LLM's capacity, noisy and inaccurate feedback\nfrom superactive robotic agents can instead hinder team performance, as it\nrequires human teammates to increase their effort to interpret and respond to a\nlarge number of communications, with limited performance return. Our results\noffer a general principle for robotic agents to dynamically adjust their levels\nand frequencies of communications to work seamlessly with humans and achieve\nimproved teaming performance.\n","authors":["Shipeng Liu","FNU Shrutika","Boshen Zhang","Zhehui Huang","Gaurav Sukhatme","Feifei Qian"],"pdf_url":"https://arxiv.org/pdf/2412.06808v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07764v1","updated":"2025-02-11T18:47:53Z","published":"2025-02-11T18:47:53Z","title":"Polynomial-Time Approximability of Constrained Reinforcement Learning","summary":"  We study the computational complexity of approximating general constrained\nMarkov decision processes. Our primary contribution is the design of a\npolynomial time $(0,\\epsilon)$-additive bicriteria approximation algorithm for\nfinding optimal constrained policies across a broad class of recursively\ncomputable constraints, including almost-sure, chance, expectation, and their\nanytime variants. Matching lower bounds imply our approximation guarantees are\noptimal so long as $P \\neq NP$. The generality of our approach results in\nanswers to several long-standing open complexity questions in the constrained\nreinforcement learning literature. Specifically, we are the first to prove\npolynomial-time approximability for the following settings: policies under\nchance constraints, deterministic policies under multiple expectation\nconstraints, policies under non-homogeneous constraints (i.e., constraints of\ndifferent types), and policies under constraints for continuous-state\nprocesses.\n","authors":["Jeremy McMahan"],"pdf_url":"https://arxiv.org/pdf/2502.07764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10366v2","updated":"2025-02-11T18:44:46Z","published":"2024-07-15T00:13:53Z","title":"Accessing Vision Foundation Models via ImageNet-1K","summary":"  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n","authors":["Yitian Zhang","Xu Ma","Yue Bai","Huan Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2407.10366v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2406.08598v3","updated":"2025-02-11T18:42:44Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02027v3","updated":"2025-02-11T18:33:27Z","published":"2025-02-04T05:24:44Z","title":"From Fog to Failure: How Dehazing Can Harm Clear Image Object Detection","summary":"  This study explores the challenges of integrating human visual cue-based\ndehazing into object detection, given the selective nature of human perception.\nWhile human vision adapts dynamically to environmental conditions,\ncomputational dehazing does not always enhance detection uniformly. We propose\na multi-stage framework where a lightweight detector identifies regions of\ninterest (RoIs), which are then enhanced via spatial attention-based dehazing\nbefore final detection by a heavier model. Though effective in foggy\nconditions, this approach unexpectedly degrades the performance on clear\nimages. We analyze this phenomenon, investigate possible causes, and offer\ninsights for designing hybrid pipelines that balance enhancement and detection.\nOur findings highlight the need for selective preprocessing and challenge\nassumptions about universal benefits from cascading transformations.\n","authors":["Ashutosh Kumar","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2502.02027v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07755v1","updated":"2025-02-11T18:32:24Z","published":"2025-02-11T18:32:24Z","title":"An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa\n  and Dynamic Contextual Positional Gating","summary":"  This paper presents a novel Natural Language Processing (NLP) framework for\nenhancing medical diagnosis through the integration of advanced techniques in\ndata augmentation, feature extraction, and classification. The proposed\napproach employs back-translation to generate diverse paraphrased datasets,\nimproving robustness and mitigating overfitting in classification tasks.\nLeveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with\nDynamic Contextual Positional Gating (DCPG), the model captures fine-grained\ncontextual and positional relationships, dynamically adjusting the influence of\npositional information based on semantic context to produce high-quality text\nembeddings. For classification, an Attention-Based Feedforward Neural Network\n(ABFNN) is utilized, effectively focusing on the most relevant features to\nimprove decision-making accuracy. Applied to the classification of symptoms,\nclinical notes, and other medical texts, this architecture demonstrates its\nability to address the complexities of medical data. The combination of data\naugmentation, contextual embedding generation, and advanced classification\nmechanisms offers a robust and accurate diagnostic tool, with potential\napplications in automated medical diagnosis and clinical decision support. This\nmethod demonstrates the effectiveness of the proposed NLP framework for medical\ndiagnosis, achieving remarkable results with an accuracy of 99.78%, recall of\n99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only\nunderscore the model's robust performance in classifying medical texts with\nexceptional precision and reliability but also highlight its superiority over\nexisting methods, making it a highly promising tool for automated diagnostic\nsystems.\n","authors":["Mohammad Ali Labbaf Khaniki","Sahabeh Saadati","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2502.07755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07752v1","updated":"2025-02-11T18:27:19Z","published":"2025-02-11T18:27:19Z","title":"Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension","summary":"  Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.\n","authors":["Wenbo Gong","Meyer Scetbon","Chao Ma","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.07752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07750v1","updated":"2025-02-11T18:25:48Z","published":"2025-02-11T18:25:48Z","title":"PFedDST: Personalized Federated Learning with Decentralized Selection\n  Training","summary":"  Distributed Learning (DL) enables the training of machine learning models\nacross multiple devices, yet it faces challenges like non-IID data\ndistributions and device capability disparities, which can impede training\nefficiency. Communication bottlenecks further complicate traditional Federated\nLearning (FL) setups. To mitigate these issues, we introduce the Personalized\nFederated Learning with Decentralized Selection Training (PFedDST) framework.\nPFedDST enhances model training by allowing devices to strategically evaluate\nand select peers based on a comprehensive communication score. This score\nintegrates loss, task similarity, and selection frequency, ensuring optimal\npeer connections. This selection strategy is tailored to increase local\npersonalization and promote beneficial peer collaborations to strengthen the\nstability and efficiency of the training process. Our experiments demonstrate\nthat PFedDST not only enhances model accuracy but also accelerates convergence.\nThis approach outperforms state-of-the-art methods in handling data\nheterogeneity, delivering both faster and more effective training in diverse\nand decentralized systems.\n","authors":["Mengchen Fan","Keren Li","Tianyun Zhang","Qing Tian","Baocheng Geng"],"pdf_url":"https://arxiv.org/pdf/2502.07750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07747v1","updated":"2025-02-11T18:14:44Z","published":"2025-02-11T18:14:44Z","title":"WHODUNIT: Evaluation benchmark for culprit detection in mystery stories","summary":"  We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.\n","authors":["Kshitij Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15332v2","updated":"2025-02-11T18:01:40Z","published":"2024-08-27T18:00:06Z","title":"What makes math problems hard for reinforcement learning: a case study","summary":"  Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple perspectives, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the context\ndefined by the Andrews-Curtis conjecture, we propose algorithmic enhancements\nand a topological hardness measure with implications for a broad class of\nsearch problems. As part of our study, we also address several open\nmathematical questions. Notably, we demonstrate the length reducibility of all\nbut two presentations in the Akbulut-Kirby series (1981), and resolve various\npotential counterexamples in the Miller-Schupp series (1991), including three\ninfinite subfamilies.\n","authors":["Ali Shehper","Anibal M. Medina-Mardones","Lucas Fagan","Bartomiej Lewandowski","Angus Gruen","Yang Qiu","Piotr Kucharski","Zhenghan Wang","Sergei Gukov"],"pdf_url":"https://arxiv.org/pdf/2408.15332v2.pdf","comment":"58 pages, 25 figures, 1 table. Try it:\n  https://github.com/shehper/AC-Solver"},{"id":"http://arxiv.org/abs/2502.07737v1","updated":"2025-02-11T17:57:53Z","published":"2025-02-11T17:57:53Z","title":"Next Block Prediction: Video Generation via Semi-Auto-Regressive\n  Modeling","summary":"  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n","authors":["Shuhuai Ren","Shuming Ma","Xu Sun","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.07737v1.pdf","comment":"project page: https://renshuhuai-andy.github.io/NBP-project/"},{"id":"http://arxiv.org/abs/2502.07734v1","updated":"2025-02-11T17:53:33Z","published":"2025-02-11T17:53:33Z","title":"EdgeEar: Efficient and Accurate Ear Recognition for Edge Devices","summary":"  Ear recognition is a contactless and unobtrusive biometric technique with\napplications across various domains. However, deploying high-performing ear\nrecognition models on resource-constrained devices is challenging, limiting\ntheir applicability and widespread adoption. This paper introduces EdgeEar, a\nlightweight model based on a proposed hybrid CNN-transformer architecture to\nsolve this problem. By incorporating low-rank approximations into specific\nlinear layers, EdgeEar reduces its parameter count by a factor of 50 compared\nto the current state-of-the-art, bringing it below two million while\nmaintaining competitive accuracy. Evaluation on the Unconstrained Ear\nRecognition Challenge (UERC2023) benchmark shows that EdgeEar achieves the\nlowest EER while significantly reducing computational costs. These findings\ndemonstrate the feasibility of efficient and accurate ear recognition, which we\nbelieve will contribute to the wider adoption of ear biometrics.\n","authors":["Camile Lendering","Bernardo Perrone Ribeiro","iga Emeri","Peter Peer"],"pdf_url":"https://arxiv.org/pdf/2502.07734v1.pdf","comment":"Submitted to IEEE FG 2025"},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06530v3","updated":"2025-02-11T17:49:04Z","published":"2024-10-09T04:07:20Z","title":"TopoTune : A Framework for Generalized Combinatorial Complex Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) excel in learning from relational datasets,\nprocessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems -- such as biological or social\nnetworks--involve multiway complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nTopological Deep Learning (TDL) aims to accommodate and leverage these\nhigher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly\ngeneral TDL models, have been shown to be more expressive and better performing\nthan GNNs. However, differently from the GNN ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting\nits accessibility and applicability. To address this issue, we introduce\nGeneralized CCNNs (GCCNs), a novel simple yet powerful family of TDL models\nthat can be used to systematically transform any (graph) neural network into\nits TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while\nextensive experiments on a diverse class of GCCNs show that these architectures\nconsistently match or outperform CCNNs, often with less model complexity. In an\neffort to accelerate and democratize TDL, we introduce TopoTune, a lightweight\nsoftware for defining, building, and training GCCNs with unprecedented\nflexibility and ease.\n","authors":["Mathilde Papillon","Guillermo Bernrdez","Claudio Battiloro","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2410.06530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07728v1","updated":"2025-02-11T17:42:07Z","published":"2025-02-11T17:42:07Z","title":"Verifying LLM-Generated Code in the Context of Software Verification\n  with Ada/SPARK","summary":"  Large language models (LLMs) have demonstrated remarkable code generation\ncapabilities, but the correctness of the generated code cannot be inherently\ntrusted. This paper explores the feasibility of using formal software\nverification, specifically the SPARK framework for Ada, to ensure the\nreliability of LLM-generated code. We present Marmaragan, a tool that leverages\nan LLM in order to generate SPARK annotations for existing programs, enabling\nformal verification of the code. The tool is benchmarked on a curated set of\nSPARK programs, with annotations selectively removed to test specific\ncapabilities. The performance of Marmaragan with GPT-4o on the benchmark is\npromising, with correct annotations having been generated for 50.7% of the\nbenchmark cases. The results establish a foundation for future work on\ncombining the power of LLMs with the reliability of formal software\nverification.\n","authors":["Marcos Cramer","Lucian McIntyre"],"pdf_url":"https://arxiv.org/pdf/2502.07728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v2","updated":"2025-02-11T17:40:41Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v2.pdf","comment":"Accepted to ACM Computing Surveys; 55 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2310.08731v3","updated":"2025-02-11T17:38:13Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Wei Zhou","Robert Wright","Mark O. Riedl"],"pdf_url":"https://arxiv.org/pdf/2310.08731v3.pdf","comment":"RLC Safety 2024"},{"id":"http://arxiv.org/abs/2502.07721v1","updated":"2025-02-11T17:33:48Z","published":"2025-02-11T17:33:48Z","title":"TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning","summary":"  The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments.\n","authors":["Mengyang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05701v3","updated":"2025-02-11T17:14:43Z","published":"2024-09-09T15:13:56Z","title":"pFedGPA: Diffusion-based Generative Parameter Aggregation for\n  Personalized Federated Learning","summary":"  Federated Learning (FL) offers a decentralized approach to model training,\nwhere data remains local and only model parameters are shared between the\nclients and the central server. Traditional methods, such as Federated\nAveraging (FedAvg), linearly aggregate these parameters which are usually\ntrained on heterogeneous data distributions, potentially overlooking the\ncomplex, high-dimensional nature of the parameter space. This can result in\ndegraded performance of the aggregated model. While personalized FL approaches\ncan mitigate the heterogeneous data issue to some extent, the limitation of\nlinear aggregation remains unresolved. To alleviate this issue, we investigate\nthe generative approach of diffusion model and propose a novel generative\nparameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this\nframework, we deploy a diffusion model on the server to integrate the diverse\nparameter distributions and propose a parameter inversion method to efficiently\ngenerate a set of personalized parameters for each client. This inversion\nmethod transforms the uploaded parameters into a latent code, which is then\naggregated through denoising sampling to produce the final personalized\nparameters. By encoding the dependence of a client's model parameters on the\nspecific data distribution using the high-capacity diffusion model,\n\\texttt{pFedGPA} can effectively decouple the complexity of the overall\ndistribution of all clients' model parameters from the complexity of each\nindividual client's parameter distribution. Our experimental results\nconsistently demonstrate the superior performance of the proposed method across\nmultiple datasets, surpassing baseline approaches.\n","authors":["Jiahao Lai","Jiaqi Li","Jian Xu","Yanru Wu","Boshi Tang","Siqi Chen","Yongfeng Huang","Wenbo Ding","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2409.05701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07709v1","updated":"2025-02-11T17:08:00Z","published":"2025-02-11T17:08:00Z","title":"MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces","summary":"  Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.\n","authors":["Loris Gaven","Thomas Carta","Clment Romac","Cdric Colas","Sylvain Lamprier","Olivier Sigaud","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2502.07709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17165v3","updated":"2025-02-11T17:06:45Z","published":"2023-11-28T19:01:09Z","title":"(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions","summary":"  The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.\n","authors":["Olivia Macmillan-Scott","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2311.17165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06663v4","updated":"2025-02-11T16:57:29Z","published":"2024-08-13T06:28:43Z","title":"Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","summary":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","authors":["Kaiser Sun","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2408.06663v4.pdf","comment":"Updated Draft"},{"id":"http://arxiv.org/abs/2412.16247v2","updated":"2025-02-11T16:54:45Z","published":"2024-12-20T00:01:16Z","title":"Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models","summary":"  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n","authors":["Konstantin Donhauser","Kristina Ulicna","Gemma Elyse Moran","Aditya Ravuri","Kian Kenyon-Dean","Cian Eastwood","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2412.16247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07693v1","updated":"2025-02-11T16:46:56Z","published":"2025-02-11T16:46:56Z","title":"SoK: A Classification for AI-driven Personalized Privacy Assistants","summary":"  To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.\n","authors":["Victor Morel","Leonardo Iwaya","Simone Fischer-Hbner"],"pdf_url":"https://arxiv.org/pdf/2502.07693v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.00507v2","updated":"2025-02-11T16:39:55Z","published":"2025-02-01T17:55:58Z","title":"A statistically consistent measure of Semantic Variability using\n  Language Models","summary":"  To address the issue of variability in the output generated by a language\nmodel, we present a measure of semantic variability that is statistically\nconsistent under mild assumptions. This measure, denoted as semantic spectral\nentropy, is a easy to implement algorithm that requires just off the shelf\nlanguage models. We put very few restrictions on the language models and we\nhave shown in a clear simulation studies that such method can generate accurate\nmetric despite randomness that arise from the language models.\n","authors":["Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12645v3","updated":"2025-02-11T16:36:32Z","published":"2024-06-18T14:13:13Z","title":"Evaluating Evidence Attribution in Generated Fact Checking Explanations","summary":"  Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.\n","authors":["Rui Xing","Timothy Baldwin","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2406.12645v3.pdf","comment":"Accepted to NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2405.20880v2","updated":"2025-02-11T16:29:04Z","published":"2024-05-31T14:55:11Z","title":"Paying to Do Better: Games with Payments between Learning Agents","summary":"  In repeated games, such as auctions, players typically use learning\nalgorithms to choose their actions. The use of such autonomous learning agents\nhas become widespread on online platforms. In this paper, we explore the impact\nof players incorporating monetary transfer policies into their agents'\nalgorithms, aiming to influence behavior in their favor through the dynamics\nbetween the agents. Our focus is on understanding when players have incentives\nto make use of monetary transfers, how such payments may affect learning\ndynamics, and what the implications are for welfare and its distribution among\nthe players. We propose a simple and general game-theoretic model to capture\nsuch scenarios. Our results on general games show that in a very broad class of\ngames, self-interested players benefit from letting their learning agents make\npayments to other learners during the game dynamics, and that in many cases,\nthis kind of behavior improves welfare for all players. Our results on first-\nand second-price auctions show that in equilibria of the ``payment policy\ngame,'' the agents' dynamics reach strong collusive outcomes with low revenue\nfor the auctioneer. These results raise new questions and highlight a challenge\nfor mechanism design in systems where automated learning agents can benefit\nfrom interacting with their peers in the digital ecosystem and outside the\nboundaries of the mechanism.\n","authors":["Yoav Kolumbus","Joe Halpern","va Tardos"],"pdf_url":"https://arxiv.org/pdf/2405.20880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18768v3","updated":"2025-02-11T16:24:23Z","published":"2024-09-27T14:12:49Z","title":"Learning from Demonstration with Implicit Nonlinear Dynamics Models","summary":"  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.\n","authors":["Peter David Fagan","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.18768v3.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.02153v2","updated":"2025-02-11T16:23:39Z","published":"2024-12-03T04:28:14Z","title":"Revisiting the Initial Steps in Adaptive Gradient Descent Optimization","summary":"  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.02153v2.pdf","comment":"Conference on Parsimony and Learning (CPAL) 2025"},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2502.06314v2","updated":"2025-02-11T16:04:15Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Schlkopf","Julius von Kgelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.05670v2","updated":"2025-02-11T16:02:57Z","published":"2025-02-08T19:13:40Z","title":"Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences","summary":"  Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.\n","authors":["Ada Defne Tur","Gaurav Kamath","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2502.05670v2.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.07663v1","updated":"2025-02-11T15:56:22Z","published":"2025-02-11T15:56:22Z","title":"Human Decision-making is Susceptible to AI-driven Manipulation","summary":"  Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.\n","authors":["Sahand Sabour","June M. Liu","Siyang Liu","Chris Z. Yao","Shiyao Cui","Xuanming Zhang","Wen Zhang","Yaru Cao","Advait Bhat","Jian Guan","Wei Wu","Rada Mihalcea","Tim Althoff","Tatia M. C. Lee","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07663v1.pdf","comment":"Work in progress. Code and data will be made available via\n  https://github.com/Sahandfer/Manipulation-Susceptibility"},{"id":"http://arxiv.org/abs/2403.06828v3","updated":"2025-02-11T15:47:43Z","published":"2024-03-11T15:44:38Z","title":"NeuPAN: Direct Point Robot Navigation with End-to-End Model-based\n  Learning","summary":"  Navigating a nonholonomic robot in a cluttered, unknown environment requires\naccurate perception and precise motion control for real-time collision\navoidance. This paper presents NeuPAN: a real-time, highly accurate, map-free,\neasy-to-deploy, and environment-invariant robot motion planner. Leveraging a\ntightly coupled perception-to-control framework, NeuPAN has two key innovations\ncompared to existing approaches: 1) it directly maps raw point cloud data to a\nlatent distance feature space for collision-free motion generation, avoiding\nerror propagation from the perception to control pipeline; 2) it is\ninterpretable from an end-to-end model-based learning perspective. The crux of\nNeuPAN is solving an end-to-end mathematical model with numerous point-level\nconstraints using a plug-and-play (PnP) proximal alternating-minimization\nnetwork (PAN), incorporating neurons in the loop. This allows NeuPAN to\ngenerate real-time, physically interpretable motions. It seamlessly integrates\ndata and knowledge engines, and its network parameters can be fine-tuned via\nbackpropagation. We evaluate NeuPAN on a ground mobile robot, a wheel-legged\nrobot, and an autonomous vehicle, in extensive simulated and real-world\nenvironments. Results demonstrate that NeuPAN outperforms existing baselines in\nterms of accuracy, efficiency, robustness, and generalization capabilities\nacross various environments, including the cluttered sandbox, office, corridor,\nand parking lot. We show that NeuPAN works well in unknown and unstructured\nenvironments with arbitrarily shaped objects, transforming impassable paths\ninto passable ones.\n","authors":["Ruihua Han","Shuai Wang","Shuaijun Wang","Zeqing Zhang","Jianjun Chen","Shijie Lin","Chengyang Li","Chengzhong Xu","Yonina C. Eldar","Qi Hao","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.06828v3.pdf","comment":"Accepted by TRO 2025; project website:\n  https://hanruihua.github.io/neupan_project/"},{"id":"http://arxiv.org/abs/2502.07656v1","updated":"2025-02-11T15:43:49Z","published":"2025-02-11T15:43:49Z","title":"A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders","summary":"  We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.\n","authors":["Daqian Shao","Thomas Kleine Buening","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2502.07656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08770v2","updated":"2025-02-11T15:39:08Z","published":"2024-07-11T17:52:03Z","title":"Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing","summary":"  Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics\n","authors":["Huanqian Wang","Yang Yue","Rui Lu","Jingxin Shi","Andrew Zhao","Shenzhi Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2407.08770v2.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.07644v1","updated":"2025-02-11T15:34:00Z","published":"2025-02-11T15:34:00Z","title":"SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models","summary":"  To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods.\n","authors":["Shihao Xia","Mengting He","Shuai Shao","Tingting Yu","Yiying Zhang","Linhai Song"],"pdf_url":"https://arxiv.org/pdf/2502.07644v1.pdf","comment":"16 pages. arXiv admin note: text overlap with arXiv:2404.04306"},{"id":"http://arxiv.org/abs/2502.07640v1","updated":"2025-02-11T15:27:35Z","published":"2025-02-11T15:27:35Z","title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving","summary":"  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.\n","authors":["Yong Lin","Shange Tang","Bohan Lyu","Jiayun Wu","Hongzhou Lin","Kaiyu Yang","Jia Li","Mengzhou Xia","Danqi Chen","Sanjeev Arora","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.07640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05147v2","updated":"2025-02-11T15:25:02Z","published":"2025-02-07T18:25:28Z","title":"LP-DETR: Layer-wise Progressive Relations for Object Detection","summary":"  This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach\nthat enhances DETR-based object detection through multi-scale relation\nmodeling. Our method introduces learnable spatial relationships between object\nqueries through a relation-aware self-attention mechanism, which adaptively\nlearns to balance different scales of relations (local, medium and global)\nacross decoder layers. This progressive design enables the model to effectively\ncapture evolving spatial dependencies throughout the detection pipeline.\nExtensive experiments on COCO 2017 dataset demonstrate that our method improves\nboth convergence speed and detection accuracy compared to standard\nself-attention module. The proposed method achieves competitive results,\nreaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50\nbackbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore,\nour analysis reveals an interesting pattern: the model naturally learns to\nprioritize local spatial relations in early decoder layers while gradually\nshifting attention to broader contexts in deeper layers, providing valuable\ninsights for future research in object detection.\n","authors":["Zhengjian Kang","Ye Zhang","Xiaoyu Deng","Xintao Li","Yongzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05147v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.07635v1","updated":"2025-02-11T15:23:05Z","published":"2025-02-11T15:23:05Z","title":"Distributed Value Decomposition Networks with Networked Agents","summary":"  We investigate the problem of distributed training under partial\nobservability, whereby cooperative multi-agent reinforcement learning agents\n(MARL) maximize the expected cumulative joint reward. We propose distributed\nvalue decomposition networks (DVDN) that generate a joint Q-function that\nfactorizes into agent-wise Q-functions. Whereas the original value\ndecomposition networks rely on centralized training, our approach is suitable\nfor domains where centralized training is not possible and agents must learn by\ninteracting with the physical environment in a decentralized manner while\ncommunicating with their peers. DVDN overcomes the need for centralized\ntraining by locally estimating the shared objective. We contribute with two\ninnovative algorithms, DVDN and DVDN (GT), for the heterogeneous and\nhomogeneous agents settings respectively. Empirically, both algorithms\napproximate the performance of value decomposition networks, in spite of the\ninformation loss during communication, as demonstrated in ten MARL tasks in\nthree standard environments.\n","authors":["Guilherme S. Varela","Alberto Sardinha","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2502.07635v1.pdf","comment":"21 pages, 15 figures, to be published in Proceedings of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS"},{"id":"http://arxiv.org/abs/2412.01175v2","updated":"2025-02-11T14:59:40Z","published":"2024-12-02T06:31:28Z","title":"OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?","summary":"  We introduce OBI-Bench, a holistic benchmark crafted to systematically\nevaluate large multi-modal models (LMMs) on whole-process oracle bone\ninscriptions (OBI) processing tasks demanding expert-level domain knowledge and\ndeliberate cognition. OBI-Bench includes 5,523 meticulously collected\ndiverse-sourced images, covering five key domain problems: recognition,\nrejoining, classification, retrieval, and deciphering. These images span\ncenturies of archaeological findings and years of research by front-line\nscholars, comprising multi-stage font appearances from excavation to synthesis,\nsuch as original oracle bone, inked rubbings, oracle bone fragments, cropped\nsingle characters, and handprinted characters. Unlike existing benchmarks,\nOBI-Bench focuses on advanced visual perception and reasoning with OBI-specific\nknowledge, challenging LMMs to perform tasks akin to those faced by experts.\nThe evaluation of 6 proprietary LMMs as well as 17 open-source LMMs highlights\nthe substantial challenges and demands posed by OBI-Bench. Even the latest\nversions of GPT-4o, Gemini 1.5 Pro, and Qwen-VL-Max are still far from\npublic-level humans in some fine-grained perception tasks. However, they\nperform at a level comparable to untrained humans in deciphering tasks,\nindicating remarkable capabilities in offering new interpretative perspectives\nand generating creative guesses. We hope OBI-Bench can facilitate the community\nto develop domain-specific multi-modal foundation models towards ancient\nlanguage research and delve deeper to discover and enhance these untapped\npotentials of LMMs.\n","authors":["Zijian Chen","Tingzhu Chen","Wenjun Zhang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.01175v2.pdf","comment":"Accepted by ICLR 2025 as a Poster. 31 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.14317v2","updated":"2025-02-11T14:51:08Z","published":"2024-08-26T14:45:03Z","title":"Claim Verification in the Age of Large Language Models: A Survey","summary":"  The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.\n","authors":["Alphaeus Dmonte","Roland Oruche","Marcos Zampieri","Prasad Calyam","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2408.14317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06097v2","updated":"2025-02-11T14:44:47Z","published":"2025-02-10T02:06:17Z","title":"NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems","summary":"  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n","authors":["Shuli Wang","Xue Wei","Senjie Kou","Chi Wang","Wenshuai Chen","Qi Tang","Yinhua Zhu","Xiong Xiao","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06097v2.pdf","comment":"Accepted by WWW 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.07591v1","updated":"2025-02-11T14:40:57Z","published":"2025-02-11T14:40:57Z","title":"DMWM: Dual-Mind World Model with Long-Term Imagination","summary":"  Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07586v1","updated":"2025-02-11T14:34:05Z","published":"2025-02-11T14:34:05Z","title":"We Can't Understand AI Using our Existing Vocabulary","summary":"  This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.\n","authors":["John Hewitt","Robert Geirhos","Been Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07586v1.pdf","comment":"Position paper"},{"id":"http://arxiv.org/abs/2501.07913v2","updated":"2025-02-11T14:30:44Z","published":"2025-01-14T07:55:18Z","title":"Governing AI Agents","summary":"  The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.\n","authors":["Noam Kolt"],"pdf_url":"https://arxiv.org/pdf/2501.07913v2.pdf","comment":"Notre Dame Law Review, Vol. 101, Forthcoming"},{"id":"http://arxiv.org/abs/2502.07577v1","updated":"2025-02-11T14:23:13Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v3","updated":"2025-02-11T14:18:17Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Samples for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11054v2","updated":"2025-02-11T14:15:13Z","published":"2024-08-20T17:58:59Z","title":"Near, far: Patch-ordering enhances vision foundation models' scene\n  understanding","summary":"  We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised\ntraining loss that enforces patch-level nearest neighbor consistency across a\nstudent and teacher model. Compared to contrastive approaches that only yield\nbinary learning signals, i.e., 'attract' and 'repel', this approach benefits\nfrom the more fine-grained learning signal of sorting spatially dense features\nrelative to reference patches. Our method leverages differentiable sorting\napplied on top of pretrained representations, such as DINOv2-registers to\nbootstrap the learning signal and further improve upon them. This dense\npost-pretraining leads to superior performance across various models and\ndatasets, despite requiring only 19 hours on a single GPU. This method\ngenerates high-quality dense feature encoders and establishes several new\nstate-of-the-art results such as +5.5% and +6% for non-parametric in-context\nsemantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear\nsegmentation evaluations on COCO-Things and -Stuff and improvements in the 3D\nunderstanding of multi-view consistency on SPair-71k, by more than 1.5%.\n","authors":["Valentinos Pariza","Mohammadreza Salehi","Gertjan Burghouts","Francesco Locatello","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2408.11054v2.pdf","comment":"Accepted at ICLR25. The webpage is accessible at:\n  https://vpariza.github.io/NeCo/"},{"id":"http://arxiv.org/abs/2404.07664v2","updated":"2025-02-11T14:05:29Z","published":"2024-04-11T11:55:42Z","title":"Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of\n  Out-of-Distribution Objects Using Prototypes","summary":"  Detecting and localising unknown or out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision, particularly in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play framework - PRototype-based OOD detection Without Labels\n(PROWL). It is an inference-based method that does not require training on the\ndomain dataset and relies on extracting relevant features from self-supervised\npre-trained models. PROWL can be easily adapted to detect in-domain objects in\nany operational design domain (ODD) in a zero-shot manner by specifying a list\nof known classes from this domain. PROWL, as a first zero-shot unsupervised\nmethod, achieves state-of-the-art results on the RoadAnomaly and RoadObstacle\ndatasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and\nFishyscapes, as well as comparable performance against existing supervised\nmethods trained without auxiliary OOD data. We also demonstrate its\ngeneralisability to other domains such as rail and maritime.\n","authors":["Poulami Sinhamahapatra","Franziska Schwaiger","Shirsha Bose","Huiyu Wang","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2404.07664v2.pdf","comment":"Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2502.07562v1","updated":"2025-02-11T14:00:12Z","published":"2025-02-11T14:00:12Z","title":"LoRP-TTS: Low-Rank Personalized Text-To-Speech","summary":"  Speech synthesis models convert written text into natural-sounding audio.\nWhile earlier models were limited to a single speaker, recent advancements have\nled to the development of zero-shot systems that generate realistic speech from\na wide range of speakers using their voices as additional prompts. However,\nthey still struggle with imitating non-studio-quality samples that differ\nsignificantly from the training datasets. In this work, we demonstrate that\nutilizing Low-Rank Adaptation (LoRA) allows us to successfully use even single\nrecordings of spontaneous speech in noisy environments as prompts. This\napproach enhances speaker similarity by up to $30pp$ while preserving content\nand naturalness. It represents a significant step toward creating truly diverse\nspeech corpora, that is crucial in all speech-related tasks.\n","authors":["ukasz Bondaruk","Jakub Kubiak"],"pdf_url":"https://arxiv.org/pdf/2502.07562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00535v3","updated":"2025-02-11T13:59:11Z","published":"2024-10-01T09:21:29Z","title":"The Causal Information Bottleneck and Optimal Causal Variable\n  Abstractions","summary":"  To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v3.pdf","comment":"Submitted to UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2501.08962v2","updated":"2025-02-11T13:55:01Z","published":"2025-01-15T17:18:46Z","title":"An analysis of data variation and bias in image-based dermatological\n  datasets for machine learning classification","summary":"  AI algorithms have become valuable in aiding professionals in healthcare. The\nincreasing confidence obtained by these models is helpful in critical decision\ndemands. In clinical dermatology, classification models can detect malignant\nlesions on patients' skin using only RGB images as input. However, most\nlearning-based methods employ data acquired from dermoscopic datasets on\ntraining, which are large and validated by a gold standard. Clinical models aim\nto deal with classification on users' smartphone cameras that do not contain\nthe corresponding resolution provided by dermoscopy. Also, clinical\napplications bring new challenges. It can contain captures from uncontrolled\nenvironments, skin tone variations, viewpoint changes, noises in data and\nlabels, and unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of samples is\nlow, it can cause degradations on the model's performance; the source\ndistribution used in training differs from the test set. This work aims to\nevaluate the gap between dermoscopic and clinical samples and understand how\nthe dataset variations impact training. It assesses the main differences\nbetween distributions that disturb the model's prediction. Finally, from\nexperiments on different architectures, we argue how to combine the data from\ndivergent distributions, decreasing the impact on the model's final accuracy.\n","authors":["Francisco Filho","Emanoel Santos","Rodrigo Mota","Kelvin Cunha","Fabio Papais","Amanda Arruda","Mateus Baltazar","Camila Vieira","Jos Gabriel Tavares","Rafael Barros","Othon Souza","Thales Bezerra","Natalia Lopes","rico Moutinho","Jssica Guido","Shirley Cruz","Paulo Borba","Tsang Ing Ren"],"pdf_url":"https://arxiv.org/pdf/2501.08962v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.01752v3","updated":"2025-02-11T13:46:36Z","published":"2024-04-02T09:07:12Z","title":"Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous\n  Space","summary":"  In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in\ncontinuous space. The difficulty of the problem arises from the extremely large\nsearch space caused by the combinatorial nature of the problem and the\ncontinuous state space. We propose a two-level approach where the low level is\na sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a\ncollision-free trajectory for individual robots. The high level can use any\nmethod that can resolve inter-robot conflicts where we employ two\nrepresentative methods that are Prioritized Planning (SI-CPP) and Conflict\nBased Search (SI-CCBS). Experimental results show that SI-RRT* can quickly find\na high-quality solution with a few samples. SI-CPP exhibits improved\nscalability while SI-CCBS produces higher-quality solutions compared to the\nstate-of-the-art planners for continuous space.\n","authors":["Joonyeol Sim","Joonkyung Kim","Changjoo Nam"],"pdf_url":"https://arxiv.org/pdf/2404.01752v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07552v1","updated":"2025-02-11T13:41:06Z","published":"2025-02-11T13:41:06Z","title":"Unsupervised Translation of Emergent Communication","summary":"  Emergent Communication (EC) provides a unique window into the language\nsystems that emerge autonomously when agents are trained to jointly achieve\nshared goals. However, it is difficult to interpret EC and evaluate its\nrelationship with natural languages (NL). This study employs unsupervised\nneural machine translation (UNMT) techniques to decipher ECs formed during\nreferential games with varying task complexities, influenced by the semantic\ndiversity of the environment. Our findings demonstrate UNMT's potential to\ntranslate EC, illustrating that task complexity characterized by semantic\ndiversity enhances EC translatability, while higher task complexity with\nconstrained semantic variability exhibits pragmatic EC, which, although\nchallenging to interpret, remains suitable for translation. This research marks\nthe first attempt, to our knowledge, to translate EC without the aid of\nparallel data.\n","authors":["Ido Levy","Orr Paradise","Boaz Carmeli","Ron Meir","Shafi Goldwasser","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2502.07552v1.pdf","comment":"19 pages (including appendix and bibliography), Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07549v1","updated":"2025-02-11T13:39:35Z","published":"2025-02-11T13:39:35Z","title":"HGTUL: A Hypergraph-based Model For Trajectory User Linking","summary":"  Trajectory User Linking (TUL), which links anonymous trajectories with users\nwho generate them, plays a crucial role in modeling human mobility. Despite\nsignificant advancements in this field, existing studies primarily neglect the\nhigh-order inter-trajectory relationships, which represent complex associations\namong multiple trajectories, manifested through multi-location co-occurrence\npatterns emerging when trajectories intersect at various Points of Interest\n(POIs). Furthermore, they also overlook the variable influence of POIs on\ndifferent trajectories, as well as the user class imbalance problem caused by\ndisparities in user activity levels and check-in frequencies. To address these\nlimitations, we propose a novel HyperGraph-based multi-perspective Trajectory\nUser Linking model (HGTUL). Our model learns trajectory representations from\nboth relational and spatio-temporal perspectives: (1) it captures high-order\nassociations among trajectories by constructing a trajectory hypergraph and\nleverages a hypergraph attention network to learn the variable impact of POIs\non trajectories; (2) it models the spatio-temporal characteristics of\ntrajectories by incorporating their temporal and spatial information into a\nsequential encoder. Moreover, we design a data balancing method to effectively\naddress the user class imbalance problem and experimentally validate its\nsignificance in TUL. Extensive experiments on three real-world datasets\ndemonstrate that HGTUL outperforms state-of-the-art baselines, achieving\nimprovements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,\nrespectively.\n","authors":["Fengjie Chang","Xinning Zhu","Zheng Hu","Yang Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07549v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.07542v1","updated":"2025-02-11T13:29:58Z","published":"2025-02-11T13:29:58Z","title":"Exoplanet Transit Candidate Identification in TESS Full-Frame Images via\n  a Transformer-Based Algorithm","summary":"  The Transiting Exoplanet Survey Satellite (TESS) is surveying a large\nfraction of the sky, generating a vast database of photometric time series data\nthat requires thorough analysis to identify exoplanetary transit signals.\nAutomated learning approaches have been successfully applied to identify\ntransit signals. However, most existing methods focus on the classification and\nvalidation of candidates, while few efforts have explored new techniques for\nthe search of candidates. To search for new exoplanet transit candidates, we\npropose an approach to identify exoplanet transit signals without the need for\nphase folding or assuming periodicity in the transit signals, such as those\nobserved in multi-transit light curves. To achieve this, we implement a new\nneural network inspired by Transformers to directly process Full Frame Image\n(FFI) light curves to detect exoplanet transits. Transformers, originally\ndeveloped for natural language processing, have recently demonstrated\nsignificant success in capturing long-range dependencies compared to previous\napproaches focused on sequential data. This ability allows us to employ\nmulti-head self-attention to identify exoplanet transit signals directly from\nthe complete light curves, combined with background and centroid time series,\nwithout requiring prior transit parameters. The network is trained to learn\ncharacteristics of the transit signal, like the dip shape, which helps\ndistinguish planetary transits from other variability sources. Our model\nsuccessfully identified 214 new planetary system candidates, including 122\nmulti-transit light curves, 88 single-transit and 4 multi-planet systems from\nTESS sectors 1-26 with a radius > 0.27 $R_{\\mathrm{Jupiter}}$, demonstrating\nits ability to detect transits regardless of their periodicity.\n","authors":["Helem Salinas","Rafael Brahm","Greg Olmschenk","Richard K. Barry","Karim Pichara","Stela Ishitani Silva","Vladimir Araujo"],"pdf_url":"https://arxiv.org/pdf/2502.07542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00641v2","updated":"2025-02-11T13:12:16Z","published":"2025-02-02T03:07:45Z","title":"Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance","summary":"  The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.\n","authors":["Borui Xu","Yao Chen","Zeyi Wen","Weiguo Liu","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2502.00641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v1","updated":"2025-02-11T13:11:59Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07527v1","updated":"2025-02-11T13:08:03Z","published":"2025-02-11T13:08:03Z","title":"NatureLM: Deciphering the Language of Nature for Scientific Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v1.pdf","comment":"81 pages"},{"id":"http://arxiv.org/abs/2501.15442v2","updated":"2025-02-11T13:05:36Z","published":"2025-01-26T08:10:13Z","title":"Overview of the Amphion Toolkit (v0.2)","summary":"  Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.\n","authors":["Jiaqi Li","Xueyao Zhang","Yuancheng Wang","Haorui He","Chaoren Wang","Li Wang","Huan Liao","Junyi Ao","Zeyu Xie","Yiqiao Huang","Junan Zhang","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2501.15442v2.pdf","comment":"Github: https://github.com/open-mmlab/Amphion"},{"id":"http://arxiv.org/abs/2502.05398v2","updated":"2025-02-11T12:57:13Z","published":"2025-02-08T01:10:56Z","title":"Probabilistic Foundations for Metacognition via Hybrid-AI","summary":"  Metacognition is the concept of reasoning about an agent's own internal\nprocesses, and it has recently received renewed attention with respect to\nartificial intelligence (AI) and, more specifically, machine learning systems.\nThis paper reviews a hybrid-AI approach known as \"error detecting and\ncorrecting rules\" (EDCR) that allows for the learning of rules to correct\nperceptual (e.g., neural) models. Additionally, we introduce a probabilistic\nframework that adds rigor to prior empirical studies, and we use this framework\nto prove results on necessary and sufficient conditions for metacognitive\nimprovement, as well as limits to the approach. A set of future\n","authors":["Paulo Shakarian","Gerardo I. Simari","Nathaniel D. Bastian"],"pdf_url":"https://arxiv.org/pdf/2502.05398v2.pdf","comment":"Accepted to AAAI-MAKE 2025"},{"id":"http://arxiv.org/abs/2502.07523v1","updated":"2025-02-11T12:55:32Z","published":"2025-02-11T12:55:32Z","title":"Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization","summary":"  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n","authors":["Daniel Palenicek","Florian Vogt","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.07523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19458v4","updated":"2025-02-11T12:41:08Z","published":"2024-05-29T19:12:08Z","title":"MemControl: Mitigating Memorization in Diffusion Models via Automated\n  Parameter Selection","summary":"  Diffusion models excel in generating images that closely resemble their\ntraining data but are also susceptible to data memorization, raising privacy,\nethical, and legal concerns, particularly in sensitive domains such as medical\nimaging. We hypothesize that this memorization stems from the\noverparameterization of deep models and propose that regularizing model\ncapacity during fine-tuning can mitigate this issue. Firstly, we empirically\nshow that regulating the model capacity via Parameter-efficient fine-tuning\n(PEFT) mitigates memorization to some extent, however, it further requires the\nidentification of the exact parameter subsets to be fine-tuned for high-quality\ngeneration. To identify these subsets, we introduce a bi-level optimization\nframework, MemControl, that automates parameter selection using memorization\nand generation quality metrics as rewards during fine-tuning. The parameter\nsubsets discovered through MemControl achieve a superior tradeoff between\ngeneration quality and memorization. For the task of medical image generation,\nour approach outperforms existing state-of-the-art memorization mitigation\nstrategies by fine-tuning as few as 0.019% of model parameters. Moreover, we\ndemonstrate that the discovered parameter subsets are transferable to\nnon-medical domains. Our framework is scalable to large datasets, agnostic to\nreward functions, and can be integrated with existing approaches for further\nmemorization mitigation. To the best of our knowledge, this is the first study\nto empirically evaluate memorization in medical images and propose a targeted\nyet universal mitigation strategy. The code is available at\nhttps://github.com/Raman1121/Diffusion_Memorization_HPO.\n","authors":["Raman Dutt","Ondrej Bohdal","Pedro Sanchez","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2405.19458v4.pdf","comment":"Accepted into WACV 2025 (Applications Track)"},{"id":"http://arxiv.org/abs/2502.07516v1","updated":"2025-02-11T12:36:00Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07799v3","updated":"2025-02-11T12:32:59Z","published":"2023-10-11T18:32:21Z","title":"Domain-invariant Clinical Representation Learning by Bridging Data\n  Distribution Shift across EMR Datasets","summary":"  Emerging diseases present challenges in symptom recognition and timely\nclinical intervention due to limited available information. An effective\nprognostic model could assist physicians in making accurate diagnoses and\ndesigning personalized treatment plans to prevent adverse outcomes. However, in\nthe early stages of disease emergence, several factors hamper model\ndevelopment: limited data collection, insufficient clinical experience, and\nprivacy and ethical concerns restrict data availability and complicate accurate\nlabel assignment. Furthermore, Electronic Medical Record (EMR) data from\ndifferent diseases or sources often exhibit significant cross-dataset feature\nmisalignment, severely impacting the effectiveness of deep learning models. We\npresent a domain-invariant representation learning method that constructs a\ntransition model between source and target datasets. By constraining the\ndistribution shift of features generated across different domains, we capture\ndomain-invariant features specifically relevant to downstream tasks, developing\na unified domain-invariant encoder that achieves better feature representation\nacross various task domains. Experimental results across multiple target tasks\ndemonstrate that our proposed model surpasses competing baseline methods and\nachieves faster training convergence, particularly when working with limited\ndata. Extensive experiments validate our method's effectiveness in providing\nmore accurate predictions for emerging pandemics and other diseases. Code is\npublicly available at https://github.com/wang1yuhang/domain_invariant_network.\n","authors":["Zhongji Zhang","Yuhang Wang","Yinghao Zhu","Xinyu Ma","Yasha Wang","Junyi Gao","Liantao Ma","Wen Tang","Xiaoyun Zhang","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17472v4","updated":"2025-02-11T12:29:00Z","published":"2024-02-27T12:53:15Z","title":"RAGFormer: Learning Semantic Attributes and Topological Structure for\n  Fraud Detection","summary":"  Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.\n","authors":["Haolin Li","Shuyang Jiang","Lifeng Zhang","Siyuan Du","Guangnan Ye","Hongfeng Chai"],"pdf_url":"https://arxiv.org/pdf/2402.17472v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.00134v4","updated":"2025-02-11T12:28:36Z","published":"2024-08-29T12:55:10Z","title":"MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale","summary":"  Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference.\n","authors":["Anton Andreychuk","Konstantin Yakovlev","Aleksandr Panov","Alexey Skrynnik"],"pdf_url":"https://arxiv.org/pdf/2409.00134v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00696v3","updated":"2025-02-11T12:21:13Z","published":"2024-09-01T11:24:54Z","title":"Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation","summary":"  Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.\n","authors":["Jasper Dekoninck","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2409.00696v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07503v1","updated":"2025-02-11T12:11:40Z","published":"2025-02-11T12:11:40Z","title":"Harnessing Language's Fractal Geometry with Recursive Inference Scaling","summary":"  Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.\n","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07503v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.04411v2","updated":"2025-02-11T12:09:51Z","published":"2025-02-06T11:26:30Z","title":"Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing","summary":"  Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.\n","authors":["Kunfeng Lai","Zhenheng Tang","Xinglin Pan","Peijie Dong","Xiang Liu","Haolan Chen","Li Shen","Bo Li","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.04411v2.pdf","comment":"work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors"},{"id":"http://arxiv.org/abs/2502.07494v1","updated":"2025-02-11T11:53:23Z","published":"2025-02-11T11:53:23Z","title":"URECA: The Chain of Two Minimum Set Cover Problems exists behind\n  Adaptation to Shifts in Semantic Code Search","summary":"  Adaptation is to make model learn the patterns shifted from the training\ndistribution. In general, this adaptation is formulated as the minimum entropy\nproblem. However, the minimum entropy problem has inherent limitation --\nshifted initialization cascade phenomenon. We extend the relationship between\nthe minimum entropy problem and the minimum set cover problem via Lebesgue\nintegral. This extension reveals that internal mechanism of the minimum entropy\nproblem ignores the relationship between disentangled representations, which\nleads to shifted initialization cascade. From the analysis, we introduce a new\nclustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).\nURECA is an efficient clustering algorithm for the leverage of the\nrelationships between disentangled representations. The update rule of URECA\ndepends on Thresholdly-Updatable Stationary Assumption to dynamics as a\nreleased version of Stationary Assumption. This assumption helps URECA to\ntransport disentangled representations with no errors based on the\nrelationships between disentangled representations. URECA also utilize\nsimulation trick to efficiently cluster disentangled representations. The wide\nrange of evaluations show that URECA achieves consistent performance gains for\nthe few-shot adaptation to diverse types of shifts along with advancement to\nState-of-The-Art performance in CoSQA in the scenario of query shift.\n","authors":["Seok-Ung Choi","Joonghyuk Hahn","Yo-Sub Han"],"pdf_url":"https://arxiv.org/pdf/2502.07494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12956v2","updated":"2025-02-11T11:50:15Z","published":"2025-01-22T15:29:09Z","title":"GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models","summary":"  Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.\n","authors":["Pengxiang Zhao","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.12956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07479v1","updated":"2025-02-11T11:40:43Z","published":"2025-02-11T11:40:43Z","title":"WebChecker: A Versatile EVL Plugin for Validating HTML Pages with\n  Bootstrap Frameworks","summary":"  WebChecker is a plugin for Epsilon Validation Language (EVL), designed to\nvalidate both static and dynamic HTML pages utilizing frameworks like\nBootstrap. By employing configurable EVL constraints, WebChecker enforces\nimplicit rules governing HTML and CSS frameworks. The effectiveness of the\nplugin is demonstrated through its application on Bootstrap, the widely adopted\nHTML, CSS, and JavaScript framework. WebChecker comes with a set of EVL\nconstraints to assess Bootstrap based web pages. To substantiate our claims, I\npresent an illustrative example featuring two solutions that effectively\nenforce implicit rules.\n","authors":["Milind Cherukuri"],"pdf_url":"https://arxiv.org/pdf/2502.07479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07469v1","updated":"2025-02-11T11:25:10Z","published":"2025-02-11T11:25:10Z","title":"5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence","summary":"  Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion.\n","authors":["Gianluca Galletti","Fabian Paischer","Paul Setinek","William Hornsby","Lorenzo Zanisi","Naomi Carey","Stanislas Pamela","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.07469v1.pdf","comment":"6 pages (+ references and appendix)"},{"id":"http://arxiv.org/abs/2408.01322v3","updated":"2025-02-11T11:18:26Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  The objects we perceive guide our eye movements when observing real-world\ndynamic scenes. Yet, gaze shifts and selective attention are critical for\nperceiving details and refining object boundaries. Object segmentation and gaze\nbehavior are, however, typically treated as two independent processes. Here, we\npresent a computational model that simulates these processes in an\ninterconnected manner and allows for hypothesis-driven investigations of\ndistinct attentional mechanisms. Drawing on an information processing pattern\nfrom robotics, we use a Bayesian filter to recursively segment the scene, which\nalso provides an uncertainty estimate for the object boundaries that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior on a dataset of dynamic real-world\nscenes, measured by scanpath statistics, including foveation duration and\nsaccade amplitude distributions used for parameter fitting and higher-level\nstatistics not used for fitting. These include how object detections,\ninspections, and returns are balanced and a delay of returning saccades without\nan explicit implementation of such temporal inhibition of return. Extensive\nsimulations and ablation studies show that uncertainty promotes balanced\nexploration and that semantic object cues are crucial to forming the perceptual\nunits used in object-based attention. Moreover, we show how our model's modular\ndesign allows for extensions, such as incorporating saccadic momentum or\npre-saccadic attention, to further align its output with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v3.pdf","comment":"40+25 pages, 8+7 figures"},{"id":"http://arxiv.org/abs/2502.07465v1","updated":"2025-02-11T11:16:59Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v1.pdf","comment":"8 pages,6 figures"},{"id":"http://arxiv.org/abs/2502.07461v1","updated":"2025-02-11T11:12:19Z","published":"2025-02-11T11:12:19Z","title":"JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed\n  Metadata","summary":"  We introduce JamendoMaxCaps, a large-scale music-caption dataset featuring\nover 200,000 freely licensed instrumental tracks from the renowned Jamendo\nplatform. The dataset includes captions generated by a state-of-the-art\ncaptioning model, enhanced with imputed metadata. We also introduce a retrieval\nsystem that leverages both musical features and metadata to identify similar\nsongs, which are then used to fill in missing metadata using a local large\nlanguage model (LLLM). This approach allows us to provide a more comprehensive\nand informative dataset for researchers working on music-language understanding\ntasks. We validate this approach quantitatively with five different\nmeasurements. By making the JamendoMaxCaps dataset publicly available, we\nprovide a high-quality resource to advance research in music-language\nunderstanding tasks such as music retrieval, multimodal representation\nlearning, and generative music models.\n","authors":["Abhinaba Roy","Renhang Liu","Tongyu Lu","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2502.07461v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.17438v2","updated":"2025-02-11T11:10:32Z","published":"2024-11-26T13:54:24Z","title":"Object-centric proto-symbolic behavioural reasoning from pixels","summary":"  Autonomous intelligent agents must bridge computational challenges at\ndisparate levels of abstraction, from the low-level spaces of sensory input and\nmotor commands to the high-level domain of abstract reasoning and planning. A\nkey question in designing such agents is how best to instantiate the\nrepresentational space that will interface between these two levels -- ideally\nwithout requiring supervision in the form of expensive data annotations. These\nobjectives can be efficiently achieved by representing the world in terms of\nobjects (grounded in perception and action). In this work, we present a novel,\nbrain-inspired, deep-learning architecture that learns from pixels to\ninterpret, control, and reason about its environment, using object-centric\nrepresentations. We show the utility of our approach through tasks in synthetic\nenvironments that require a combination of (high-level) logical reasoning and\n(low-level) continuous control. Results show that the agent can learn emergent\nconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as\nwell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land\nC)$ and XOR operations, and successfully controls its environment to satisfy\nobjectives deduced from these logical rules. The agent can adapt online to\nunexpected changes in its environment and is robust to mild violations of its\nworld model, thanks to dynamic internal desired goal generation. While the\npresent results are limited to synthetic settings (2D and 3D activated versions\nof dSprites), which fall short of real-world levels of complexity, the proposed\narchitecture shows how to manipulate grounded object representations, as a key\ninductive bias for unsupervised learning, to enable behavioral reasoning.\n","authors":["Ruben van Bergen","Justus Hbotter","Pablo Lanillos"],"pdf_url":"https://arxiv.org/pdf/2411.17438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11676v3","updated":"2025-02-11T11:09:19Z","published":"2024-07-16T12:52:29Z","title":"SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with\n  Realistic Validation On Diverse Modalities","summary":"  Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a\nlabeled source domain to perform well on an unlabeled target domain with some\ndata distribution shift. While many methods have been proposed in the\nliterature, fair and realistic evaluation remains an open question,\nparticularly due to methodological difficulties in selecting hyperparameters in\nthe unsupervised setting. With SKADA-bench, we propose a framework to evaluate\nDA methods on diverse modalities, beyond computer vision task that have been\nlargely explored in the literature. We present a complete and fair evaluation\nof existing shallow algorithms, including reweighting, mapping, and subspace\nalignment. Realistic hyperparameter selection is performed with nested\ncross-validation and various unsupervised model selection scores, on both\nsimulated datasets with controlled shifts and real-world datasets across\ndiverse modalities, such as images, text, biomedical, and tabular data. Our\nbenchmark highlights the importance of realistic validation and provides\npractical guidance for real-life applications, with key insights into the\nchoice and impact of model selection approaches. SKADA-bench is open-source,\nreproducible, and can be easily extended with novel DA methods, datasets, and\nmodel selection criteria without requiring re-evaluating competitors.\nSKADA-bench is available on Github at\nhttps://github.com/scikit-adaptation/skada-bench.\n","authors":["Yanis Lalou","Tho Gnassounou","Antoine Collas","Antoine de Mathelin","Oleksii Kachaiev","Ambroise Odonnat","Alexandre Gramfort","Thomas Moreau","Rmi Flamary"],"pdf_url":"https://arxiv.org/pdf/2407.11676v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07459v1","updated":"2025-02-11T11:07:44Z","published":"2025-02-11T11:07:44Z","title":"PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian","summary":"  Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul\n","authors":["Erfan Moosavi Monazzah","Vahid Rahimzadeh","Yadollah Yaghoobzadeh","Azadeh Shakery","Mohammad Taher Pilehvar"],"pdf_url":"https://arxiv.org/pdf/2502.07459v1.pdf","comment":"Accepted at NAACL 2025 Main Conference, the dataset is available on\n  HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)"},{"id":"http://arxiv.org/abs/2411.12502v3","updated":"2025-02-11T11:03:24Z","published":"2024-11-19T13:40:49Z","title":"Transformer Neural Processes - Kernel Regression","summary":"  Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.\n","authors":["Daniel Jenson","Jhonathan Navott","Mengyan Zhang","Makkunda Sharma","Elizaveta Semenova","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2411.12502v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07222v2","updated":"2025-02-11T11:02:10Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v2.pdf","comment":"New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2502.07455v1","updated":"2025-02-11T10:57:12Z","published":"2025-02-11T10:57:12Z","title":"RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation","summary":"  Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.\n","authors":["Viacheslav Vasilev","Julia Agafonova","Nikolai Gerasimenko","Alexander Kapitanov","Polina Mikhailova","Evelina Mironova","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2502.07455v1.pdf","comment":"Accepted for NAACL 2025 Findings, GitHub:\n  https://github.com/ai-forever/RusCode"},{"id":"http://arxiv.org/abs/2502.00321v3","updated":"2025-02-11T10:55:02Z","published":"2025-02-01T05:06:21Z","title":"MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling","summary":"  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n","authors":["Bencheng Yan","Si Chen","Shichang Jia","Jianyu Liu","Yueran Liu","Chenghan Fu","Wanxian Guan","Hui Zhao","Xiang Zhang","Kai Zhang","Wenbo Su","Pengjie Wang","Jian Xu","Bo Zheng","Baolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07452v1","updated":"2025-02-11T10:52:54Z","published":"2025-02-11T10:52:54Z","title":"Eliciting Rational Initial Weights in Gradual Argumentation","summary":"  Many semantics for weighted argumentation frameworks assume that each\nargument is associated with an initial weight. However, eliciting these initial\nweights poses challenges: (1) accurately providing a specific numerical value\nis often difficult, and (2) individuals frequently confuse initial weights with\nacceptability degrees in the presence of other arguments. To address these\nissues, we propose an elicitation pipeline that allows one to specify\nacceptability degree intervals for each argument. By employing gradual\nsemantics, we can refine these intervals when they are rational, restore\nrationality when they are not, and ultimately identify possible initial weights\nfor each argument.\n","authors":["Nir Oren","Bruno Yun"],"pdf_url":"https://arxiv.org/pdf/2502.07452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07445v1","updated":"2025-02-11T10:43:36Z","published":"2025-02-11T10:43:36Z","title":"Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon","summary":"  Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.\n","authors":["Nurit Cohen-Inger","Yehonatan Elisha","Bracha Shapira","Lior Rokach","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07443v1","updated":"2025-02-11T10:37:20Z","published":"2025-02-11T10:37:20Z","title":"Approximating Human Strategic Reasoning with LLM-Enhanced Recursive\n  Reasoners Leveraging Multi-agent Hypergames","summary":"  LLM-driven multi-agent-based simulations have been gaining traction with\napplications in game-theoretic and social simulations. While most\nimplementations seek to exploit or evaluate LLM-agentic reasoning, they often\ndo so with a weak notion of agency and simplified architectures. We implement a\nrole-based multi-agent strategic interaction framework tailored to\nsophisticated recursive reasoners, providing the means for systematic in-depth\ndevelopment and evaluation of strategic reasoning. Our game environment is\ngoverned by the umpire responsible for facilitating games, from matchmaking\nthrough move validation to environment management. Players incorporate\nstate-of-the-art LLMs in their decision mechanism, relying on a formal\nhypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty\ncontests to evaluate the recursive reasoning capabilities of the latest LLMs,\nproviding a comparison to an established baseline model from economics and data\nfrom human experiments. Furthermore, we introduce the foundations of an\nalternative semantic measure of reasoning to the k-level theory. Our\nexperiments show that artificial reasoners can outperform the baseline model in\nterms of both approximating human behaviour and reaching the optimal solution.\n","authors":["Vince Trencsenyi","Agnieszka Mensfelt","Kostas Stathis"],"pdf_url":"https://arxiv.org/pdf/2502.07443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11228v2","updated":"2025-02-11T10:35:04Z","published":"2024-09-17T14:21:02Z","title":"Learning Source Disentanglement in Neural Audio Codec","summary":"  Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.\n","authors":["Xiaoyu Bie","Xubo Liu","Gal Richard"],"pdf_url":"https://arxiv.org/pdf/2409.11228v2.pdf","comment":"ICASSP 2025, project page: https://xiaoyubie1994.github.io/sdcodec/"},{"id":"http://arxiv.org/abs/2412.04476v2","updated":"2025-02-11T10:35:02Z","published":"2024-11-19T15:40:16Z","title":"The Moral Mind(s) of Large Language Models","summary":"  As large language models (LLMs) become integrated into decision-making across\nvarious sectors, key questions arise: do they exhibit an emergent \"moral mind\"\n- a consistent set of moral principles guiding their ethical judgments - and is\nthis reasoning uniform or diverse across models? To investigate this, we\npresented approximately forty models from major providers with a structured set\nof ethical scenarios, creating one of the largest datasets of its kind. Our\nrationality tests revealed that at least one model from each provider exhibited\nbehavior consistent with approximately stable moral principles, effectively\nacting as if nearly optimizing a utility function encoding ethical reasoning.\nWe estimated these utility functions and found that models tend to cluster\naround neutral ethical stances. To further characterize moral heterogeneity, we\napplied a non-parametric permutation approach, constructing a probabilistic\nsimilarity network based on revealed preference patterns. This analysis showed\nthat while approximately rational models share a core ethical structure,\ndifferences emerged: roughly half displayed greater moral adaptability,\nbridging diverse perspectives, while the remainder adhered to more rigid\nethical structures.\n","authors":["Avner Seror"],"pdf_url":"https://arxiv.org/pdf/2412.04476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07441v1","updated":"2025-02-11T10:31:43Z","published":"2025-02-11T10:31:43Z","title":"SensPS: Sensing Personal Space Comfortable Distance between Human-Human\n  Using Multimodal Sensors","summary":"  Personal space, also known as peripersonal space, is crucial in human social\ninteraction, influencing comfort, communication, and social stress. Estimating\nand respecting personal space is essential for enhancing human-computer\ninteraction (HCI) and smart environments. Personal space preferences vary due\nto individual traits, cultural background, and contextual factors. Advanced\nmultimodal sensing technologies, including eye-tracking and wristband sensors,\noffer opportunities to develop adaptive systems that dynamically adjust to user\ncomfort levels. Integrating physiological and behavioral data enables a deeper\nunderstanding of spatial interactions. This study develops a sensor-based model\nto estimate comfortable personal space and identifies key features influencing\nspatial preferences. Our findings show that multimodal sensors, particularly\neye-tracking and physiological wristband data, can effectively predict personal\nspace preferences, with eye-tracking data playing a more significant role. An\nexperimental study involving controlled human interactions demonstrates that a\nTransformer-based model achieves the highest predictive accuracy (F1 score:\n0.87) for estimating personal space. Eye-tracking features, such as gaze point\nand pupil diameter, emerge as the most significant predictors, while\nphysiological signals from wristband sensors contribute marginally. These\nresults highlight the potential for AI-driven personalization of social space\nin adaptive environments, suggesting that multimodal sensing can be leveraged\nto develop intelligent systems that optimize spatial arrangements in\nworkplaces, educational institutions, and public settings. Future work should\nexplore larger datasets, real-world applications, and additional physiological\nmarkers to enhance model robustness.\n","authors":["Ko Watanabe","Nico Frster","Shoya Ishimaru"],"pdf_url":"https://arxiv.org/pdf/2502.07441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12663v2","updated":"2025-02-11T10:21:16Z","published":"2024-07-17T15:47:25Z","title":"Is That Rain? Understanding Effects on Visual Odometry Performance for\n  Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge","summary":"  The development of safe and reliable autonomous unmanned aerial vehicles\nrelies on the ability of the system to recognise and adapt to changes in the\nlocal environment based on sensor inputs. State-of-the-art local tracking and\ntrajectory planning are typically performed using camera sensor input to the\nflight control algorithm, but the extent to which environmental disturbances\nlike rain affect the performance of these systems is largely unknown. In this\npaper, we first describe the development of an open dataset comprising ~335k\nimages to examine these effects for seven different classes of precipitation\nconditions and show that a worst-case average tracking error of 1.5 m is\npossible for a state-of-the-art visual odometry system (VINS-Fusion). We then\nuse the dataset to train a set of deep neural network models suited to mobile\nand constrained deployment scenarios to determine the extent to which it may be\npossible to efficiently and accurately classify these `rainy' conditions. The\nmost lightweight of these models (MobileNetV3 small) can achieve an accuracy of\n90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which\nis suitable for deployment in resource-constrained and latency-sensitive\nsystems. We demonstrate a classification latency in the order of milliseconds\nusing typical flight computer hardware. Accordingly, such a model can feed into\nthe disturbance estimation component of an autonomous flight controller. In\naddition, data from unmanned aerial vehicles with the ability to accurately\ndetermine environmental conditions in real time may contribute to developing\nmore granular timely localised weather forecasting.\n","authors":["Andrea Albanese","Yanran Wang","Davide Brunelli","David Boyle"],"pdf_url":"https://arxiv.org/pdf/2407.12663v2.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07760v1","updated":"2025-02-11T18:43:07Z","published":"2025-02-11T18:43:07Z","title":"Scalable Fingerprinting of Large Language Models","summary":"  Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks.\n","authors":["Anshul Nasery","Jonathan Hayase","Creston Brooks","Peiyao Sheng","Himanshu Tyagi","Pramod Viswanath","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07760v1.pdf","comment":"23 pages 15 figures"},{"id":"http://arxiv.org/abs/2502.00950v3","updated":"2025-02-11T18:24:41Z","published":"2025-02-02T23:04:55Z","title":"Fast Audio Codec Identification Using Overlapping LCS","summary":"  Audio data are widely exchanged over telecommunications networks. Due to the\nlimitations of network resources, these data are typically compressed before\ntransmission. Various methods are available for compressing audio data. To\naccess such audio information, it is first necessary to identify the codec used\nfor compression. One of the most effective approaches for audio codec\nidentification involves analyzing the content of received packets. In these\nmethods, statistical features extracted from the packets are utilized to\ndetermine the codec employed. This paper proposes a novel method for audio\ncodec classification based on features derived from the overlapped longest\ncommon sub-string and sub-sequence (LCS). The simulation results, which\nachieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of\nthe proposed method over conventional approaches. This method divides each 8 KB\npacket into fifteen 1 KB packets with a 50% overlap. The results indicate that\nthis division has no significant impact on the simulation outcomes, while\nsignificantly speeding up the feature extraction, being eight times faster than\nthe traditional method for extracting LCS features.\n","authors":["Farzane Jafari"],"pdf_url":"https://arxiv.org/pdf/2502.00950v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2409.03743v2","updated":"2025-02-11T17:48:15Z","published":"2024-09-05T17:56:19Z","title":"Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)","summary":"  Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.\n","authors":["Hans Winderix","Marton Bognar","Lesly-Ann Daniel","Frank Piessens"],"pdf_url":"https://arxiv.org/pdf/2409.03743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07657v1","updated":"2025-02-11T15:46:03Z","published":"2025-02-11T15:46:03Z","title":"Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian\n  Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations","summary":"  We consider the problem of approximating a $d \\times d$ covariance matrix $M$\nwith a rank-$k$ matrix under $(\\varepsilon,\\delta)$-differential privacy. We\npresent and analyze a complex variant of the Gaussian mechanism and obtain\nupper bounds on the Frobenius norm of the difference between the matrix output\nby this mechanism and the best rank-$k$ approximation to $M$. Our analysis\nprovides improvements over previous bounds, particularly when the spectrum of\n$M$ satisfies natural structural assumptions. The novel insight is to view the\naddition of Gaussian noise to a matrix as a continuous-time matrix Brownian\nmotion. This viewpoint allows us to track the evolution of eigenvalues and\neigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations enable us to upper bound the\nFrobenius distance between the best rank-$k$ approximation of $M$ and that of a\nGaussian perturbation of $M$ as an integral that involves inverse eigenvalue\ngaps of the stochastically evolving matrix, as opposed to a sum of perturbation\nbounds obtained via Davis-Kahan-type theorems. Subsequently, again using the\nDyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$\nperturbed by Gaussian noise have large gaps with high probability. These\nresults also contribute to the analysis of low-rank approximations under\naverage-case perturbations, and to an understanding of eigenvalue gaps for\nrandom matrices, both of which may be of independent interest.\n","authors":["Oren Mangoubi","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2502.07657v1.pdf","comment":"Published in Journal of the ACM. arXiv admin note: substantial text\n  overlap with arXiv:2306.16648"},{"id":"http://arxiv.org/abs/2307.14409v2","updated":"2025-02-11T15:18:42Z","published":"2023-07-13T14:54:19Z","title":"Exploring the Bitcoin Mesoscale","summary":"  The open availability of the entire history of the Bitcoin transactions opens\nup the possibility to study this system at an unprecedented level of detail.\nThis contribution is devoted to the analysis of the mesoscale structural\nproperties of the Bitcoin User Network (BUN), across its entire history (i.e.\nfrom 2009 to 2017). What emerges from our analysis is that the BUN is\ncharacterized by a core-periphery structure a deeper analysis of which reveals\na certain degree of bow-tieness (i.e. the presence of a Strongly-Connected\nComponent, an IN- and an OUT-component together with some tendrils attached to\nthe IN-component). Interestingly, the evolution of the BUN structural\norganization experiences fluctuations that seem to be correlated with the\npresence of bubbles, i.e. periods of price surge and decline observed\nthroughout the entire Bitcoin history: our results, thus, further confirm the\ninterplay between structural quantities and price movements observed in\nprevious analyses.\n","authors":["Nicol Vallarano","Tiziano Squartini","Claudio J. Tessone"],"pdf_url":"https://arxiv.org/pdf/2307.14409v2.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.07594v1","updated":"2025-02-11T14:44:51Z","published":"2025-02-11T14:44:51Z","title":"Distributed Non-Interactive Zero-Knowledge Proofs","summary":"  Distributed certification is a set of mechanisms that allows an all-knowing\nprover to convince the units of a communication network that the network's\nstate has some desired property, such as being 3-colorable or triangle-free.\nClassical mechanisms, such as proof labeling schemes (PLS), consist of a\nmessage from the prover to each unit, followed by one round of communication\nbetween each unit and its neighbors. Later works consider extensions, called\ndistributed interactive proofs, where the prover and the units can have\nmultiple rounds of communication before the communication among the units.\n  Recently, Bick, Kol, and Oshman (SODA '22) defined a zero-knowledge version\nof distributed interactive proofs, where the prover convinces the units of the\nnetwork's state without revealing any other information about the network's\nstate or structure. In their work, they propose different variants of this\nmodel and show that many graph properties of interest can be certified with\nthem.\n  In this work, we define and study distributed non-interactive zero-knowledge\nproofs (dNIZK); these can be seen as a non-interactive version of the\naforementioned model, and also as a zero-knowledge version of PLS. We prove the\nfollowing:\n  - There exists a dNIZK protocol for 3-coloring with O(log n)-bit messages\nfrom the prover and O(log n)-size messages among neighbors.\n  - There exists a family of dNIZK protocols for triangle-freeness, that\npresents a trade-off between the size of the messages from the prover and the\nsize of the messages among neighbors.\n  - There exists a dNIZK protocol for any graph property in NP in the random\noracle models, which is secure against an arbitrary number of malicious\nparties.\n","authors":["Alex B. Grilo","Ami Paz","Mor Perry"],"pdf_url":"https://arxiv.org/pdf/2502.07594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07557v1","updated":"2025-02-11T13:50:50Z","published":"2025-02-11T13:50:50Z","title":"JBShield: Defending Large Language Models from Jailbreak Attacks through\n  Activated Concept Analysis and Manipulation","summary":"  Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs.\n","authors":["Shenyi Zhang","Yuchen Zhai","Keyan Guo","Hongxin Hu","Shengnan Guo","Zheng Fang","Lingchen Zhao","Chao Shen","Cong Wang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07557v1.pdf","comment":"To Appear in the 34rd USENIX Security Symposium, August 13-15, 2025"},{"id":"http://arxiv.org/abs/2412.04078v2","updated":"2025-02-11T12:16:21Z","published":"2024-12-05T11:24:27Z","title":"Mind the Gap: Towards Generalizable Autonomous Penetration Testing via\n  Domain Randomization and Meta-Reinforcement Learning","summary":"  With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as a promising research\narea. Reinforcement learning (RL) is a natural fit for studying this topic.\nHowever, two key challenges limit the applicability of RL-based autonomous\npentesting in real-world scenarios: (a) training environment dilemma --\ntraining agents in simulated environments is sample-efficient while ensuring\ntheir realism remains challenging; (b) poor generalization ability -- agents'\npolicies often perform poorly when transferred to unseen scenarios, with even\nslight changes potentially causing significant generalization gap. To this end,\nwe propose GAP, a generalizable autonomous pentesting framework that aims to\nrealizes efficient policy training in realistic environments and train\ngeneralizable agents capable of drawing inferences about other cases from one\ninstance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables\nend-to-end policy learning in unknown real environments while constructing\nrealistic simulations; (b) improves agents' generalization ability by\nleveraging domain randomization and meta-RL learning.Specially, we are among\nthe first to apply domain randomization in autonomous pentesting and propose a\nlarge language model-powered domain randomization method for synthetic\nenvironment generation. We further apply meta-RL to improve agents'\ngeneralization ability in unseen environments by leveraging synthetic\nenvironments. The combination of two methods effectively bridges the\ngeneralization gap and improves agents' policy adaptation\nperformance.Experiments are conducted on various vulnerable virtual machines,\nwith results showing that GAP can enable policy learning in various realistic\nenvironments, achieve zero-shot policy transfer in similar environments, and\nrealize rapid policy adaptation in dissimilar environments.\n","authors":["Shicheng Zhou","Jingju Liu","Yuliang Lu","Jiahai Yang","Yue Zhang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2412.04078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07498v1","updated":"2025-02-11T11:59:10Z","published":"2025-02-11T11:59:10Z","title":"Decentralized Entropy-Driven Ransomware Detection Using Autonomous\n  Neural Graph Embeddings","summary":"  The increasing sophistication of cyber threats has necessitated the\ndevelopment of advanced detection mechanisms capable of identifying and\nmitigating ransomware attacks with high precision and efficiency. A novel\nframework, termed Decentralized Entropy-Driven Detection (DED), is introduced,\nleveraging autonomous neural graph embeddings and entropy-based anomaly scoring\nto address the limitations of traditional methods. The framework operates on a\ndistributed network of nodes, eliminating single points of failure and\nenhancing resilience against targeted attacks. Experimental results demonstrate\nits ability to achieve detection accuracy exceeding 95\\%, with false positive\nrates maintained below 2\\% across diverse ransomware variants. The integration\nof graph-based modeling and machine learning techniques enables the framework\nto capture complex system interactions, facilitating the identification of\nsubtle anomalies indicative of ransomware activity. Comparative analysis\nagainst existing methods highlights its superior performance in terms of\ndetection rates and computational efficiency. Case studies further validate its\neffectiveness in real-world scenarios, showcasing its ability to detect and\nmitigate ransomware attacks within minutes of their initiation. The proposed\nframework represents a significant step forward in cybersecurity, offering a\nscalable and adaptive solution to the growing challenge of ransomware\ndetection.\n","authors":["Ekaterina Starchenko","Hugo Bellinghamshire","David Pickering","Tristan Weatherspoon","Nathaniel Berkhamstead","Elizabeth Green","Magnus Rothschild"],"pdf_url":"https://arxiv.org/pdf/2502.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07492v1","updated":"2025-02-11T11:51:12Z","published":"2025-02-11T11:51:12Z","title":"RoMA: Robust Malware Attribution via Byte-level Adversarial Training\n  with Global Perturbations and Adversarial Consistency Regularization","summary":"  Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios.\n","authors":["Yuxia Sun","Huihong Chen","Jingcai Guo","Aoxiang Sun","Zhetao Li","Haolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07492v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.12027v2","updated":"2025-02-11T11:11:29Z","published":"2024-06-17T18:51:45Z","title":"Adversarial Perturbations Cannot Reliably Protect Artists From\n  Generative AI","summary":"  Artists are increasingly concerned about advancements in image generation\nmodels that can closely replicate their unique artistic styles. In response,\nseveral protection tools against style mimicry have been developed that\nincorporate small adversarial perturbations into artworks published online. In\nthis work, we evaluate the effectiveness of popular protections -- with\nmillions of downloads -- and show they only provide a false sense of security.\nWe find that low-effort and \"off-the-shelf\" techniques, such as image\nupscaling, are sufficient to create robust mimicry methods that significantly\ndegrade existing protections. Through a user study, we demonstrate that all\nexisting protections can be easily bypassed, leaving artists vulnerable to\nstyle mimicry. We caution that tools based on adversarial perturbations cannot\nreliably protect artists from the misuse of generative AI, and urge the\ndevelopment of alternative non-technological solutions.\n","authors":["Robert Hnig","Javier Rando","Nicholas Carlini","Florian Tramr"],"pdf_url":"https://arxiv.org/pdf/2406.12027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13481v2","updated":"2025-02-11T10:56:50Z","published":"2024-05-22T09:47:54Z","title":"Locally Private Estimation with Public Features","summary":"  We initiate the study of locally differentially private (LDP) learning with\npublic features. We define semi-feature LDP, where some features are publicly\navailable while the remaining ones, along with the label, require protection\nunder local differential privacy. Under semi-feature LDP, we demonstrate that\nthe mini-max convergence rate for non-parametric regression is significantly\nreduced compared to that of classical LDP. Then we propose HistOfTree, an\nestimator that fully leverages the information contained in both public and\nprivate features. Theoretically, HistOfTree reaches the mini-max optimal\nconvergence rate. Empirically, HistOfTree achieves superior performance on both\nsynthetic and real data. We also explore scenarios where users have the\nflexibility to select features for protection manually. In such cases, we\npropose an estimator and a data-driven parameter tuning strategy, leading to\nanalogous theoretical and empirical results.\n","authors":["Yuheng Ma","Ke Jia","Hanfang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.13481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07410v1","updated":"2025-02-11T09:44:41Z","published":"2025-02-11T09:44:41Z","title":"Mining Power Destruction Attacks in the Presence of Petty-Compliant\n  Mining Pools","summary":"  Bitcoin's security relies on its Proof-of-Work consensus, where miners solve\npuzzles to propose blocks. The puzzle's difficulty is set by the difficulty\nadjustment mechanism (DAM), based on the network's available mining power.\nAttacks that destroy some portion of mining power can exploit the DAM to lower\ndifficulty, making such attacks profitable. In this paper, we analyze three\ntypes of mining power destruction attacks in the presence of petty-compliant\nmining pools: selfish mining, bribery, and mining power distraction attacks. We\nanalyze selfish mining while accounting for the distribution of mining power\namong pools, a factor often overlooked in the literature. Our findings indicate\nthat selfish mining can be more destructive when the non-adversarial mining\nshare is well distributed among pools. We also introduce a novel bribery\nattack, where the adversarial pool bribes petty-compliant pools to orphan\nothers' blocks. For small pools, we demonstrate that the bribery attack can\ndominate strategies like selfish mining or undercutting. Lastly, we present the\nmining distraction attack, where the adversarial pool incentivizes\npetty-compliant pools to abandon Bitcoin's puzzle and mine for a simpler\npuzzle, thus wasting some part of their mining power. Similar to the previous\nattacks, this attack can lower the mining difficulty, but with the difference\nthat it does not generate any evidence of mining power destruction, such as\norphan blocks.\n","authors":["Roozbeh Sarenche","Svetla Nikova","Bart Preneel"],"pdf_url":"https://arxiv.org/pdf/2502.07410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07330v1","updated":"2025-02-11T07:49:10Z","published":"2025-02-11T07:49:10Z","title":"EMERALD: Evidence Management for Continuous Certification as a Service\n  in the Cloud","summary":"  The conspicuous lack of cloud-specific security certifications, in addition\nto the existing market fragmentation, hinder transparency and accountability in\nthe provision and usage of European cloud services. Both issues ultimately\nreflect on the level of customers' trustworthiness and adoption of cloud\nservices. The upcoming demand for continuous certification has not yet been\ndefinitively addressed and it remains unclear how the level 'high' of the\nEuropean Cybersecurity Certification Scheme for Cloud Services (EUCS) shall be\ntechnologically achieved. The introduction of AI in cloud services is raising\nthe complexity of certification even further. This paper presents the EMERALD\nCertification-as-a-Service (CaaS) concept for continuous certification of\nharmonized cybersecurity schemes, like the EUCS. EMERALD CaaS aims to provide\nagile and lean re-certification to consumers that adhere to a defined level of\nsecurity and trust in a uniform way across heterogeneous environments\nconsisting of combinations of different resources (Cloud, Edge, IoT). Initial\nfindings suggest that EMERALD will significantly contribute to continuous\ncertification, boosting providers and users of cloud services to maintain\nregulatory compliance towards the latest and upcoming security schemes.\n","authors":["Christian Banse","Bjrn Fanta","Juncal Alonso","Cristina Martinez"],"pdf_url":"https://arxiv.org/pdf/2502.07330v1.pdf","comment":"Accepted for publication at CLOSER 2025"},{"id":"http://arxiv.org/abs/2502.06385v2","updated":"2025-02-11T07:04:43Z","published":"2025-02-10T12:14:33Z","title":"Recommendations to OSCE/ODIHR (on how to give better recommendations for\n  Internet voting)","summary":"  This paper takes a critical look at the recommendations OSCE/ODIHR has given\nfor the Estonian Internet voting over the 20 years it has been running. We\npresent examples of recommendations that can not be fulfilled at all, but also\nexamples where fulfilling a recommendation requires a non-trivial trade-off,\npotentially weakening the system in some other respect. In such cases\nOSCE/ODIHR should take an explicit position which trade-off it recommends. We\nalso look at the development of the recommendation to introduce end-to-end\nverifiability. In this case we expect OSCE/ODIHR to define what it exactly\nmeans by this property, as well as to give explicit criteria to determine\nwhether and to which extent end-to-end verifiability has been achieved.\n","authors":["Jan Willemson"],"pdf_url":"https://arxiv.org/pdf/2502.06385v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.07284v1","updated":"2025-02-11T06:04:24Z","published":"2025-02-11T06:04:24Z","title":"VLWE: Variety-based Learning with Errors for Vector Encryption through\n  Algebraic Geometry","summary":"  Lattice-based cryptography is a foundation for post-quantum security, with\nthe Learning with Errors (LWE) problem as a core component in key exchange,\nencryption, and homomorphic computation. Structured variants like Ring-LWE\n(RLWE) and Module-LWE (MLWE) improve efficiency using polynomial rings but\nremain constrained by traditional polynomial multiplication rules, limiting\ntheir ability to handle structured vectorized data. This work introduces\nVariety-LWE (VLWE), a new structured lattice problem based on algebraic\ngeometry. Unlike RLWE and MLWE, which use polynomial quotient rings with\nstandard multiplication, VLWE operates over multivariate polynomial rings\ndefined by algebraic varieties. A key difference is that these polynomials lack\nmixed variables, and multiplication is coordinate-wise rather than following\nstandard polynomial multiplication. This enables direct encoding and\nhomomorphic processing of high-dimensional data while preserving worst-case to\naverage-case hardness reductions. We prove VLWE's security by reducing it to\nmultiple independent Ideal-SVP instances, demonstrating resilience against\nclassical and quantum attacks. Additionally, we analyze hybrid\nalgebraic-lattice attacks, showing that existing Grobner basis and lattice\nreduction methods do not directly threaten VLWE. We further construct a vector\nhomomorphic encryption scheme based on VLWE, supporting structured computations\nwhile controlling noise growth. This scheme offers advantages in\nprivacy-preserving machine learning, encrypted search, and secure computations\nover structured data. VLWE emerges as a novel and independent paradigm in\nlattice-based cryptography, leveraging algebraic geometry to enable new\ncryptographic capabilities beyond traditional polynomial quotient rings.\n","authors":["Dongfang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.07284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18802v2","updated":"2025-02-11T06:00:39Z","published":"2024-05-29T06:46:10Z","title":"Enhancing Security and Privacy in Federated Learning using\n  Low-Dimensional Update Representation and Proximity-Based Defense","summary":"  Federated Learning (FL) is a promising privacy-preserving machine learning\nparadigm that allows data owners to collaboratively train models while keeping\ntheir data localized. Despite its potential, FL faces challenges related to the\ntrustworthiness of both clients and servers, particularly against curious or\nmalicious adversaries. In this paper, we introduce a novel framework named\n\\underline{F}ederated \\underline{L}earning with Low-Dimensional\n\\underline{U}pdate \\underline{R}epresentation and \\underline{P}roximity-Based\ndefense (FLURP), designed to address privacy preservation and resistance to\nByzantine attacks in distributed learning environments. FLURP employs\n$\\mathsf{LinfSample}$ method, enabling clients to compute the $l_{\\infty}$ norm\nacross sliding windows of updates, resulting in a Low-Dimensional Update\nRepresentation (LUR). Calculating the shared distance matrix among LURs, rather\nthan updates, significantly reduces the overhead of Secure Multi-Party\nComputation (SMPC) by three orders of magnitude while effectively\ndistinguishing between benign and poisoned updates. Additionally, FLURP\nintegrates a privacy-preserving proximity-based defense mechanism utilizing\noptimized SMPC protocols to minimize communication rounds. Our experiments\ndemonstrate FLURP's effectiveness in countering Byzantine adversaries with low\ncommunication and runtime overhead. FLURP offers a scalable framework for\nsecure and reliable FL in distributed environments, facilitating its\napplication in scenarios requiring robust data management and security.\n","authors":["Wenjie Li","Kai Fan","Jingyuan Zhang","Hui Li","Wei Yang Bryan Lim","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.18802v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2405.19272v3","updated":"2025-02-11T04:20:07Z","published":"2024-05-29T17:03:31Z","title":"Differentially Private Clustered Federated Learning","summary":"  Federated learning (FL), which is a decentralized machine learning (ML)\napproach, often incorporates differential privacy (DP) to provide rigorous data\nprivacy guarantees. Previous works attempted to address high structured data\nheterogeneity in vanilla FL settings through clustering clients (a.k.a\nclustered FL), but these methods remain sensitive and prone to errors, further\nexacerbated by the DP noise. This vulnerability makes the previous methods\ninappropriate for differentially private FL (DPFL) settings with structured\ndata heterogeneity. To address this gap, we propose an algorithm for\ndifferentially private clustered FL, which is robust to the DP noise in the\nsystem and identifies the underlying clients' clusters correctly. To this end,\nwe propose to cluster clients based on both their model updates and training\nloss values. Furthermore, for clustering clients' model updates at the end of\nthe first round, our proposed approach addresses the server's uncertainties by\nemploying large batch sizes as well as Gaussian Mixture Models (GMM) to reduce\nthe impact of DP and stochastic noise and avoid potential clustering errors.\nThis idea is efficient especially in privacy-sensitive scenarios with more DP\nnoise. We provide theoretical analysis to justify our approach and evaluate it\nacross diverse data distributions and privacy budgets. Our experimental results\nshow its effectiveness in addressing large structured data heterogeneity in\nDPFL.\n","authors":["Saber Malekmohammadi","Afaf Taik","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2405.19272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07231v1","updated":"2025-02-11T03:46:35Z","published":"2025-02-11T03:46:35Z","title":"Revisiting the Auxiliary Data in Backdoor Purification","summary":"  Backdoor attacks occur when an attacker subtly manipulates machine learning\nmodels during the training phase, leading to unintended behaviors when specific\ntriggers are present. To mitigate such emerging threats, a prevalent strategy\nis to cleanse the victim models by various backdoor purification techniques.\nDespite notable achievements, current state-of-the-art (SOTA) backdoor\npurification techniques usually rely on the availability of a small clean\ndataset, often referred to as auxiliary dataset. However, acquiring an ideal\nauxiliary dataset poses significant challenges in real-world applications. This\nstudy begins by assessing the SOTA backdoor purification techniques across\ndifferent types of real-world auxiliary datasets. Our findings indicate that\nthe purification effectiveness fluctuates significantly depending on the type\nof auxiliary dataset used. Specifically, a high-quality in-distribution\nauxiliary dataset is essential for effective purification, whereas datasets\nfrom varied or out-of-distribution sources significantly degrade the defensive\nperformance. Based on this, we propose Guided Input Calibration (GIC), which\naims to improve purification efficacy by employing a learnable transformation.\nGuided by the victim model itself, GIC aligns the characteristics of the\nauxiliary dataset with those of the original training set. Comprehensive\nexperiments demonstrate that GIC can substantially enhance purification\nperformance across diverse types of auxiliary datasets. The code and data will\nbe available via https://github.com/shawkui/BackdoorBenchER.\n","authors":["Shaokui Wei","Shanchao Yang","Jiayin Liu","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2502.07231v1.pdf","comment":"Preprint. Code and data are being finalized and will be released\n  incrementally"},{"id":"http://arxiv.org/abs/2502.06348v2","updated":"2025-02-11T03:40:13Z","published":"2025-02-10T10:58:09Z","title":"AiRacleX: Automated Detection of Price Oracle Manipulations via\n  LLM-Driven Knowledge Mining and Prompt Generation","summary":"  Decentralized finance (DeFi) applications depend on accurate price oracles to\nensure secure transactions, yet these oracles are highly vulnerable to\nmanipulation, enabling attackers to exploit smart contract vulnerabilities for\nunfair asset valuation and financial gain. Detecting such manipulations\ntraditionally relies on the manual effort of experienced experts, presenting\nsignificant challenges. In this paper, we propose a novel LLM-driven framework\nthat automates the detection of price oracle manipulations by leveraging the\ncomplementary strengths of different LLM models (LLMs). Our approach begins\nwith domain-specific knowledge extraction, where an LLM model synthesizes\nprecise insights about price oracle vulnerabilities from top-tier academic\npapers, eliminating the need for profound expertise from developers or\nauditors. This knowledge forms the foundation for a second LLM model to\ngenerate structured, context-aware chain of thought prompts, which guide a\nthird LLM model in accurately identifying manipulation patterns in smart\ncontracts. We validate the effectiveness of framework through experiments on 60\nknown vulnerabilities from 46 real-world DeFi attacks or projects spanning 2021\nto 2023. The best performing combination of LLMs (Haiku-Haiku-4o-mini)\nidentified by AiRacleX demonstrate a 2.58-times improvement in recall (0.667 vs\n0.259) compared to the state-of-the-art tool GPTScan, while maintaining\ncomparable precision. Furthermore, our framework demonstrates the feasibility\nof replacing commercial models with open-source alternatives, enhancing privacy\nand security for developers.\n","authors":["Bo Gao","Yuan Wang","Qingsong Wei","Yong Liu","Rick Siow Mong Goh","David Lo"],"pdf_url":"https://arxiv.org/pdf/2502.06348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09585v2","updated":"2025-02-11T03:32:37Z","published":"2024-11-14T16:54:06Z","title":"Backdoor Mitigation by Distance-Driven Detoxification","summary":"  Backdoor attacks undermine the integrity of machine learning models by\nallowing attackers to manipulate predictions using poisoned training data. Such\nattacks lead to targeted misclassification when specific triggers are present,\nwhile the model behaves normally under other conditions. This paper considers a\npost-training backdoor defense task, aiming to detoxify the backdoors in\npre-trained models. We begin by analyzing the underlying issues of vanilla\nfine-tuning and observe that it is often trapped in regions with low loss for\nboth clean and poisoned samples. Motivated by such observations, we propose\nDistance-Driven Detoxification (D3), an innovative approach that reformulates\nbackdoor defense as a constrained optimization problem. Specifically, D3\npromotes the model's departure from the vicinity of its initial weights,\neffectively reducing the influence of backdoors. Extensive experiments on\nstate-of-the-art (SOTA) backdoor attacks across various model architectures and\ndatasets demonstrate that D3 not only matches but often surpasses the\nperformance of existing SOTA post-training defense techniques.\n","authors":["Shaokui Wei","Jiayin Liu","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2411.09585v2.pdf","comment":"Preprint version. Fix issues in Figure 1. Thanks for the readers'\n  comments"},{"id":"http://arxiv.org/abs/2502.07207v1","updated":"2025-02-11T03:06:03Z","published":"2025-02-11T03:06:03Z","title":"A Study on the Importance of Features in Detecting Advanced Persistent\n  Threats Using Machine Learning","summary":"  Advanced Persistent Threats (APTs) pose a significant security risk to\norganizations and industries. These attacks often lead to severe data breaches\nand compromise the system for a long time. Mitigating these sophisticated\nattacks is highly challenging due to the stealthy and persistent nature of\nAPTs. Machine learning models are often employed to tackle this challenge by\nbringing automation and scalability to APT detection. Nevertheless, these\nintelligent methods are data-driven, and thus, highly affected by the quality\nand relevance of input data. This paper aims to analyze measurements considered\nwhen recording network traffic and conclude which features contribute more to\ndetecting APT samples. To do this, we study the features associated with\nvarious APT cases and determine their importance using a machine learning\nframework. To ensure the generalization of our findings, several feature\nselection techniques are employed and paired with different classifiers to\nevaluate their effectiveness. Our findings provide insights into how APT\ndetection can be enhanced in real-world scenarios.\n","authors":["Ehsan Hallaji","Roozbeh Razavi-Far","Mehrdad Saif"],"pdf_url":"https://arxiv.org/pdf/2502.07207v1.pdf","comment":"Accepted for publication in the 2024 International Conference on\n  Computational Science and Computational Intelligence (CSCI'24)"},{"id":"http://arxiv.org/abs/2502.07159v1","updated":"2025-02-11T00:54:24Z","published":"2025-02-11T00:54:24Z","title":"Pseudorandomness Properties of Random Reversible Circuits","summary":"  Motivated by practical concerns in cryptography, we study pseudorandomness\nproperties of permutations on $\\{0,1\\}^n$ computed by random circuits made from\nreversible $3$-bit gates (permutations on $\\{0,1\\}^3$). Our main result is that\na random circuit of depth $\\sqrt{n} \\cdot \\tilde{O}(k^3)$, with each layer\nconsisting of $\\Theta(n)$ random gates in a fixed two-dimensional\nnearest-neighbor architecture, yields approximate $k$-wise independent\npermutations.\n  Our result can be seen as a particularly simple/practical block cipher\nconstruction that gives provable statistical security against attackers with\naccess to $k$~input-output pairs within few rounds.\n  The main technical component of our proof consists of two parts:\n  1. We show that the Markov chain on $k$-tuples of $n$-bit strings induced by\na single random $3$-bit one-dimensional nearest-neighbor gate has spectral gap\nat least $1/n \\cdot \\tilde{O}(k)$. Then we infer that a random circuit with\nlayers of random gates in a fixed one-dimensional gate architecture yields\napproximate $k$-wise independent permutations of $\\{0,1\\}^n$ in depth $n\\cdot\n\\tilde{O}(k^2)$\n  2. We show that if the $n$ wires are layed out on a two-dimensional lattice\nof bits, then repeatedly alternating applications of approximate $k$-wise\nindependent permutations of $\\{0,1\\}^{\\sqrt n}$ to the rows and columns of the\nlattice yields an approximate $k$-wise independent permutation of $\\{0,1\\}^n$\nin small depth.\n  Our work improves on the original work of Gowers, who showed a gap of\n$1/\\mathrm{poly}(n,k)$ for one random gate (with non-neighboring inputs); and,\non subsequent work improving the gap to $\\Omega(1/n^2k)$ in the same setting.\n","authors":["William Gay","William He","Nicholas Kocurek","Ryan O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2502.07159v1.pdf","comment":"Merge of arXiv:2404.1464(8) and arXiv:2409.1461(4). Results in\n  arXiv:2404.1464(8) on candidate constructions of computationally pseudorandom\n  permutations from one-way functions have been withdrawn due to an error"},{"id":"http://arxiv.org/abs/2502.08013v1","updated":"2025-02-11T23:20:58Z","published":"2025-02-11T23:20:58Z","title":"Hierarchical Manifold Projection for Ransomware Detection: A Novel\n  Geometric Approach to Identifying Malicious Encryption Patterns","summary":"  Encryption-based cyber threats continue to evolve, employing increasingly\nsophisticated techniques to bypass traditional detection mechanisms. Many\nexisting classification strategies depend on static rule sets, signature-based\nmatching, or machine learning models that require extensive labeled datasets,\nmaking them ineffective against emerging ransomware families that exhibit\npolymorphic and adversarial behaviors. A novel classification framework\nstructured through hierarchical manifold projection introduces a mathematical\napproach to detecting malicious encryption workflows, preserving geometric\nconsistencies that differentiate ransomware-induced modifications from benign\ncryptographic operations. The proposed methodology transforms encryption\nsequences into structured manifold embeddings, ensuring classification\nrobustness through non-Euclidean feature separability rather than reliance on\nstatic indicators. Generalization capabilities remain stable across diverse\nransomware variants, as hierarchical decomposition techniques capture\nmulti-scale encryption characteristics while maintaining resilience against\ncode obfuscation and execution flow modifications. Empirical analysis\ndemonstrates that detection accuracy remains high even when encryption key\nvariability, delayed execution tactics, or API call obfuscation strategies are\nintroduced, reinforcing the reliability of manifold-based classification.\nReal-time scalability assessments confirm that the proposed approach maintains\ncomputational efficiency across increasing dataset volumes, validating its\napplicability to large-scale threat detection scenarios.\n","authors":["Frederick Pembroke","Eleanor Featherstonehaugh","Sebastian Wetherington","Harriet Fitzgerald","Maximilian Featherington","Peter Idliman"],"pdf_url":"https://arxiv.org/pdf/2502.08013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08008v1","updated":"2025-02-11T23:07:14Z","published":"2025-02-11T23:07:14Z","title":"An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models","summary":"  Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.\n","authors":["Kasra Ahmadi","Rouzbeh Behnia","Reza Ebrahimi","Mehran Mozaffari Kermani","Jeremiah Birrell","Jason Pacheco","Attila A Yavuz"],"pdf_url":"https://arxiv.org/pdf/2502.08008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08001v1","updated":"2025-02-11T22:48:49Z","published":"2025-02-11T22:48:49Z","title":"Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation","summary":"  Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems.\n","authors":["Haonan Shi","Tu Ouyang","An Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08001v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07925v1","updated":"2025-02-11T19:58:17Z","published":"2025-02-11T19:58:17Z","title":"PIXHELL: When Pixels Learn to Scream","summary":"  This paper presents a technique for generating sound by leveraging the\nelectrical properties of liquid crystal displays (LCDs). The phenomenon occurs\ndue to vibrational noise produced by capacitors within the LCD panel during\nrapid pixel state transitions. By modulating these transitions through\nspecially crafted bitmap patterns projected onto the screen, we demonstrate how\nweak yet audible acoustic signals can be generated directly from the display.\nWe designed, implemented, evaluated, and tested a system that repurposes the\nLCD as a sound-emitting device. Potential applications for this technique\ninclude low-power auditory feedback systems, short-range device communication,\nair-gap covert channels, secure auditory signaling, and innovative approaches\nto human-computer interaction.\n","authors":["Mordechai Guri"],"pdf_url":"https://arxiv.org/pdf/2502.07925v1.pdf","comment":"Version of this paper accepted to 2024 IEEE 48th Annual Computers,\n  Software, and Applications Conference (COMPSAC)"},{"id":"http://arxiv.org/abs/2410.11876v3","updated":"2025-02-11T19:56:20Z","published":"2024-10-10T01:23:16Z","title":"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based\n  Chatbots","summary":"  The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4o.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI.\n","authors":["Jijie Zhou","Eryue Xu","Yaoyao Wu","Tianshi Li"],"pdf_url":"https://arxiv.org/pdf/2410.11876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11295v4","updated":"2025-02-11T19:42:26Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2407.00075v3","updated":"2025-02-11T19:08:08Z","published":"2024-06-21T19:18:16Z","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","summary":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","authors":["Anton Xue","Avishree Khare","Rajeev Alur","Surbhi Goel","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2407.00075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07159v1","updated":"2025-02-11T00:54:24Z","published":"2025-02-11T00:54:24Z","title":"Pseudorandomness Properties of Random Reversible Circuits","summary":"  Motivated by practical concerns in cryptography, we study pseudorandomness\nproperties of permutations on $\\{0,1\\}^n$ computed by random circuits made from\nreversible $3$-bit gates (permutations on $\\{0,1\\}^3$). Our main result is that\na random circuit of depth $\\sqrt{n} \\cdot \\tilde{O}(k^3)$, with each layer\nconsisting of $\\Theta(n)$ random gates in a fixed two-dimensional\nnearest-neighbor architecture, yields approximate $k$-wise independent\npermutations.\n  Our result can be seen as a particularly simple/practical block cipher\nconstruction that gives provable statistical security against attackers with\naccess to $k$~input-output pairs within few rounds.\n  The main technical component of our proof consists of two parts:\n  1. We show that the Markov chain on $k$-tuples of $n$-bit strings induced by\na single random $3$-bit one-dimensional nearest-neighbor gate has spectral gap\nat least $1/n \\cdot \\tilde{O}(k)$. Then we infer that a random circuit with\nlayers of random gates in a fixed one-dimensional gate architecture yields\napproximate $k$-wise independent permutations of $\\{0,1\\}^n$ in depth $n\\cdot\n\\tilde{O}(k^2)$\n  2. We show that if the $n$ wires are layed out on a two-dimensional lattice\nof bits, then repeatedly alternating applications of approximate $k$-wise\nindependent permutations of $\\{0,1\\}^{\\sqrt n}$ to the rows and columns of the\nlattice yields an approximate $k$-wise independent permutation of $\\{0,1\\}^n$\nin small depth.\n  Our work improves on the original work of Gowers, who showed a gap of\n$1/\\mathrm{poly}(n,k)$ for one random gate (with non-neighboring inputs); and,\non subsequent work improving the gap to $\\Omega(1/n^2k)$ in the same setting.\n","authors":["William Gay","William He","Nicholas Kocurek","Ryan O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2502.07159v1.pdf","comment":"Merge of arXiv:2404.14648 and arXiv:2409.14614. Results in\n  arXiv:2404.14648 on candidate constructions of computationally pseudorandom\n  permutations from one-way functions have been withdrawn due to an error"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.07783v1","updated":"2025-02-11T18:59:57Z","published":"2025-02-11T18:59:57Z","title":"Curvature Tuning: Provable Training-free Model Steering From a Single\n  Parameter","summary":"  The scaling of model size and data size has reshaped the paradigm of AI. As a\nresult, the common protocol to leverage the latest models is to steer them\ntowards a specific downstream task of interest through {\\em fine-tuning}.\nDespite its importance, the main methods for fine-tuning remain limited to full\nor low-rank adapters--containing countless hyper-parameters and lacking\ninterpretability. In this paper, we take a step back and demonstrate how novel\nand explainable post-training steering solutions can be derived theoretically\nfrom {\\em spline operators}, a rich mathematical framing of Deep Networks that\nwas recently developed. Our method--coined \\textbf{Curvature Tuning (CT)}--has\na single parameter that provably modulates the curvature of the model's\ndecision boundary henceforth allowing training-free steering. This makes CT\nboth more efficient and interpretable than conventional fine-tuning methods. We\nempirically validate its effectiveness in improving generalization and\nrobustness of pretrained models. For example, CT improves out-of-distribution\ntransfer performances of ResNet-18/50 by 2.57\\%/1.74\\% across seventeen\ndownstream datasets, and improves RobustBench robust accuracy by\n11.76\\%/348.44\\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving\ntheir generalization on nine downstream datasets by 2.43\\%/3.33\\%. Our code is\navailable at\n\\href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.\n","authors":["Leyang Hu","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.07783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v3","updated":"2025-02-11T18:59:47Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07780v1","updated":"2025-02-11T18:59:35Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07774v1","updated":"2025-02-11T18:57:18Z","published":"2025-02-11T18:57:18Z","title":"Optimistic Interior Point Methods for Sequential Hypothesis Testing by\n  Betting","summary":"  The technique of \"testing by betting\" frames nonparametric sequential\nhypothesis testing as a multiple-round game, where a player bets on future\nobservations that arrive in a streaming fashion, accumulates wealth that\nquantifies evidence against the null hypothesis, and rejects the null once the\nwealth exceeds a specified threshold while controlling the false positive\nerror. Designing an online learning algorithm that achieves a small regret in\nthe game can help rapidly accumulate the bettor's wealth, which in turn can\nshorten the time to reject the null hypothesis under the alternative $H_1$.\nHowever, many of the existing works employ the Online Newton Step (ONS) to\nupdate within a halved decision space to avoid a gradient explosion issue,\nwhich is potentially conservative for rapid wealth accumulation. In this paper,\nwe introduce a novel strategy utilizing interior-point methods in optimization\nthat allows updates across the entire interior of the decision space without\nthe risk of gradient explosion. Our approach not only maintains strong\nstatistical guarantees but also facilitates faster null hypothesis rejection in\ncritical scenarios, overcoming the limitations of existing approaches.\n","authors":["Can Chen","Jun-Kun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.06774v2","updated":"2025-02-11T18:54:30Z","published":"2025-02-10T18:52:22Z","title":"ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection","summary":"  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n","authors":["Giacomo Lastrucci","Artur M. Schweidtmann"],"pdf_url":"https://arxiv.org/pdf/2502.06774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07764v1","updated":"2025-02-11T18:47:53Z","published":"2025-02-11T18:47:53Z","title":"Polynomial-Time Approximability of Constrained Reinforcement Learning","summary":"  We study the computational complexity of approximating general constrained\nMarkov decision processes. Our primary contribution is the design of a\npolynomial time $(0,\\epsilon)$-additive bicriteria approximation algorithm for\nfinding optimal constrained policies across a broad class of recursively\ncomputable constraints, including almost-sure, chance, expectation, and their\nanytime variants. Matching lower bounds imply our approximation guarantees are\noptimal so long as $P \\neq NP$. The generality of our approach results in\nanswers to several long-standing open complexity questions in the constrained\nreinforcement learning literature. Specifically, we are the first to prove\npolynomial-time approximability for the following settings: policies under\nchance constraints, deterministic policies under multiple expectation\nconstraints, policies under non-homogeneous constraints (i.e., constraints of\ndifferent types), and policies under constraints for continuous-state\nprocesses.\n","authors":["Jeremy McMahan"],"pdf_url":"https://arxiv.org/pdf/2502.07764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19392v2","updated":"2025-02-11T18:45:12Z","published":"2025-01-31T18:47:42Z","title":"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models","summary":"  Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.\n","authors":["Alina Shutova","Vladimir Malinovskii","Vage Egiazarian","Denis Kuznedelev","Denis Mazur","Nikita Surkov","Ivan Ermakov","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2501.19392v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2407.10366v2","updated":"2025-02-11T18:44:46Z","published":"2024-07-15T00:13:53Z","title":"Accessing Vision Foundation Models via ImageNet-1K","summary":"  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n","authors":["Yitian Zhang","Xu Ma","Yue Bai","Huan Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2407.10366v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07760v1","updated":"2025-02-11T18:43:07Z","published":"2025-02-11T18:43:07Z","title":"Scalable Fingerprinting of Large Language Models","summary":"  Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks.\n","authors":["Anshul Nasery","Jonathan Hayase","Creston Brooks","Peiyao Sheng","Himanshu Tyagi","Pramod Viswanath","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07760v1.pdf","comment":"23 pages 15 figures"},{"id":"http://arxiv.org/abs/2502.07758v1","updated":"2025-02-11T18:38:02Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago Due","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rlle","Christina C. Westhoff","Bndicte Lenoir","Niels Halama","Inka Zrnig","Dirk Jger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v1.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.07752v1","updated":"2025-02-11T18:27:19Z","published":"2025-02-11T18:27:19Z","title":"Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension","summary":"  Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.\n","authors":["Wenbo Gong","Meyer Scetbon","Chao Ma","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.07752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07750v1","updated":"2025-02-11T18:25:48Z","published":"2025-02-11T18:25:48Z","title":"PFedDST: Personalized Federated Learning with Decentralized Selection\n  Training","summary":"  Distributed Learning (DL) enables the training of machine learning models\nacross multiple devices, yet it faces challenges like non-IID data\ndistributions and device capability disparities, which can impede training\nefficiency. Communication bottlenecks further complicate traditional Federated\nLearning (FL) setups. To mitigate these issues, we introduce the Personalized\nFederated Learning with Decentralized Selection Training (PFedDST) framework.\nPFedDST enhances model training by allowing devices to strategically evaluate\nand select peers based on a comprehensive communication score. This score\nintegrates loss, task similarity, and selection frequency, ensuring optimal\npeer connections. This selection strategy is tailored to increase local\npersonalization and promote beneficial peer collaborations to strengthen the\nstability and efficiency of the training process. Our experiments demonstrate\nthat PFedDST not only enhances model accuracy but also accelerates convergence.\nThis approach outperforms state-of-the-art methods in handling data\nheterogeneity, delivering both faster and more effective training in diverse\nand decentralized systems.\n","authors":["Mengchen Fan","Keren Li","Tianyun Zhang","Qing Tian","Baocheng Geng"],"pdf_url":"https://arxiv.org/pdf/2502.07750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07749v1","updated":"2025-02-11T18:25:14Z","published":"2025-02-11T18:25:14Z","title":"Whole-Genome Phenotype Prediction with Machine Learning: Open Problems\n  in Bacterial Genomics","summary":"  How can we identify causal genetic mechanisms that govern bacterial traits?\nInitial efforts entrusting machine learning models to handle the task of\npredicting phenotype from genotype return high accuracy scores. However,\nattempts to extract any meaning from the predictive models are found to be\ncorrupted by falsely identified \"causal\" features. Relying solely on pattern\nrecognition and correlations is unreliable, significantly so in bacterial\ngenomics settings where high-dimensionality and spurious associations are the\nnorm. Though it is not yet clear whether we can overcome this hurdle,\nsignificant efforts are being made towards discovering potential high-risk\nbacterial genetic variants. In view of this, we set up open problems\nsurrounding phenotype prediction from bacterial whole-genome datasets and\nextending those to learning causal effects, and discuss challenges that impact\nthe reliability of a machine's decision-making when faced with datasets of this\nnature.\n","authors":["Tamsin James","Ben Williamson","Peter Tino","Nicole Wheeler"],"pdf_url":"https://arxiv.org/pdf/2502.07749v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.08096v3","updated":"2025-02-11T18:25:07Z","published":"2024-02-12T22:32:12Z","title":"An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning","summary":"  Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal.\n","authors":["Andrew Bai","Chih-Kuan Yeh","Cho-Jui Hsieh","Ankur Taly"],"pdf_url":"https://arxiv.org/pdf/2402.08096v3.pdf","comment":"13 pages, 9 figures. Published in NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2502.07746v1","updated":"2025-02-11T18:13:29Z","published":"2025-02-11T18:13:29Z","title":"HiPoNet: A Topology-Preserving Multi-View Neural Network For High\n  Dimensional Point Cloud and Single-Cell Data","summary":"  In this paper, we propose HiPoNet, an end-to-end differentiable neural\nnetwork for regression, classification, and representation learning on\nhigh-dimensional point clouds. Single-cell data can have high dimensionality\nexceeding the capabilities of existing methods point cloud tailored for 3D\ndata. Moreover, modern single-cell and spatial experiments now yield entire\ncohorts of datasets (i.e. one on every patient), necessitating models that can\nprocess large, high-dimensional point clouds at scale. Most current approaches\nbuild a single nearest-neighbor graph, discarding important geometric\ninformation. In contrast, HiPoNet forms higher-order simplicial complexes\nthrough learnable feature reweighting, generating multiple data views that\ndisentangle distinct biological processes. It then employs simplicial wavelet\ntransforms to extract multi-scale features - capturing both local and global\ntopology. We empirically show that these components preserve topological\ninformation in the learned representations, and that HiPoNet significantly\noutperforms state-of-the-art point-cloud and graph-based models on single cell.\nWe also show an application of HiPoNet on spatial transcriptomics datasets\nusing spatial co-ordinates as one of the views. Overall, HiPoNet offers a\nrobust and scalable solution for high-dimensional data analysis.\n","authors":["Siddharth Viswanath","Hiren Madhu","Dhananjay Bhaskar","Jake Kovalic","Dave Johnson","Rex Ying","Christopher Tape","Ian Adelstein","Michael Perlmutter","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2502.07746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01985v4","updated":"2025-02-11T18:09:35Z","published":"2024-09-03T15:26:51Z","title":"UNSURE: self-supervised learning with Unknown Noise level and Stein's\n  Unbiased Risk Estimate","summary":"  Recently, many self-supervised learning methods for image reconstruction have\nbeen proposed that can learn from noisy data alone, bypassing the need for\nground-truth references. Most existing methods cluster around two classes: i)\nStein's Unbiased Risk Estimate (SURE) and similar approaches that assume full\nknowledge of the noise distribution, and ii) Noise2Self and similar\ncross-validation methods that require very mild knowledge about the noise\ndistribution. The first class of methods tends to be impractical, as the noise\nlevel is often unknown in real-world applications, and the second class is\noften suboptimal compared to supervised learning. In this paper, we provide a\ntheoretical framework that characterizes this expressivity-robustness trade-off\nand propose a new approach based on SURE, but unlike the standard SURE, does\nnot require knowledge about the noise level. Throughout a series of\nexperiments, we show that the proposed estimator outperforms other existing\nself-supervised methods on various imaging inverse problems.\n","authors":["Julin Tachella","Mike Davies","Laurent Jacques"],"pdf_url":"https://arxiv.org/pdf/2409.01985v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04463v2","updated":"2025-02-11T18:06:02Z","published":"2025-02-06T19:18:16Z","title":"Training Language Models to Reason Efficiently","summary":"  Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.\n","authors":["Daman Arora","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.04463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07741v1","updated":"2025-02-11T18:05:54Z","published":"2025-02-11T18:05:54Z","title":"Advancing climate model interpretability: Feature attribution for Arctic\n  melt anomalies","summary":"  The focus of our work is improving the interpretability of anomalies in\nclimate models and advancing our understanding of Arctic melt dynamics. The\nArctic and Antarctic ice sheets are experiencing rapid surface melting and\nincreased freshwater runoff, contributing significantly to global sea level\nrise. Understanding the mechanisms driving snowmelt in these regions is\ncrucial. ERA5, a widely used reanalysis dataset in polar climate studies,\noffers extensive climate variables and global data assimilation. However, its\nsnowmelt model employs an energy imbalance approach that may oversimplify the\ncomplexity of surface melt. In contrast, the Glacier Energy and Mass Balance\n(GEMB) model incorporates additional physical processes, such as snow\naccumulation, firn densification, and meltwater percolation/refreezing,\nproviding a more detailed representation of surface melt dynamics. In this\nresearch, we focus on analyzing surface snowmelt dynamics of the Greenland Ice\nSheet using feature attribution for anomalous melt events in ERA5 and GEMB\nmodels. We present a novel unsupervised attribution method leveraging\ncounterfactual explanation method to analyze detected anomalies in ERA5 and\nGEMB. Our anomaly detection results are validated using MEaSUREs ground-truth\ndata, and the attributions are evaluated against established feature ranking\nmethods, including XGBoost, Shapley values, and Random Forest. Our attribution\nframework identifies the physics behind each model and the climate features\ndriving melt anomalies. These findings demonstrate the utility of our\nattribution method in enhancing the interpretability of anomalies in climate\nmodels and advancing our understanding of Arctic melt dynamics.\n","authors":["Tolulope Ale","Nicole-Jeanne Schlegel","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2502.07741v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.15332v2","updated":"2025-02-11T18:01:40Z","published":"2024-08-27T18:00:06Z","title":"What makes math problems hard for reinforcement learning: a case study","summary":"  Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple perspectives, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the context\ndefined by the Andrews-Curtis conjecture, we propose algorithmic enhancements\nand a topological hardness measure with implications for a broad class of\nsearch problems. As part of our study, we also address several open\nmathematical questions. Notably, we demonstrate the length reducibility of all\nbut two presentations in the Akbulut-Kirby series (1981), and resolve various\npotential counterexamples in the Miller-Schupp series (1991), including three\ninfinite subfamilies.\n","authors":["Ali Shehper","Anibal M. Medina-Mardones","Lucas Fagan","Bartomiej Lewandowski","Angus Gruen","Yang Qiu","Piotr Kucharski","Zhenghan Wang","Sergei Gukov"],"pdf_url":"https://arxiv.org/pdf/2408.15332v2.pdf","comment":"58 pages, 25 figures, 1 table. Try it:\n  https://github.com/shehper/AC-Solver"},{"id":"http://arxiv.org/abs/2502.07739v1","updated":"2025-02-11T17:59:35Z","published":"2025-02-11T17:59:35Z","title":"HRP: High-Rank Preheating for Superior LoRA Initialization","summary":"  This paper studies the crucial impact of initialization on the convergence\nproperties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that\nrandom initialization, a widely used schema, will likely lead LoRA to random\nlow-rank results, rather than the best low-rank result. While this issue can be\nmitigated by adjusting initialization towards a well-informed direction, it\nrelies on prior knowledge of the target, which is typically unknown in\nreal-world scenarios. To approximate this well-informed initial direction, we\npropose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few\nsteps and uses the singular value decomposition of the preheated result as a\nsuperior initialization. HRP initialization is theory-supported to combine the\nconvergence strengths of high-rank LoRA and the generalization strengths of\nlow-rank LoRA. Extensive experiments demonstrate that HRP significantly\nenhances LoRA's effectiveness across various models and tasks, achieving\nperformance comparable to full-parameter fine-tuning and outperforming other\ninitialization strategies.\n","authors":["Yuzhu Chen","Yingjie Wang","Shi Fu","Li Shen","Yongcheng Jing","Xinmei Tian","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.07739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07735v1","updated":"2025-02-11T17:55:03Z","published":"2025-02-11T17:55:03Z","title":"Revisiting Non-Acyclic GFlowNets in Discrete Environments","summary":"  Generative Flow Networks (GFlowNets) are a family of generative models that\nlearn to sample objects from a given probability distribution, potentially\nknown up to a normalizing constant. Instead of working in the object space,\nGFlowNets proceed by sampling trajectories in an appropriately constructed\ndirected acyclic graph environment, greatly relying on the acyclicity of the\ngraph. In our paper, we revisit the theory that relaxes the acyclicity\nassumption and present a simpler theoretical framework for non-acyclic\nGFlowNets in discrete environments. Moreover, we provide various novel\ntheoretical insights related to training with fixed backward policies, the\nnature of flow functions, and connections between entropy-regularized RL and\nnon-acyclic GFlowNets, which naturally generalize the respective concepts and\ntheoretical results from the acyclic setting. In addition, we experimentally\nre-examine the concept of loss stability in non-acyclic GFlowNet training, as\nwell as validate our own theoretical findings.\n","authors":["Nikita Morozov","Ian Maksimov","Daniil Tiapkin","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2502.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20562v2","updated":"2025-02-11T17:53:46Z","published":"2024-09-30T17:59:03Z","title":"SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes","summary":"  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n","authors":["Tianchang Shen","Zhaoshuo Li","Marc Law","Matan Atzmon","Sanja Fidler","James Lucas","Jun Gao","Nicholas Sharp"],"pdf_url":"https://arxiv.org/pdf/2409.20562v2.pdf","comment":"published at SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06530v3","updated":"2025-02-11T17:49:04Z","published":"2024-10-09T04:07:20Z","title":"TopoTune : A Framework for Generalized Combinatorial Complex Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) excel in learning from relational datasets,\nprocessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems -- such as biological or social\nnetworks--involve multiway complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nTopological Deep Learning (TDL) aims to accommodate and leverage these\nhigher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly\ngeneral TDL models, have been shown to be more expressive and better performing\nthan GNNs. However, differently from the GNN ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting\nits accessibility and applicability. To address this issue, we introduce\nGeneralized CCNNs (GCCNs), a novel simple yet powerful family of TDL models\nthat can be used to systematically transform any (graph) neural network into\nits TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while\nextensive experiments on a diverse class of GCCNs show that these architectures\nconsistently match or outperform CCNNs, often with less model complexity. In an\neffort to accelerate and democratize TDL, we introduce TopoTune, a lightweight\nsoftware for defining, building, and training GCCNs with unprecedented\nflexibility and ease.\n","authors":["Mathilde Papillon","Guillermo Bernrdez","Claudio Battiloro","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2410.06530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15065v2","updated":"2025-02-11T17:47:11Z","published":"2024-08-27T13:48:15Z","title":"The Benefits of Balance: From Information Projections to Variance\n  Reduction","summary":"  Data balancing across multiple modalities and sources appears in various\nforms in foundation models in machine learning and AI, e.g. in CLIP and DINO.\nWe show that data balancing across modalities and sources actually offers an\nunsuspected benefit: variance reduction. We present a non-asymptotic\nstatistical bound that quantifies this variance reduction effect and relates it\nto the eigenvalue decay of Markov operators. Furthermore, we describe how\nvarious forms of data balancing in contrastive multimodal learning and\nself-supervised clustering can be better understood, and even improved upon,\nowing to our variance reduction viewpoint.\n","authors":["Lang Liu","Ronak Mehta","Soumik Pal","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2408.15065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08281v3","updated":"2025-02-11T17:43:59Z","published":"2024-01-16T11:12:36Z","title":"The Faiss library","summary":"  Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.\n","authors":["Matthijs Douze","Alexandr Guzhva","Chengqi Deng","Jeff Johnson","Gergely Szilvasy","Pierre-Emmanuel Mazar","Maria Lomeli","Lucas Hosseini","Herv Jgou"],"pdf_url":"https://arxiv.org/pdf/2401.08281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08731v3","updated":"2025-02-11T17:38:13Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Wei Zhou","Robert Wright","Mark O. Riedl"],"pdf_url":"https://arxiv.org/pdf/2310.08731v3.pdf","comment":"RLC Safety 2024"},{"id":"http://arxiv.org/abs/2501.11779v2","updated":"2025-02-11T17:36:32Z","published":"2025-01-20T23:10:13Z","title":"Glinthawk: A Two-Tiered Architecture for Offline LLM Inference","summary":"  We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.\n","authors":["Pouya Hamadanian","Sadjad Fouladi"],"pdf_url":"https://arxiv.org/pdf/2501.11779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04667v2","updated":"2025-02-11T17:36:13Z","published":"2025-01-08T18:28:12Z","title":"Natural Variational Annealing for Multimodal Optimization","summary":"  We introduce a new multimodal optimization approach called Natural\nVariational Annealing (NVA) that combines the strengths of three foundational\nconcepts to simultaneously search for multiple global and local modes of\nblack-box nonconvex objectives. First, it implements a simultaneous search by\nusing variational posteriors, such as, mixtures of Gaussians. Second, it\napplies annealing to gradually trade off exploration for exploitation. Finally,\nit learns the variational search distribution using natural-gradient learning\nwhere updates resemble well-known and easy-to-implement algorithms. The three\nconcepts come together in NVA giving rise to new algorithms and also allowing\nus to incorporate \"fitness shaping\", a core concept from evolutionary\nalgorithms. We assess the quality of search on simulations and compare them to\nmethods using gradient descent and evolution strategies. We also provide an\napplication to a real-world inverse problem in planetary science.\n","authors":["Tm Le Minh","Julyan Arbel","Thomas Mllenhoff","Mohammad Emtiyaz Khan","Florence Forbes"],"pdf_url":"https://arxiv.org/pdf/2501.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07721v1","updated":"2025-02-11T17:33:48Z","published":"2025-02-11T17:33:48Z","title":"TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning","summary":"  The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments.\n","authors":["Mengyang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10763v2","updated":"2025-02-11T17:28:34Z","published":"2024-03-16T02:06:14Z","title":"Drago: Primal-Dual Coupled Variance Reduction for Faster\n  Distributionally Robust Optimization","summary":"  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses learning\nusing $f$-DRO and spectral/$L$-risk minimization. We present Drago, a\nstochastic primal-dual algorithm that combines cyclic and randomized components\nwith a carefully regularized primal update to achieve dual variance reduction.\nOwing to its design, Drago enjoys a state-of-the-art linear convergence rate on\nstrongly convex-strongly concave DRO problems with a fine-grained dependency on\nprimal and dual condition numbers. Theoretical results are supported by\nnumerical benchmarks on regression and classification tasks.\n","authors":["Ronak Mehta","Jelena Diakonikolas","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2403.10763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07715v1","updated":"2025-02-11T17:15:55Z","published":"2025-02-11T17:15:55Z","title":"Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) problems are being considered under increasingly\nmore complex structures. While tabular and linear models have been thoroughly\nexplored, the analytical study of RL under nonlinear function approximation,\nespecially kernel-based models, has recently gained traction for their strong\nrepresentational capacity and theoretical tractability. In this context, we\nexamine the question of statistical efficiency in kernel-based RL within the\nreward-free RL framework, specifically asking: how many samples are required to\ndesign a near-optimal policy? Existing work addresses this question under\nrestrictive assumptions about the class of kernel functions. We first explore\nthis question by assuming a generative model, then relax this assumption at the\ncost of increasing the sample complexity by a factor of H, the length of the\nepisode. We tackle this fundamental problem using a broad class of kernels and\na simpler algorithm compared to prior work. Our approach derives new confidence\nintervals for kernel ridge regression, specific to our RL setting, which may be\nof broader applicability. We further validate our theoretical findings through\nsimulations.\n","authors":["Aya Kayal","Sattar Vakili","Laura Toni","Alberto Bernacchia"],"pdf_url":"https://arxiv.org/pdf/2502.07715v1.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2409.05701v3","updated":"2025-02-11T17:14:43Z","published":"2024-09-09T15:13:56Z","title":"pFedGPA: Diffusion-based Generative Parameter Aggregation for\n  Personalized Federated Learning","summary":"  Federated Learning (FL) offers a decentralized approach to model training,\nwhere data remains local and only model parameters are shared between the\nclients and the central server. Traditional methods, such as Federated\nAveraging (FedAvg), linearly aggregate these parameters which are usually\ntrained on heterogeneous data distributions, potentially overlooking the\ncomplex, high-dimensional nature of the parameter space. This can result in\ndegraded performance of the aggregated model. While personalized FL approaches\ncan mitigate the heterogeneous data issue to some extent, the limitation of\nlinear aggregation remains unresolved. To alleviate this issue, we investigate\nthe generative approach of diffusion model and propose a novel generative\nparameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this\nframework, we deploy a diffusion model on the server to integrate the diverse\nparameter distributions and propose a parameter inversion method to efficiently\ngenerate a set of personalized parameters for each client. This inversion\nmethod transforms the uploaded parameters into a latent code, which is then\naggregated through denoising sampling to produce the final personalized\nparameters. By encoding the dependence of a client's model parameters on the\nspecific data distribution using the high-capacity diffusion model,\n\\texttt{pFedGPA} can effectively decouple the complexity of the overall\ndistribution of all clients' model parameters from the complexity of each\nindividual client's parameter distribution. Our experimental results\nconsistently demonstrate the superior performance of the proposed method across\nmultiple datasets, surpassing baseline approaches.\n","authors":["Jiahao Lai","Jiaqi Li","Jian Xu","Yanru Wu","Boshi Tang","Siqi Chen","Yongfeng Huang","Wenbo Ding","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2409.05701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17165v3","updated":"2025-02-11T17:06:45Z","published":"2023-11-28T19:01:09Z","title":"(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions","summary":"  The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.\n","authors":["Olivia Macmillan-Scott","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2311.17165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16247v2","updated":"2025-02-11T16:54:45Z","published":"2024-12-20T00:01:16Z","title":"Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models","summary":"  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n","authors":["Konstantin Donhauser","Kristina Ulicna","Gemma Elyse Moran","Aditya Ravuri","Kian Kenyon-Dean","Cian Eastwood","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2412.16247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10737v3","updated":"2025-02-11T16:47:17Z","published":"2024-06-15T20:47:38Z","title":"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n","authors":["Yunbei Zhang","Akshay Mehra","Shuaicheng Niu","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18768v3","updated":"2025-02-11T16:24:23Z","published":"2024-09-27T14:12:49Z","title":"Learning from Demonstration with Implicit Nonlinear Dynamics Models","summary":"  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.\n","authors":["Peter David Fagan","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.18768v3.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.02153v2","updated":"2025-02-11T16:23:39Z","published":"2024-12-03T04:28:14Z","title":"Revisiting the Initial Steps in Adaptive Gradient Descent Optimization","summary":"  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.02153v2.pdf","comment":"Conference on Parsimony and Learning (CPAL) 2025"},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2502.06314v2","updated":"2025-02-11T16:04:15Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Schlkopf","Julius von Kgelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2402.11355v5","updated":"2025-02-11T16:03:35Z","published":"2024-02-17T18:12:02Z","title":"A Practical Method for Generating String Counterfactuals","summary":"  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n","authors":["Matan Avitan","Ryan Cotterell","Yoav Goldberg","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2402.11355v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.11061v8","updated":"2025-02-11T15:59:51Z","published":"2024-10-14T20:14:39Z","title":"Learning to Optimize for Mixed-Integer Non-linear Programming","summary":"  Mixed-integer nonlinear programs (MINLPs) arise in diverse domains such as\nenergy systems and transportation but are notoriously difficult to solve,\nparticularly on a large scale. While learning-to-optimize methods have been\nsuccessful at continuous optimization, extending them to MINLPs is still\nchallenging due to the integer constraints. To overcome this, we propose a\nnovel deep-learning approach with two learnable correction layers to ensure\nsolution integrality and a post-processing step to improve solution\nfeasibility. Our experiments show that this is the first general method capable\nof efficiently solving large-scale MINLPs with up to tens of thousands of\nvariables in milliseconds, delivering high-quality solutions even when\ntraditional solvers and heuristics fail. This is the first general learning\nmethod for MINLP, successfully solving some of the largest instances reported\nto date.\n","authors":["Bo Tang","Elias B. Khalil","Jn Drgoa"],"pdf_url":"https://arxiv.org/pdf/2410.11061v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12042v3","updated":"2025-02-11T15:58:10Z","published":"2024-06-17T19:22:04Z","title":"Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image\n  Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have demonstrated impressive image\ngeneration capabilities. Still, their computational intensity prohibits\nresource-constrained organizations from deploying T2I models after fine-tuning\nthem on their internal target data. While pruning techniques offer a potential\nsolution to reduce the computational burden of T2I models, static pruning\nmethods use the same pruned model for all input prompts, overlooking the\nvarying capacity requirements of different prompts. Dynamic pruning addresses\nthis issue by utilizing a separate sub-network for each prompt, but it prevents\nbatch parallelism on GPUs. To overcome these limitations, we introduce Adaptive\nPrompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed\nfor T2I diffusion models. Central to our approach is a prompt router model,\nwhich learns to determine the required capacity for an input text prompt and\nroutes it to an architecture code, given a total desired compute budget for\nprompts. Each architecture code represents a specialized model tailored to the\nprompts assigned to it, and the number of codes is a hyperparameter. We train\nthe prompt router and architecture codes using contrastive learning, ensuring\nthat similar prompts are mapped to nearby codes. Further, we employ optimal\ntransport to prevent the codes from collapsing into a single one. We\ndemonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using\nCC3M and COCO as target datasets. APTP outperforms the single-model pruning\nbaselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters\nlearned by APTP reveals they are semantically meaningful. We also show that\nAPTP can automatically discover previously empirically found challenging\nprompts for SD, e.g. prompts for generating text images, assigning them to\nhigher capacity codes.\n","authors":["Alireza Ganjdanesh","Reza Shirkavand","Shangqian Gao","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.12042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07661v1","updated":"2025-02-11T15:51:23Z","published":"2025-02-11T15:51:23Z","title":"Partial-Label Learning with Conformal Candidate Cleaning","summary":"  Real-world data is often ambiguous; for example, human annotation produces\ninstances with multiple conflicting class labels. Partial-label learning (PLL)\naims at training a classifier in this challenging setting, where each instance\nis associated with a set of candidate labels and one correct, but unknown,\nclass label. A multitude of algorithms targeting this setting exists and, to\nenhance their prediction quality, several extensions that are applicable across\na wide range of PLL methods have been introduced. While many of these\nextensions rely on heuristics, this article proposes a novel enhancing method\nthat incrementally prunes candidate sets using conformal prediction. To work\naround the missing labeled validation set, which is typically required for\nconformal prediction, we propose a strategy that alternates between training a\nPLL classifier to label the validation set, leveraging these predicted class\nlabels for calibration, and pruning candidate labels that are not part of the\nresulting conformal sets. In this sense, our method alternates between\nempirical risk minimization and candidate set pruning. We establish that our\npruning method preserves the conformal validity with respect to the unknown\nground truth. Our extensive experiments on artificial and real-world data show\nthat the proposed approach significantly improves the test set accuracies of\nseveral state-of-the-art PLL classifiers.\n","authors":["Tobias Fuchs","Florian Kalinke"],"pdf_url":"https://arxiv.org/pdf/2502.07661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11160v4","updated":"2025-02-11T15:50:27Z","published":"2023-04-21T17:59:08Z","title":"Isotonic Mechanism for Exponential Family Estimation in Machine Learning\n  Peer Review","summary":"  In 2023, the International Conference on Machine Learning (ICML) required\nauthors with multiple submissions to rank their submissions based on perceived\nquality. In this paper, we aim to employ these author-specified rankings to\nenhance peer review in machine learning and artificial intelligence conferences\nby extending the Isotonic Mechanism to exponential family distributions. This\nmechanism generates adjusted scores that closely align with the original scores\nwhile adhering to author-specified rankings. Despite its applicability to a\nbroad spectrum of exponential family distributions, implementing this mechanism\ndoes not require knowledge of the specific distribution form. We demonstrate\nthat an author is incentivized to provide accurate rankings when her utility\ntakes the form of a convex additive function of the adjusted review scores. For\na certain subclass of exponential family distributions, we prove that the\nauthor reports truthfully only if the question involves only pairwise\ncomparisons between her submissions, thus indicating the optimality of ranking\nin truthful information elicitation. Moreover, we show that the adjusted scores\nimprove dramatically the estimation accuracy compared to the original scores\nand achieve nearly minimax optimality when the ground-truth scores have bounded\ntotal variation. We conclude with a numerical analysis of the ICML 2023 ranking\ndata, showing substantial estimation gains in approximating a proxy\nground-truth quality of the papers using the Isotonic Mechanism.\n","authors":["Yuling Yan","Weijie J. Su","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2304.11160v4.pdf","comment":"accepted to the Journal of the Royal Statistical Society: Series B"},{"id":"http://arxiv.org/abs/2502.07657v1","updated":"2025-02-11T15:46:03Z","published":"2025-02-11T15:46:03Z","title":"Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian\n  Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations","summary":"  We consider the problem of approximating a $d \\times d$ covariance matrix $M$\nwith a rank-$k$ matrix under $(\\varepsilon,\\delta)$-differential privacy. We\npresent and analyze a complex variant of the Gaussian mechanism and obtain\nupper bounds on the Frobenius norm of the difference between the matrix output\nby this mechanism and the best rank-$k$ approximation to $M$. Our analysis\nprovides improvements over previous bounds, particularly when the spectrum of\n$M$ satisfies natural structural assumptions. The novel insight is to view the\naddition of Gaussian noise to a matrix as a continuous-time matrix Brownian\nmotion. This viewpoint allows us to track the evolution of eigenvalues and\neigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations enable us to upper bound the\nFrobenius distance between the best rank-$k$ approximation of $M$ and that of a\nGaussian perturbation of $M$ as an integral that involves inverse eigenvalue\ngaps of the stochastically evolving matrix, as opposed to a sum of perturbation\nbounds obtained via Davis-Kahan-type theorems. Subsequently, again using the\nDyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$\nperturbed by Gaussian noise have large gaps with high probability. These\nresults also contribute to the analysis of low-rank approximations under\naverage-case perturbations, and to an understanding of eigenvalue gaps for\nrandom matrices, both of which may be of independent interest.\n","authors":["Oren Mangoubi","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2502.07657v1.pdf","comment":"Published in Journal of the ACM. arXiv admin note: substantial text\n  overlap with arXiv:2306.16648"},{"id":"http://arxiv.org/abs/2502.07656v1","updated":"2025-02-11T15:43:49Z","published":"2025-02-11T15:43:49Z","title":"A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders","summary":"  We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.\n","authors":["Daqian Shao","Thomas Kleine Buening","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2502.07656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03736v3","updated":"2025-02-11T15:42:19Z","published":"2024-06-06T04:22:11Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data","summary":"  Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.\n","authors":["Jingyang Ou","Shen Nie","Kaiwen Xue","Fengqi Zhu","Jiacheng Sun","Zhenguo Li","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.03736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07650v1","updated":"2025-02-11T15:39:47Z","published":"2025-02-11T15:39:47Z","title":"Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold","summary":"  Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to approximate the natural gradient of the\nKL divergence efficiently without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms.\n","authors":["Song Liu","Leyang Wang","Yakun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07646v1","updated":"2025-02-11T15:35:15Z","published":"2025-02-11T15:35:15Z","title":"Causal Additive Models with Unobserved Causal Paths and Backdoor Paths","summary":"  Causal additive models have been employed as tractable yet expressive\nframeworks for causal discovery involving hidden variables. State-of-the-art\nmethodologies suggest that determining the causal relationship between a pair\nof variables is infeasible in the presence of an unobserved backdoor or an\nunobserved causal path. Contrary to this assumption, we theoretically show that\nresolving the causal direction is feasible in certain scenarios by\nincorporating two novel components into the theory. The first component\nintroduces a novel characterization of regression sets within independence\nbetween regression residuals. The second component leverages conditional\nindependence among the observed variables. We also provide a search algorithm\nthat integrates these innovations and demonstrate its competitive performance\nagainst existing methods.\n","authors":["Thong Pham","Takashi Nicholas Maeda","Shohei Shimizu"],"pdf_url":"https://arxiv.org/pdf/2502.07646v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07642v1","updated":"2025-02-11T15:33:17Z","published":"2025-02-11T15:33:17Z","title":"FoQA: A Faroese Question-Answering Dataset","summary":"  We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.\n","authors":["Annika Simonsen","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.07642v1.pdf","comment":"Camera-ready version for RESOURCEFUL workshop, 2025"},{"id":"http://arxiv.org/abs/2502.07640v1","updated":"2025-02-11T15:27:35Z","published":"2025-02-11T15:27:35Z","title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving","summary":"  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.\n","authors":["Yong Lin","Shange Tang","Bohan Lyu","Jiayun Wu","Hongzhou Lin","Kaiyu Yang","Jia Li","Mengzhou Xia","Danqi Chen","Sanjeev Arora","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.07640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07636v1","updated":"2025-02-11T15:23:14Z","published":"2025-02-11T15:23:14Z","title":"Consistency Training with Physical Constraints","summary":"  We propose a physics-aware Consistency Training (CT) method that accelerates\nsampling in Diffusion Models with physical constraints. Our approach leverages\na two-stage strategy: (1) learning the noise-to-data mapping via CT, and (2)\nincorporating physics constraints as a regularizer. Experiments on toy examples\nshow that our method generates samples in a single step while adhering to the\nimposed constraints. This approach has the potential to efficiently solve\npartial differential equations (PDEs) using deep generative modeling.\n","authors":["Che-Chia Chang","Chen-Yang Dai","Te-Sheng Lin","Ming-Chih Lai","Chieh-Hsin Lai"],"pdf_url":"https://arxiv.org/pdf/2502.07636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07635v1","updated":"2025-02-11T15:23:05Z","published":"2025-02-11T15:23:05Z","title":"Distributed Value Decomposition Networks with Networked Agents","summary":"  We investigate the problem of distributed training under partial\nobservability, whereby cooperative multi-agent reinforcement learning agents\n(MARL) maximize the expected cumulative joint reward. We propose distributed\nvalue decomposition networks (DVDN) that generate a joint Q-function that\nfactorizes into agent-wise Q-functions. Whereas the original value\ndecomposition networks rely on centralized training, our approach is suitable\nfor domains where centralized training is not possible and agents must learn by\ninteracting with the physical environment in a decentralized manner while\ncommunicating with their peers. DVDN overcomes the need for centralized\ntraining by locally estimating the shared objective. We contribute with two\ninnovative algorithms, DVDN and DVDN (GT), for the heterogeneous and\nhomogeneous agents settings respectively. Empirically, both algorithms\napproximate the performance of value decomposition networks, in spite of the\ninformation loss during communication, as demonstrated in ten MARL tasks in\nthree standard environments.\n","authors":["Guilherme S. Varela","Alberto Sardinha","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2502.07635v1.pdf","comment":"21 pages, 15 figures, to be published in Proceedings of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS"},{"id":"http://arxiv.org/abs/2502.04907v2","updated":"2025-02-11T15:17:43Z","published":"2025-02-07T13:23:40Z","title":"Scalable and consistent embedding of probability measures into Hilbert\n  spaces via measure quantization","summary":"  This paper is focused on statistical learning from data that come as\nprobability measures. In this setting, popular approaches consist in embedding\nsuch data into a Hilbert space with either Linearized Optimal Transport or\nKernel Mean Embedding. However, the cost of computing such embeddings prohibits\ntheir direct use in large-scale settings. We study two methods based on measure\nquantization for approximating input probability measures with discrete\nmeasures of small-support size. The first one is based on optimal quantization\nof each input measure, while the second one relies on mean-measure\nquantization. We study the consistency of such approximations, and its\nimplication for scalable embeddings of probability measures into a Hilbert\nspace at a low computational cost. We finally illustrate our findings with\nvarious numerical experiments.\n","authors":["Erell Gachon","Elsa Cazelles","Jrmie Bigot"],"pdf_url":"https://arxiv.org/pdf/2502.04907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07630v1","updated":"2025-02-11T15:17:29Z","published":"2025-02-11T15:17:29Z","title":"Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF\n  Corrections","summary":"  PET is a functional imaging method that visualizes metabolic processes. TOF\ninformation can be derived from coincident detector signals and incorporated\ninto image reconstruction to enhance the SNR. PET detectors are typically\nassessed by their CTR, but timing performance is degraded by various factors.\nResearch on timing calibration seeks to mitigate these degradations and restore\naccurate timing information. While many calibration methods use analytical\napproaches, machine learning techniques have recently gained attention due to\ntheir flexibility. We developed a residual physics-based calibration approach\nthat combines prior domain knowledge with the power of machine learning models.\nThis approach begins with an initial analytical calibration addressing\nfirst-order skews. The remaining deviations, regarded as residual effects, are\nused to train machine learning models to eliminate higher-order skews. The key\nadvantage is that the experimenter guides the learning process through the\ndefinition of timing residuals. In earlier studies, we developed models that\ndirectly predicted the expected time difference, which offered corrections only\nimplicitly (implicit correction models). In this study, we introduce a new\ndefinition for timing residuals, enabling us to train models that directly\npredict correction values (explicit correction models). The explicit correction\napproach significantly simplifies data acquisition, improves linearity, and\nenhances timing performance from $371 \\pm 6$ ps to $281 \\pm 5$ ps for\ncoincidences from 430 keV to 590 keV. Additionally, the new definition reduces\nmodel size, making it suitable for high-throughput applications like PET\nscanners. Experiments were conducted using two detector stacks composed of $4\n\\times 4$ LYSO:Ce,Ca crystals ($3.8\\times 3.8\\times 20$ mm$^{3}$) coupled to $4\n\\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.\n","authors":["Stephan Naunheim","Luis Lopes de Paiva","Vanessa Nadig","Yannick Kuhl","Stefan Gundacker","Florian Mueller","Volkmar Schulz"],"pdf_url":"https://arxiv.org/pdf/2502.07630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13779v2","updated":"2025-02-11T15:09:49Z","published":"2024-01-24T20:00:23Z","title":"Faster Convergence with Less Communication: Broadcast-Based Subgraph\n  Sampling for Decentralized Learning over Wireless Networks","summary":"  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely\nadopted algorithm for decentralized training of machine learning models across\nnetworked agents. A crucial part of D-SGD is the consensus-based model\naveraging, which heavily relies on information exchange and fusion among the\nnodes. Specifically, for consensus averaging over wireless networks,\ncommunication coordination is necessary to determine when and how a node can\naccess the channel and transmit (or receive) information to (or from) its\nneighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph\nsampling method designed to accelerate the convergence of D-SGD while\nconsidering the actual communication cost per iteration. $\\texttt{BASS}$\ncreates a set of mixing matrix candidates that represent sparser subgraphs of\nthe base topology. In each consensus iteration, one mixing matrix is sampled,\nleading to a specific scheduling decision that activates multiple\ncollision-free subsets of nodes. The sampling occurs in a probabilistic manner,\nand the elements of the mixing matrices, along with their sampling\nprobabilities, are jointly optimized. Simulation results demonstrate that\n$\\texttt{BASS}$ enables faster convergence with fewer transmission slots\ncompared to existing link-based scheduling methods. In conclusion, the inherent\nbroadcasting nature of wireless channels offers intrinsic advantages in\naccelerating the convergence of decentralized optimization and learning.\n","authors":["Daniel Prez Herrera","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2401.13779v2.pdf","comment":"14 pages, 10 figures, accepted for publication at IEEE Open Journals\n  of Communication. arXiv admin note: text overlap with arXiv:2310.16106"},{"id":"http://arxiv.org/abs/2502.07620v1","updated":"2025-02-11T15:09:05Z","published":"2025-02-11T15:09:05Z","title":"Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift","summary":"  The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07620v1.pdf","comment":"17pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.07616v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Tractable Transformers for Flexible Conditional Generation","summary":"  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n","authors":["Anji Liu","Xuejie Liu","Dayuan Zhao","Mathias Niepert","Yitao Liang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2502.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07608v1","updated":"2025-02-11T14:58:54Z","published":"2025-02-11T14:58:54Z","title":"Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing","summary":"  Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks.\n","authors":["Arvind Pillai","Dimitris Spathis","Subigya Nepal","Amanda C Collins","Daniel M Mackin","Michael V Heinz","Tess Z Griffin","Nicholas C Jacobson","Andrew Campbell"],"pdf_url":"https://arxiv.org/pdf/2502.07608v1.pdf","comment":"Under review at CHIL 2025"},{"id":"http://arxiv.org/abs/2412.07041v3","updated":"2025-02-11T14:57:40Z","published":"2024-12-09T23:01:04Z","title":"Generalized Least Squares Kernelized Tensor Factorization","summary":"  Completing multidimensional tensor-structured data with missing entries is a\nfundamental task for many real-world applications involving incomplete or\ncorrupted datasets. For data with spatial or temporal side information,\nlow-rank factorization models with smoothness constraints have demonstrated\nstrong performance. Although effective at capturing global and long-range\ncorrelations, these models often struggle to capture short-scale,\nhigh-frequency variations in the data. To address this limitation, we propose\nthe Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework\nfor tensor completion. GLSKF integrates smoothness-constrained low-rank\nfactorization with a locally correlated residual process; the resulting\nadditive structure enables effective characterization of both global\ndependencies and local variations. Specifically, we define the covariance norm\nto enforce the smoothness of factor matrices in the global low-rank\nfactorization, and use structured covariance/kernel functions to model the\nlocal processes. For model estimation, we develop an alternating least squares\n(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes\nzero-padding and slicing operations based on projection matrices which preserve\nthe Kronecker structure of covariances, facilitating efficient computations\nthrough the conjugate gradient (CG) method. The proposed framework is evaluated\non four real-world datasets across diverse tasks. Experimental results\ndemonstrate that GLSKF achieves superior performance and scalability,\nestablishing it as a novel solution for multidimensional tensor completion.\n","authors":["Mengying Lei","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07606v1","updated":"2025-02-11T14:56:16Z","published":"2025-02-11T14:56:16Z","title":"Algorithmic Aspects of Strategic Trading","summary":"  Algorithmic trading in modern financial markets is widely acknowledged to\nexhibit strategic, game-theoretic behaviors whose complexity can be difficult\nto model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress\nin the setting of trading for position building. Here parties wish to buy or\nsell a fixed number of shares in a fixed time period in the presence of both\ntemporary and permanent market impact, resulting in exponentially large\nstrategy spaces. While these papers primarily consider the existence and\nstructural properties of equilibrium strategies, in this work we focus on the\nalgorithmic aspects of the proposed model. We give an efficient algorithm for\ncomputing best responses, and show that while the temporary impact only setting\nyields a potential game, best response dynamics do not generally converge for\nthe general setting, for which no fast algorithm for (Nash) equilibrium\ncomputation is known. This leads us to consider the broader notion of Coarse\nCorrelated Equilibria (CCE), which we show can be computed efficiently via an\nimplementation of Follow the Perturbed Leader (FTPL). We illustrate the model\nand our results with an experimental investigation, where FTPL exhibits\ninteresting behavior in different regimes of the relative weighting between\ntemporary and permanent market impact.\n","authors":["Michael Kearns","Mirah Shi"],"pdf_url":"https://arxiv.org/pdf/2502.07606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02525v4","updated":"2025-02-11T14:54:12Z","published":"2022-09-06T14:30:18Z","title":"Generalisation under gradient descent via deterministic PAC-Bayes","summary":"  We establish disintegrated PAC-Bayesian generalisation bounds for models\ntrained with gradient descent methods or continuous gradient flows. Contrary to\nstandard practice in the PAC-Bayesian setting, our result applies to\noptimisation algorithms that are deterministic, without requiring any\nde-randomisation step. Our bounds are fully computable, depending on the\ndensity of the initial distribution and the Hessian of the training objective\nover the trajectory. We show that our framework can be applied to a variety of\niterative optimisation algorithms, including stochastic gradient descent (SGD),\nmomentum-based schemes, and damped Hamiltonian dynamics.\n","authors":["Eugenio Clerico","Tyler Farghly","George Deligiannidis","Benjamin Guedj","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2209.02525v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07591v1","updated":"2025-02-11T14:40:57Z","published":"2025-02-11T14:40:57Z","title":"DMWM: Dual-Mind World Model with Long-Term Imagination","summary":"  Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20385v2","updated":"2025-02-11T14:37:00Z","published":"2024-12-29T07:21:13Z","title":"A Particle Algorithm for Mean-Field Variational Inference","summary":"  Variational inference is a fast and scalable alternative to Markov chain\nMonte Carlo and has been widely applied to posterior inference tasks in\nstatistics and machine learning. A traditional approach for implementing\nmean-field variational inference (MFVI) is coordinate ascent variational\ninference (CAVI), which relies crucially on parametric assumptions on complete\nconditionals. In this paper, we introduce a novel particle-based algorithm for\nmean-field variational inference, which we term PArticle VI (PAVI). Notably,\nour algorithm does not rely on parametric assumptions on complete conditionals,\nand it applies to the nonparametric setting. We provide non-asymptotic\nfinite-particle convergence guarantee for our algorithm. To our knowledge, this\nis the first end-to-end guarantee for particle-based MFVI.\n","authors":["Qiang Du","Kaizheng Wang","Edith Zhang","Chenyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2412.20385v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.07587v1","updated":"2025-02-11T14:36:39Z","published":"2025-02-11T14:36:39Z","title":"SEMU: Singular Value Decomposition for Efficient Machine Unlearning","summary":"  While the capabilities of generative foundational models have advanced\nrapidly in recent years, methods to prevent harmful and unsafe behaviors remain\nunderdeveloped. Among the pressing challenges in AI safety, machine unlearning\n(MU) has become increasingly critical to meet upcoming safety regulations. Most\nexisting MU approaches focus on altering the most significant parameters of the\nmodel. However, these methods often require fine-tuning substantial portions of\nthe model, resulting in high computational costs and training instabilities,\nwhich are typically mitigated by access to the original training dataset.\n  In this work, we address these limitations by leveraging Singular Value\nDecomposition (SVD) to create a compact, low-dimensional projection that\nenables the selective forgetting of specific data points. We propose Singular\nValue Decomposition for Efficient Machine Unlearning (SEMU), a novel approach\ndesigned to optimize MU in two key aspects. First, SEMU minimizes the number of\nmodel parameters that need to be modified, effectively removing unwanted\nknowledge while making only minimal changes to the model's weights. Second,\nSEMU eliminates the dependency on the original training dataset, preserving the\nmodel's previously acquired knowledge without additional data requirements.\n  Extensive experiments demonstrate that SEMU achieves competitive performance\nwhile significantly improving efficiency in terms of both data usage and the\nnumber of modified parameters.\n","authors":["Marcin Sendera","ukasz Struski","Kamil Ksiek","Kryspin Musiol","Jacek Tabor","Dawid Rymarczyk"],"pdf_url":"https://arxiv.org/pdf/2502.07587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07584v1","updated":"2025-02-11T14:31:32Z","published":"2025-02-11T14:31:32Z","title":"Understanding the Generalization Error of Markov algorithms through\n  Poissonization","summary":"  Using continuous-time stochastic differential equation (SDE) proxies to\nstochastic optimization algorithms has proven fruitful for understanding their\ngeneralization abilities. A significant part of these approaches are based on\nthe so-called ``entropy flows'', which greatly simplify the generalization\nanalysis. Unfortunately, such well-structured entropy flows cannot be obtained\nfor most discrete-time algorithms, and the existing SDE approaches remain\nlimited to specific noise and algorithmic structures. We aim to alleviate this\nissue by introducing a generic framework for analyzing the generalization error\nof Markov algorithms through `Poissonization', a continuous-time approximation\nof discrete-time processes with formal approximation guarantees. Through this\napproach, we first develop a novel entropy flow, which directly leads to\nPAC-Bayesian generalization bounds. We then draw novel links to modified\nversions of the celebrated logarithmic Sobolev inequalities (LSI), identify\ncases where such LSIs are satisfied, and obtain improved bounds. Beyond its\ngenerality, our framework allows exploiting specific properties of learning\nalgorithms. In particular, we incorporate the noise structure of different\nalgorithm types - namely, those with additional noise injections (noisy) and\nthose without (non-noisy) - through various technical tools. This illustrates\nthe capacity of our methods to achieve known (yet, Poissonized) and new\ngeneralization bounds.\n","authors":["Benjamin Dupuis","Maxime Haddouche","George Deligiannidis","Umut Simsekli"],"pdf_url":"https://arxiv.org/pdf/2502.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07580v1","updated":"2025-02-11T14:27:10Z","published":"2025-02-11T14:27:10Z","title":"Generative Modeling with Bayesian Sample Inference","summary":"  We derive a novel generative model from the simple act of Gaussian posterior\ninference. Treating the generated sample as an unknown variable to infer lets\nus formulate the sampling process in the language of Bayesian probability. Our\nmodel uses a sequence of prediction and posterior update steps to narrow down\nthe unknown sample from a broad initial belief. In addition to a rigorous\ntheoretical analysis, we establish a connection between our model and diffusion\nmodels and show that it includes Bayesian Flow Networks (BFNs) as a special\ncase. In our experiments, we demonstrate improved performance over both BFNs\nand Variational Diffusion Models, achieving competitive likelihood scores on\nCIFAR10 and ImageNet.\n","authors":["Marten Lienen","Marcel Kollovieh","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2502.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07579v1","updated":"2025-02-11T14:25:52Z","published":"2025-02-11T14:25:52Z","title":"Single-Step Consistent Diffusion Samplers","summary":"  Sampling from unnormalized target distributions is a fundamental yet\nchallenging task in machine learning and statistics. Existing sampling\nalgorithms typically require many iterative steps to produce high-quality\nsamples, leading to high computational costs that limit their practicality in\ntime-sensitive or resource-constrained settings. In this work, we introduce\nconsistent diffusion samplers, a new class of samplers designed to generate\nhigh-fidelity samples in a single step. We first develop a distillation\nalgorithm to train a consistent diffusion sampler from a pretrained diffusion\nmodel without pre-collecting large datasets of samples. Our algorithm leverages\nincomplete sampling trajectories and noisy intermediate states directly from\nthe diffusion process. We further propose a method to train a consistent\ndiffusion sampler from scratch, fully amortizing exploration by training a\nsingle model that both performs diffusion sampling and skips intermediate steps\nusing a self-consistency loss. Through extensive experiments on a variety of\nunnormalized distributions, we show that our approach yields high-fidelity\nsamples using less than 1% of the network evaluations required by traditional\ndiffusion samplers.\n","authors":["Pascal Jutras-Dub","Patrick Pynadath","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v1","updated":"2025-02-11T14:23:13Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03006v2","updated":"2025-02-11T14:19:32Z","published":"2024-11-05T11:12:11Z","title":"Neural Networks and (Virtual) Extended Formulations","summary":"  Neural networks with piecewise linear activation functions, such as rectified\nlinear units (ReLU) or maxout, are among the most fundamental models in modern\nmachine learning. We make a step towards proving lower bounds on the size of\nsuch neural networks by linking their representative capabilities to the notion\nof the extension complexity $\\mathrm{xc}(P)$ of a polytope $P$. This is a\nwell-studied quantity in combinatorial optimization and polyhedral geometry\ndescribing the number of inequalities needed to model $P$ as a linear program.\nWe show that $\\mathrm{xc}(P)$ is a lower bound on the size of any monotone or\ninput-convex neural network that solves the linear optimization problem over\n$P$. This implies exponential lower bounds on such neural networks for a\nvariety of problems, including the polynomially solvable maximum weight\nmatching problem.\n  In an attempt to prove similar bounds also for general neural networks, we\nintroduce the notion of virtual extension complexity $\\mathrm{vxc}(P)$, which\ngeneralizes $\\mathrm{xc}(P)$ and describes the number of inequalities needed to\nrepresent the linear optimization problem over $P$ as a difference of two\nlinear programs. We prove that $\\mathrm{vxc}(P)$ is a lower bound on the size\nof any neural network that optimizes over $P$. While it remains an open\nquestion to derive useful lower bounds on $\\mathrm{vxc}(P)$, we argue that this\nquantity deserves to be studied independently from neural networks by proving\nthat one can efficiently optimize over a polytope $P$ using a small virtual\nextended formulation.\n","authors":["Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2411.03006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2410.00535v3","updated":"2025-02-11T13:59:11Z","published":"2024-10-01T09:21:29Z","title":"The Causal Information Bottleneck and Optimal Causal Variable\n  Abstractions","summary":"  To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v3.pdf","comment":"Submitted to UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2501.18005v3","updated":"2025-02-11T13:58:39Z","published":"2025-01-29T21:40:32Z","title":"Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces","summary":"  Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets.\n","authors":["Neetha Jambigi","Bartosz Bogacz","Moritz Mueller","Thomas Bach","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2501.18005v3.pdf","comment":"I do not have the necessary approvals to out the paper on Arxiv from\n  my organization yet. I was too soon to do this"},{"id":"http://arxiv.org/abs/2502.07553v1","updated":"2025-02-11T13:41:30Z","published":"2025-02-11T13:41:30Z","title":"Attention Learning is Needed to Efficiently Learn Parity Function","summary":"  Transformers, with their attention mechanisms, have emerged as the\nstate-of-the-art architectures of sequential modeling and empirically\noutperform feed-forward neural networks (FFNNs) across many fields, such as\nnatural language processing and computer vision. However, their generalization\nability, particularly for low-sensitivity functions, remains less studied. We\nbridge this gap by analyzing transformers on the $k$-parity problem. Daniely\nand Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7\n\\log k)$ parameters can learn $k$-parity, where the input length $n$ is\ntypically much larger than $k$. In this paper, we prove that FFNNs require at\nleast $\\Omega(n)$ parameters to learn $k$-parity, while transformers require\nonly $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.\nWe further prove that this parameter efficiency cannot be achieved with fixed\nattention heads. Our work establishes transformers as theoretically superior to\nFFNNs in learning parity function, showing how their attention mechanisms\nenable parameter-efficient generalization in functions with low sensitivity.\n","authors":["Yaomengxi Han","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2502.07553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07551v1","updated":"2025-02-11T13:40:15Z","published":"2025-02-11T13:40:15Z","title":"Early Stopping Against Label Noise Without Validation Data","summary":"  Early stopping methods in deep learning face the challenge of balancing the\nvolume of training and validation data, especially in the presence of label\nnoise. Concretely, sparing more data for validation from training data would\nlimit the performance of the learned model, yet insufficient validation data\ncould result in a sub-optimal selection of the desired model. In this paper, we\npropose a novel early stopping method called Label Wave, which does not require\nvalidation data for selecting the desired model in the presence of label noise.\nIt works by tracking the changes in the model's predictions on the training set\nduring the training process, aiming to halt training before the model unduly\nfits mislabeled data. This method is empirically supported by our observation\nthat minimum fluctuations in predictions typically occur at the training epoch\nbefore the model excessively fits mislabeled data. Through extensive\nexperiments, we show both the effectiveness of the Label Wave method across\nvarious settings and its capability to enhance the performance of existing\nmethods for learning with noisy labels.\n","authors":["Suqin Yuan","Lei Feng","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07551v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2502.07549v1","updated":"2025-02-11T13:39:35Z","published":"2025-02-11T13:39:35Z","title":"HGTUL: A Hypergraph-based Model For Trajectory User Linking","summary":"  Trajectory User Linking (TUL), which links anonymous trajectories with users\nwho generate them, plays a crucial role in modeling human mobility. Despite\nsignificant advancements in this field, existing studies primarily neglect the\nhigh-order inter-trajectory relationships, which represent complex associations\namong multiple trajectories, manifested through multi-location co-occurrence\npatterns emerging when trajectories intersect at various Points of Interest\n(POIs). Furthermore, they also overlook the variable influence of POIs on\ndifferent trajectories, as well as the user class imbalance problem caused by\ndisparities in user activity levels and check-in frequencies. To address these\nlimitations, we propose a novel HyperGraph-based multi-perspective Trajectory\nUser Linking model (HGTUL). Our model learns trajectory representations from\nboth relational and spatio-temporal perspectives: (1) it captures high-order\nassociations among trajectories by constructing a trajectory hypergraph and\nleverages a hypergraph attention network to learn the variable impact of POIs\non trajectories; (2) it models the spatio-temporal characteristics of\ntrajectories by incorporating their temporal and spatial information into a\nsequential encoder. Moreover, we design a data balancing method to effectively\naddress the user class imbalance problem and experimentally validate its\nsignificance in TUL. Extensive experiments on three real-world datasets\ndemonstrate that HGTUL outperforms state-of-the-art baselines, achieving\nimprovements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,\nrespectively.\n","authors":["Fengjie Chang","Xinning Zhu","Zheng Hu","Yang Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07549v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.01175v4","updated":"2025-02-11T13:35:23Z","published":"2024-06-03T10:14:32Z","title":"NeoRL: Efficient Exploration for Nonepisodic RL","summary":"  We study the problem of nonepisodic reinforcement learning (RL) for nonlinear\ndynamical systems, where the system dynamics are unknown and the RL agent has\nto learn from a single trajectory, i.e., without resets. We propose Nonepisodic\nOptimistic RL (NeoRL), an approach based on the principle of optimism in the\nface of uncertainty. NeoRL uses well-calibrated probabilistic models and plans\noptimistically w.r.t. the epistemic uncertainty about the unknown dynamics.\nUnder continuity and bounded energy assumptions on the system, we provide a\nfirst-of-its-kind regret bound of $O(\\Gamma_T \\sqrt{T})$ for general nonlinear\nsystems with Gaussian process dynamics. We compare NeoRL to other baselines on\nseveral deep RL environments and empirically demonstrate that NeoRL achieves\nthe optimal average cost while incurring the least regret.\n","authors":["Bhavya Sukhija","Lenart Treven","Florian Drfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2406.01175v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07547v1","updated":"2025-02-11T13:34:09Z","published":"2025-02-11T13:34:09Z","title":"Instance-dependent Early Stopping","summary":"  In machine learning practice, early stopping has been widely used to\nregularize models and can save computational costs by halting the training\nprocess when the model's performance on a validation set stops improving.\nHowever, conventional early stopping applies the same stopping criterion to all\ninstances without considering their individual learning statuses, which leads\nto redundant computations on instances that are already well-learned. To\nfurther improve the efficiency, we propose an Instance-dependent Early Stopping\n(IES) method that adapts the early stopping mechanism from the entire training\nset to the instance level, based on the core principle that once the model has\nmastered an instance, the training on it should stop. IES considers an instance\nas mastered if the second-order differences of its loss value remain within a\nsmall range around zero. This offers a more consistent measure of an instance's\nlearning status compared with directly using the loss value, and thus allows\nfor a unified threshold to determine when an instance can be excluded from\nfurther backpropagation. We show that excluding mastered instances from\nbackpropagation can increase the gradient norms, thereby accelerating the\ndecrease of the training loss and speeding up the training process. Extensive\nexperiments on benchmarks demonstrate that IES method can reduce\nbackpropagation instances by 10%-50% while maintaining or even slightly\nimproving the test accuracy and transfer learning performance of a model.\n","authors":["Suqin Yuan","Runqi Lin","Lei Feng","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07547v1.pdf","comment":"Accepted by ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.14281v2","updated":"2025-02-11T13:28:10Z","published":"2024-10-18T08:38:12Z","title":"PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models","summary":"  Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec.\n","authors":["Tonglong Wei","Yan Lin","Youfang Lin","Shengnan Guo","Jilin Hu","Haitao Yuan","Gao Cong","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.14281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01567v2","updated":"2025-02-11T13:27:59Z","published":"2024-05-24T18:39:20Z","title":"MeMo: Meaningful, Modular Controllers via Noise Injection","summary":"  Robots are often built from standardized assemblies, (e.g. arms, legs, or\nfingers), but each robot must be trained from scratch to control all the\nactuators of all the parts together. In this paper we demonstrate a new\napproach that takes a single robot and its controller as input and produces a\nset of modular controllers for each of these assemblies such that when a new\nrobot is built from the same parts, its control can be quickly learned by\nreusing the modular controllers. We achieve this with a framework called MeMo\nwhich learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a\nnovel modularity objective to learn an appropriate division of labor among the\nmodules. We demonstrate that this objective can be optimized simultaneously\nwith standard behavior cloning loss via noise injection. We benchmark our\nframework in locomotion and grasping environments on simple to complex robot\nmorphology transfer. We also show that the modules help in task transfer. On\nboth structure and task transfer, MeMo achieves improved training efficiency to\ngraph neural network and Transformer baselines.\n","authors":["Megan Tjandrasuwita","Jie Xu","Armando Solar-Lezama","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2407.01567v2.pdf","comment":"NeurIPS 2024; 29 pages, 21 figures"},{"id":"http://arxiv.org/abs/2402.01138v4","updated":"2025-02-11T13:18:57Z","published":"2024-02-02T04:30:58Z","title":"Graph Neural Networks in EEG-based Emotion Recognition: A Survey","summary":"  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n","authors":["Chenyu Liu","Xinliang Zhou","Yihao Wu","Ruizhi Yang","Zhongruo Wang","Liming Zhai","Ziyu Jia","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01138v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v1","updated":"2025-02-11T13:15:16Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v1","updated":"2025-02-11T13:11:59Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07529v1","updated":"2025-02-11T13:10:34Z","published":"2025-02-11T13:10:34Z","title":"Training Deep Learning Models with Norm-Constrained LMOs","summary":"  In this work, we study optimization methods that leverage the linear\nminimization oracle (LMO) over a norm-ball. We propose a new stochastic family\nof algorithms that uses the LMO to adapt to the geometry of the problem and,\nperhaps surprisingly, show that they can be applied to unconstrained problems.\nThe resulting update rule unifies several existing optimization methods under a\nsingle framework. Furthermore, we propose an explicit choice of norm for deep\narchitectures, which, as a side benefit, leads to the transferability of\nhyperparameters across model sizes. Experimentally, we demonstrate significant\nspeedups on nanoGPT training without any reliance on Adam. The proposed method\nis memory-efficient, requiring only one set of model weights and one set of\ngradients, which can be stored in half-precision.\n","authors":["Thomas Pethick","Wanyun Xie","Kimon Antonakopoulos","Zhenyu Zhu","Antonio Silveti-Falls","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.07529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07528v1","updated":"2025-02-11T13:09:09Z","published":"2025-02-11T13:09:09Z","title":"Forecasting the future development in quality and value of professional\n  football players for applications in team management","summary":"  Transfers in professional football (soccer) are risky investments because of\nthe large transfer fees and high risks involved. Although data-driven models\ncan be used to improve transfer decisions, existing models focus on describing\nplayers' historical progress, leaving their future performance unknown.\nMoreover, recent developments have called for the use of explainable models\ncombined with uncertainty quantification of predictions. This paper assesses\nexplainable machine learning models based on predictive accuracy and\nuncertainty quantification methods for the prediction of the future development\nin quality and transfer value of professional football players. Using a\nhistorical data set of data-driven indicators describing player quality and the\ntransfer value of a football player, the models are trained to forecast player\nquality and player value one year ahead. These two prediction problems\ndemonstrate the efficacy of tree-based models, particularly random forest and\nXGBoost, in making accurate predictions. In general, the random forest model is\nfound to be the most suitable model because it provides accurate predictions as\nwell as an uncertainty quantification method that naturally arises from the\nbagging procedure of the random forest model. Additionally, our research shows\nthat the development of player performance contains nonlinear patterns and\ninteractions between variables, and that time series information can provide\nuseful information for the modeling of player performance metrics. Our research\nprovides models to help football clubs make more informed, data-driven transfer\ndecisions by forecasting player quality and transfer value.\n","authors":["Koen W. van Arem","Floris Goes-Smit","Jakob Shl"],"pdf_url":"https://arxiv.org/pdf/2502.07528v1.pdf","comment":"The article itself is on the pages 1-27. The data set used in this\n  article is described in the appendix at the pages 28-35"},{"id":"http://arxiv.org/abs/2502.07527v1","updated":"2025-02-11T13:08:03Z","published":"2025-02-11T13:08:03Z","title":"NatureLM: Deciphering the Language of Nature for Scientific Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v1.pdf","comment":"81 pages"},{"id":"http://arxiv.org/abs/2502.07523v1","updated":"2025-02-11T12:55:32Z","published":"2025-02-11T12:55:32Z","title":"Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization","summary":"  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n","authors":["Daniel Palenicek","Florian Vogt","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.07523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11107v2","updated":"2025-02-11T12:51:40Z","published":"2024-07-15T15:22:52Z","title":"Latent Linear Quadratic Regulator for Robotic Control Tasks","summary":"  Model predictive control (MPC) has played a more crucial role in various\nrobotic control tasks, but its high computational requirements are concerning,\nespecially for nonlinear dynamical models. This paper presents a\n$\\textbf{la}$tent $\\textbf{l}$inear $\\textbf{q}$uadratic $\\textbf{r}$egulator\n(LaLQR) that maps the state space into a latent space, on which the dynamical\nmodel is linear and the cost function is quadratic, allowing the efficient\napplication of LQR. We jointly learn this alternative system by imitating the\noriginal MPC. Experiments show LaLQR's superior efficiency and generalization\ncompared to other baselines.\n","authors":["Yuan Zhang","Shaohui Yang","Toshiyuki Ohtsuka","Colin Jones","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.11107v2.pdf","comment":"Accepted at RSS 2024 workshop on Koopman Operators in Robotics"},{"id":"http://arxiv.org/abs/2501.02737v2","updated":"2025-02-11T12:38:35Z","published":"2025-01-06T03:11:12Z","title":"Holistic Semantic Representation for Navigational Trajectory Generation","summary":"  Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.\n","authors":["Ji Cao","Tongya Zheng","Qinghong Guo","Yu Wang","Junshu Dai","Shunyu Liu","Jie Yang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.02737v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07516v1","updated":"2025-02-11T12:36:00Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07514v1","updated":"2025-02-11T12:33:33Z","published":"2025-02-11T12:33:33Z","title":"A Near-optimal, Scalable and Corruption-tolerant Framework for\n  Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond","summary":"  We investigate various stochastic bandit problems in the presence of\nadversarial corruption. A seminal contribution to this area is the\nBARBAR~\\citep{gupta2019better} algorithm, which is both simple and efficient,\ntolerating significant levels of corruption with nearly no degradation in\nperformance. However, its regret upper bound exhibits a complexity of $O(KC)$,\nwhile the lower bound is $\\Omega(C)$. In this paper, we enhance the BARBAR\nalgorithm by proposing a novel framework called BARBAT, which eliminates the\nfactor of $K$ and achieves an optimal regret bound up to a logarithmic factor.\nWe also demonstrate how BARBAT can be extended to various settings, including\ngraph bandits, combinatorial semi-bandits, batched bandits and multi-agent\nbandits. In comparison to the Follow-The-Regularized-Leader (FTRL) family of\nmethods, which provide a best-of-both-worlds guarantee, our approach is more\nefficient and parallelizable. Notably, FTRL-based methods face challenges in\nscaling to batched and multi-agent settings.\n","authors":["Zicheng Hu","Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07799v3","updated":"2025-02-11T12:32:59Z","published":"2023-10-11T18:32:21Z","title":"Domain-invariant Clinical Representation Learning by Bridging Data\n  Distribution Shift across EMR Datasets","summary":"  Emerging diseases present challenges in symptom recognition and timely\nclinical intervention due to limited available information. An effective\nprognostic model could assist physicians in making accurate diagnoses and\ndesigning personalized treatment plans to prevent adverse outcomes. However, in\nthe early stages of disease emergence, several factors hamper model\ndevelopment: limited data collection, insufficient clinical experience, and\nprivacy and ethical concerns restrict data availability and complicate accurate\nlabel assignment. Furthermore, Electronic Medical Record (EMR) data from\ndifferent diseases or sources often exhibit significant cross-dataset feature\nmisalignment, severely impacting the effectiveness of deep learning models. We\npresent a domain-invariant representation learning method that constructs a\ntransition model between source and target datasets. By constraining the\ndistribution shift of features generated across different domains, we capture\ndomain-invariant features specifically relevant to downstream tasks, developing\na unified domain-invariant encoder that achieves better feature representation\nacross various task domains. Experimental results across multiple target tasks\ndemonstrate that our proposed model surpasses competing baseline methods and\nachieves faster training convergence, particularly when working with limited\ndata. Extensive experiments validate our method's effectiveness in providing\nmore accurate predictions for emerging pandemics and other diseases. Code is\npublicly available at https://github.com/wang1yuhang/domain_invariant_network.\n","authors":["Zhongji Zhang","Yuhang Wang","Yinghao Zhu","Xinyu Ma","Yasha Wang","Junyi Gao","Liantao Ma","Wen Tang","Xiaoyun Zhang","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17472v4","updated":"2025-02-11T12:29:00Z","published":"2024-02-27T12:53:15Z","title":"RAGFormer: Learning Semantic Attributes and Topological Structure for\n  Fraud Detection","summary":"  Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.\n","authors":["Haolin Li","Shuyang Jiang","Lifeng Zhang","Siyuan Du","Guangnan Ye","Hongfeng Chai"],"pdf_url":"https://arxiv.org/pdf/2402.17472v4.pdf","comment":"Preprint"}]},"2025-02-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08638v1","updated":"2025-02-12T18:54:37Z","published":"2025-02-12T18:54:37Z","title":"Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples","summary":"  The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.\n","authors":["Andrianos Michail","Simon Clematide","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2502.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04354v3","updated":"2025-02-12T18:24:34Z","published":"2023-11-07T21:27:17Z","title":"Uncovering Intermediate Variables in Transformers using Circuit Probing","summary":"  Neural network models have achieved high performance on a wide variety of\ncomplex tasks, but the algorithms that they implement are notoriously difficult\nto interpret. It is often necessary to hypothesize intermediate variables\ninvolved in a network's computation in order to understand these algorithms.\nFor example, does a language model depend on particular syntactic properties\nwhen generating a sentence? Yet, existing analysis tools make it difficult to\ntest hypotheses of this type. We propose a new analysis technique - circuit\nprobing - that automatically uncovers low-level circuits that compute\nhypothesized intermediate variables. This enables causal analysis through\ntargeted ablation at the level of model parameters. We apply this method to\nmodels trained on simple arithmetic tasks, demonstrating its effectiveness at\n(1) deciphering the algorithms that models have learned, (2) revealing modular\nstructure within a model, and (3) tracking the development of circuits over\ntraining. Across these three experiments we demonstrate that circuit probing\ncombines and extends the capabilities of existing methods, providing one\nunified approach for a variety of analyses. Finally, we demonstrate circuit\nprobing on a real-world use case: uncovering circuits that are responsible for\nsubject-verb agreement and reflexive anaphora in GPT2-Small and Medium.\n","authors":["Michael A. Lepori","Thomas Serre","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2311.04354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08599v1","updated":"2025-02-12T17:38:27Z","published":"2025-02-12T17:38:27Z","title":"SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent","summary":"  Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.\n","authors":["Keyeun Lee","Seo Hyeong Kim","Seolhee Lee","Jinsu Eun","Yena Ko","Hayeon Jeon","Esther Hehsun Kim","Seonghye Cho","Soeun Yang","Eun-mee Kim","Hajin Lim"],"pdf_url":"https://arxiv.org/pdf/2502.08599v1.pdf","comment":"21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main"},{"id":"http://arxiv.org/abs/2407.07313v3","updated":"2025-02-12T17:20:56Z","published":"2024-07-10T02:20:19Z","title":"ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.\n","authors":["Benjamin G. Ascoli","Yasoda Sai Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08561v1","updated":"2025-02-12T16:49:52Z","published":"2025-02-12T16:49:52Z","title":"Quality-Aware Decoding: Unifying Quality Estimation and Decoding","summary":"  An emerging research direction in NMT involves the use of Quality Estimation\n(QE) models, which have demonstrated high correlations with human judgment and\ncan enhance translations through Quality-Aware Decoding. Although several\napproaches have been proposed based on sampling multiple candidate\ntranslations, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (upto $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal.\n","authors":["Sai Koneru","Matthias Huck","Miriam Exel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2502.08561v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08550v1","updated":"2025-02-12T16:31:21Z","published":"2025-02-12T16:31:21Z","title":"LLMs can implicitly learn from mistakes in-context","summary":"  Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.\n","authors":["Lisa Alazraki","Maximilian Mozes","Jon Ander Campos","Yi Chern Tan","Marek Rei","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2502.08550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06766v2","updated":"2025-02-12T15:55:37Z","published":"2025-02-10T18:47:04Z","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs","summary":"  There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard).\n","authors":["Ryan Synk","Monte Hoover","John Kirchenbauer","Neel Jain","Alex Stein","Manli Shu","Josue Melendez Sanchez","Ramani Duraiswami","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.06766v2.pdf","comment":"9 pages, 9 figures, 2 tables in main body"},{"id":"http://arxiv.org/abs/2502.08514v1","updated":"2025-02-12T15:46:50Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08512v1","updated":"2025-02-12T15:46:34Z","published":"2025-02-12T15:46:34Z","title":"Measuring Diversity in Synthetic Datasets","summary":"  Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.\n","authors":["Yuchang Zhu","Huizhe Zhang","Bingzhe Wu","Jintang Li","Zibin Zheng","Peilin Zhao","Liang Chen","Yatao Bian"],"pdf_url":"https://arxiv.org/pdf/2502.08512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08507v1","updated":"2025-02-12T15:41:43Z","published":"2025-02-12T15:41:43Z","title":"Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction","summary":"  Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples.\n","authors":["Wei Li","Wen Luo","Guangyue Peng","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08507v1.pdf","comment":"Accepted by NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.08489v1","updated":"2025-02-12T15:26:08Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc Pmies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","Jos Javier Saiz","Ferran Espua","Jaume Prats","Javier Aula-Blasco","Mario Mina","Adrin Rubio","Alexander Shvets","Anna Salls","Iaki Lacunza","Iigo Pikabea","Jorge Palomar","Jlia Falco","Luca Tormo","Luis Vasquez-Reina","Montserrat Marimon","Valle Ruz-Fernndez","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.08468v1","updated":"2025-02-12T15:03:33Z","published":"2025-02-12T15:03:33Z","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data","summary":"  Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08458v1","updated":"2025-02-12T14:53:04Z","published":"2025-02-12T14:53:04Z","title":"Examining Spanish Counseling with MIDAS: a Motivational Interviewing\n  Dataset in Spanish","summary":"  Cultural and language factors significantly influence counseling, but Natural\nLanguage Processing research has not yet examined whether the findings of\nconversational analysis for counseling conducted in English apply to other\nlanguages. This paper presents a first step towards this direction. We\nintroduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling\ndataset created from public video sources that contains expert annotations for\ncounseling reflections and questions. Using this dataset, we explore\nlanguage-based differences in counselor behavior in English and Spanish and\ndevelop classifiers in monolingual and multilingual settings, demonstrating its\napplications in counselor behavioral coding tasks.\n","authors":["Aylin Gunal","Bowen Yi","John Piette","Rada Mihalcea","Vernica Prez-Rosas"],"pdf_url":"https://arxiv.org/pdf/2502.08458v1.pdf","comment":"To appear in NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2408.02416v2","updated":"2025-02-12T14:52:56Z","published":"2024-08-05T12:20:39Z","title":"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models","summary":"  The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval.\n","authors":["Zi Liang","Haibo Hu","Qingqing Ye","Yaxin Xiao","Haoyang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02416v2.pdf","comment":"Source Code: https://github.com/liangzid/PromptExtractionEval"},{"id":"http://arxiv.org/abs/2402.17400v2","updated":"2025-02-12T14:46:43Z","published":"2024-02-27T10:47:24Z","title":"Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications","summary":"  Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains.\n","authors":["aatay Yldz","Nishaanth Kanna Ravichandran","Nitin Sharma","Matthias Bethge","Beyza Ermis"],"pdf_url":"https://arxiv.org/pdf/2402.17400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08450v1","updated":"2025-02-12T14:41:20Z","published":"2025-02-12T14:41:20Z","title":"Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated\n  Essay Scoring","summary":"  In automated essay scoring (AES), recent efforts have shifted toward\ncross-prompt settings that score essays on unseen prompts for practical\napplicability. However, prior methods trained with essay-score pairs of\nspecific prompts pose challenges in obtaining prompt-generalized essay\nrepresentation. In this work, we propose a grammar-aware cross-prompt trait\nscoring (GAPS), which internally captures prompt-independent syntactic aspects\nto learn generic essay representation. We acquire grammatical error-corrected\ninformation in essays via the grammar error correction technique and design the\nAES model to seamlessly integrate such information. By internally referring to\nboth the corrected and the original essays, the model can focus on generic\nfeatures during training. Empirical experiments validate our method's\ngeneralizability, showing remarkable improvements in prompt-independent and\ngrammar-related traits. Furthermore, GAPS achieves notable QWK gains in the\nmost challenging cross-prompt scenario, highlighting its strength in evaluating\nunseen prompts.\n","authors":["Heejin Do","Taehee Park","Sangwon Ryu","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08450v1.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v1","updated":"2025-02-12T14:32:17Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2412.01621v2","updated":"2025-02-12T14:03:19Z","published":"2024-12-02T15:41:47Z","title":"NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers","summary":"  Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.\n","authors":["Angel Yahir Loredo Lopez","Tyler McDonald","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2412.01621v2.pdf","comment":"5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award"},{"id":"http://arxiv.org/abs/2502.08415v1","updated":"2025-02-12T13:58:42Z","published":"2025-02-12T13:58:42Z","title":"A Semantic Parsing Algorithm to Solve Linear Ordering Problems","summary":"  We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs.\n","authors":["Maha Alkhairy","Vincent Homer","Brendan O'Connor"],"pdf_url":"https://arxiv.org/pdf/2502.08415v1.pdf","comment":"3 figures, 9 pages main paper and 6 pages references and appendix"},{"id":"http://arxiv.org/abs/2502.08395v1","updated":"2025-02-12T13:37:03Z","published":"2025-02-12T13:37:03Z","title":"IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance","summary":"  Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.\n","authors":["Paul Rttger","Musashi Hinck","Valentin Hofmann","Kobi Hackenburg","Valentina Pyatkin","Faeze Brahman","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2502.08395v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.08371v1","updated":"2025-02-12T13:03:43Z","published":"2025-02-12T13:03:43Z","title":"Unveiling Global Discourse Structures: Theoretical Analysis and NLP\n  Applications in Argument Mining","summary":"  Particularly in the structure of global discourse, coherence plays a pivotal\nrole in human text comprehension and is a hallmark of high-quality text. This\nis especially true for persuasive texts, where coherent argument structures\nsupport claims effectively. This paper discusses and proposes methods for\ndetecting, extracting and representing these global discourse structures in a\nproccess called Argument(ation) Mining. We begin by defining key terms and\nprocesses of discourse structure analysis, then continue to summarize existing\nresearch on the matter, and identify shortcomings in current argument component\nextraction and classification methods. Furthermore, we will outline an\narchitecture for argument mining that focuses on making models more\ngeneralisable while overcoming challenges in the current field of research by\nutilizing novel NLP techniques. This paper reviews current knowledge,\nsummarizes recent works, and outlines our NLP pipeline, aiming to contribute to\nthe theoretical understanding of global discourse structures.\n","authors":["Christopher van Le"],"pdf_url":"https://arxiv.org/pdf/2502.08371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v2","updated":"2025-02-12T13:03:09Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v2.pdf","comment":"accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08363v1","updated":"2025-02-12T12:50:15Z","published":"2025-02-12T12:50:15Z","title":"Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding","summary":"  The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.\n","authors":["Konstantin Berestizshevsky","Renzo Andri","Lukas Cavigelli"],"pdf_url":"https://arxiv.org/pdf/2502.08363v1.pdf","comment":"8 pages, 11 figures, work under submission"},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.08356v1","updated":"2025-02-12T12:39:51Z","published":"2025-02-12T12:39:51Z","title":"Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.\n","authors":["Kushagra Bhushan","Yatin Nandwani","Dinesh Khandelwal","Sonam Gupta","Gaurav Pandey","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08356v1.pdf","comment":"22 pages, 14 tables, to be published in NAACL 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08323v1","updated":"2025-02-12T11:44:19Z","published":"2025-02-12T11:44:19Z","title":"Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning","summary":"  Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures.\n","authors":["Barnaby Schmitt","Alistair Grosvenor","Matthias Cunningham","Clementine Walsh","Julius Pembrokeshire","Jonathan Teel"],"pdf_url":"https://arxiv.org/pdf/2502.08323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10827v3","updated":"2025-02-12T11:41:16Z","published":"2024-12-14T13:12:50Z","title":"Rethinking Chain-of-Thought from the Perspective of Self-Training","summary":"  Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.\n","authors":["Zongqian Wu","Baoduo Xu","Ruochen Cui","Mengmeng Zhan","Xiaofeng Zhu","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10827v3.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08319v1","updated":"2025-02-12T11:35:20Z","published":"2025-02-12T11:35:20Z","title":"MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection","summary":"  Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1.\n","authors":["Lubna Al-Henaki","Hend Al-Khalifa","Abdulmalik Al-Salman","Hajar Alqubayshi","Hind Al-Twailay","Gheeda Alghamdi","Hawra Aljasim"],"pdf_url":"https://arxiv.org/pdf/2502.08319v1.pdf","comment":"12 pages, 3 figuers, 4 tabels"},{"id":"http://arxiv.org/abs/2502.08317v1","updated":"2025-02-12T11:32:19Z","published":"2025-02-12T11:32:19Z","title":"Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting","summary":"  Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.\n","authors":["Jiarui Wu","Zhuo Liu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.08317v1.pdf","comment":"19 pages, accepted to NAACL Findings"},{"id":"http://arxiv.org/abs/2502.08312v1","updated":"2025-02-12T11:30:28Z","published":"2025-02-12T11:30:28Z","title":"Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs","summary":"  This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.\n","authors":["Tanguy Cazalets","Joni Dambre"],"pdf_url":"https://arxiv.org/pdf/2502.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08301v1","updated":"2025-02-12T11:02:59Z","published":"2025-02-12T11:02:59Z","title":"Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks","summary":"  Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.\n","authors":["Laurne Vaugrante","Francesca Carlon","Maluna Menke","Thilo Hagendorff"],"pdf_url":"https://arxiv.org/pdf/2502.08301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08298v1","updated":"2025-02-12T10:58:57Z","published":"2025-02-12T10:58:57Z","title":"Improving Existing Optimization Algorithms with LLMs","summary":"  The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/\n","authors":["Camilo Chacn Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2502.08298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03824v2","updated":"2025-02-12T10:45:25Z","published":"2025-02-06T07:19:59Z","title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","summary":"  LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.\n","authors":["Minsang Kim","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2502.03824v2.pdf","comment":"the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted"},{"id":"http://arxiv.org/abs/2311.17696v7","updated":"2025-02-12T10:45:02Z","published":"2023-11-29T15:02:46Z","title":"How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)","summary":"  Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts.\n","authors":["Chenxi Dong","Yimin Yuan","Kan Chen","Shupei Cheng","Chujie Wen"],"pdf_url":"https://arxiv.org/pdf/2311.17696v7.pdf","comment":"6 pages, 6 figures, ICEIT 2025"},{"id":"http://arxiv.org/abs/2502.08281v1","updated":"2025-02-12T10:38:22Z","published":"2025-02-12T10:38:22Z","title":"Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification","summary":"  Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs.\n","authors":["Jipeng Qiang","Minjiang Huang","Yi Zhu","Yunhao Yuan","Chaowei Zhang","Kui Yu"],"pdf_url":"https://arxiv.org/pdf/2502.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08279v1","updated":"2025-02-12T10:36:55Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.02873 by other authors"},{"id":"http://arxiv.org/abs/2502.08266v1","updated":"2025-02-12T10:19:50Z","published":"2025-02-12T10:19:50Z","title":"Dealing with Annotator Disagreement in Hate Speech Classification","summary":"  Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.\n","authors":["Somaiyeh Dehghan","Mehmet Umut Sen","Berrin Yanikoglu"],"pdf_url":"https://arxiv.org/pdf/2502.08266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08265v1","updated":"2025-02-12T10:17:18Z","published":"2025-02-12T10:17:18Z","title":"Exploring the Potential of Large Language Models to Simulate Personality","summary":"  With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.\n","authors":["Maria Molchanova","Anna Mikhailova","Anna Korzanova","Lidiia Ostyakova","Alexandra Dolidze"],"pdf_url":"https://arxiv.org/pdf/2502.08265v1.pdf","comment":"Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on\n  EMNLP2024"},{"id":"http://arxiv.org/abs/2407.15588v5","updated":"2025-02-12T09:50:02Z","published":"2024-07-22T12:25:48Z","title":"Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple\n  Matching with Entity and Relation Texts","summary":"  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n","authors":["Soojin Yoon","Sungho Ko","Tongyoung Kim","SeongKu Kang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15588v5.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2502.08246v1","updated":"2025-02-12T09:39:54Z","published":"2025-02-12T09:39:54Z","title":"Inference-time sparse attention with asymmetric indexing","summary":"  Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compliant vector search algorithms, yet the standard partitioning\nmethods yield poor results in this context, because (1) keys and queries follow\ndifferent distributions and (2) the effect of RoPE positional encoding.\n  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),\nwhich overcomes these problems. It is an asymmetrical indexing technique that\nemploys distinct partitions for keys and queries, thereby approximating\nself-attention with a data-adaptive sparsity pattern.\n  It works on pretrained language models without finetuning, as it only\nrequires to train (offline) a small query classifier. On a long context Llama\n3.1-8b model, with sequences ranging from 100k to 500k tokens, our method\ntypically reduces by a factor 20 the fraction of memory that needs to be\nlooked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.\n","authors":["Pierre-Emmanuel Mazar","Gergely Szilvasy","Maria Lomeli","Francisco Massa","Naila Murray","Herv Jgou","Matthijs Douze"],"pdf_url":"https://arxiv.org/pdf/2502.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08213v1","updated":"2025-02-12T08:48:55Z","published":"2025-02-12T08:48:55Z","title":"LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention","summary":"  In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.\n","authors":["Konstantin Kolomeitsev"],"pdf_url":"https://arxiv.org/pdf/2502.08213v1.pdf","comment":"Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules"},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10967v2","updated":"2025-02-12T08:10:06Z","published":"2025-01-19T07:00:46Z","title":"Advancing General Multimodal Capability of Vision-language Models with\n  Pyramid-descent Visual Position Encoding","summary":"  Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.\n","authors":["Zhanpeng Chen","Mingxiao Li","Ziyang Chen","Nan Du","Xiaolong Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2501.10967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02385v2","updated":"2025-02-12T07:55:10Z","published":"2025-01-04T21:23:36Z","title":"Guiding Medical Vision-Language Models with Explicit Visual Prompts:\n  Framework Design and Comprehensive Exploration of Prompt Variations","summary":"  While mainstream vision-language models (VLMs) have advanced rapidly in\nunderstanding image level information, they still lack the ability to focus on\nspecific areas designated by humans. Rather, they typically rely on large\nvolumes of high-quality image-text paired data to learn and generate posterior\nattention maps. To address this critical issue, we propose leveraging visual\nprompts:simple visual markers in various forms to guide and enhance the\nformation of region-specific attention. Thus, we introduce MedVP, a pioneering\nframework that integrates medical entity extraction, visual prompt generation,\nand dataset adaptation for visual prompt guided fine-tuning. We successfully\noutperform recent state-of-the-art large models across multiple medical VQA\ndatasets. Extensive experiments and Human evaluation are conducted to analyze\nthe impact of different visual prompt forms and how they contribute to\nperformance improvement. The results demonstrate both the effectiveness and\nclinical significance of our approach.\n","authors":["Kangyu Zhu","Ziyuan Qin","Huahui Yi","Zekun Jiang","Qicheng Lao","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2501.02385v2.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08180v1","updated":"2025-02-12T07:37:39Z","published":"2025-02-12T07:37:39Z","title":"Enhancing LLM Character-Level Manipulation via Divide and Conquer","summary":"  Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.\n","authors":["Zhen Xiong","Yujun Cai","Bryan Hooi","Nanyun Peng","Kai-Wei Chang","Zhecheng Li","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08178v1","updated":"2025-02-12T07:32:48Z","published":"2025-02-12T07:32:48Z","title":"ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation","summary":"  While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers.\n","authors":["Ruobing Yao","Yifei Zhang","Shuang Song","Yuhua Liu","Neng Gao","Chenyang Tu"],"pdf_url":"https://arxiv.org/pdf/2502.08178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15151v2","updated":"2025-02-12T07:25:24Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v1","updated":"2025-02-12T07:19:36Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  In the field of synthetic aperture radar (SAR) remote sensing image\ninterpretation, although Vision language models (VLMs) have made remarkable\nprogress in natural language processing and image understanding, their\napplications remain limited in professional domains due to insufficient domain\nexpertise. This paper innovatively proposes the first large-scale multimodal\ndialogue dataset for SAR images, named SARChat-2M, which contains approximately\n2 million high-quality image-text pairs, encompasses diverse scenarios with\ndetailed target annotations. This dataset not only supports several key tasks\nsuch as visual understanding and object detection tasks, but also has unique\ninnovative aspects: this study develop a visual-language dataset and benchmark\nfor the SAR domain, enabling and evaluating VLMs' capabilities in SAR image\ninterpretation, which provides a paradigmatic framework for constructing\nmultimodal datasets across various remote sensing vertical domains. Through\nexperiments on 16 mainstream VLMs, the effectiveness of the dataset has been\nfully verified, and the first multi-task dialogue benchmark in the SAR field\nhas been successfully established. The project will be released at\nhttps://github.com/JimmyMa99/SARChat, aiming to promote the in-depth\ndevelopment and wide application of SAR visual language models.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16821v2","updated":"2025-02-12T06:54:53Z","published":"2024-11-25T17:15:41Z","title":"KL-geodesics flow matching with a novel sampling scheme","summary":"  Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.\n","authors":["Egor Sevriugov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2411.16821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00784v2","updated":"2025-02-12T06:34:11Z","published":"2024-10-17T06:44:18Z","title":"FIRE: Fact-checking with Iterative Retrieval and Verification","summary":"  Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.\n","authors":["Zhuohan Xie","Rui Xing","Yuxia Wang","Jiahui Geng","Hasan Iqbal","Dhruv Sahnan","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2411.00784v2.pdf","comment":"4 figures, 8 tables, accepted to Findings of NAACL"},{"id":"http://arxiv.org/abs/2502.05206v2","updated":"2025-02-12T06:16:00Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v2.pdf","comment":"47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2501.09997v2","updated":"2025-02-12T06:15:17Z","published":"2025-01-17T07:30:01Z","title":"Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models","summary":"  Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.\n","authors":["Qiang Liu","Xinlong Chen","Yue Ding","Shizhen Xu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07316v2","updated":"2025-02-12T05:58:21Z","published":"2025-02-11T07:26:50Z","title":"CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction","summary":"  Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.\n","authors":["Junlong Li","Daya Guo","Dejian Yang","Runxin Xu","Yu Wu","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2502.07316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08141v1","updated":"2025-02-12T05:48:26Z","published":"2025-02-12T05:48:26Z","title":"LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits","summary":"  Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.\n","authors":["Zikai Zhou","Qizheng Zhang","Hermann Kumbong","Kunle Olukotun"],"pdf_url":"https://arxiv.org/pdf/2502.08141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15504v2","updated":"2025-02-12T05:45:21Z","published":"2024-12-20T02:35:39Z","title":"Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework","summary":"  Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA.\n","authors":["Zhenjie Xu","Wenqing Chen","Yi Tang","Xuanying Li","Cheng Hu","Zhixuan Chu","Kui Ren","Zibin Zheng","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15504v2.pdf","comment":"This work has been accepted at The 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2407.18418v3","updated":"2025-02-12T05:42:09Z","published":"2024-07-25T22:31:50Z","title":"Know Your Limits: A Survey of Abstention in Large Language Models","summary":"  Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future research, such as\nwhether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, and opportunities to optimize abstention abilities\nin specific contexts. In doing so, we aim to broaden the scope and impact of\nabstention methodologies in AI systems.\n","authors":["Bingbing Wen","Jihan Yao","Shangbin Feng","Chenjun Xu","Yulia Tsvetkov","Bill Howe","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18418v3.pdf","comment":"TACL 2024"},{"id":"http://arxiv.org/abs/2501.14249v3","updated":"2025-02-12T05:39:05Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Tung Nguyen","Daron Anderson","Imad Ali Shah","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Jaeho Lee","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","Robert Gerbicz","John-Clark Levin","Serguei Popov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Mstyslav Kazakov","Geoff Galgon","Johannes Schmitt","Alvaro Sanchez","Yongki Lee","Will Yeadon","Scott Sauers","Marc Roth","Chidozie Agu","Sren Riis","Fabian Giska","Saiteja Utpala","Antrell Cheatom","Zachary Giboney","Gashaw M. Goshu","Sarah-Jane Crowson","Mohinder Maheshbhai Naiya","Noah Burns","Lennart Finke","Zerui Cheng","Hyunwoo Park","Francesco Fournier-Facio","Jennifer Zampese","John Wydallis","John B. Wydallis","Ryan G. Hoerr","Mark Nandor","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Jungbae Nam","Edwin Taylor","Jun Jin","Gautier Abou Loume","Hangrui Cao","Alexis C Garretson","Damien Sileo","Qiuyu Ren","Doru Cojoc","Pavel Arkhipov","Usman Qazi","Aras Bacho","Lianghui Li","Sumeet Motwani","Christian Schroeder de Witt","Alexei Kopylov","Johannes Veith","Eric Singer","Paolo Rissone","Jaehyeok Jin","Jack Wei Lun Shi","Chris G. Willcocks","Ameya Prabhu","Longke Tang","Kevin Zhou","Emily de Oliveira Santos","Andrey Pupasov Maksimov","Edward Vendrow","Kengo Zenitani","Joshua Robinson","Aleksandar Mikov","Julien Guillod","Yuqi Li","Ben Pageler","Joshua Vendrow","Vladyslav Kuchkin","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Andrew Gritsevskiy","Dakotah Martinez","Nick Crispino","Dimitri Zvonkine","Natanael Wildner Fraga","Saeed Soori","Ori Press","Henry Tang","Julian Salazar","Sean R. Green","Lina Brssel","Moon Twayana","Aymeric Dieuleveut","T. Ryan Rogers","Wenjin Zhang","Ross Finocchio","Bikun Li","Jinzhou Yang","Arun Rao","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Ariel Ghislain Kemogne Kamdoum","Tad Hogg","Alvin Jin","Carlo Bosio","Gongbo Sun","Brian P Coppola","Haline Heidinger","Rafael Sayous","Stefan Ivanov","Joseph M Cavanagh","Jiawei Shen","Joseph Marvin Imperial","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Brecht Verbeken","Kelsey Van den Houte","Lynn Van Der Sypt","David Noever","Lisa Schut","Ilia Sucholutsky","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Shankar Sivarajan","Tong Yang","John Maar","Julian Wykowski","Mart Oller","Jennifer Sandlin","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Felipe Meneguitti Dias","Tobias Kreiman","Kaivalya Rawal","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Jeremy Nguyen","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Sergey Ivanov","Rafa Powiata","Chenguang Wang","Daofeng Li","Donato Crisostomi","Ali Dehghan","Andrea Achilleos","John Arnold Ambay","Benjamin Myklebust","Archan Sen","David Perrella","Nurdin Kaparov","Mark H Inlow","Allen Zang","Kalyan Ramakrishnan","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Dan Bar Hava","Aleksey Kuchkin","Robert Lauff","David Holmes","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Daniel Pyda","Zakayo Kazibwe","Mukhwinder Singh","Don Clarke","Dae Hyun Kim","Sara Fish","Veit Elser","Victor Efren Guadarrama Vilchis","Immo Klose","Christoph Demian","Ujjwala Anantheswaran","Adam Zweiger","Guglielmo Albani","Jeffery Li","Nicolas Daans","Maksim Radionov","Vclav Rozho","Vincent Ginis","Ziqiao Ma","Christian Stump","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Marco Piccardo","Niv Cohen","Virendra Singh","Josef Tkadlec","Paul Rosu","Alan Goldfarb","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Aline Menezes","Arkil Patel","Zixuan Wang","Jamie Tucker-Foltz","Jack Stade","Declan Grabb","Tom Goertzen","Fereshteh Kazemi","Jeremiah Milbauer","Abhishek Shukla","Hossam Elgnainy","Yan Carlos Leyva Labrador","Hao He","Ling Zhang","Alan Givr","Hew Wolff","Gzdenur Demir","Muhammad Fayez Aziz","Younesse Kaddar","Ivar ngquist","Yanxu Chen","Elliott Thornley","Robin Zhang","Jiayi Pan","Antonio Terpin","Niklas Muennighoff","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Jainam Shah","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Andrew Ho","Shaul Barkan","Jiaqi Wang","Martin Stehberger","Egor Kretov","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Zaki Hossain","Ido Akov","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Micah Carroll","Orr Paradise","Jan Hendrik Kirchner","Stefan Steinerberger","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Paolo Giordano","Philipp Petersen","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Antonella Pinto","Shreyas Verma","Prashant Joshi","Eli Meril","Zheng-Xin Yong","Allison Tee","Jrmy Androletti","Orion Weller","Raghav Singhal","Gang Zhang","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Hamid Mostaghimi","Kunvar Thaman","Qijia Chen","Tran Quoc Khnh","Jacob Loader","Stefano Cavalleri","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Jonathan Roberts","William Alley","Kunyang Sun","Ryan Stendall","Max Lamparth","Anka Reuel","Ting Wang","Hanmeng Xu","Pablo Hernndez-Cmara","Freddie Martin","Thomas Preu","Tomek Korbak","Marcus Abramovitch","Dominic Williamson","Ida Bosio","Ziye Chen","Bir Blint","Eve J. Y. Lo","Maria Ins S. Nunes","Yibo Jiang","M Saiful Bari","Peyman Kassani","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Guillaume Douville","Daniel Tordera","George Balabanian","Earth Anderson","Lynna Kvistad","Alejandro Jos Moyano","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Isaac C. McAlister","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Ronald Clark","Sherwin Abdoli","Tim Santens","Harrison K Wang","Evan Chen","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Niels Mndler","Avi Semler","Emma Rodman","Jacob Drori","Carl J Fossum","Luk Gloor","Milind Jagota","Ronak Pradeep","Honglu Fan","Tej Shah","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobc","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Siranut Usawasutsakorn","Mohammadreza Mofayezi","Alexander Piperski","Marc Carauleanu","David K. Zhang","Kostiantyn Dobarskyi","Dylan Ler","Roman Leventov","Ignat Soroko","Thorben Jansen","Scott Creighton","Pascal Lauer","Joshua Duersch","Vage Taamazyan","Dario Bezzi","Wiktor Morak","Wenjie Ma","William Held","Tran uc Huy","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Hossein Shahrtash","Edson Oliveira","Joseph W. Jackson","Daniel Espinosa Gonzalez","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Emilien Duc","Bita Golshani","David Stap","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Lukas Lewark","Miguel Orbegozo Rodriguez","Mtys Vincze","Dustin Wehr","Colin Tang","Shaun Phillips","Fortuna Samuele","Jiang Muzhen","Fredrik Ekstrm","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peaflor","Haile Kassahun","Alena Friedrich","Claire Sparrow","Rayner Hernandez Perez","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Samuel Albanie","Will Cai","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Jasdeep Sidhu","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Brian Weber","Harsh Kumar","Tong Jiang","Arunim Agarwal","Chiara Ceconello","Warren S. Vaz","Chao Zhuang","Haon Park","Andrew R. Tawfeek","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Shreen Gul","Gunjan Chhablani","Zhehang Du","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Florencia de la Rosa","Xiuyu Li","Guillaume Malod","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiit Yaln","Gbenga Daniel Obikoya","Luca Arnaboldi"," Rai","Filippo Bigi","M. C. Bosc","Oleg Shumar","Kaniuar Bacho","Pierre Clavier","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Denis Peskoff","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Olle Hggstrm","Emil Verkama","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Yiyang Fan","Gabriel Poesia Reis e Silva","Linwei Xin","Yosi Kratish","Jakub ucki","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Justin Xu","Kevin Joseph Scaria","Freddie Vargus","Farzad Habibi"," Long"," Lian","Emanuele Rodol","Jules Robins","Vincent Cheng","Tony Fruhauff","Brad Raynor","Hao Qi","Xi Jiang","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Sarah Hoback","Rodrigo De Oliveira Pena","Glen Sherman","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Sandra Mendoza","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Ashley Cartwright","Daphiny Pottmaier","Omid Taheri","David Outevsky","Stanley Stepanic","Samuel Perry","Luke Askew","Ral Adrin Huerta Rodrguez","Ali M. R. Minissi","Sam Ali","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Sk Md Salauddin","Murat Islam","Juan Gonzalez","Josh Ducey","Maja Somrak","Vasilios Mavroudis","Eric Vergo","Juehang Qin","Benjmin Borbs","Eric Chu","Jack Lindsey","Anil Radhakrishnan","Antoine Jallon","I. M. J. McInnis","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Javier Gimenez","Roselynn Grace Montecillo","Russell Campbell","Asankhaya Sharma","Khalida Meer","Xavier Alapont","Deepakkumar Patil","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Sergei Bogdanov","Sren Mller","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Innocent Enyekwe","Ragavendran P V","Zienab EL-Wasif","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Song Bian","John Lai","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Alex Hoover","Joseph McGowan","Tejal Patwardhan","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v3.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08130v1","updated":"2025-02-12T05:24:21Z","published":"2025-02-12T05:24:21Z","title":"Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models","summary":"  Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to $4.4$ on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks.\n","authors":["Sonam Gupta","Yatin Nandwani","Asaf Yehudai","Dinesh Khandelwal","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08130v1.pdf","comment":"10 pages, Accepted to NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.07072v2","updated":"2025-02-12T05:14:41Z","published":"2025-02-10T22:07:02Z","title":"IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models","summary":"  Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.\n","authors":["Sayem Mohammad Imtiaz","Astha Singh","Fraol Batole","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2502.07072v2.pdf","comment":"Accepted as full research paper at FSE'2025"},{"id":"http://arxiv.org/abs/2502.08127v1","updated":"2025-02-12T05:13:04Z","published":"2025-02-12T05:13:04Z","title":"Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance","summary":"  Recent advancements in large language models (LLMs) have shown strong general\nreasoning abilities, yet their effectiveness in financial reasoning remains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension, long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improve financial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and\nreinforcement learning with domain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such as multi-table reasoning, long-context processing, and\nfinancial terminology comprehension. All our datasets, models, and codes are\npublicly available. Furthermore, we introduce a leaderboard for benchmarking\nfuture datasets and models.\n","authors":["Lingfei Qian","Weipeng Zhou","Yan Wang","Xueqing Peng","Jimin Huang","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2502.08127v1.pdf","comment":"Ongoing work, 13 pages, 2 figures, 3 Tables"},{"id":"http://arxiv.org/abs/2501.02629v2","updated":"2025-02-12T04:55:19Z","published":"2025-01-05T19:06:03Z","title":"Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense","summary":"  As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher\n","authors":["Yang Ouyang","Hengrui Gu","Shuhang Lin","Wenyue Hua","Jie Peng","Bhavya Kailkhura","Meijun Gao","Tianlong Chen","Kaixiong Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.02629v2.pdf","comment":"14 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2501.04961v2","updated":"2025-02-12T04:52:08Z","published":"2025-01-09T04:26:15Z","title":"Demystifying Domain-adaptive Post-training for Financial LLMs","summary":"  Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.\n","authors":["Zixuan Ke","Yifei Ming","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2501.04961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02936v4","updated":"2025-02-12T04:41:34Z","published":"2024-04-03T04:25:01Z","title":"Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large\n  Language Models","summary":"  The problem of pre-training data detection for large language models (LLMs)\nhas received growing attention due to its implications in critical issues like\ncopyright violation and test data contamination. Despite improved performance,\nexisting methods (including the state-of-the-art, Min-K%) are mostly developed\nupon simple heuristics and lack solid, reasonable foundations. In this work, we\npropose a novel and theoretically motivated methodology for pre-training data\ndetection, named Min-K%++. Specifically, we present a key insight that training\nsamples tend to be local maxima of the modeled distribution along each input\ndimension through maximum likelihood training, which in turn allow us to\ninsightfully translate the problem into identification of local maxima. Then,\nwe design our method accordingly that works under the discrete distribution\nmodeled by LLMs, whose core idea is to determine whether the input forms a mode\nor has relatively high probability under the conditional categorical\ndistribution. Empirically, the proposed method achieves new SOTA performance\nacross multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the\nrunner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the\nmore challenging MIMIR benchmark, it consistently improves upon reference-free\nmethods while performing on par with reference-based method that requires an\nextra reference model.\n","authors":["Jingyang Zhang","Jingwei Sun","Eric Yeats","Yang Ouyang","Martin Kuo","Jianyi Zhang","Hao Frank Yang","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.02936v4.pdf","comment":"ICLR'25 Spotlight. Project page and code is available at\n  https://zjysteven.github.io/mink-plus-plus/"},{"id":"http://arxiv.org/abs/2501.17630v2","updated":"2025-02-12T04:28:04Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v2.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2410.09807v2","updated":"2025-02-12T04:24:19Z","published":"2024-10-13T11:48:09Z","title":"Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based\n  Sentiment Analysis Evaluation","summary":"  Aspect-based sentiment analysis (ABSA) is a challenging task of extracting\nsentiments along with their corresponding aspects and opinion terms from the\ntext. The inherent subjectivity of span annotation makes variability in the\nsurface forms of extracted terms, complicating the evaluation process.\nTraditional evaluation methods often constrain ground truths (GT) to a single\nterm, potentially misrepresenting the accuracy of semantically valid\npredictions that differ in surface form. To address this limitation, we propose\na novel and fully automated pipeline that expands existing evaluation sets by\nadding alternative valid terms for aspect and opinion. Our approach facilitates\nan equitable assessment of language models by accommodating multiple-answer\ncandidates, resulting in enhanced human agreement compared to single-answer\ntest sets (achieving up to a 10\\%p improvement in Kendall's Tau score).\nExperimental results demonstrate that our expanded evaluation set helps uncover\nthe capabilities of large language models (LLMs) in ABSA tasks, which is\nconcealed by the single-answer GT sets. Consequently, our work contributes to\nthe development of a flexible evaluation framework for ABSA by embracing\ndiverse surface forms to span extraction tasks in a cost-effective and\nreproducible manner. Our code and dataset is open at\nhttps://github.com/dudrrm/zoom-in-n-out-absa.\n","authors":["Soyoung Yang","Hojun Cho","Jiyoung Lee","Sohee Yoon","Edward Choi","Jaegul Choo","Won Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2410.09807v2.pdf","comment":"NAACL 2025 camera-ready"},{"id":"http://arxiv.org/abs/2502.08109v1","updated":"2025-02-12T04:17:02Z","published":"2025-02-12T04:17:02Z","title":"HuDEx: Integrating Hallucination Detection and Explainability for\n  Enhancing the Reliability of LLM responses","summary":"  Recent advances in large language models (LLMs) have shown promising\nimprovements, often surpassing existing methods across a wide range of\ndownstream tasks in natural language processing. However, these models still\nface challenges, which may hinder their practical applicability. For example,\nthe phenomenon of hallucination is known to compromise the reliability of LLMs,\nespecially in fields that demand high factual precision. Current benchmarks\nprimarily focus on hallucination detection and factuality evaluation but do not\nextend beyond identification. This paper proposes an explanation enhanced\nhallucination-detection model, coined as HuDEx, aimed at enhancing the\nreliability of LLM-generated responses by both detecting hallucinations and\nproviding detailed explanations. The proposed model provides a novel approach\nto integrate detection with explanations, and enable both users and the LLM\nitself to understand and reduce errors. Our measurement results demonstrate\nthat the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations.\nFurthermore, the proposed model performs well in both zero-shot and other test\nenvironments, showcasing its adaptability across diverse benchmark datasets.\nThe proposed approach further enhances the hallucination detection research by\nintroducing a novel approach to integrating interpretability with hallucination\ndetection, which further enhances the performance and reliability of evaluating\nhallucinations in language models.\n","authors":["Sujeong Lee","Hayoung Lee","Seongsoo Heo","Wonik Choi"],"pdf_url":"https://arxiv.org/pdf/2502.08109v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.05589v2","updated":"2025-02-12T04:15:47Z","published":"2025-02-08T14:28:36Z","title":"On Memory Construction and Retrieval for Personalized Conversational\n  Agents","summary":"  To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques. In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nTurn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as\n\\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing\nmemory retrieval accuracy across different granularities. Building on these\ninsights, we propose SeCom, a method that constructs a memory bank with topical\nsegments by introducing a conversation Segmentation model, while performing\nmemory retrieval based on Compressed memory units. Experimental results show\nthat SeCom outperforms turn-level, session-level, and several\nsummarization-based methods on long-term conversation benchmarks such as LOCOMO\nand Long-MT-Bench+. Additionally, the proposed conversation segmentation method\ndemonstrates superior performance on dialogue segmentation datasets such as\nDialSeg711, TIAGE, and SuperDialSeg.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Xufang Luo","Hao Cheng","Dongsheng Li","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05589v2.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.07555v2","updated":"2025-02-12T03:33:06Z","published":"2025-02-11T13:48:10Z","title":"O1 Embedder: Let Retrievers Think Before Action","summary":"  The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.\n","authors":["Ruiran Yan","Zheng Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2502.07555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08092v1","updated":"2025-02-12T03:33:06Z","published":"2025-02-12T03:33:06Z","title":"GCoT: Chain-of-Thought Prompt Learning for Graphs","summary":"  Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.\n","authors":["Xingtong Yu","Chang Zhou","Zhongwei Kuai","Xinming Zhang","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2502.08092v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.15633v4","updated":"2025-02-12T03:32:34Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Samples for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07677v2","updated":"2025-02-12T03:31:45Z","published":"2025-02-11T16:27:28Z","title":"Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach","summary":"  Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat\nhttps://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing\n","authors":["Param Kulkarni","Yingchi Liu","Hao-Ming Fu","Shaohua Yang","Isuru Gunasekara","Matt Peloquin","Noah Spitzer-Williams","Xiaotian Zhou","Xiaozhong Liu","Zhengping Ji","Yasser Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2502.07677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09150v3","updated":"2025-02-12T03:00:20Z","published":"2024-08-17T09:49:40Z","title":"CogLM: Tracking Cognitive Development of Large Language Models","summary":"  Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) In our testing\nframework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive\nabilities, comparable to those of a 20-year-old human. (2) The parameter size\nand optimization objective are two key factors affecting the cognitive levels\nof LLMs. (3) The performance on downstream tasks is positively correlated with\nthe level of cognitive abilities. These findings fill the gap in research on\nthe cognitive abilities of LLMs, tracing the development of LLMs from a\ncognitive perspective and guiding the future direction of their evolution.\n","authors":["Xinglin Wang","Peiwen Yuan","Shaoxiong Feng","Yiwei Li","Boyuan Pan","Heda Wang","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2408.09150v3.pdf","comment":"NAACL2025 Main"},{"id":"http://arxiv.org/abs/2502.08080v1","updated":"2025-02-12T02:54:12Z","published":"2025-02-12T02:54:12Z","title":"NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals","summary":"  Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts.\n","authors":["Neha Srikanth","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2502.08080v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.13457v3","updated":"2025-02-12T02:52:25Z","published":"2024-08-24T04:03:35Z","title":"Make Every Penny Count: Difficulty-Adaptive Self-Consistency for\n  Cost-Efficient Reasoning","summary":"  Self-consistency (SC), a widely used decoding strategy for chain-of-thought\nreasoning, shows significant gains across various multi-step reasoning tasks\nbut comes with a high cost due to multiple sampling with the preset size. Its\nvariants, Adaptive self-consistency (ASC) and Early-stopping self-consistency\n(ESC), dynamically adjust the number of samples based on the posterior\ndistribution of a set of pre-samples, reducing the cost of SC with minimal\nimpact on performance. Both methods, however, do not exploit the prior\ninformation about question difficulty. It often results in unnecessary repeated\nsampling for easy questions that could be accurately answered with just one\nattempt, wasting resources. To tackle this problem, we propose\nDifficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty\ninformation of batch queries from both prior and posterior perspectives to\nadaptively allocate inference resources, further reducing the overall cost of\nSC. To demonstrate the effectiveness of DSC, we conduct extensive experiments\non three popular categories of reasoning tasks: arithmetic, commonsense and\nsymbolic reasoning on six benchmarks. The empirical results show that DSC\nconsistently surpasses the strong baseline ASC and ESC in terms of costs by a\nsignificant margin, while attaining comparable performances.\n","authors":["Xinglin Wang","Shaoxiong Feng","Yiwei Li","Peiwen Yuan","Yueqi Zhang","Chuyi Tan","Boyuan Pan","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2408.13457v3.pdf","comment":"NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2406.10281v2","updated":"2025-02-12T02:11:10Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"  Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.\n","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2406.10281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15512v2","updated":"2025-02-12T02:06:35Z","published":"2024-10-20T21:17:49Z","title":"Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad)\n  that it Can't Answer?","summary":"  Question answering (QA), giving correct answers to questions, is a popular\ntask, but we test reverse question answering (RQA): for an input answer, give a\nquestion with that answer. Past work tests QA and RQA separately, but we test\nthem jointly, comparing their difficulty, aiding benchmark design, and checking\nreasoning consistency. We run 16 LLMs on QA and RQA with trivia\nquestions/answers, revealing: 1) Versus QA, LLMs are much less accurate in RQA\nfor numerical answers, but slightly more accurate in RQA for textual answers;\n2) LLMs often answer their own invalid questions from RQA accurately in QA, so\nRQA errors are not from knowledge gaps alone; 3) RQA errors correlate with\nquestion difficulty and inversely correlate with answer frequencies in the\nDolma corpus; and 4) LLMs struggle to provide valid multi-hop questions. By\nfinding question and answer types that lead to RQA errors, we suggest\nimprovements for LLM reasoning.\n","authors":["Nishant Balepur","Feng Gu","Abhilasha Ravichander","Shi Feng","Jordan Boyd-Graber","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.15512v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08059v1","updated":"2025-02-12T01:54:21Z","published":"2025-02-12T01:54:21Z","title":"On Mechanistic Circuits for Extractive Question-Answering","summary":"  Large language models are increasingly used to process documents and\nfacilitate question-answering on them. In our paper, we extract mechanistic\ncircuits for this real-world language modeling task: context-augmented language\nmodeling for extractive question-answering (QA) tasks and understand the\npotential benefits of circuits towards downstream applications such as data\nattribution to context information. We extract circuits as a function of\ninternal model components (e.g., attention heads, MLPs) using causal mediation\nanalysis techniques. Leveraging the extracted circuits, we first understand the\ninterplay between the model's usage of parametric memory and retrieved context\ntowards a better mechanistic understanding of context-augmented language\nmodels. We then identify a small set of attention heads in our circuit which\nperforms reliable data attribution by default, thereby obtaining attribution\nfor free in just the model's forward pass. Using this insight, we then\nintroduce ATTNATTRIB, a fast data attribution algorithm which obtains\nstate-of-the-art attribution results across various extractive QA benchmarks.\nFinally, we show the possibility to steer the language model towards answering\nfrom the context, instead of the parametric memory by using the attribution\nfrom ATTNATTRIB as an additional signal during the forward pass. Beyond\nmechanistic understanding, our paper provides tangible applications of circuits\nin the form of reliable data attribution and model steering.\n","authors":["Samyadeep Basu","Vlad Morariu","Zichao Wang","Ryan Rossi","Cherry Zhao","Soheil Feizi","Varun Manjunatha"],"pdf_url":"https://arxiv.org/pdf/2502.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08045v1","updated":"2025-02-12T01:04:13Z","published":"2025-02-12T01:04:13Z","title":"Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs","summary":"  A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.\n","authors":["Mohsinul Kabir","Ajwad Abrar","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2502.08045v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.08037v1","updated":"2025-02-12T00:38:11Z","published":"2025-02-12T00:38:11Z","title":"Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery","summary":"  The capabilities of Large Language Models (LLMs) in low-resource languages\nlag far behind those in English, making their universal accessibility a\nsignificant challenge. To alleviate this, we present\n$\\textit{Franken-Adapter}$, a modular language adaptation approach for\ndecoder-only LLMs with embedding surgery. Our method begins by creating\ncustomized vocabularies for target languages and performing language adaptation\nthrough embedding tuning on multilingual data. These pre-trained embeddings are\nsubsequently integrated with LLMs that have been instruction-tuned on English\nalignment data to enable zero-shot cross-lingual transfer. Our experiments on\n$\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of\nup to 20% across 96 languages, spanning both discriminative and generative\ntasks, with minimal regressions ($<$1%) in English. Further in-depth analysis\nreveals the critical role of customizing tokenizers in enhancing language\nadaptation, while boosting inference efficiency. Additionally, we show the\nversatility of our method by achieving a 14% improvement over a math-optimized\nLLM across 20 languages, offering a modular solution to transfer reasoning\nabilities across languages post hoc.\n","authors":["Fan Jiang","Honglin Yu","Grace Chung","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2502.08037v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2502.08026v1","updated":"2025-02-12T00:00:37Z","published":"2025-02-12T00:00:37Z","title":"Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations","summary":"  Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach.\n","authors":["Alistair Wren","Beatrice Loxley","Hamish Cadwallader","Simon Beckwith","Fabian Pargeter","James Blades"],"pdf_url":"https://arxiv.org/pdf/2502.08026v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.08644v1","updated":"2025-02-12T18:58:34Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.08643v1","updated":"2025-02-12T18:57:22Z","published":"2025-02-12T18:57:22Z","title":"A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards","summary":"  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n","authors":["Shivansh Patel","Xinchen Yin","Wenlong Huang","Shubham Garg","Hooshang Nayyeri","Li Fei-Fei","Svetlana Lazebnik","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2502.08643v1.pdf","comment":"ICRA 2025, Project Page: https://iker-robot.github.io/"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08631v1","updated":"2025-02-12T18:42:42Z","published":"2025-02-12T18:42:42Z","title":"Ensemble based approach to quantifying uncertainty of LLM based\n  classifications","summary":"  The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.\n","authors":["Srijith Rajamohan","Ahmed Salhin","Josh Frazier","Rohit Kumar","Yu-Cheng Tsai","Todd Cook"],"pdf_url":"https://arxiv.org/pdf/2502.08631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2502.08610v1","updated":"2025-02-12T17:57:54Z","published":"2025-02-12T17:57:54Z","title":"Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis\n  of Gaps in Current AI Standards","summary":"  As AI systems integrate into critical infrastructure, security gaps in AI\ncompliance frameworks demand urgent attention. This paper audits and quantifies\nsecurity risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI\nand Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk\nassessment methodology, we develop four key metrics: Risk Severity Index (RSI),\nAttack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and\nRoot Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns\nacross the frameworks, exposing significant gaps. NIST fails to address 69.23\npercent of identified risks, ALTAI has the highest attack vector vulnerability\n(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with\n80.00 percent of high-risk concerns remaining unresolved. Root cause analysis\nhighlights under-defined processes (ALTAI RCVS = 033) and weak implementation\nguidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings\nemphasize the need for stronger, enforceable security controls in AI\ncompliance. We offer targeted recommendations to enhance security posture and\nbridge the gap between compliance and real-world AI risks.\n","authors":["Keerthana Madhavan","Abbas Yazdinejad","Fattane Zarrinkalam","Ali Dehghantanha"],"pdf_url":"https://arxiv.org/pdf/2502.08610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05655v2","updated":"2025-02-12T17:55:53Z","published":"2024-09-09T14:22:19Z","title":"Interactive incremental learning of generalizable skills with local\n  trajectory modulation","summary":"  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n","authors":["Markus Knauer","Alin Albu-Schffer","Freek Stulp","Joo Silvrio"],"pdf_url":"https://arxiv.org/pdf/2409.05655v2.pdf","comment":"Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19\n  figures, 6 tables. See\n  https://github.com/DLR-RM/interactive-incremental-learning for further\n  information and video"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.17883v2","updated":"2025-02-12T17:51:51Z","published":"2024-10-23T13:57:00Z","title":"Lightweight Neural App Control","summary":"  This paper introduces a novel mobile phone control architecture, Lightweight\nMulti-modal App Control (LiMAC), for efficient interactions and control across\nvarious Android apps. LiMAC takes as input a textual goal and a sequence of\npast mobile observations, such as screenshots and corresponding UI trees, to\ngenerate precise actions. To address the computational constraints inherent to\nsmartphones, we introduce a small Action Transformer (AcT) integrated with a\nfine-tuned vision-language model (VLM) for real-time decision-making and task\nexecution. We evaluate LiMAC on two open-source mobile control datasets,\ndemonstrating the superior performance of our small-form-factor approach\nagainst fine-tuned versions of open-source VLMs, such as Florence2 and\nQwen2-VL. It also significantly outperforms prompt engineering baselines\nutilising closed-source foundation models like GPT-4o. More specifically, LiMAC\nincreases the overall action accuracy by up to 19% compared to fine-tuned VLMs,\nand up to 42% compared to prompt-engineering baselines.\n","authors":["Filippos Christianos","Georgios Papoudakis","Thomas Coste","Jianye Hao","Jun Wang","Kun Shao"],"pdf_url":"https://arxiv.org/pdf/2410.17883v2.pdf","comment":"ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2502.08605v1","updated":"2025-02-12T17:49:46Z","published":"2025-02-12T17:49:46Z","title":"CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection","summary":"  Does the intrinsic curvature of complex networks hold the key to unveiling\ngraph anomalies that conventional approaches overlook? Reconstruction-based\ngraph anomaly detection (GAD) methods overlook such geometric outliers,\nfocusing only on structural and attribute-level anomalies. To this end, we\npropose CurvGAD - a mixed-curvature graph autoencoder that introduces the\nnotion of curvature-based geometric anomalies. CurvGAD introduces two parallel\npipelines for enhanced anomaly interpretability: (1) Curvature-equivariant\ngeometry reconstruction, which focuses exclusively on reconstructing the edge\ncurvatures using a mixed-curvature, Riemannian encoder and Gaussian\nkernel-based decoder; and (2) Curvature-invariant structure and attribute\nreconstruction, which decouples structural and attribute anomalies from\ngeometric irregularities by regularizing graph curvature under discrete\nOllivier-Ricci flow, thereby isolating the non-geometric anomalies. By\nleveraging curvature, CurvGAD refines the existing anomaly classifications and\nidentifies new curvature-driven anomalies. Extensive experimentation over 10\nreal-world datasets (both homophilic and heterophilic) demonstrates an\nimprovement of up to 6.5% over state-of-the-art GAD methods.\n","authors":["Karish Grover","Geoffrey J. Gordon","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2502.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08597v1","updated":"2025-02-12T17:34:04Z","published":"2025-02-12T17:34:04Z","title":"Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners","summary":"  We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.\n","authors":["David Easley","Yoav Kolumbus","Eva Tardos"],"pdf_url":"https://arxiv.org/pdf/2502.08597v1.pdf","comment":"Learning in Markets, Heterogeneous Agents, Regret and Survival"},{"id":"http://arxiv.org/abs/2502.08586v1","updated":"2025-02-12T17:19:36Z","published":"2025-02-12T17:19:36Z","title":"Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks","summary":"  A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.\n","authors":["Ang Li","Yin Zhou","Vethavikashini Chithrra Raghuram","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2502.08586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05905v5","updated":"2025-02-12T17:10:53Z","published":"2024-05-09T17:01:31Z","title":"Truthful Aggregation of LLMs with an Application to Online Advertising","summary":"  The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.\n","authors":["Ermis Soumalias","Michael J. Curry","Sven Seuken"],"pdf_url":"https://arxiv.org/pdf/2405.05905v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08577v1","updated":"2025-02-12T17:10:53Z","published":"2025-02-12T17:10:53Z","title":"FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning","summary":"  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n","authors":["Davide Domini","Gianluca Aguzzi","Lukas Esterle","Mirko Viroli"],"pdf_url":"https://arxiv.org/pdf/2502.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08576v1","updated":"2025-02-12T17:10:34Z","published":"2025-02-12T17:10:34Z","title":"Mapping the Landscape of Generative AI in Network Monitoring and\n  Management","summary":"  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n","authors":["Giampaolo Bovenzi","Francesco Cerasuolo","Domenico Ciuonzo","Davide Di Monda","Idio Guarino","Antonio Montieri","Valerio Persico","Antonio Pescap"],"pdf_url":"https://arxiv.org/pdf/2502.08576v1.pdf","comment":"32 pages, 9 figure, 10 tables"},{"id":"http://arxiv.org/abs/2502.08574v1","updated":"2025-02-12T17:09:13Z","published":"2025-02-12T17:09:13Z","title":"COAST: Intelligent Time-Adaptive Neural Operators","summary":"  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a\nnovel neural operator learning method that leverages a causal language model\n(CLM) framework to dynamically adapt time steps. Our method predicts both the\nevolution of a system and its optimal time step, intelligently balancing\ncomputational efficiency and accuracy. We find that COAST generates variable\nstep sizes that correlate with the underlying system intrinsicities, both\nwithin and across dynamical systems. Within a single trajectory, smaller steps\nare taken in regions of high complexity, while larger steps are employed in\nsimpler regions. Across different systems, more complex dynamics receive more\ngranular time steps. Benchmarked on diverse systems with varied dynamics, COAST\nconsistently outperforms state-of-the-art methods, achieving superior\nperformance in both efficiency and accuracy. This work underscores the\npotential of CLM-based intelligent adaptive solvers for scalable operator\nlearning of dynamical systems.\n","authors":["Zhikai Wu","Shiyang Zhang","Sizhuang He","Sifan Wang","Min Zhu","Anran Jiao","Lu Lu","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.08574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08573v1","updated":"2025-02-12T17:07:43Z","published":"2025-02-12T17:07:43Z","title":"A Novel Approach to for Multimodal Emotion Recognition : Multimodal\n  semantic information fusion","summary":"  With the advancement of artificial intelligence and computer vision\ntechnologies, multimodal emotion recognition has become a prominent research\ntopic. However, existing methods face challenges such as heterogeneous data\nfusion and the effective utilization of modality correlations. This paper\nproposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on\nthe integration of contrastive learning and visual sequence compression. The\nproposed method enhances cross-modal feature fusion through contrastive\nlearning and reduces redundancy in the visual modality by leveraging visual\nsequence compression. Experimental results on two public datasets, IEMOCAP and\nMELD, demonstrate that DeepMSI-MER significantly improves the accuracy and\nrobustness of emotion recognition, validating the effectiveness of multimodal\nfeature fusion and the proposed approach.\n","authors":["Wei Dai","Dequan Zheng","Feng Yu","Yanrong Zhang","Yaohui Hou"],"pdf_url":"https://arxiv.org/pdf/2502.08573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18304v4","updated":"2025-02-12T17:02:06Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v4.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07693v2","updated":"2025-02-12T16:52:51Z","published":"2025-02-11T16:46:56Z","title":"SoK: A Classification for AI-driven Personalized Privacy Assistants","summary":"  To help users make privacy-related decisions, personalized privacy assistants\nbased on AI technology have been developed in recent years. These AI-driven\nPersonalized Privacy Assistants (AI-driven PPAs) can reap significant benefits\nfor users, who may otherwise struggle to make decisions regarding their\npersonal data in environments saturated with privacy-related decision requests.\nHowever, no study systematically inquired about the features of these AI-driven\nPPAs, their underlying technologies, or the accuracy of their decisions. To\nfill this gap, we present a Systematization of Knowledge (SoK) to map the\nexisting solutions found in the scientific literature. We screened 1697 unique\nresearch papers over the last decade (2013-2023), constructing a classification\nfrom 39 included papers. As a result, this SoK reviews several aspects of\nexisting research on AI-driven PPAs in terms of types of publications,\ncontributions, methodological quality, and other quantitative insights.\nFurthermore, we provide a comprehensive classification for AI-driven PPAs,\ndelving into their architectural choices, system contexts, types of AI used,\ndata sources, types of decisions, and control over decisions, among other\nfacets. Based on our SoK, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.\n","authors":["Victor Morel","Leonardo Iwaya","Simone Fischer-Hbner"],"pdf_url":"https://arxiv.org/pdf/2502.07693v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.20163v3","updated":"2025-02-12T16:49:56Z","published":"2024-12-28T14:27:45Z","title":"Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems","summary":"  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n","authors":["Minhye Jeon","Seokho Ahn","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20163v3.pdf","comment":"Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.08560v1","updated":"2025-02-12T16:47:41Z","published":"2025-02-12T16:47:41Z","title":"Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion","summary":"  The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP.\n","authors":["Lemuel Puglisi","Daniel C. Alexander","Daniele Rav"],"pdf_url":"https://arxiv.org/pdf/2502.08560v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.03328"},{"id":"http://arxiv.org/abs/2502.06914v2","updated":"2025-02-12T16:47:32Z","published":"2025-02-10T09:46:26Z","title":"UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge","summary":"  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n","authors":["Chenao Li","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06914v2.pdf","comment":"18 pages,8 figures"},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08554v1","updated":"2025-02-12T16:35:41Z","published":"2025-02-12T16:35:41Z","title":"Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies","summary":"  Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs.\n","authors":["Sunnie S. Y. Kim","Jennifer Wortman Vaughan","Q. Vera Liao","Tania Lombrozo","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2502.08554v1.pdf","comment":"CHI 2025. This version includes the appendix"},{"id":"http://arxiv.org/abs/2502.08550v1","updated":"2025-02-12T16:31:21Z","published":"2025-02-12T16:31:21Z","title":"LLMs can implicitly learn from mistakes in-context","summary":"  Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.\n","authors":["Lisa Alazraki","Maximilian Mozes","Jon Ander Campos","Yi Chern Tan","Marek Rei","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2502.08550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08547v1","updated":"2025-02-12T16:29:39Z","published":"2025-02-12T16:29:39Z","title":"Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data","summary":"  The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.\n","authors":["Doudou Zhou","Han Tong","Linshanshan Wang","Suqi Liu","Xin Xiong","Ziming Gan","Romain Griffier","Boris Hejblum","Yun-Chung Liu","Chuan Hong","Clara-Lea Bonzel","Tianrun Cai","Kevin Pan","Yuk-Lam Ho","Lauren Costa","Vidul A. Panickan","J. Michael Gaziano","Kenneth Mandl","Vianney Jouhet","Rodolphe Thiebaut","Zongqi Xia","Kelly Cho","Katherine Liao","Tianxi Cai"],"pdf_url":"https://arxiv.org/pdf/2502.08547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08534v1","updated":"2025-02-12T16:15:03Z","published":"2025-02-12T16:15:03Z","title":"Input convex neural networks: universal approximation theorem and\n  implementation for isotropic polyconvex hyperelastic energies","summary":"  This paper presents a novel framework of neural networks for isotropic\nhyperelasticity that enforces necessary physical and mathematical constraints\nwhile simultaneously satisfying the universal approximation theorem. The two\nkey ingredients are an input convex network architecture and a formulation in\nthe elementary polynomials of the signed singular values of the deformation\ngradient. In line with previously published networks, it can rigorously capture\nframe-indifference and polyconvexity - as well as further constraints like\nbalance of angular momentum and growth conditions. However and in contrast to\nprevious networks, a universal approximation theorem for the proposed approach\nis proven. To be more explicit, the proposed network can approximate any\nframe-indifferent, isotropic polyconvex energy (provided the network is large\nenough). This is possible by working with a sufficient and necessary criterion\nfor frame-indifferent, isotropic polyconvex functions. Comparative studies with\nexisting approaches identify the advantages of the proposed method,\nparticularly in approximating non-polyconvex energies as well as computing\npolyconvex hulls.\n","authors":["Gian-Luca Geuken","Patrick Kurzeja","David Wiedemann","Jrn Mosler"],"pdf_url":"https://arxiv.org/pdf/2502.08534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18138v5","updated":"2025-02-12T15:56:41Z","published":"2023-11-29T23:01:33Z","title":"Algorithmic Persuasion Through Simulation","summary":"  We study a Bayesian persuasion game where a sender wants to persuade a\nreceiver to take a binary action, such as purchasing a product. The sender is\ninformed about the (real-valued) state of the world, such as the quality of the\nproduct, but only has limited information about the receiver's beliefs and\nutilities. Motivated by customer surveys, user studies, and recent advances in\nAI, we allow the sender to learn more about the receiver by querying an oracle\nthat simulates the receiver's behavior. After a fixed number of queries, the\nsender commits to a messaging policy and the receiver takes the action that\nmaximizes her expected utility given the message she receives. We characterize\nthe sender's optimal messaging policy given any distribution over receiver\ntypes. We then design a polynomial-time querying algorithm that optimizes the\nsender's expected utility in this game. We also consider approximate oracles,\nmore general query structures, and costly queries.\n","authors":["Keegan Harris","Nicole Immorlica","Brendan Lucier","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2311.18138v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08518v1","updated":"2025-02-12T15:54:56Z","published":"2025-02-12T15:54:56Z","title":"FedMHO: Heterogeneous One-Shot Federated Learning Towards\n  Resource-Constrained Edge Devices","summary":"  Federated Learning (FL) is increasingly adopted in edge computing scenarios,\nwhere a large number of heterogeneous clients operate under constrained or\nsufficient resources. The iterative training process in conventional FL\nintroduces significant computation and communication overhead, which is\nunfriendly for resource-constrained edge devices. One-shot FL has emerged as a\npromising approach to mitigate communication overhead, and model-heterogeneous\nFL solves the problem of diverse computing resources across clients. However,\nexisting methods face challenges in effectively managing model-heterogeneous\none-shot FL, often leading to unsatisfactory global model performance or\nreliance on auxiliary datasets. To address these challenges, we propose a novel\nFL framework named FedMHO, which leverages deep classification models on\nresource-sufficient clients and lightweight generative models on\nresource-constrained devices. On the server side, FedMHO involves a two-stage\nprocess that includes data generation and knowledge fusion. Furthermore, we\nintroduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem\nduring the knowledge fusion stage, and an unsupervised data optimization\nsolution to improve the quality of synthetic samples. Comprehensive experiments\ndemonstrate the effectiveness of our methods, as they outperform\nstate-of-the-art baselines in various experimental setups.\n","authors":["Dezhong Yao","Yuexin Shi","Tongtong Liu","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08512v1","updated":"2025-02-12T15:46:34Z","published":"2025-02-12T15:46:34Z","title":"Measuring Diversity in Synthetic Datasets","summary":"  Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.\n","authors":["Yuchang Zhu","Huizhe Zhang","Bingzhe Wu","Jintang Li","Zibin Zheng","Peilin Zhao","Liang Chen","Yatao Bian"],"pdf_url":"https://arxiv.org/pdf/2502.08512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05980v3","updated":"2025-02-12T15:40:35Z","published":"2024-02-08T06:48:01Z","title":"Do Large Code Models Understand Programming Concepts? Counterfactual\n  Analysis for Code Predicates","summary":"  Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.\n","authors":["Ashish Hooda","Mihai Christodorescu","Miltiadis Allamanis","Aaron Wilson","Kassem Fawaz","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2402.05980v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14679v2","updated":"2025-02-12T15:37:22Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08503v1","updated":"2025-02-12T15:34:45Z","published":"2025-02-12T15:34:45Z","title":"Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?","summary":"  In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.\n","authors":["Jiahe Jin","Yanheng He","Mingyan Yang"],"pdf_url":"https://arxiv.org/pdf/2502.08503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2409.08678v2","updated":"2025-02-12T15:18:21Z","published":"2024-09-13T09:46:41Z","title":"Shadow Program Inversion with Differentiable Planning: A Framework for\n  Unified Robot Program Parameter and Trajectory Optimization","summary":"  This paper presents SPI-DP, a novel first-order optimizer capable of\noptimizing robot programs with respect to both high-level task objectives and\nmotion-level constraints. To that end, we introduce DGPMP2-ND, a differentiable\ncollision-free motion planner for serial N-DoF kinematics, and integrate it\ninto an iterative, gradient-based optimization approach for generic,\nparameterized robot program representations. SPI-DP allows first-order\noptimization of planned trajectories and program parameters with respect to\nobjectives such as cycle time or smoothness subject to e.g. collision\nconstraints, while enabling humans to understand, modify or even certify the\noptimized programs. We provide a comprehensive evaluation on two practical\nhousehold and industrial applications.\n","authors":["Benjamin Alt","Claudius Kienle","Darko Katic","Rainer Jkel","Michael Beetz"],"pdf_url":"https://arxiv.org/pdf/2409.08678v2.pdf","comment":"8 pages, 6 figures, accepted at the 2025 IEEE International\n  Conference on Robotics & Automation (ICRA)"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2411.02540v3","updated":"2025-02-12T15:14:01Z","published":"2024-11-04T19:21:06Z","title":"GraphXAIN: Narratives to Explain Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.\n","authors":["Mateusz Cedro","David Martens"],"pdf_url":"https://arxiv.org/pdf/2411.02540v3.pdf","comment":"19 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.08468v1","updated":"2025-02-12T15:03:33Z","published":"2025-02-12T15:03:33Z","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data","summary":"  Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07737v2","updated":"2025-02-12T14:50:50Z","published":"2025-02-11T17:57:53Z","title":"Next Block Prediction: Video Generation via Semi-Autoregressive Modeling","summary":"  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n","authors":["Shuhuai Ren","Shuming Ma","Xu Sun","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.07737v2.pdf","comment":"project page: https://renshuhuai-andy.github.io/NBP-project/"},{"id":"http://arxiv.org/abs/2407.08649v2","updated":"2025-02-12T14:49:16Z","published":"2024-07-11T16:28:31Z","title":"Confidence-based Estimators for Predictive Performance in Model\n  Monitoring","summary":"  After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.\n","authors":["Juhani Kivimki","Jakub Biaek","Jukka K. Nurminen","Wojtek Kuberski"],"pdf_url":"https://arxiv.org/pdf/2407.08649v2.pdf","comment":"This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]"},{"id":"http://arxiv.org/abs/2502.08450v1","updated":"2025-02-12T14:41:20Z","published":"2025-02-12T14:41:20Z","title":"Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated\n  Essay Scoring","summary":"  In automated essay scoring (AES), recent efforts have shifted toward\ncross-prompt settings that score essays on unseen prompts for practical\napplicability. However, prior methods trained with essay-score pairs of\nspecific prompts pose challenges in obtaining prompt-generalized essay\nrepresentation. In this work, we propose a grammar-aware cross-prompt trait\nscoring (GAPS), which internally captures prompt-independent syntactic aspects\nto learn generic essay representation. We acquire grammatical error-corrected\ninformation in essays via the grammar error correction technique and design the\nAES model to seamlessly integrate such information. By internally referring to\nboth the corrected and the original essays, the model can focus on generic\nfeatures during training. Empirical experiments validate our method's\ngeneralizability, showing remarkable improvements in prompt-independent and\ngrammar-related traits. Furthermore, GAPS achieves notable QWK gains in the\nmost challenging cross-prompt scenario, highlighting its strength in evaluating\nunseen prompts.\n","authors":["Heejin Do","Taehee Park","Sangwon Ryu","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08450v1.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2502.08449v1","updated":"2025-02-12T14:41:14Z","published":"2025-02-12T14:41:14Z","title":"CordViP: Correspondence-based Visuomotor Policy for Dexterous\n  Manipulation in Real-World","summary":"  Achieving human-level dexterity in robots is a key objective in the field of\nrobotic manipulation. Recent advancements in 3D-based imitation learning have\nshown promising results, providing an effective pathway to achieve this goal.\nHowever, obtaining high-quality 3D representations presents two key problems:\n(1) the quality of point clouds captured by a single-view camera is\nsignificantly affected by factors such as camera resolution, positioning, and\nocclusions caused by the dexterous hand; (2) the global point clouds lack\ncrucial contact information and spatial correspondences, which are necessary\nfor fine-grained dexterous manipulation tasks. To eliminate these limitations,\nwe propose CordViP, a novel framework that constructs and learns\ncorrespondences by leveraging the robust 6D pose estimation of objects and\nrobot proprioception. Specifically, we first introduce the interaction-aware\npoint clouds, which establish correspondences between the object and the hand.\nThese point clouds are then used for our pre-training policy, where we also\nincorporate object-centric contact maps and hand-arm coordination information,\neffectively capturing both spatial and temporal dynamics. Our method\ndemonstrates exceptional dexterous manipulation capabilities with an average\nsuccess rate of 90\\% in four real-world tasks, surpassing other baselines by a\nlarge margin. Experimental results also highlight the superior generalization\nand robustness of CordViP to different objects, viewpoints, and scenarios. Code\nand videos are available on https://aureleopku.github.io/CordViP.\n","authors":["Yankai Fu","Qiuxuan Feng","Ning Chen","Zichen Zhou","Mengzhen Liu","Mingdong Wu","Tianxing Chen","Shanyu Rong","Jiaming Liu","Hao Dong","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v1","updated":"2025-02-12T14:32:17Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2412.01621v2","updated":"2025-02-12T14:03:19Z","published":"2024-12-02T15:41:47Z","title":"NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers","summary":"  Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.\n","authors":["Angel Yahir Loredo Lopez","Tyler McDonald","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2412.01621v2.pdf","comment":"5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award"},{"id":"http://arxiv.org/abs/2502.08417v1","updated":"2025-02-12T13:59:37Z","published":"2025-02-12T13:59:37Z","title":"Handwritten Text Recognition: A Survey","summary":"  Handwritten Text Recognition (HTR) has become an essential field within\npattern recognition and machine learning, with applications spanning historical\ndocument preservation to modern data entry and accessibility solutions. The\ncomplexity of HTR lies in the high variability of handwriting, which makes it\nchallenging to develop robust recognition systems. This survey examines the\nevolution of HTR models, tracing their progression from early heuristic-based\napproaches to contemporary state-of-the-art neural models, which leverage deep\nlearning techniques. The scope of the field has also expanded, with models\ninitially capable of recognizing only word-level content progressing to recent\nend-to-end document-level approaches. Our paper categorizes existing work into\ntwo primary levels of recognition: (1) \\emph{up to line-level}, encompassing\nword and line recognition, and (2) \\emph{beyond line-level}, addressing\nparagraph- and document-level challenges. We provide a unified framework that\nexamines research methodologies, recent advances in benchmarking, key datasets\nin the field, and a discussion of the results reported in the literature.\nFinally, we identify pressing research challenges and outline promising future\ndirections, aiming to equip researchers and practitioners with a roadmap for\nadvancing the field.\n","authors":["Carlos Garrido-Munoz","Antonio Rios-Vila","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2502.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05431v2","updated":"2025-02-12T13:54:01Z","published":"2025-02-08T03:41:16Z","title":"APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding","summary":"  Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.\n","authors":["Xinyu Yang","Tianqi Chen","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.05431v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2408.08707v2","updated":"2025-02-12T13:29:11Z","published":"2024-08-16T12:40:01Z","title":"Beam Prediction based on Large Language Models","summary":"  In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.\n","authors":["Yucheng Sheng","Kai Huang","Le Liang","Peng Liu","Shi Jin","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.08707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2402.04059v2","updated":"2025-02-12T13:16:29Z","published":"2024-02-06T15:03:53Z","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","summary":"  Missing values are ubiquitous in multivariate time series (MTS) data, posing\nsignificant challenges for accurate analysis and downstream applications. In\nrecent years, deep learning-based methods have successfully handled missing\ndata by leveraging complex temporal dependencies and learned data\ndistributions. In this survey, we provide a comprehensive summary of deep\nlearning approaches for multivariate time series imputation (MTSI) tasks. We\npropose a novel taxonomy that categorizes existing methods based on two key\nperspectives: imputation uncertainty and neural network architecture.\nFurthermore, we summarize existing MTSI toolkits with a particular emphasis on\nthe PyPOTS Ecosystem, which provides an integrated and standardized foundation\nfor MTSI research. Finally, we discuss key challenges and future research\ndirections, which give insight for further MTSI research. This survey aims to\nserve as a valuable resource for researchers and practitioners in the field of\ntime series analysis and missing data imputation tasks.\n","authors":["Jun Wang","Wenjie Du","Yiyuan Yang","Linglong Qian","Wei Cao","Keli Zhang","Wenjia Wang","Yuxuan Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.04059v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.08378v1","updated":"2025-02-12T13:10:09Z","published":"2025-02-12T13:10:09Z","title":"Learning Humanoid Standing-up Control across Diverse Postures","summary":"  Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/.\n","authors":["Tao Huang","Junli Ren","Huayi Wang","Zirui Wang","Qingwei Ben","Muning Wen","Xiao Chen","Jianan Li","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.08378v1.pdf","comment":"Humanoid Standing-up Control, 12 pages"},{"id":"http://arxiv.org/abs/2502.08373v1","updated":"2025-02-12T13:05:24Z","published":"2025-02-12T13:05:24Z","title":"Uncertainty Aware Human-machine Collaboration in Camouflaged Object\n  Detection","summary":"  Camouflaged Object Detection (COD), the task of identifying objects concealed\nwithin their environments, has seen rapid growth due to its wide range of\npractical applications. A key step toward developing trustworthy COD systems is\nthe estimation and effective utilization of uncertainty. In this work, we\npropose a human-machine collaboration framework for classifying the presence of\ncamouflaged objects, leveraging the complementary strengths of computer vision\n(CV) models and noninvasive brain-computer interfaces (BCIs). Our approach\nintroduces a multiview backbone to estimate uncertainty in CV model\npredictions, utilizes this uncertainty during training to improve efficiency,\nand defers low-confidence cases to human evaluation via RSVP-based BCIs during\ntesting for more reliable decision-making. We evaluated the framework in the\nCAMO dataset, achieving state-of-the-art results with an average improvement of\n4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to\nexisting methods. For the best-performing participants, the improvements\nreached 7.6\\% in BA and 6.66\\% in the F1 score. Analysis of the training\nprocess revealed a strong correlation between our confidence measures and\nprecision, while an ablation study confirmed the effectiveness of the proposed\ntraining policy and the human-machine collaboration strategy. In general, this\nwork reduces human cognitive load, improves system reliability, and provides a\nstrong foundation for advancements in real-world COD applications and\nhuman-computer interaction. Our code and data are available at:\nhttps://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.\n","authors":["Ziyue Yang","Kehan Wang","Yuhang Ming","Yong Peng","Han Yang","Qiong Chen","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2502.08373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v2","updated":"2025-02-12T13:03:09Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v2.pdf","comment":"accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08365v1","updated":"2025-02-12T12:51:36Z","published":"2025-02-12T12:51:36Z","title":"Towards Principled Multi-Agent Task Agnostic Exploration","summary":"  In reinforcement learning, we typically refer to task-agnostic exploration\nwhen we aim to explore the environment without access to the task specification\na priori. In a single-agent setting the problem has been extensively studied\nand mostly understood. A popular approach cast the task-agnostic objective as\nmaximizing the entropy of the state distribution induced by the agent's policy,\nfrom which principles and methods follows. In contrast, little is known about\ntask-agnostic exploration in multi-agent settings, which are ubiquitous in the\nreal world. How should different agents explore in the presence of others? In\nthis paper, we address this question through a generalization to multiple\nagents of the problem of maximizing the state distribution entropy. First, we\ninvestigate alternative formulations, highlighting respective positives and\nnegatives. Then, we present a scalable, decentralized, trust-region policy\nsearch algorithm to address the problem in practical settings. Finally, we\nprovide proof of concept experiments to both corroborate the theoretical\nfindings and pave the way for task-agnostic exploration in challenging\nmulti-agent settings.\n","authors":["Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2502.08365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08363v1","updated":"2025-02-12T12:50:15Z","published":"2025-02-12T12:50:15Z","title":"Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding","summary":"  The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.\n","authors":["Konstantin Berestizshevsky","Renzo Andri","Lukas Cavigelli"],"pdf_url":"https://arxiv.org/pdf/2502.08363v1.pdf","comment":"8 pages, 11 figures, work under submission"},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.07071v2","updated":"2025-02-12T12:38:13Z","published":"2025-01-31T19:43:13Z","title":"TRADES: Generating Realistic Market Simulations with Diffusion Models","summary":"  Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.\n","authors":["Leonardo Berti","Bardh Prenkaj","Paola Velardi"],"pdf_url":"https://arxiv.org/pdf/2502.07071v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.08353v1","updated":"2025-02-12T12:28:39Z","published":"2025-02-12T12:28:39Z","title":"Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy","summary":"  With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.\n","authors":["Ruizhan Xue","Huimin Deng","Fang He","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08353v1.pdf","comment":"Submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08340v1","updated":"2025-02-12T12:07:09Z","published":"2025-02-12T12:07:09Z","title":"Hierarchical Learning-based Graph Partition for Large-scale Vehicle\n  Routing Problems","summary":"  Neural solvers based on the divide-and-conquer approach for Vehicle Routing\nProblems (VRPs) in general, and capacitated VRP (CVRP) in particular,\nintegrates the global partition of an instance with local constructions for\neach subproblem to enhance generalization. However, during the global partition\nphase, misclusterings within subgraphs have a tendency to progressively\ncompound throughout the multi-step decoding process of the learning-based\npartition policy. This suboptimal behavior in the global partition phase, in\nturn, may lead to a dramatic deterioration in the performance of the overall\ndecomposition-based system, despite using optimal local constructions. To\naddress these challenges, we propose a versatile Hierarchical Learning-based\nGraph Partition (HLGP) framework, which is tailored to benefit the partition of\nCVRP instances by synergistically integrating global and local partition\npolicies. Specifically, the global partition policy is tasked with creating the\ncoarse multi-way partition to generate the sequence of simpler two-way\npartition subtasks. These subtasks mark the initiation of the subsequent K\nlocal partition levels. At each local partition level, subtasks exclusive for\nthis level are assigned to the local partition policy which benefits from the\ninsensitive local topological features to incrementally alleviate the\ncompounded errors. This framework is versatile in the sense that it optimizes\nthe involved partition policies towards a unified objective harmoniously\ncompatible with both reinforcement learning (RL) and supervised learning (SL).\n(*Due to the notification of arXiv \"The Abstract field cannot be longer than\n1,920 characters\", the appeared Abstract is shortened. For the full Abstract,\nplease download the Article.)\n","authors":["Yuxin Pan","Ruohong Liu","Yize Chen","Zhiguang Cao","Fangzhen Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08340v1.pdf","comment":"Accepted as a Full Paper at AAMAS 2025 (24th International Conference\n  on Autonomous Agents and Multiagent Systems)"},{"id":"http://arxiv.org/abs/2502.08337v1","updated":"2025-02-12T12:00:58Z","published":"2025-02-12T12:00:58Z","title":"Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled\n  Data Center Clusters","summary":"  Reducing the environmental impact of cloud computing requires efficient\nworkload distribution across geographically dispersed Data Center Clusters\n(DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time\nshift of workloads within individual data centers (DC). This paper introduces\nGreen-DCC, which proposes a Reinforcement Learning (RL) based hierarchical\ncontroller to optimize both workload and liquid cooling dynamically in a DCC.\nBy incorporating factors such as weather, carbon intensity, and resource\navailability, Green-DCC addresses realistic constraints and interdependencies.\nWe demonstrate how the system optimizes multiple data centers synchronously,\nenabling the scope of digital twins, and compare the performance of various RL\napproaches based on carbon emissions and sustainability metrics while also\noffering a framework and benchmark simulation for broader ML research in\nsustainability.\n","authors":["Soumyendu Sarkar","Avisek Naug","Antonio Guillen","Vineet Gundecha","Ricardo Luna Gutierrez","Sahand Ghorbanpour","Sajad Mousavi","Ashwin Ramesh Babu","Desik Rengarajan","Cullen Bash"],"pdf_url":"https://arxiv.org/pdf/2502.08337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08336v1","updated":"2025-02-12T12:00:16Z","published":"2025-02-12T12:00:16Z","title":"Salience-Invariant Consistent Policy Learning for Generalization in\n  Visual Reinforcement Learning","summary":"  Generalizing policies to unseen scenarios remains a critical challenge in\nvisual reinforcement learning, where agents often overfit to the specific\nvisual observations of the training environment. In unseen environments,\ndistracting pixels may lead agents to extract representations containing\ntask-irrelevant information. As a result, agents may deviate from the optimal\nbehaviors learned during training, thereby hindering visual generalization.To\naddress this issue, we propose the Salience-Invariant Consistent Policy\nLearning (SCPL) algorithm, an efficient framework for zero-shot generalization.\nOur approach introduces a novel value consistency module alongside a dynamics\nmodule to effectively capture task-relevant representations. The value\nconsistency module, guided by saliency, ensures the agent focuses on\ntask-relevant pixels in both original and perturbed observations, while the\ndynamics module uses augmented data to help the encoder capture dynamic- and\nreward-relevant representations. Additionally, our theoretical analysis\nhighlights the importance of policy consistency for generalization. To\nstrengthen this, we introduce a policy consistency module with a KL divergence\nconstraint to maintain consistent policies across original and perturbed\nobservations.Extensive experiments on the DMC-GB, Robotic Manipulation, and\nCARLA benchmarks demonstrate that SCPL significantly outperforms\nstate-of-the-art methods in terms of generalization. Notably, SCPL achieves\naverage performance improvements of 14\\%, 39\\%, and 69\\% in the challenging DMC\nvideo hard setting, the Robotic hard setting, and the CARLA benchmark,\nrespectively.Project Page: https://sites.google.com/view/scpl-rl.\n","authors":["Sun Jingbo","Tu Songjun","Zhang Qichao","Chen Ke","Zhao Dongbin"],"pdf_url":"https://arxiv.org/pdf/2502.08336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08332v1","updated":"2025-02-12T11:56:40Z","published":"2025-02-12T11:56:40Z","title":"Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark","summary":"  The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark.\n","authors":["Yuhang Cai","Yaofei Wang","Donghui Hu","Gu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00003v4","updated":"2025-02-12T11:47:03Z","published":"2024-10-15T06:53:30Z","title":"Unsupervised Training of Diffusion Models for Feasible Solution\n  Generation in Neural Combinatorial Optimization","summary":"  Recent advancements in neural combinatorial optimization (NCO) methods have\nshown promising results in generating near-optimal solutions without the need\nfor expert-crafted heuristics. However, high performance of these approaches\noften rely on problem-specific human-expertise-based search after generating\ncandidate solutions, limiting their applicability to commonly solved CO\nproblems such as Traveling Salesman Problem (TSP). In this paper, we present\nIC/DC, an unsupervised CO framework that directly trains a diffusion model from\nscratch. We train our model in a self-supervised way to minimize the cost of\nthe solution while adhering to the problem-specific constraints. IC/DC is\nspecialized in addressing CO problems involving two distinct sets of items, and\nit does not need problem-specific search processes to generate valid solutions.\nIC/DC employs a novel architecture capable of capturing the intricate\nrelationships between items, and thereby enabling effective optimization in\nchallenging CO scenarios. IC/DC achieves state-of-the-art performance relative\nto existing NCO methods on the Parallel Machine Scheduling Problem (PMSP) and\nAsymmetric Traveling Salesman Problem (ATSP).\n","authors":["Seong-Hyun Hong","Hyun-Sung Kim","Zian Jang","Deunsol Yoon","Hyungseok Song","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2411.00003v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10827v3","updated":"2025-02-12T11:41:16Z","published":"2024-12-14T13:12:50Z","title":"Rethinking Chain-of-Thought from the Perspective of Self-Training","summary":"  Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.\n","authors":["Zongqian Wu","Baoduo Xu","Ruochen Cui","Mengmeng Zhan","Xiaofeng Zhu","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10827v3.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08317v1","updated":"2025-02-12T11:32:19Z","published":"2025-02-12T11:32:19Z","title":"Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting","summary":"  Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.\n","authors":["Jiarui Wu","Zhuo Liu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.08317v1.pdf","comment":"19 pages, accepted to NAACL Findings"},{"id":"http://arxiv.org/abs/2501.18071v2","updated":"2025-02-12T11:31:48Z","published":"2025-01-30T00:42:43Z","title":"Towards Transparent and Accurate Diabetes Prediction Using Machine\n  Learning and Explainable Artificial Intelligence","summary":"  Diabetes mellitus (DM) is a global health issue of significance that must be\ndiagnosed as early as possible and managed well. This study presents a\nframework for diabetes prediction using Machine Learning (ML) models,\ncomplemented with eXplainable Artificial Intelligence (XAI) tools, to\ninvestigate both the predictive accuracy and interpretability of the\npredictions from ML models. Data Preprocessing is based on the Synthetic\nMinority Oversampling Technique (SMOTE) and feature scaling used on the\nDiabetes Binary Health Indicators dataset to deal with class imbalance and\nvariability of clinical features. The ensemble model provided high accuracy,\nwith a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General\nHealth, Income, and Physical Activity were the most influential predictors\nobtained from the model explanations. The results of this study suggest that ML\ncombined with XAI is a promising means of developing accurate and\ncomputationally transparent tools for use in healthcare systems.\n","authors":["Pir Bakhsh Khokhar","Viviana Pentangelo","Fabio Palomba","Carmine Gravino"],"pdf_url":"https://arxiv.org/pdf/2501.18071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08302v1","updated":"2025-02-12T11:03:51Z","published":"2025-02-12T11:03:51Z","title":"HDT: Hierarchical Discrete Transformer for Multivariate Time Series\n  Forecasting","summary":"  Generative models have gained significant attention in multivariate time\nseries forecasting (MTS), particularly due to their ability to generate\nhigh-fidelity samples. Forecasting the probability distribution of multivariate\ntime series is a challenging yet practical task. Although some recent attempts\nhave been made to handle this task, two major challenges persist: 1) some\nexisting generative methods underperform in high-dimensional multivariate time\nseries forecasting, which is hard to scale to higher dimensions; 2) the\ninherent high-dimensional multivariate attributes constrain the forecasting\nlengths of existing generative models. In this paper, we point out that\ndiscrete token representations can model high-dimensional MTS with faster\ninference time, and forecasting the target with long-term trends of itself can\nextend the forecasting length with high accuracy. Motivated by this, we propose\na vector quantized framework called Hierarchical Discrete Transformer (HDT)\nthat models time series into discrete token representations with l2\nnormalization enhanced vector quantized strategy, in which we transform the MTS\nforecasting into discrete tokens generation. To address the limitations of\ngenerative models in long-term forecasting, we propose a hierarchical discrete\nTransformer. This model captures the discrete long-term trend of the target at\nthe low level and leverages this trend as a condition to generate the discrete\nrepresentation of the target at the high level that introduces the features of\nthe target itself to extend the forecasting length in high-dimensional MTS.\nExtensive experiments on five popular MTS datasets verify the effectiveness of\nour proposed method.\n","authors":["Shibo Feng","Peilin Zhao","Liu Liu","Pengcheng Wu","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.08302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08301v1","updated":"2025-02-12T11:02:59Z","published":"2025-02-12T11:02:59Z","title":"Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks","summary":"  Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.\n","authors":["Laurne Vaugrante","Francesca Carlon","Maluna Menke","Thilo Hagendorff"],"pdf_url":"https://arxiv.org/pdf/2502.08301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08298v1","updated":"2025-02-12T10:58:57Z","published":"2025-02-12T10:58:57Z","title":"Improving Existing Optimization Algorithms with LLMs","summary":"  The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/\n","authors":["Camilo Chacn Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2502.08298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09388v2","updated":"2025-02-12T10:55:54Z","published":"2024-12-12T15:56:20Z","title":"All You Need in Knowledge Distillation Is a Tailored Coordinate System","summary":"  Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.\n","authors":["Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.09388v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.03824v2","updated":"2025-02-12T10:45:25Z","published":"2025-02-06T07:19:59Z","title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","summary":"  LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.\n","authors":["Minsang Kim","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2502.03824v2.pdf","comment":"the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted"},{"id":"http://arxiv.org/abs/2502.08287v1","updated":"2025-02-12T10:44:45Z","published":"2025-02-12T10:44:45Z","title":"CRISP: A Framework for Cryo-EM Image Segmentation and Processing with\n  Conditional Random Field","summary":"  Differentiating signals from the background in micrographs is a critical\ninitial step for cryogenic electron microscopy (cryo-EM), yet it remains\nlaborious due to low signal-to-noise ratio (SNR), the presence of contaminants\nand densely packed particles of varying sizes. Although image segmentation has\nrecently been introduced to distinguish particles at the pixel level, the low\nSNR complicates the automated generation of accurate annotations for training\nsupervised models. Moreover, platforms for systematically comparing different\ndesign choices in pipeline construction are lacking. Thus, a modular framework\nis essential to understand the advantages and limitations of this approach and\ndrive further development. To address these challenges, we present a pipeline\nthat automatically generates high-quality segmentation maps from cryo-EM data\nto serve as ground truth labels. Our modular framework enables the selection of\nvarious segmentation models and loss functions. We also integrate Conditional\nRandom Fields (CRFs) with different solvers and feature sets to refine coarse\npredictions, thereby producing fine-grained segmentation. This flexibility\nfacilitates optimal configurations tailored to cryo-EM datasets. When trained\non a limited set of micrographs, our approach achieves over 90% accuracy,\nrecall, precision, Intersection over Union (IoU), and F1-score on synthetic\ndata. Furthermore, to demonstrate our framework's efficacy in downstream\nanalyses, we show that the particles extracted by our pipeline produce 3D\ndensity maps with higher resolution than those generated by existing particle\npickers on real experimental datasets, while achieving performance comparable\nto that of manually curated datasets from experts.\n","authors":["Szu-Chi Chung","Po-Cheng Chou"],"pdf_url":"https://arxiv.org/pdf/2502.08287v1.pdf","comment":"31 pages, 28 Figures"},{"id":"http://arxiv.org/abs/2502.08282v1","updated":"2025-02-12T10:41:21Z","published":"2025-02-12T10:41:21Z","title":"Individualised Treatment Effects Estimation with Composite Treatments\n  and Composite Outcomes","summary":"  Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods.\n","authors":["Vinod Kumar Chauhan","Lei Clifton","Gaurav Nigam","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2502.08282v1.pdf","comment":"6 pages (double column), 4 figures"},{"id":"http://arxiv.org/abs/2502.08279v1","updated":"2025-02-12T10:36:55Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.02873 by other authors"},{"id":"http://arxiv.org/abs/2501.14856v2","updated":"2025-02-12T10:36:26Z","published":"2025-01-24T17:15:49Z","title":"Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative\n  Framework for Imitation Learning from Observation","summary":"  This paper introduces a new imitation learning framework based on\nenergy-based generative models capable of learning complex, physics-dependent,\nrobot motion policies through state-only expert motion trajectories. Our\nalgorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR),\nconstructs several perturbed versions of the expert's motion data distribution\nand learns smooth, and well-defined representations of the data distribution's\nenergy function using denoising score matching. We propose to use these learnt\nenergy functions as reward functions to learn imitation policies via\nreinforcement learning. We also present a strategy to gradually switch between\nthe learnt energy functions, ensuring that the learnt rewards are always\nwell-defined in the manifold of policy-generated samples. We evaluate our\nalgorithm on complex humanoid tasks such as locomotion and martial arts and\ncompare it with state-only adversarial imitation learning algorithms like\nAdversarial Motion Priors (AMP). Our framework sidesteps the optimisation\nchallenges of adversarial imitation learning techniques and produces results\ncomparable to AMP in several quantitative metrics across multiple imitation\nsettings.\n","authors":["Anish Abhijit Diwan","Julen Urain","Jens Kober","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2501.14856v2.pdf","comment":"Accepted as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2025. Revised to include review feedback"},{"id":"http://arxiv.org/abs/2403.15405v3","updated":"2025-02-12T10:33:58Z","published":"2024-02-20T13:42:50Z","title":"Predicting Parkinson's disease trajectory using clinical and functional\n  MRI features: a reproduction and replication study","summary":"  Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly\nunderstood physiopathology and no established biomarkers for the diagnosis of\nearly stages and for prediction of disease progression. Several neuroimaging\nbiomarkers have been studied recently, but these are susceptible to several\nsources of variability related for instance to cohort selection or image\nanalysis. In this context, an evaluation of the robustness of such biomarkers\nto variations in the data processing workflow is essential. This study is part\nof a larger project investigating the replicability of potential neuroimaging\nbiomarkers of PD. Here, we attempt to reproduce (re-implementing the\nexperiments with the same data, same method) and replicate (different data\nand/or method) the models described in [1] to predict individual's PD current\nstate and progression using demographic, clinical and neuroimaging features\n(fALFF and ReHo extracted from resting-state fMRI). We use the Parkinson's\nProgression Markers Initiative dataset (PPMI, ppmi-info.org), as in [1] and aim\nto reproduce the original cohort, imaging features and machine learning models\nas closely as possible using the information available in the paper and the\ncode. We also investigated methodological variations in cohort selection,\nfeature extraction pipelines and sets of input features. Different criteria\nwere used to evaluate the reproduction and compare the reproduced results with\nthe original ones. Notably, we obtained significantly better than chance\nperformance using the analysis pipeline closest to that in the original study\n(R2 \\&gt; 0), which is consistent with its findings. Moreover, using derived\ndata provided by the authors of the original study, we were able to make an\nexact reproduction and managed to obtain results that were close to the\noriginal ones. The challenges encountered while reproducing and replicating the\noriginal work are likely explained by the complexity of neuroimaging studies,\nin particular in clinical settings. We provide recommendations to further\nfacilitate the reproducibility of such studies in the future.\n","authors":["Elodie Germani","Nikhil Baghwat","Mathieu Dugr","Rmi Gau","Albert Montillo","Kevin Nguyen","Andrzej Sokolowski","Madeleine Sharp","Jean-Baptiste Poline","Tristan Glatard"],"pdf_url":"https://arxiv.org/pdf/2403.15405v3.pdf","comment":"PLoS ONE, In press"},{"id":"http://arxiv.org/abs/2405.18272v2","updated":"2025-02-12T10:22:58Z","published":"2024-05-28T15:23:46Z","title":"Metaheuristics and Large Language Models Join Forces: Toward an\n  Integrated Optimization Approach","summary":"  Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern.\n","authors":["Camilo Chacn Sartori","Christian Blum","Filippo Bistaffa","Guillem Rodrguez Corominas"],"pdf_url":"https://arxiv.org/pdf/2405.18272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08266v1","updated":"2025-02-12T10:19:50Z","published":"2025-02-12T10:19:50Z","title":"Dealing with Annotator Disagreement in Hate Speech Classification","summary":"  Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.\n","authors":["Somaiyeh Dehghan","Mehmet Umut Sen","Berrin Yanikoglu"],"pdf_url":"https://arxiv.org/pdf/2502.08266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08265v1","updated":"2025-02-12T10:17:18Z","published":"2025-02-12T10:17:18Z","title":"Exploring the Potential of Large Language Models to Simulate Personality","summary":"  With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.\n","authors":["Maria Molchanova","Anna Mikhailova","Anna Korzanova","Lidiia Ostyakova","Alexandra Dolidze"],"pdf_url":"https://arxiv.org/pdf/2502.08265v1.pdf","comment":"Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on\n  EMNLP2024"},{"id":"http://arxiv.org/abs/2309.17170v2","updated":"2025-02-12T10:09:27Z","published":"2023-09-29T12:07:08Z","title":"Robotic Grasping of Harvested Tomato Trusses Using Vision and Online\n  Learning","summary":"  Currently, truss tomato weighing and packaging require significant manual\nwork. The main obstacle to automation lies in the difficulty of developing a\nreliable robotic grasping system for already harvested trusses. We propose a\nmethod to grasp trusses that are stacked in a crate with considerable clutter,\nwhich is how they are commonly stored and transported after harvest. The method\nconsists of a deep learning-based vision system to first identify the\nindividual trusses in the crate and then determine a suitable grasping location\non the stem. To this end, we have introduced a grasp pose ranking algorithm\nwith online learning capabilities. After selecting the most promising grasp\npose, the robot executes a pinch grasp without needing touch sensors or\ngeometric models. Lab experiments with a robotic manipulator equipped with an\neye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all\ntrusses from a pile. 93% of the trusses were successfully grasped on the first\ntry, while the remaining 7% required more attempts.\n","authors":["Luuk van den Bent","Toms Coleman","Robert Babuka"],"pdf_url":"https://arxiv.org/pdf/2309.17170v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.10957v2","updated":"2025-02-12T10:06:09Z","published":"2025-01-19T06:11:02Z","title":"MARIO: A Mixed Annotation Framework For Polyp Segmentation","summary":"  Existing polyp segmentation models are limited by high labeling costs and the\nsmall size of datasets. Additionally, vast polyp datasets remain underutilized\nbecause these models typically rely on a single type of annotation. To address\nthis dilemma, we introduce MARIO, a mixed supervision model designed to\naccommodate various annotation types, significantly expanding the range of\nusable data. MARIO learns from underutilized datasets by incorporating five\nforms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and\npoint-level. Each form of supervision is associated with a tailored loss that\neffectively leverages the supervision labels while minimizing the noise. This\nallows MARIO to move beyond the constraints of relying on a single annotation\ntype. Furthermore, MARIO primarily utilizes dataset with weak and cheap\nannotations, reducing the dependence on large-scale, fully annotated ones.\nExperimental results across five benchmark datasets demonstrate that MARIO\nconsistently outperforms existing methods, highlighting its efficacy in\nbalancing trade-offs between different forms of supervision and maximizing\npolyp segmentation performance\n","authors":["Haoyang Li","Yiwen Hu","Jun Wei","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2501.10957v2.pdf","comment":"Accepted by IEEE ISBI 2025 4-page paper"},{"id":"http://arxiv.org/abs/2502.08259v1","updated":"2025-02-12T10:05:25Z","published":"2025-02-12T10:05:25Z","title":"Balancing optimism and pessimism in offline-to-online learning","summary":"  We consider what we call the offline-to-online learning setting, focusing on\nstochastic finite-armed bandit problems. In offline-to-online learning, a\nlearner starts with offline data collected from interactions with an unknown\nenvironment in a way that is not under the learner's control. Given this data,\nthe learner begins interacting with the environment, gradually improving its\ninitial strategy as it collects more data to maximize its total reward. The\nlearner in this setting faces a fundamental dilemma: if the policy is deployed\nfor only a short period, a suitable strategy (in a number of senses) is the\nLower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can\neffectively compete with any policy that is sufficiently \"covered\" by the\noffline data. However, for longer time horizons, a preferred strategy is the\nUpper Confidence Bound (UCB) algorithm, which is based on optimism. Over time,\nUCB converges to the performance of the optimal policy at a rate that is nearly\nthe best possible among all online algorithms. In offline-to-online learning,\nhowever, UCB initially explores excessively, leading to worse short-term\nperformance compared to LCB. This suggests that a learner not in control of how\nlong its policy will be in use should start with LCB for short horizons and\ngradually transition to a UCB-like strategy as more rounds are played. This\narticle explores how and why this transition should occur. Our main result\nshows that our new algorithm performs nearly as well as the better of LCB and\nUCB at any point in time. The core idea behind our algorithm is broadly\napplicable, and we anticipate that our results will extend beyond the\nmulti-armed bandit setting.\n","authors":["Sentenac Flore","Lee Albin","Szepesvari Csaba"],"pdf_url":"https://arxiv.org/pdf/2502.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01969v2","updated":"2025-02-12T09:58:15Z","published":"2024-11-04T10:44:46Z","title":"Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning","summary":"  Toddlers learn to recognize objects from different viewpoints with almost no\nsupervision. Recent works argue that toddlers develop this ability by mapping\nclose-in-time visual inputs to similar representations while interacting with\nobjects. High acuity vision is only available in the central visual field,\nwhich May explain why toddlers (much like adults) constantly move around their\ngaze during such interactions. It is unclear whether/how much toddlers curate\ntheir visual experience through these eye movements to support their learning\nof object representations. In this work, we explore whether a bio-inspired\nvisual learning model can harness toddlers' gaze behavior during a play session\nto develop view-invariant object recognition. Exploiting head-mounted eye\ntracking during dyadic play, we simulate toddlers' central visual field\nexperience by cropping image regions centered on the gaze location. This visual\nstream feeds time-based self-supervised learning algorithms. Our experiments\ndemonstrate that toddlers' gaze strategy supports the learning of invariant\nobject representations. Our analysis also reveals that the limited size of the\ncentral visual field where acuity is high is crucial for this. We further find\nthat toddlers' visual experience elicits more robust representations compared\nto adults', mostly because toddlers look at objects they hold themselves for\nlonger bouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.\n","authors":["Zhengyang Yu","Arthur Aubret","Marcel C. Raabe","Jane Yang","Chen Yu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2411.01969v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.15588v5","updated":"2025-02-12T09:50:02Z","published":"2024-07-22T12:25:48Z","title":"Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple\n  Matching with Entity and Relation Texts","summary":"  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n","authors":["Soojin Yoon","Sungho Ko","Tongyoung Kim","SeongKu Kang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15588v5.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2412.16205v2","updated":"2025-02-12T09:48:29Z","published":"2024-12-17T10:53:12Z","title":"Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface\n  Vehicles","summary":"  Unmanned Surface Vehicles (USVs) have become critical tools for marine\nexploration, environmental monitoring, and autonomous navigation. Accurate\nestimation of wave direction is essential for improving USV navigation and\nensuring operational safety, but traditional methods often suffer from high\ncosts and limited spatial resolution. This paper proposes a machine\nlearning-based approach leveraging LSTM (Long Short-Term Memory) networks to\npredict wave direction using sensor data collected from USVs. Experimental\nresults show the capability of the LSTM model to learn temporal dependencies\nand provide accurate predictions, outperforming simpler baselines.\n","authors":["Manele Ait Habouche","Mickal Kerboeuf","Goulven Guillou","Jean-Philippe Babau"],"pdf_url":"https://arxiv.org/pdf/2412.16205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12676v3","updated":"2025-02-12T09:47:58Z","published":"2023-12-20T00:31:43Z","title":"Bayesian Analysis of Combinatorial Gaussian Process Bandits","summary":"  We consider the combinatorial volatile Gaussian process (GP) semi-bandit\nproblem. Each round, an agent is provided a set of available base arms and must\nselect a subset of them to maximize the long-term cumulative reward. We study\nthe Bayesian setting and provide novel Bayesian cumulative regret bounds for\nthree GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend\nprevious results for GP-UCB and GP-TS to the infinite, volatile and\ncombinatorial setting, and to the best of our knowledge, we provide the first\nregret bound for GP-BayesUCB. Volatile arms encompass other widely considered\nbandit problems such as contextual bandits. Furthermore, we employ our\nframework to address the challenging real-world problem of online\nenergy-efficient navigation, where we demonstrate its effectiveness compared to\nthe alternatives.\n","authors":["Jack Sandberg","Niklas kerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2312.12676v3.pdf","comment":"34 pages, 9 figures. Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2409.02483v5","updated":"2025-02-12T09:39:06Z","published":"2024-09-04T07:20:01Z","title":"TASAR: Transfer-based Attack on Skeletal Action Recognition","summary":"  Skeletal sequence data, as a widely employed representation of human actions,\nare crucial in Human Activity Recognition (HAR). Recently, adversarial attacks\nhave been proposed in this area, which exposes potential security concerns, and\nmore importantly provides a good tool for model robustness test. Within this\nresearch, transfer-based attack is an important tool as it mimics the\nreal-world scenario where an attacker has no knowledge of the target model, but\nis under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR\nattacks exhibit weak adversarial transferability and the reason remains largely\nunknown. In this paper, we investigate this phenomenon via the characterization\nof the loss function. We find that one prominent indicator of poor\ntransferability is the low smoothness of the loss function. Led by this\nobservation, we improve the transferability by properly smoothening the loss\nwhen computing the adversarial examples. This leads to the first Transfer-based\nAttack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened\nmodel posterior of pre-trained surrogates, which is achieved by a new\npost-train Dual Bayesian optimization strategy. Furthermore, unlike existing\ntransfer-based methods which overlook the temporal coherence within sequences,\nTASAR incorporates motion dynamics into the Bayesian attack, effectively\ndisrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation,\nwe build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR\nmodels, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive\nresults demonstrate the superiority of TASAR. Our benchmark enables easy\ncomparisons for future studies, with the code available in the\nhttps://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Ajian Liu","Xiaoshuai Hao","Xingxing Wei","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02483v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08235v1","updated":"2025-02-12T09:23:26Z","published":"2025-02-12T09:23:26Z","title":"The Danger of Overthinking: Examining the Reasoning-Action Dilemma in\n  Agentic Tasks","summary":"  Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving\ncapabilities, but their effectiveness in interactive environments can be\nlimited. This paper introduces and analyzes overthinking in LRMs. A phenomenon\nwhere models favor extended internal reasoning chains over environmental\ninteraction. Through experiments on software engineering tasks using SWE Bench\nVerified, we observe three recurring patterns: Analysis Paralysis, Rogue\nActions, and Premature Disengagement. We propose a framework to study these\nbehaviors, which correlates with human expert assessments, and analyze 4018\ntrajectories. We observe that higher overthinking scores correlate with\ndecreased performance, with reasoning models exhibiting stronger tendencies\ntoward overthinking compared to non-reasoning models. Our analysis reveals that\nsimple efforts to mitigate overthinking in agentic environments, such as\nselecting the solution with the lower overthinking score, can improve model\nperformance by almost 30% while reducing computational costs by 43%. These\nresults suggest that mitigating overthinking has strong practical implications.\nWe suggest that by leveraging native function-calling capabilities and\nselective reinforcement learning overthinking tendencies could be mitigated. We\nalso open-source our evaluation framework and dataset to facilitate research in\nthis direction at https://github.com/AlexCuadron/Overthinking.\n","authors":["Alejandro Cuadron","Dacheng Li","Wenjie Ma","Xingyao Wang","Yichuan Wang","Siyuan Zhuang","Shu Liu","Luis Gaspar Schroeder","Tian Xia","Huanzhi Mao","Nicholas Thumiger","Aditya Desai","Ion Stoica","Ana Klimovic","Graham Neubig","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2502.08235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v1","updated":"2025-02-12T09:12:30Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v1.pdf","comment":"Under review at ICML 2025, 8 pages 5 figures"},{"id":"http://arxiv.org/abs/2502.07709v2","updated":"2025-02-12T08:52:52Z","published":"2025-02-11T17:08:00Z","title":"MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces","summary":"  Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.\n","authors":["Loris Gaven","Thomas Carta","Clment Romac","Cdric Colas","Sylvain Lamprier","Olivier Sigaud","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2502.07709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08211v1","updated":"2025-02-12T08:40:57Z","published":"2025-02-12T08:40:57Z","title":"Quality over Quantity: Boosting Data Efficiency Through Ensembled\n  Multimodal Data Curation","summary":"  In an era overwhelmed by vast amounts of data, the effective curation of\nweb-crawl datasets is essential for optimizing model performance. This paper\ntackles the challenges associated with the unstructured and heterogeneous\nnature of such datasets. Traditional heuristic curation methods often\ninadequately capture complex features, resulting in biases and the exclusion of\nrelevant data. We introduce an advanced, learning-driven approach, Ensemble\nCuration Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel\nquality-guided deduplication method to ensure balanced feature distributions.\nEcoDatum strategically integrates various unimodal and multimodal data curation\noperators within a weak supervision ensemble framework, utilizing automated\noptimization to score each data point effectively. EcoDatum, which\nsignificantly improves the data curation quality and efficiency, outperforms\nexisting state-of-the-art (SOTA) techniques, ranked 1st on the DataComp\nleaderboard, with an average performance score of 0.182 across 38 diverse\nevaluation datasets. This represents a 28% improvement over the DataComp\nbaseline method, demonstrating its effectiveness in improving dataset curation\nand model training efficiency.\n","authors":["Jinda Xu","Yuhao Song","Daming Wang","Weiwei Zhao","Minghua Chen","Kangliang Chen","Qinya Li"],"pdf_url":"https://arxiv.org/pdf/2502.08211v1.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.08610v1","updated":"2025-02-12T17:57:54Z","published":"2025-02-12T17:57:54Z","title":"Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis\n  of Gaps in Current AI Standards","summary":"  As AI systems integrate into critical infrastructure, security gaps in AI\ncompliance frameworks demand urgent attention. This paper audits and quantifies\nsecurity risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI\nand Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk\nassessment methodology, we develop four key metrics: Risk Severity Index (RSI),\nAttack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and\nRoot Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns\nacross the frameworks, exposing significant gaps. NIST fails to address 69.23\npercent of identified risks, ALTAI has the highest attack vector vulnerability\n(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with\n80.00 percent of high-risk concerns remaining unresolved. Root cause analysis\nhighlights under-defined processes (ALTAI RCVS = 033) and weak implementation\nguidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings\nemphasize the need for stronger, enforceable security controls in AI\ncompliance. We offer targeted recommendations to enhance security posture and\nbridge the gap between compliance and real-world AI risks.\n","authors":["Keerthana Madhavan","Abbas Yazdinejad","Fattane Zarrinkalam","Ali Dehghantanha"],"pdf_url":"https://arxiv.org/pdf/2502.08610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11546v2","updated":"2025-02-12T17:25:47Z","published":"2025-01-20T15:37:53Z","title":"An Exploratory Study on the Engineering of Security Features","summary":"  Software security is of utmost importance for most software systems.\nDevelopers must systematically select, plan, design, implement, and especially,\nmaintain and evolve security features -- functionalities to mitigate attacks or\nprotect personal data such as cryptography or access control -- to ensure the\nsecurity of their software. Although security features are usually available in\nlibraries, integrating security features requires writing and maintaining\nadditional security-critical code. While there have been studies on the use of\nsuch libraries, surprisingly little is known about how developers engineer\nsecurity features, how they select what security features to implement and\nwhich ones may require custom implementation, and the implications for\nmaintenance. As a result, we currently rely on assumptions that are largely\nbased on common sense or individual examples. However, to provide them with\neffective solutions, researchers need hard empirical data to understand what\npractitioners need and how they view security -- data that we currently lack.\nTo fill this gap, we contribute an exploratory study with 26 knowledgeable\nindustrial participants. We study how security features of software systems are\nselected and engineered in practice, what their code-level characteristics are,\nand what challenges practitioners face. Based on the empirical data gathered,\nwe provide insights into engineering practices and validate four common\nassumptions.\n","authors":["Kevin Hermann","Sven Peldszus","Jan-Philipp Steghfer","Thorsten Berger"],"pdf_url":"https://arxiv.org/pdf/2501.11546v2.pdf","comment":"Accepted at the 47th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2025)"},{"id":"http://arxiv.org/abs/2304.14540v7","updated":"2025-02-12T16:35:11Z","published":"2023-04-27T21:44:59Z","title":"Efficient IAM Greybox Penetration Testing","summary":"  Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nmisconfigured IAM can lead to privilege escalation (PE) attacks, causing\nsignificant economic loss. Third-party cloud security services detect such\nissues using whitebox penetration testing, which requires full access to IAM\nconfigurations. However, since these configurations often contain sensitive\ndata, customers must manually anonymize them to protect their privacy. To\naddress the dual challenges of anonymization and data privacy, we introduce\nTAC, the first greybox penetration testing approach for third-party services to\nefficiently detect IAM PEs. Instead of requiring customers to blindly anonymize\ntheir entire IAM configuration, TAC intelligently interacts with customers by\nquerying only a small fraction of information in the IAM configuration that is\nnecessary for PE detection. To achieve this, TAC integrates two key\ninnovations: (1) a comprehensive IAM modeling approach to detect a wide range\nof IAM PEs using partial information collected from query responses, and (2) a\nquery optimization mechanism leveraging Reinforcement Learning (RL) and Graph\nNeural Networks (GNNs) to minimize customer inputs. Additionally, to address\nthe scarcity of real-world IAM PE datasets, we introduce IAMVulGen, a\nsynthesizer that generates a large number of diverse IAM PEs that mimic\nreal-world scenarios. Experimental results on both synthetic and real-world\nbenchmarks show that TAC, as a greybox approach, achieves competitively low\nand, in some cases, significantly lower false negative rates than\nstate-ofthe-art whitebox approaches, while utilizing a limited number of\nqueries.\n","authors":["Yang Hu","Wenxi Wang","Sarfraz Khurshid","Mohit Tiwari"],"pdf_url":"https://arxiv.org/pdf/2304.14540v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08535v1","updated":"2025-02-12T16:18:18Z","published":"2025-02-12T16:18:18Z","title":"The Forest Behind the Tree: Revealing Hidden Smart Home Communication\n  Patterns via Strategic Traffic Blocking","summary":"  Network-connected Smart Home devices are becoming increasingly common,\ncreating potential security and privacy risks. Previous research has shown\nthese devices follow predictable network communication patterns, allowing\nresearchers to model their normal network behavior and detect potential\nsecurity breaches. However, existing approaches only observe traffic passively\nrather than actively trying to disturb it. We present a framework that\ngenerates comprehensive network signatures for Smart Home devices by\nsystematically blocking previously observed traffic patterns to reveal new,\nhidden patterns that other methods miss. These signatures are structured as\nbehavior trees, where each child node represents network flows that occur when\nthe parent node's traffic is blocked. We applied this framework on ten\nreal-world devices under 26 usage scenarios, discovering 138 unique flows, of\nwhich 27 (20%) are information gained through our multi-level tree approach,\ncompared to state-of-the-art single-level traffic analysis.\n","authors":["Franois De Keersmaeker","Rmi Van Boxem","Cristel Pelsser","Ramin Sadre"],"pdf_url":"https://arxiv.org/pdf/2502.08535v1.pdf","comment":"16 pages of body text, 19 pages total, single column, ACM conference\n  template"},{"id":"http://arxiv.org/abs/2502.08467v1","updated":"2025-02-12T15:02:30Z","published":"2025-02-12T15:02:30Z","title":"Dancer in the Dark: Synthesizing and Evaluating Polyglots for Blind\n  Cross-Site Scripting","summary":"  Cross-Site Scripting (XSS) is a prevalent and well known security problem in\nweb applications. Numerous methods to automatically analyze and detect these\nvulnerabilities exist. However, all of these methods require that either code\nor feedback from the application is available to guide the detection process.\nIn larger web applications, inputs can propagate from a frontend to an internal\nbackend that provides no feedback to the outside. None of the previous\napproaches are applicable in this scenario, known as blind XSS (BXSS). In this\npaper, we address this problem and present the first comprehensive study on\nBXSS. As no feedback channel exists, we verify the presence of vulnerabilities\nthrough blind code execution. For this purpose, we develop a method for\nsynthesizing polyglots, small XSS payloads that execute in all common injection\ncontexts. Seven of these polyglots are already sufficient to cover a\nstate-of-the-art XSS testbed. In a validation on real-world client-side\nvulnerabilities, we show that their XSS detection rate is on par with existing\ntaint tracking approaches. Based on these polyglots, we conduct a study of BXSS\nvulnerabilities on the Tranco Top 100,000 websites. We discover 20\nvulnerabilities in 18 web-based backend systems. These findings demonstrate the\nefficacy of our detection approach and point at a largely unexplored attack\nsurface in web security.\n","authors":["Robin Kirchner","Jonas Mller","Marius Musch","David Klein","Konrad Rieck","Martin Johns"],"pdf_url":"https://arxiv.org/pdf/2502.08467v1.pdf","comment":"SEC '24: Proceedings of the 33rd USENIX Conference on Security\n  Symposium Article No.: 376, Pages 6723 - 6740, ISBN 978-1-939133-44-1"},{"id":"http://arxiv.org/abs/2408.02416v2","updated":"2025-02-12T14:52:56Z","published":"2024-08-05T12:20:39Z","title":"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models","summary":"  The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval.\n","authors":["Zi Liang","Haibo Hu","Qingqing Ye","Yaxin Xiao","Haoyang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02416v2.pdf","comment":"Source Code: https://github.com/liangzid/PromptExtractionEval"},{"id":"http://arxiv.org/abs/2502.08447v1","updated":"2025-02-12T14:39:30Z","published":"2025-02-12T14:39:30Z","title":"Deserialization Gadget Chains are not a Pathological Problem in\n  Android:an In-Depth Study of Java Gadget Chains in AOSP","summary":"  Inter-app communication is a mandatory and security-critical functionality of\noperating systems, such as Android. On the application level, Android\nimplements this facility through Intents, which can also transfer non-primitive\nobjects using Java's Serializable API. However, the Serializable API has a long\nhistory of deserialization vulnerabilities, specifically deserialization gadget\nchains. Research endeavors have been heavily directed towards the detection of\ndeserialization gadget chains on the Java platform. Yet, there is little\nknowledge about the existence of gadget chains within the Android platform. We\naim to close this gap by searching gadget chains in the Android SDK, Android's\nofficial development libraries, as well as frequently used third-party\nlibraries. To handle this large dataset, we design a gadget chain detection\ntool optimized for soundness and efficiency. In a benchmark on the full\nYsoserial dataset, it achieves similarly sound results to the state-of-the-art\nin significantly less time. Using our tool, we first show that the Android SDK\ncontains almost the same trampoline gadgets as the Java Class Library. We also\nfind that one can trigger Java native serialization through Android's Parcel\nAPI. Yet, running our tool on the Android SDK and 1,200 Android dependencies,\nin combination with a comprehensive sink dataset, yields no security-critical\ngadget chains. This result opposes the general notion of Java deserialization\ngadget chains being a widespread problem. Instead, the issue appears to be more\nnuanced, and we provide a perspective on where to direct further research.\n","authors":["Bruno Kreyssig","Timothe Riom","Sabine Houy","Alexandre Bartel","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2502.08447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01240v2","updated":"2025-02-12T14:23:38Z","published":"2025-02-03T11:01:11Z","title":"The Impact of Logic Locking on Confidentiality: An Automated Evaluation","summary":"  Logic locking secures hardware designs in untrusted foundries by\nincorporating key-driven gates to obscure the original blueprint. While this\nmethod safeguards the integrated circuit from malicious alterations during\nfabrication, its influence on data confidentiality during runtime has been\nignored. In this study, we employ path sensitization to formally examine the\nimpact of logic locking on confidentiality. By applying three representative\nlogic locking mechanisms on open-source cryptographic benchmarks, we utilize an\nautomatic test pattern generation framework to evaluate the effect of locking\non cryptographic encryption keys and sensitive data signals. Our analysis\nreveals that logic locking can inadvertently cause sensitive data leakage when\nincorrect logic locking keys are used. We show that a single malicious logic\nlocking key can expose over 70% of an encryption key. If an adversary gains\ncontrol over other inputs, the entire encryption key can be compromised. This\nresearch uncovers a significant security vulnerability in logic locking and\nemphasizes the need for comprehensive security assessments that extend beyond\nkey-recovery attacks.\n","authors":["Lennart M. Reimann","Evgenii Rezunov","Dominik Germek","Luca Collini","Christian Pilato","Ramesh Karri","Rainer Leupers"],"pdf_url":"https://arxiv.org/pdf/2502.01240v2.pdf","comment":"8 pages, accepted at 26th International Symposium on Quality\n  Electronic Design (ISQED'25)"},{"id":"http://arxiv.org/abs/2502.08401v1","updated":"2025-02-12T13:45:59Z","published":"2025-02-12T13:45:59Z","title":"Presentations of Racks","summary":"  Presentations of racks is studied and a cryptographic protocol defined on\nracks is proposed.\n","authors":["Seluk Kayacan"],"pdf_url":"https://arxiv.org/pdf/2502.08401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14987v2","updated":"2025-02-12T13:16:19Z","published":"2024-05-23T18:40:15Z","title":"Simultaneous quantum identity authentication scheme utilizing\n  entanglement swapping with secret key preservation","summary":"  Unconditional security in quantum key distribution (QKD) relies on\nauthenticating the identities of users involved in key distribution. While\nclassical identity authentication schemes were initially utilized in QKD\nimplementations, concerns regarding their vulnerability have prompted the\nexploration of quantum identity authentication (QIA) protocols. In this study,\nwe introduce a new protocol for QIA, derived from the concept of controlled\nsecure direct quantum communication. Our proposed scheme facilitates\nsimultaneous authentication between two users, Alice and Bob, leveraging Bell\nstates with the assistance of a third party, Charlie. Through rigorous security\nanalysis, we demonstrate that the proposed protocol withstands various known\nattacks, including impersonation, intercept and resend and impersonated\nfraudulent attacks. Additionally, we establish the relevance of the proposed\nprotocol by comparing it with the existing protocols of similar type.\n","authors":["Arindam Dutta","Anirban Pathak"],"pdf_url":"https://arxiv.org/pdf/2405.14987v2.pdf","comment":"A new scheme for quantum identity authentication is proposed"},{"id":"http://arxiv.org/abs/2502.08341v1","updated":"2025-02-12T12:10:26Z","published":"2025-02-12T12:10:26Z","title":"SoK: Where to Fuzz? Assessing Target Selection Methods in Directed\n  Fuzzing","summary":"  A common paradigm for improving fuzzing performance is to focus on selected\nregions of a program rather than its entirety. While previous work has largely\nexplored how these locations can be reached, their selection, that is, the\nwhere, has received little attention so far. A common paradigm for improving\nfuzzing performance is to focus on selected regions of a program rather than\nits entirety. While previous work has largely explored how these locations can\nbe reached, their selection, that is, the where, has received little attention\nso far. In this paper, we fill this gap and present the first comprehensive\nanalysis of target selection methods for fuzzing. To this end, we examine\npapers from leading security and software engineering conferences, identifying\nprevalent methods for choosing targets. By modeling these methods as general\nscoring functions, we are able to compare and measure their efficacy on a\ncorpus of more than 1,600 crashes from the OSS-Fuzz project. Our analysis\nprovides new insights for target selection in practice: First, we find that\nsimple software metrics significantly outperform other methods, including\ncommon heuristics used in directed fuzzing, such as recently modified code or\nlocations with sanitizer instrumentation. Next to this, we identify language\nmodels as a promising choice for target selection. In summary, our work offers\na new perspective on directed fuzzing, emphasizing the role of target selection\nas an orthogonal dimension to improve performance.\n","authors":["Felix Weissberg","Jonas Mller","Tom Ganz","Erik Imgrund","Lukas Pirch","Lukas Seidel","Moritz Schloegel","Thorsten Eisenhofer","Konrad Rieck"],"pdf_url":"https://arxiv.org/pdf/2502.08341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08332v1","updated":"2025-02-12T11:56:40Z","published":"2025-02-12T11:56:40Z","title":"Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark","summary":"  The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark.\n","authors":["Yuhang Cai","Yaofei Wang","Donghui Hu","Gu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01110v4","updated":"2025-02-12T10:53:03Z","published":"2025-02-03T07:01:21Z","title":"The Nonlinear Filter Model of Stream Cipher Redivivus","summary":"  The nonlinear filter model is an old and well understood approach to the\ndesign of secure stream ciphers. Extensive research over several decades has\nshown how to attack stream ciphers based on this model and has identified the\nsecurity properties required of the Boolean function used as the filtering\nfunction to resist such attacks. This led to the problem of constructing\nBoolean functions which provide adequate security \\textit{and} at the same time\nare efficient to implement. Unfortunately, over the last two decades no good\nsolutions to this problem appeared in the literature. The lack of good\nsolutions has effectively led to nonlinear filter model becoming more or less\nobsolete. This is a big loss to the cryptographic design toolkit, since the\ngreat advantages of the nonlinear filter model are its simplicity, well\nunderstood security and the potential to provide low cost solutions for\nhardware oriented stream ciphers. In this paper, we revive the nonlinear filter\nmodel by constructing appropriate Boolean functions which provide required\nsecurity and are also efficient to implement. We put forward concrete\nsuggestions of stream ciphers which are $\\kappa$-bit secure against known types\nof attacks for $\\kappa=80,128,160,192,224$ and $256$. For the $80$-bit,\n$128$-bit, and the $256$-bit security levels, the circuits for the\ncorresponding stream ciphers require about 1743.5, 2771.5, and 5607.5 NAND\ngates respectively. For the $80$-bit and the $128$-bit security levels, the\ngate count estimates compare quite well to the famous ciphers Trivium and\nGrain-128a respectively, while for the $256$-bit security level, we do not know\nof any other stream cipher design which has such a low gate count.\n","authors":["Claude Carlet","Palash Sarkar"],"pdf_url":"https://arxiv.org/pdf/2502.01110v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09119v2","updated":"2025-02-12T09:38:31Z","published":"2024-12-12T09:54:38Z","title":"The Utility and Complexity of in- and out-of-Distribution Machine\n  Unlearning","summary":"  Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility.\n","authors":["Youssef Allouah","Joshua Kazdan","Rachid Guerraoui","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2412.09119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08240v1","updated":"2025-02-12T09:30:37Z","published":"2025-02-12T09:30:37Z","title":"Lazy Gatekeepers: A Large-Scale Study on SPF Configuration in the Wild","summary":"  The Sender Policy Framework (SPF) is a basic mechanism for authorizing the\nuse of domains in email. In combination with other mechanisms, it serves as a\ncornerstone for protecting users from forged senders. In this paper, we\ninvestigate the configuration of SPF across the Internet. To this end, we\nanalyze SPF records from 12 million domains in the wild. Our analysis shows a\ngrowing adoption, with 56.5 % of the domains providing SPF records. However, we\nalso uncover notable security issues: First, 2.9 % of the SPF records have\nerrors, undefined content or ineffective rules, undermining the intended\nprotection. Second, we observe a large number of very lax configurations. For\nexample, 34.7 % of the domains allow emails to be sent from over 100 000 IP\naddresses. We explore the reasons for these loose policies and demonstrate that\nthey facilitate email forgery. As a remedy, we derive recommendations for an\nadequate configuration and notify all operators of domains with misconfigured\nSPF records.\n","authors":["Stefan Czybik","Micha Horlboge","Konrad Rieck"],"pdf_url":"https://arxiv.org/pdf/2502.08240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16497v3","updated":"2025-02-12T09:09:30Z","published":"2024-02-26T11:30:34Z","title":"SAND: Decoupling Sanitization from Fuzzing for Low Overhead","summary":"  Sanitizers provide robust test oracles for various software vulnerabilities.\nFuzzing on sanitizer-enabled programs has been the best practice to find\nsoftware bugs. Since sanitizers need to heavily instrument a target program to\ninsert run-time checks, sanitizer-enabled programs have much higher overhead\ncompared to normally built programs. In this paper, we present SAND, a new\nfuzzing framework that decouples sanitization from the fuzzing loop. SAND\nperforms fuzzing on a normally built program and only invokes sanitizer-enabled\nprograms when input is shown to be interesting. Since most of the generated\ninputs are not interesting, i.e., not bug-triggering, SAND allows most of the\nfuzzing time to be spent on the normally built program. To identify interesting\ninputs, we introduce execution pattern for a practical execution analysis on\nthe normally built program. We realize SAND on top of AFL++ and evaluate it on\n12 real-world programs. Our extensive evaluation highlights its effectiveness:\nin 24 hours, compared to all the baseline fuzzers, SAND significantly discovers\nmore bugs while not missing any.\n","authors":["Ziqiao Kong","Shaohua Li","Heqing Huang","Zhendong Su"],"pdf_url":"https://arxiv.org/pdf/2402.16497v3.pdf","comment":"Camera-ready version"},{"id":"http://arxiv.org/abs/2501.08834v2","updated":"2025-02-12T08:58:44Z","published":"2025-01-15T14:38:18Z","title":"Smart Contract Fuzzing Towards Profitable Vulnerabilities","summary":"  Billions of dollars are transacted through smart contracts, making\nvulnerabilities a major financial risk. One focus in the security arms race is\non profitable vulnerabilities that attackers can exploit. Fuzzing is a key\nmethod for identifying these vulnerabilities. However, current solutions face\ntwo main limitations: a lack of profit-centric techniques for expediting\ndetection, and insufficient automation in maximizing the profitability of\ndiscovered vulnerabilities, leaving the analysis to human experts. To address\nthese gaps, we have developed VERITE, a profit-centric smart contract fuzzing\nframework that not only effectively detects those profitable vulnerabilities\nbut also maximizes the exploited profits.\n  VERITE has three key features: 1) DeFi action-based mutators for boosting the\nexploration of transactions with different fund flows; 2) potentially\nprofitable candidates identification criteria, which checks whether the input\nhas caused abnormal fund flow properties during testing; 3) a gradient\ndescent-based profit maximization strategy for these identified candidates.\n  VERITE is fully developed from scratch and evaluated on a dataset consisting\nof 61 exploited real-world DeFi projects with an average of over 1.1 million\ndollars loss. The results show that VERITE can automatically extract more than\n18 million dollars in total and is significantly better than state-of-the-art\nfuzzer ITYFUZZ in both detection (29/10) and exploitation (134 times more\nprofits gained on average). Remarkably, in 12 targets, it gains more profits\nthan real-world attacking exploits (1.01 to 11.45 times more). VERITE is also\napplied by auditors in contract auditing, where 6 (5 high severity) zero-day\nvulnerabilities are found with over $2,500 bounty rewards.\n","authors":["Ziqiao Kong","Cen Zhang","Maoyi Xie","Ming Hu","Yue Xue","Ye Liu","Haijun Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08834v2.pdf","comment":"Camera-ready version"},{"id":"http://arxiv.org/abs/2502.08219v1","updated":"2025-02-12T08:57:57Z","published":"2025-02-12T08:57:57Z","title":"Tracking Down Software Cluster Bombs: A Current State Analysis of the\n  Free/Libre and Open Source Software (FLOSS) Ecosystem","summary":"  Throughout computer history, it has been repeatedly demonstrated that\ncritical software vulnerabilities can significantly affect the components\ninvolved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most\nsoftware is distributed through package repositories. Nowadays, monitoring\ncritical dependencies in a software system is essential for maintaining robust\nsecurity practices. This is particularly important due to new legal\nrequirements, such as the European Cyber Resilience Act, which necessitate that\nsoftware projects maintain a transparent track record with Software Bill of\nMaterials (SBOM) and ensure a good overall state. This study provides a summary\nof the current state of available FLOSS package repositories and addresses the\nchallenge of identifying problematic areas within a software ecosystem. These\nareas are analyzed in detail, quantifying the current state of the FLOSS\necosystem. The results indicate that while there are well-maintained projects\nwithin the FLOSS ecosystem, there are also high-impact projects that are\nsusceptible to supply chain attacks. This study proposes a method for analyzing\nthe current state and identifies missing elements, such as interfaces, for\nfuture research.\n","authors":["Stefan Tatschner","Michael P. Heinl","Nicole Pappler","Tobias Specht","Sven Plaga","Thomas Newe"],"pdf_url":"https://arxiv.org/pdf/2502.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08217v1","updated":"2025-02-12T08:54:49Z","published":"2025-02-12T08:54:49Z","title":"Investigating Vulnerabilities of GPS Trip Data to Trajectory-User\n  Linking Attacks","summary":"  Open human mobility data is considered an essential basis for the profound\nresearch and analysis required for the transition to sustainable mobility and\nsustainable urban planning. Cycling data has especially been the focus of data\ncollection endeavors in recent years. Although privacy risks regarding location\ndata are widely known, practitioners often refrain from advanced privacy\nmechanisms to prevent utility losses. Removing user identifiers from trips is\nthereby deemed a major privacy gain, as it supposedly prevents linking single\ntrips to obtain entire movement patterns. In this paper, we propose a novel\nattack to reconstruct user identifiers in GPS trip datasets consisting of\nsingle trips, unlike previous ones that are dedicated to evaluating\ntrajectory-user linking in the context of check-in data. We evaluate the\nremaining privacy risk for users in such datasets and our empirical findings\nfrom two real-world datasets show that the risk of re-identification is\nsignificant even when personal identifiers have been removed, and that\ntruncation as a simple additional privacy mechanism may not be effective in\nprotecting user privacy. Further investigations indicate that users who\nfrequently visit locations that are only visited by a small number of others,\ntend to be more vulnerable to re-identification.\n","authors":["Benedikt Strbl","Alexandra Kapp"],"pdf_url":"https://arxiv.org/pdf/2502.08217v1.pdf","comment":"32 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.03677v2","updated":"2025-02-12T08:52:57Z","published":"2024-11-06T05:34:55Z","title":"Physical Layer Deception in OFDM Systems","summary":"  As a promising technology, physical layer security (PLS) enhances security by\nleveraging the physical characteristics of communication channels. However, it\ncommonly takes the legitimate user more effort to secure its data, compared to\nthat required by the eavesdropper to intercept the communication. To address\nthis imbalance, we propose a physical layer deception (PLD) framework, which\napplies random deceptive ciphering combined with orthogonal frequency-division\nmultiplexing (OFDM) to deceive eavesdroppers with falsified information,\npreventing them from wiretapping. While ensuring the same level of\nconfidentiality as traditional PLS methods, the PLD approach additionally\nintroduces a deception mechanism, which remains effective even when the\neavesdropper has the same knowledge about the transmitter as the legitimate\nreceiver. Through detailed theoretical analysis and numerical simulations, we\nprove the superiority of our method over the conventional PLS approach.\n","authors":["Wenwen Chen","Bin Han","Yao Zhu","Anke Schmeink","Hans D. Schotten"],"pdf_url":"https://arxiv.org/pdf/2411.03677v2.pdf","comment":"Submitted to EuCNC 2025 (appendices are excluded from the submitted\n  version due to length limit)"},{"id":"http://arxiv.org/abs/2502.08193v1","updated":"2025-02-12T08:10:25Z","published":"2025-02-12T08:10:25Z","title":"Typographic Attacks in a Multi-Image Setting","summary":"  Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP.\n","authors":["Xiaomeng Wang","Zhengyu Zhao","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2502.08193v1.pdf","comment":"Accepted by NAACL2025. Our code is available at\n  https://github.com/XiaomengWang-AI/Typographic-Attacks-in-a-Multi-Image-Setting"},{"id":"http://arxiv.org/abs/2502.07053v2","updated":"2025-02-12T07:30:44Z","published":"2025-02-10T21:41:11Z","title":"TOCTOU Resilient Attestation for IoT Networks (Full Version)","summary":"  Internet-of-Things (IoT) devices are increasingly common in both consumer and\nindustrial settings, often performing safety-critical functions. Although\nsecuring these devices is vital, manufacturers typically neglect security\nissues or address them as an afterthought. This is of particular importance in\nIoT networks, e.g., in the industrial automation settings.\n  To this end, network attestation -- verifying the software state of all\ndevices in a network -- is a promising mitigation approach. However, current\nnetwork attestation schemes have certain shortcomings: (1) lengthy TOCTOU\n(Time-Of-Check-Time-Of-Use) vulnerability windows, (2) high latency and\nresource overhead, and (3) susceptibility to interference from compromised\ndevices. To address these limitations, we construct TRAIN (TOCTOU-Resilient\nAttestation for IoT Networks), an efficient technique that minimizes TOCTOU\nwindows, ensures constant-time per-device attestation, and maintains resilience\neven with multiple compromised devices. We demonstrate TRAIN's viability and\nevaluate its performance via a fully functional and publicly available\nprototype.\n","authors":["Pavel Frolikov","Youngil Kim","Renascence Tarafder Prapty","Gene Tsudik"],"pdf_url":"https://arxiv.org/pdf/2502.07053v2.pdf","comment":"15 pages,8 figues, 6 tables, To appear at SenSys 2025"},{"id":"http://arxiv.org/abs/2502.03868v2","updated":"2025-02-12T07:18:09Z","published":"2025-02-06T08:28:41Z","title":"Time-based GNSS attack detection","summary":"  To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable.\n","authors":["Marco Spanghero","Panos Papadimitratos"],"pdf_url":"https://arxiv.org/pdf/2502.03868v2.pdf","comment":"IEEE Transactions on Aerospace and Electronic Systems (Early Access)"},{"id":"http://arxiv.org/abs/2409.20002v3","updated":"2025-02-12T07:02:06Z","published":"2024-09-30T06:55:00Z","title":"The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems","summary":"  The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.\n","authors":["Linke Song","Zixuan Pang","Wenhao Wang","Zihao Wang","XiaoFeng Wang","Hongbo Chen","Wei Song","Yier Jin","Dan Meng","Rui Hou"],"pdf_url":"https://arxiv.org/pdf/2409.20002v3.pdf","comment":"This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results"},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08151v1","updated":"2025-02-12T06:37:26Z","published":"2025-02-12T06:37:26Z","title":"Local Differential Privacy is Not Enough: A Sample Reconstruction Attack\n  against Federated Learning with Local Differential Privacy","summary":"  Reconstruction attacks against federated learning (FL) aim to reconstruct\nusers' samples through users' uploaded gradients. Local differential privacy\n(LDP) is regarded as an effective defense against various attacks, including\nsample reconstruction in FL, where gradients are clipped and perturbed.\nExisting attacks are ineffective in FL with LDP since clipped and perturbed\ngradients obliterate most sample information for reconstruction. Besides,\nexisting attacks embed additional sample information into gradients to improve\nthe attack effect and cause gradient expansion, leading to a more severe\ngradient clipping in FL with LDP. In this paper, we propose a sample\nreconstruction attack against LDP-based FL with any target models to\nreconstruct victims' sensitive samples to illustrate that FL with LDP is not\nflawless. Considering gradient expansion in reconstruction attacks and noise in\nLDP, the core of the proposed attack is gradient compression and reconstructed\nsample denoising. For gradient compression, an inference structure based on\nsample characteristics is presented to reduce redundant gradients against LDP.\nFor reconstructed sample denoising, we artificially introduce zero gradients to\nobserve noise distribution and scale confidence interval to filter the noise.\nTheoretical proof guarantees the effectiveness of the proposed attack.\nEvaluations show that the proposed attack is the only attack that reconstructs\nvictims' training samples in LDP-based FL and has little impact on the target\nmodel's accuracy. We conclude that LDP-based FL needs further improvements to\ndefend against sample reconstruction attacks effectively.\n","authors":["Zhichao You","Xuewen Dong","Shujun Li","Ximeng Liu","Siqi Ma","Yulong Shen"],"pdf_url":"https://arxiv.org/pdf/2502.08151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05206v2","updated":"2025-02-12T06:16:00Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v2.pdf","comment":"47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2502.08123v1","updated":"2025-02-12T05:05:40Z","published":"2025-02-12T05:05:40Z","title":"Provably Robust Federated Reinforcement Learning","summary":"  Federated reinforcement learning (FRL) allows agents to jointly learn a\nglobal decision-making policy under the guidance of a central server. While FRL\nhas advantages, its decentralized design makes it prone to poisoning attacks.\nTo mitigate this, Byzantine-robust aggregation techniques tailored for FRL have\nbeen introduced. Yet, in our work, we reveal that these current\nByzantine-robust techniques are not immune to our newly introduced Normalized\nattack. Distinct from previous attacks that targeted enlarging the distance of\npolicy updates before and after an attack, our Normalized attack emphasizes on\nmaximizing the angle of deviation between these updates. To counter these\nthreats, we develop an ensemble FRL approach that is provably secure against\nboth known and our newly proposed attacks. Our ensemble method involves\ntraining multiple global policies, where each is learnt by a group of agents\nusing any foundational aggregation rule. These well-trained global policies\nthen individually predict the action for a specific test state. The ultimate\naction is chosen based on a majority vote for discrete action systems or the\ngeometric median for continuous ones. Our experimental results across different\nsettings show that the Normalized attack can greatly disrupt non-ensemble\nByzantine-robust methods, and our ensemble approach offers substantial\nresistance against poisoning attacks.\n","authors":["Minghong Fang","Xilong Wang","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2502.08123v1.pdf","comment":"To appear in The Web Conference 2025"},{"id":"http://arxiv.org/abs/2502.02913v4","updated":"2025-02-12T04:59:16Z","published":"2025-02-05T06:20:20Z","title":"Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage","summary":"  The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.\n","authors":["Jiayang Meng","Tao Huang","Hong Chen","Xin Shi","Qingyu Huang","Chen Hou"],"pdf_url":"https://arxiv.org/pdf/2502.02913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02629v2","updated":"2025-02-12T04:55:19Z","published":"2025-01-05T19:06:03Z","title":"Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense","summary":"  As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher\n","authors":["Yang Ouyang","Hengrui Gu","Shuhang Lin","Wenyue Hua","Jie Peng","Bhavya Kailkhura","Meijun Gao","Tianlong Chen","Kaixiong Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.02629v2.pdf","comment":"14 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2404.14648v4","updated":"2025-02-12T04:07:54Z","published":"2024-04-23T00:50:57Z","title":"Pseudorandom Permutations from Random Reversible Circuits","summary":"  We study pseudorandomness properties of permutations on $\\{0,1\\}^n$ computed\nby random circuits made from reversible $3$-bit gates (permutations on\n$\\{0,1\\}^3$). Our main result is that a random circuit of depth $n \\cdot\n\\tilde{O}(k^2)$, with each layer consisting of $\\approx n/3$ random gates in a\nfixed nearest-neighbor architecture, yields almost $k$-wise independent\npermutations. The main technical component is showing that the Markov chain on\n$k$-tuples of $n$-bit strings induced by a single random $3$-bit\nnearest-neighbor gate has spectral gap at least $1/n \\cdot \\tilde{O}(k)$. This\nimproves on the original work of Gowers [Gowers96], who showed a gap of\n$1/\\mathrm{poly}(n,k)$ for one random gate (with non-neighboring inputs); and,\non subsequent work [HMMR05,BH08] improving the gap to $\\Omega(1/n^2k)$ in the\nsame setting.\n  From the perspective of cryptography, our result can be seen as a\nparticularly simple/practical block cipher construction that gives provable\nstatistical security against attackers with access to $k$~input-output pairs\nwithin few rounds. We also show that the Luby--Rackoff construction of\npseudorandom permutations from pseudorandom functions can be implemented with\nreversible circuits. From this, we make progress on the complexity of the\nMinimum Reversible Circuit Size Problem (MRCSP), showing that block ciphers of\nfixed polynomial size are computationally secure against arbitrary\npolynomial-time adversaries, assuming the existence of one-way functions\n(OWFs).\n","authors":["William He","Ryan O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2404.14648v4.pdf","comment":"Merged with arXiv:2409.14614; for merged paper see arXiv:2502.07159.\n  A previous version of one of the merged components of this paper contained\n  candidate constructions of computationally pseudorandom permutations from\n  one-way functions. There was an error in the proof of security, and we have\n  withdrawn this result"},{"id":"http://arxiv.org/abs/2307.07815v2","updated":"2025-02-12T04:01:25Z","published":"2023-07-15T14:43:40Z","title":"HyperGo: Probability-based Directed Hybrid Fuzzing","summary":"  Directed grey-box fuzzing (DGF) is a target-guided fuzzing intended for\ntesting specific targets (e.g., the potential buggy code). Despite numerous\ntechniques proposed to enhance directedness, the existing DGF techniques still\nface challenges, such as taking into account the difficulty of reaching\ndifferent basic blocks when designing the fitness metric, and promoting the\neffectiveness of symbolic execution (SE) when solving the complex constraints\nin the path to the target. In this paper, we propose a directed hybrid fuzzer\ncalled HyperGo. To address the challenges, we introduce the concept of path\nprobability and combine the probability with distance to form an adaptive\nfitness metric called probability-based distance. By combining the two factors,\nprobability-based distance can adaptively guide DGF toward paths that are\ncloser to the target and have more easy-to-satisfy path constraints. Then, we\nput forward an Optimized Symbolic Execution Complementary (OSEC) scheme to\ncombine DGF and SE in a complementary manner. The OSEC would prune the\nunreachable branches and unsolvable branches, and prioritize symbolic execution\nof the seeds whose paths are closer to the target and have more branches that\nare difficult to be covered by DGF. We evaluated HyperGo on 2 benchmarks\nconsisting of 21 programs with a total of 100 target sites. The experimental\nresults show that HyperGo achieves 38.47$\\times$, 30.89$\\times$, 28.52$\\times$,\n106.09$\\times$ and 143.22$\\times$ speedup compared to AFLGo, AFLGoSy, BEACON,\nWindRanger, and ParmeSan, respectively in reaching target sites, and\n3.44$\\times$, 3.63$\\times$, 4.10$\\times$, 3.26$\\times$, and 3.00$\\times$\nspeedup in exposing known vulnerabilities. Moreover, HyperGo discovered 37\nundisclosed vulnerabilities from 7 real-world programs.\n","authors":["Peihong Lin","Pengfei Wang","Xu Zhou","Wei Xie","Kai Lu","Gen Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.07815v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.08097v1","updated":"2025-02-12T03:52:36Z","published":"2025-02-12T03:52:36Z","title":"ID-Cloak: Crafting Identity-Specific Cloaks Against Personalized\n  Text-to-Image Generation","summary":"  Personalized text-to-image models allow users to generate images of new\nconcepts from several reference photos, thereby leading to critical concerns\nregarding civil privacy. Although several anti-personalization techniques have\nbeen developed, these methods typically assume that defenders can afford to\ndesign a privacy cloak corresponding to each specific image. However, due to\nextensive personal images shared online, image-specific methods are limited by\nreal-world practical applications. To address this issue, we are the first to\ninvestigate the creation of identity-specific cloaks (ID-Cloak) that safeguard\nall images belong to a specific identity. Specifically, we first model an\nidentity subspace that preserves personal commonalities and learns diverse\ncontexts to capture the image distribution to be protected. Then, we craft\nidentity-specific cloaks with the proposed novel objective that encourages the\ncloak to guide the model away from its normal output within the subspace.\nExtensive experiments show that the generated universal cloak can effectively\nprotect the images. We believe our method, along with the proposed\nidentity-specific cloak setting, marks a notable advance in realistic privacy\nprotection.\n","authors":["Qianrui Teng","Xing Cui","Xuannan Liu","Peipei Li","Zekun Li","Huaibo Huang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2502.08097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02097v3","updated":"2025-02-12T02:36:42Z","published":"2024-10-02T23:34:18Z","title":"DomainHarvester: Harvesting Infrequently Visited Yet Trustworthy Domain\n  Names","summary":"  In cybersecurity, allow lists play a crucial role in distinguishing safe\nwebsites from potential threats. Conventional methods for compiling allow\nlists, focusing heavily on website popularity, often overlook infrequently\nvisited legitimate domains. This paper introduces DomainHarvester, a system\naimed at generating allow lists that include trustworthy yet infrequently\nvisited domains. By adopting an innovative bottom-up methodology that leverages\nthe web's hyperlink structure, DomainHarvester identifies legitimate yet\nunderrepresented domains. The system uses seed URLs to gather domain names,\nemploying machine learning with a Transformer-based approach to assess their\ntrustworthiness. DomainHarvester has developed two distinct allow lists: one\nwith a global focus and another emphasizing local relevance. Compared to six\nexisting top lists, DomainHarvester's allow lists show minimal overlaps, 4\\%\nglobally and 0.1\\% locally, while significantly reducing the risk of including\nmalicious domains, thereby enhancing security. The contributions of this\nresearch are substantial, illuminating the overlooked aspect of trustworthy yet\nunderrepresented domains and introducing DomainHarvester, a system that goes\nbeyond traditional popularity-based metrics. Our methodology enhances the\ninclusivity and precision of allow lists, offering significant advantages to\nusers and businesses worldwide, especially in non-English speaking regions.\n","authors":["Daiki Chiba","Hiroki Nakano","Takashi Koide"],"pdf_url":"https://arxiv.org/pdf/2410.02097v3.pdf","comment":"Originally presented at IEEE CCNC 2025. An extended version of this\n  work has been published in IEEE Access:\n  https://doi.org/10.1109/ACCESS.2025.3539882"},{"id":"http://arxiv.org/abs/2409.14614v2","updated":"2025-02-12T02:26:30Z","published":"2024-09-22T22:28:14Z","title":"Faster Mixing of Higher-Dimensional Random Reversible Circuits","summary":"  We continue the study of the approximate $k$-wise independence of random\nreversible circuits as permutations of $\\{\\pm1\\}^n$. Our main result is the\nfirst construction of a natural class of random reversible circuits with a\nsublinear-in-$n$ dependence on depth. Our construction is motivated by\nconsiderations in practical cryptography and is somewhat inspired by the design\nof practical block ciphers, such as DES and AES. Previous constructions of He\nand O'Donnell [HO24], which were built with gate architectures on\none-dimensional lattices, suffered from an inherent linear-in-$n$ dependence on\ndepth. The main novelty of our circuit model is a gate architecture built on\nhigher-dimensional lattices.\n","authors":["William Gay","William He","Nicholas Kocurek"],"pdf_url":"https://arxiv.org/pdf/2409.14614v2.pdf","comment":"Merged with arXiv:2404.14648. For merged paper see arXiv:2502.07159"},{"id":"http://arxiv.org/abs/2406.10281v2","updated":"2025-02-12T02:11:10Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"  Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.\n","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2406.10281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08055v1","updated":"2025-02-12T01:31:39Z","published":"2025-02-12T01:31:39Z","title":"SLVR: Securely Leveraging Client Validation for Robust Federated\n  Learning","summary":"  Federated Learning (FL) enables collaborative model training while keeping\nclient data private. However, exposing individual client updates makes FL\nvulnerable to reconstruction attacks. Secure aggregation mitigates such privacy\nrisks but prevents the server from verifying the validity of each client\nupdate, creating a privacy-robustness tradeoff. Recent efforts attempt to\naddress this tradeoff by enforcing checks on client updates using\nzero-knowledge proofs, but they support limited predicates and often depend on\npublic validation data. We propose SLVR, a general framework that securely\nleverages clients' private data through secure multi-party computation. By\nutilizing clients' data, SLVR not only eliminates the need for public\nvalidation data, but also enables a wider range of checks for robustness,\nincluding cross-client accuracy validation. It also adapts naturally to\ndistribution shifts in client data as it can securely refresh its validation\ndata up-to-date. Our empirical evaluations show that SLVR improves robustness\nagainst model poisoning attacks, particularly outperforming existing methods by\nup to 50% under adaptive attacks. Additionally, SLVR demonstrates effective\nadaptability and stable convergence under various distribution shift scenarios.\n","authors":["Jihye Choi","Sai Rahul Rachuri","Ke Wang","Somesh Jha","Yizhen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08055v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2502.08843v1","updated":"2025-02-12T23:29:06Z","published":"2025-02-12T23:29:06Z","title":"Hierarchical Entropy Disruption for Ransomware Detection: A\n  Computationally-Driven Framework","summary":"  The rapid evolution of encryption-based threats has rendered conventional\ndetection mechanisms increasingly ineffective against sophisticated attack\nstrategies. Monitoring entropy variations across hierarchical system levels\noffers an alternative approach to identifying unauthorized data modifications\nwithout relying on static signatures. A framework leveraging hierarchical\nentropy disruption was introduced to analyze deviations in entropy\ndistributions, capturing behavioral anomalies indicative of malicious\nencryption operations. Evaluating the framework across multiple ransomware\nvariants demonstrated its capability to achieve high detection accuracy while\nmaintaining minimal computational overhead. Entropy distributions across\ndifferent system directories revealed that encryption activities predominantly\ntargeted user-accessible files, aligning with observed attacker strategies.\nDetection latency analysis indicated that early-stage identification was\nfeasible, mitigating potential data loss before critical system impact\noccurred. The framework's ability to operate efficiently in real-time\nenvironments was validated through an assessment of resource utilization,\nconfirming a balanced trade-off between detection precision and computational\nefficiency. Comparative benchmarking against established detection methods\nhighlighted the limitations of conventional approaches in identifying novel\nransomware variants, whereas entropy-based anomaly detection provided\nresilience against obfuscation techniques.\n","authors":["Hayden Srynn","Gilbert Pomeroy","Florence Lytton","Godfrey Ashcombe","Valentine Harcourt","Duncan Pettigrew"],"pdf_url":"https://arxiv.org/pdf/2502.08843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07049v2","updated":"2025-02-12T23:19:23Z","published":"2025-02-10T21:33:38Z","title":"LLMs in Software Security: A Survey of Vulnerability Detection\n  Techniques and Insights","summary":"  Large Language Models (LLMs) are emerging as transformative tools for\nsoftware vulnerability detection, addressing critical challenges in the\nsecurity domain. Traditional methods, such as static and dynamic analysis,\noften falter due to inefficiencies, high false positive rates, and the growing\ncomplexity of modern software systems. By leveraging their ability to analyze\ncode structures, identify patterns, and generate repair sugges- tions, LLMs,\nexemplified by models like GPT, BERT, and CodeBERT, present a novel and\nscalable approach to mitigating vulnerabilities. This paper provides a detailed\nsurvey of LLMs in vulnerability detection. It examines key aspects, including\nmodel architectures, application methods, target languages, fine-tuning\nstrategies, datasets, and evaluation metrics. We also analyze the scope of\ncurrent research problems, highlighting the strengths and weaknesses of\nexisting approaches. Further, we address challenges such as cross-language\nvulnerability detection, multimodal data integration, and repository-level\nanalysis. Based on these findings, we propose solutions for issues like dataset\nscalability, model interpretability, and applications in low-resource\nscenarios. Our contributions are threefold: (1) a systematic review of how LLMs\nare applied in vulnerability detection; (2) an analysis of shared patterns and\ndifferences across studies, with a unified framework for understanding the\nfield; and (3) a summary of key challenges and future research directions. This\nwork provides valuable insights for advancing LLM-based vulnerability\ndetection. We also maintain and regularly update latest selected paper on\nhttps://github.com/OwenSanzas/LLM-For-Vulnerability-Detection\n","authors":["Ze Sheng","Zhicheng Chen","Shuning Gu","Heqing Huang","Guofei Gu","Jeff Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07049v2.pdf","comment":"33 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.08832v1","updated":"2025-02-12T22:45:46Z","published":"2025-02-12T22:45:46Z","title":"LSM Trees in Adversarial Environments","summary":"  The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated.\n","authors":["Hayder Tirmazi"],"pdf_url":"https://arxiv.org/pdf/2502.08832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08830v1","updated":"2025-02-12T22:38:50Z","published":"2025-02-12T22:38:50Z","title":"Investigation of Advanced Persistent Threats Network-based Tactics,\n  Techniques and Procedures","summary":"  The scarcity of data and the high complexity of Advanced Persistent Threats\n(APTs) attacks have created challenges in comprehending their behavior and\nhindered the exploration of effective detection techniques. To create an\neffective APT detection strategy, it is important to examine the Tactics,\nTechniques, and Procedures (TTPs) that have been reported by the industry.\nThese TTPs can be difficult to classify as either malicious or legitimate. When\ndeveloping an approach for the next generation of network intrusion detection\nsystems (NIDS), it is necessary to take into account the specific context of\nthe attack explained in this paper.\n  In this study, we select 33 APT campaigns based on the fair distribution over\nthe past 22 years to observe the evolution of APTs over time. We focus on their\nevasion techniques and how they stay undetected for months or years. We found\nthat APTs cannot continue their operations without C&C servers, which are\nmostly addressed by Domain Name System (DNS). We identify several TTPs used for\nDNS, such as Dynamic DNS, typosquatting, and TLD squatting. The next step for\nAPT operators is to start communicating with a victim. We found that the most\npopular protocol to deploy evasion techniques is using HTTP(S) with 81% of APT\ncampaigns. HTTP(S) can evade firewall filtering and pose as legitimate\nweb-based traffic. DNS protocol is also widely used by 45% of APTs for DNS\nresolution and tunneling. We identify and analyze the TTPs associated with\nusing HTTP(S) based on real artifacts.\n","authors":["Almuthanna Alageel","Sergio Maffeis","Imperial College London"],"pdf_url":"https://arxiv.org/pdf/2502.08830v1.pdf","comment":"27 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.02890v3","updated":"2025-02-12T20:39:01Z","published":"2024-10-03T18:28:10Z","title":"Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach","summary":"  Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively.\n","authors":["Haiyun He","Yepeng Liu","Ziqiao Wang","Yongyi Mao","Yuheng Bu"],"pdf_url":"https://arxiv.org/pdf/2410.02890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03470v3","updated":"2025-02-12T20:06:17Z","published":"2023-11-06T19:14:17Z","title":"Orion: A Fully Homomorphic Encryption Framework for Deep Learning","summary":"  Fully Homomorphic Encryption (FHE) has the potential to substantially improve\nprivacy and security by enabling computation directly on encrypted data. This\nis especially true with deep learning, as today, many popular user services are\npowered by neural networks in the cloud. Beyond its well-known high\ncomputational costs, one of the major challenges facing wide-scale deployment\nof FHE-secured neural inference is effectively mapping these networks to FHE\nprimitives. FHE poses many programming challenges including packing large\nvectors, managing accumulated noise, and translating arbitrary and\ngeneral-purpose programs to the limited instruction set provided by FHE. These\nchallenges make building large FHE neural networks intractable using the tools\navailable today.\n  In this paper we address these challenges with Orion, a fully-automated\nframework for private neural inference using FHE. Orion accepts deep neural\nnetworks written in PyTorch and translates them into efficient FHE programs. We\nachieve this by proposing a novel single-shot multiplexed packing strategy for\narbitrary convolutions and through a new, efficient technique to automate\nbootstrap placement and scale management. We evaluate Orion on common\nbenchmarks used by the FHE deep learning community and outperform\nstate-of-the-art by 2.38x on ResNet-20, the largest network they report.\nOrion's techniques enable processing much deeper and larger networks. We\ndemonstrate this by evaluating ResNet-50 on ImageNet and present the first\nhigh-resolution FHE object detection experiments using a YOLO-v1 model with 139\nmillion parameters. Orion is open-source for all to use at:\nhttps://github.com/baahl-nyu/orion\n","authors":["Austin Ebel","Karthik Garimella","Brandon Reagen"],"pdf_url":"https://arxiv.org/pdf/2311.03470v3.pdf","comment":"Accepted to the 30th edition of the Architectural Support for\n  Programming Languages and Operating Systems (ASPLOS) 2025 Conference, 13\n  pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.17392v2","updated":"2025-02-12T19:37:28Z","published":"2025-01-29T03:01:01Z","title":"Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed\n  Computing","summary":"  Federated learning (FL) has gained attention as a distributed learning\nparadigm for its data privacy benefits and accelerated convergence through\nparallel computation. Traditional FL relies on a server-client (SC)\narchitecture, where a central server coordinates multiple clients to train a\nglobal model, but this approach faces scalability challenges due to server\ncommunication bottlenecks. To overcome this, the ring-all-reduce (RAR)\narchitecture has been introduced, eliminating the central server and achieving\nbandwidth optimality. However, the tightly coupled nature of RAR's ring\ntopology exposes it to unique Byzantine attack risks not present in SC-based\nFL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms\nremains an open problem. To address this gap, we propose BRACE\n(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve\nboth Byzantine robustness and communication efficiency. We provide theoretical\nguarantees for the convergence of BRACE under Byzantine attacks, demonstrate\nits bandwidth efficiency, and validate its practical effectiveness through\nexperiments. Our work offers a foundational understanding of Byzantine-robust\nRAR-based FL design.\n","authors":["Minghong Fang","Zhuqing Liu","Xuecen Zhao","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2501.17392v2.pdf","comment":"To appear in The Web Conference 2025"},{"id":"http://arxiv.org/abs/2205.08287v4","updated":"2025-02-12T16:45:19Z","published":"2022-05-17T12:34:56Z","title":"Bankrupting DoS Attackers","summary":"  Can we make a denial-of-service attacker pay more than the server and honest\nclients? Consider a model where a server sees a stream of jobs sent by either\nhonest clients or an adversary. The server sets a price for servicing each job\nwith the aid of an estimator, which provides approximate statistical\ninformation about the distribution of previously occurring good jobs.\n  We describe and analyze pricing algorithms for the server under different\nmodels of synchrony, with total cost parameterized by the accuracy of the\nestimator. Given a reasonably accurate estimator, the algorithm's cost provably\ngrows more slowly than the attacker's cost, as the attacker's cost grows large.\nAdditionally, we prove a lower bound, showing that our pricing algorithm yields\nasymptotically tight results when the estimator is accurate within constant\nfactors.\n","authors":["Trisha Chakraborty","Abir Islam","Valerie King","Daniel Rayborn","Jared Saia","Maxwell Young"],"pdf_url":"https://arxiv.org/pdf/2205.08287v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08679v1","updated":"2025-02-12T08:56:35Z","published":"2025-02-12T08:56:35Z","title":"Deep Learning-Driven Malware Classification with API Call Sequence\n  Analysis and Concept Drift Handling","summary":"  Malware classification in dynamic environments presents a significant\nchallenge due to concept drift, where the statistical properties of malware\ndata evolve over time, complicating detection efforts. To address this issue,\nwe propose a deep learning framework enhanced with a genetic algorithm to\nimprove malware classification accuracy and adaptability. Our approach\nincorporates mutation operations and fitness score evaluations within genetic\nalgorithms to continuously refine the deep learning model, ensuring robustness\nagainst evolving malware threats. Experimental results demonstrate that this\nhybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach\noffers a promising solution for real-time malware classification in\never-changing cybersecurity landscapes.\n","authors":["Bishwajit Prasad Gond","Durga Prasad Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2502.08679v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.08644v1","updated":"2025-02-12T18:58:34Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08637v1","updated":"2025-02-12T18:54:10Z","published":"2025-02-12T18:54:10Z","title":"Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or\n  Learning-Based?","summary":"  A novel pinching antenna system (PASS)-enabled downlink multi-user\nmultiple-input single-output (MISO) framework is proposed. PASS consists of\nmultiple waveguides spanning over thousands of wavelength, which equip numerous\nlow-cost dielectric particles, named pinching antennas (PAs), to radiate\nsignals into free space. The positions of PAs can be reconfigured to change\nboth the large-scale path losses and phases of signals, thus facilitating the\nnovel pinching beamforming design. A sum rate maximization problem is\nformulated, which jointly optimizes the transmit and pinching beamforming to\nadaptively achieve constructive signal enhancement and destructive interference\nmitigation. To solve this highly coupled and nonconvex problem, both\noptimization-based and learning-based methods are proposed. 1) For the\noptimization-based method, a majorization-minimization and penalty dual\ndecomposition (MM-PDD) algorithm is developed, which handles the nonconvex\ncomplex exponential component using a Lipschitz surrogate function and then\ninvokes PDD for problem decoupling. 2) For the learning-based method, a novel\nKarush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which\nenables KKT solutions to be reconstructed in a data-driven manner by learning\ndual variables. Following this idea, a KDL-Tranformer algorithm is developed,\nwhich captures both inter-PA/inter-user dependencies and\nchannel-state-information (CSI)-beamforming dependencies by attention\nmechanisms. Simulation results demonstrate that: i) The proposed PASS framework\nsignificantly outperforms conventional massive multiple input multiple output\n(MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve\nover 30% system performance than MM-PDD algorithm, while achieving a\nmillisecond-level response on modern GPUs.\n","authors":["Xiaoxia Xu","Xidong Mu","Yuanwei Liu","Arumugam Nallanathan"],"pdf_url":"https://arxiv.org/pdf/2502.08637v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2502.08634v1","updated":"2025-02-12T18:48:12Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation","summary":"  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08632v1","updated":"2025-02-12T18:47:13Z","published":"2025-02-12T18:47:13Z","title":"Necessary and Sufficient Oracles: Toward a Computational Taxonomy For\n  Reinforcement Learning","summary":"  Algorithms for reinforcement learning (RL) in large state spaces crucially\nrely on supervised learning subroutines to estimate objects such as value\nfunctions or transition probabilities. Since only the simplest supervised\nlearning problems can be solved provably and efficiently, practical performance\nof an RL algorithm depends on which of these supervised learning \"oracles\" it\nassumes access to (and how they are implemented). But which oracles are better\nor worse? Is there a minimal oracle?\n  In this work, we clarify the impact of the choice of supervised learning\noracle on the computational complexity of RL, as quantified by the oracle\nstrength. First, for the task of reward-free exploration in Block MDPs in the\nstandard episodic access model -- a ubiquitous setting for RL with function\napproximation -- we identify two-context regression as a minimal oracle, i.e.\nan oracle that is both necessary and sufficient (under a mild regularity\nassumption). Second, we identify one-context regression as a near-minimal\noracle in the stronger reset access model, establishing a provable\ncomputational benefit of resets in the process. Third, we broaden our focus to\nLow-Rank MDPs, where we give cryptographic evidence that the analogous oracle\nfrom the Block MDP setting is insufficient.\n","authors":["Dhruv Rohatgi","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2502.08632v1.pdf","comment":"84 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2501.18823v2","updated":"2025-02-12T18:35:15Z","published":"2025-01-31T00:36:30Z","title":"Transcoders Beat Sparse Autoencoders for Interpretability","summary":"  Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose skip transcoders, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.\n","authors":["Gonalo Paulo","Stepan Shabalin","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2501.18823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13734v3","updated":"2025-02-12T18:32:37Z","published":"2025-01-23T15:10:51Z","title":"Sample complexity of data-driven tuning of model hyperparameters in\n  neural networks with structured parameter-dependent dual function","summary":"  Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter; our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks.\n","authors":["Maria-Florina Balcan","Anh Tuan Nguyen","Dravyansh Sharma"],"pdf_url":"https://arxiv.org/pdf/2501.13734v3.pdf","comment":"50 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08628v1","updated":"2025-02-12T18:30:36Z","published":"2025-02-12T18:30:36Z","title":"Concentration Inequalities for the Stochastic Optimization of Unbounded\n  Objectives with Application to Denoising Score Matching","summary":"  We derive novel concentration inequalities that bound the statistical error\nfor a large class of stochastic optimization problems, focusing on the case of\nunbounded objective functions. Our derivations utilize the following tools: 1)\nA new form of McDiarmid's inequality that is based on sample dependent one\ncomponent difference bounds and which leads to a novel uniform law of large\nnumbers result for unbounded functions. 2) A Rademacher complexity bound for\nfamilies of functions that satisfy an appropriate local Lipschitz property. As\nan application of these results, we derive statistical error bounds for\ndenoising score matching (DSM), an application that inherently requires one to\nconsider unbounded objective functions, even when the data distribution has\nbounded support. In addition, our results establish the benefit of sample reuse\nin algorithms that employ easily sampled auxiliary random variables in addition\nto the training data, e.g., as in DSM, which uses auxiliary Gaussian random\nvariables.\n","authors":["Jeremiah Birrell"],"pdf_url":"https://arxiv.org/pdf/2502.08628v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2410.06976v2","updated":"2025-02-12T18:27:29Z","published":"2024-10-09T15:15:40Z","title":"Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation","summary":"  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain,\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nMatcha, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the htop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, Matcha seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of Matcha on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .\n","authors":["Wenxuan Bao","Zhichen Zeng","Zhining Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2410.06976v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13312v2","updated":"2025-02-12T18:22:42Z","published":"2025-01-23T01:43:31Z","title":"Tensor-Var: Variational Data Assimilation in Tensor Product Feature\n  Space","summary":"  Variational data assimilation estimates the dynamical system states by\nminimizing a cost function that fits the numerical models with observational\ndata. The widely used method, four-dimensional variational assimilation\n(4D-Var), has two primary challenges: (1) computationally demanding for complex\nnonlinear systems and (2) relying on state-observation mappings, which are\noften not perfectly known. Deep learning (DL) has been used as a more\nexpressive class of efficient model approximators to address these challenges.\nHowever, integrating such models into 4D-Var remains challenging due to their\ninherent nonlinearities and the lack of theoretical guarantees for consistency\nin assimilation results. In this paper, we propose Tensor-Var to address these\nchallenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves\noptimization efficiency by characterizing system dynamics and state-observation\nmappings as linear operators, leading to a convex cost function in the feature\nspace. Furthermore, our method provides a new perspective to incorporate CME\ninto 4D-Var, offering theoretical guarantees of consistent assimilation results\nbetween the original and feature spaces. To improve scalability, we propose a\nmethod to learn deep features (DFs) using neural networks within the Tensor-Var\nframework. Experiments on chaotic systems and global weather prediction with\nreal-time observations show that Tensor-Var outperforms conventional and DL\nhybrid 4D-Var baselines in accuracy while achieving efficiency comparable to\nthe static 3D-Var method.\n","authors":["Yiming Yang","Xiaoyuan Cheng","Daniel Giles","Sibo Cheng","Yi He","Xiao Xue","Boli Chen","Yukun Hu"],"pdf_url":"https://arxiv.org/pdf/2501.13312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08622v1","updated":"2025-02-12T18:20:41Z","published":"2025-02-12T18:20:41Z","title":"Forecasting Drought Using Machine Learning in California","summary":"  Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.\n","authors":["Nan K. Li","Angela Chang","David Sherman"],"pdf_url":"https://arxiv.org/pdf/2502.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08620v1","updated":"2025-02-12T18:15:35Z","published":"2025-02-12T18:15:35Z","title":"Mathematical Data Science","summary":"  Can machine learning help discover new mathematical structures? In this\narticle we discuss an approach to doing this which one can call \"mathematical\ndata science\". In this paradigm, one studies mathematical objects collectively\nrather than individually, by creating datasets and doing machine learning\nexperiments and interpretations. After an overview, we present two case\nstudies: murmurations in number theory and loadings of partitions related to\nKronecker coefficients in representation theory and combinatorics.\n","authors":["Michael R. Douglas","Kyu-Hwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01512v2","updated":"2025-02-12T18:11:46Z","published":"2025-02-03T16:46:46Z","title":"Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices","summary":"  Circular and non-flat data distributions are prevalent across diverse domains\nof data science, yet their specific geometric structures often remain\nunderutilized in machine learning frameworks. A principled approach to\naccounting for the underlying geometry of such data is pivotal, particularly\nwhen extending statistical models, like the pervasive Gaussian distribution. In\nthis work, we tackle those issue by focusing on the manifold of symmetric\npositive definite matrices, a key focus in information geometry. We introduced\na non-isotropic wrapped Gaussian by leveraging the exponential map, we derive\ntheoretical properties of this distribution and propose a maximum likelihood\nframework for parameter estimation. Furthermore, we reinterpret established\nclassifiers on SPD through a probabilistic lens and introduce new classifiers\nbased on the wrapped Gaussian model. Experiments on synthetic and real-world\ndatasets demonstrate the robustness and flexibility of this geometry-aware\ndistribution, underscoring its potential to advance manifold-based data\nanalysis. This work lays the groundwork for extending classical machine\nlearning and statistical methods to more complex and structured data.\n","authors":["Thibault de Surrel","Fabien Lotte","Sylvain Chevallier","Florian Yger"],"pdf_url":"https://arxiv.org/pdf/2502.01512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08612v1","updated":"2025-02-12T18:01:04Z","published":"2025-02-12T18:01:04Z","title":"Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model","summary":"  Non-invasive patient monitoring for tracking and predicting adverse acute\nhealth events is an emerging area of research. We pursue in-hospital cardiac\narrest (IHCA) prediction using only single-channel finger photoplethysmography\n(PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator\nNetwork (FEAN) leverages powerful representations from pre-trained PPG\nfoundation models (PPG-GPT of size up to 1 Billion) stacked with sequential\nclassification models. We propose two FEAN variants (\"1H\", \"FH\") which use the\nlatest one-hour and (max) 24-hour history to make decisions respectively. Our\nstudy is the first to present IHCA prediction results in ICU patients using\nonly unimodal (continuous PPG signal) waveform deep representations. With our\nbest model, we obtain an average of 0.79 AUROC over 24~h prediction window\nbefore CA event onset with our model peaking performance at 0.82 one hour\nbefore CA. We also provide a comprehensive analysis of our model through\narchitectural tuning and PaCMAP visualization of patient health trajectory in\nlatent space.\n","authors":["Saurabh Kataria","Ran Xiao","Timothy Ruchti","Matthew Clark","Jiaying Lu","Randall J. Lee","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08611v1","updated":"2025-02-12T17:59:21Z","published":"2025-02-12T17:59:21Z","title":"Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation","summary":"  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n","authors":["Nikos Zarifis","Puqian Wang","Ilias Diakonikolas","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2502.08611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2409.05655v2","updated":"2025-02-12T17:55:53Z","published":"2024-09-09T14:22:19Z","title":"Interactive incremental learning of generalizable skills with local\n  trajectory modulation","summary":"  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n","authors":["Markus Knauer","Alin Albu-Schffer","Freek Stulp","Joo Silvrio"],"pdf_url":"https://arxiv.org/pdf/2409.05655v2.pdf","comment":"Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19\n  figures, 6 tables. See\n  https://github.com/DLR-RM/interactive-incremental-learning for further\n  information and video"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08605v1","updated":"2025-02-12T17:49:46Z","published":"2025-02-12T17:49:46Z","title":"CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection","summary":"  Does the intrinsic curvature of complex networks hold the key to unveiling\ngraph anomalies that conventional approaches overlook? Reconstruction-based\ngraph anomaly detection (GAD) methods overlook such geometric outliers,\nfocusing only on structural and attribute-level anomalies. To this end, we\npropose CurvGAD - a mixed-curvature graph autoencoder that introduces the\nnotion of curvature-based geometric anomalies. CurvGAD introduces two parallel\npipelines for enhanced anomaly interpretability: (1) Curvature-equivariant\ngeometry reconstruction, which focuses exclusively on reconstructing the edge\ncurvatures using a mixed-curvature, Riemannian encoder and Gaussian\nkernel-based decoder; and (2) Curvature-invariant structure and attribute\nreconstruction, which decouples structural and attribute anomalies from\ngeometric irregularities by regularizing graph curvature under discrete\nOllivier-Ricci flow, thereby isolating the non-geometric anomalies. By\nleveraging curvature, CurvGAD refines the existing anomaly classifications and\nidentifies new curvature-driven anomalies. Extensive experimentation over 10\nreal-world datasets (both homophilic and heterophilic) demonstrates an\nimprovement of up to 6.5% over state-of-the-art GAD methods.\n","authors":["Karish Grover","Geoffrey J. Gordon","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2502.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08603v1","updated":"2025-02-12T17:44:40Z","published":"2025-02-12T17:44:40Z","title":"Scalable Thermodynamic Second-order Optimization","summary":"  Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics.\n","authors":["Kaelan Donatella","Samuel Duffield","Denis Melanson","Maxwell Aifer","Phoebe Klett","Rajath Salegame","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2502.08603v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.05771v3","updated":"2025-02-12T17:43:56Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2501.18715v2","updated":"2025-02-12T17:41:57Z","published":"2025-01-30T19:26:05Z","title":"chebgreen: Learning and Interpolating Continuous Empirical Green's\n  Functions from Data","summary":"  In this work, we present a mesh-independent, data-driven library, chebgreen,\nto mathematically model one-dimensional systems, possessing an associated\ncontrol parameter, and whose governing partial differential equation is\nunknown. The proposed method learns an Empirical Green's Function for the\nassociated, but hidden, boundary value problem, in the form of a Rational\nNeural Network from which we subsequently construct a bivariate representation\nin a Chebyshev basis. We uncover the Green's function, at an unseen control\nparameter value, by interpolating the left and right singular functions within\na suitable library, expressed as points on a manifold of Quasimatrices, while\nthe associated singular values are interpolated with Lagrange polynomials.\n","authors":["Harshwardhan Praveen","Jacob Brown","Christopher Earls"],"pdf_url":"https://arxiv.org/pdf/2501.18715v2.pdf","comment":"Code is available at https://github.com/hsharsh/chebgreen"},{"id":"http://arxiv.org/abs/2501.07602v2","updated":"2025-02-12T17:41:23Z","published":"2025-01-10T23:33:15Z","title":"An Explainable Pipeline for Machine Learning with Functional Data","summary":"  Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.\n","authors":["Katherine Goode","J. Derek Tucker","Daniel Ries","Heike Hofmann"],"pdf_url":"https://arxiv.org/pdf/2501.07602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08600v1","updated":"2025-02-12T17:39:02Z","published":"2025-02-12T17:39:02Z","title":"Two-stage hybrid models for enhancing forecasting accuracy on\n  heterogeneous time series","summary":"  Compared to local models built in a series-by-series manner, global models\nleverage relevant information across time series, resulting in improved\nforecasting performance and generalization capacity. Constructing global models\non a set of time series is becoming mainstream in the field of time series\nforecasting. However, the advantages of global models may not always be\nrealized when dealing with heterogeneous data. While they can adapt to\nheterogeneous datasets by increasing the model complexity, the model cannot be\ninfinitely complex due to the finite sample size, which poses challenges for\nthe application of global models. Additionally, determining whether the time\nseries data is homogeneous or heterogeneous can be ambiguous in practice. To\naddress these research gaps, this paper argues that the heterogeneity of the\ndata should be defined by the global model used, and for each series, the\nportion not modelled by the global model represents heterogeneity. It further\nproposes two-stage hybrid models, which include a second stage to identify and\nmodel heterogeneous patterns. In this second stage, we can estimate either all\nlocal models or sub-global models across different domains divided based on\nheterogeneity. Experiments on four open datasets reveal that the proposed\nmethods significantly outperform five existing models, indicating they\ncontribute to fully unleash the potential of global models on heterogeneous\ndatasets.\n","authors":["Junru Ren","Shaomin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08600v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08598v1","updated":"2025-02-12T17:35:43Z","published":"2025-02-12T17:35:43Z","title":"Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio","summary":"  The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance/signal-to-noise-ratio\ndisentangled (TV/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.\n","authors":["Khaled Kahouli","Winfried Ripken","Stefan Gugler","Oliver T. Unke","Klaus-Robert Mller","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2502.08598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v1","updated":"2025-02-12T17:32:23Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Yuhao Wang","Aram Ebtekar","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03943v2","updated":"2025-02-12T17:29:54Z","published":"2024-10-04T22:00:13Z","title":"Oscillatory State-Space Models","summary":"  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with\nsequences of length 50k.\n","authors":["T. Konstantin Rusch","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.03943v2.pdf","comment":"ICLR (Oral)"},{"id":"http://arxiv.org/abs/2408.05486v2","updated":"2025-02-12T17:29:28Z","published":"2024-08-10T08:27:58Z","title":"Topological Blindspots: Understanding and Extending Topological Deep\n  Learning Through the Lens of Expressivity","summary":"  Topological deep learning (TDL) is a rapidly growing field that seeks to\nleverage topological structure in data and facilitate learning from data\nsupported on topological objects, ranging from molecules to 3D shapes. Most TDL\narchitectures can be unified under the framework of higher-order\nmessage-passing (HOMP), which generalizes graph message-passing to higher-order\ndomains. In the first part of the paper, we explore HOMP's expressive power\nfrom a topological perspective, demonstrating the framework's inability to\ncapture fundamental topological and metric invariants such as diameter,\norientability, planarity, and homology. In addition, we demonstrate HOMP's\nlimitations in fully leveraging lifting and pooling methods on graphs. To the\nbest of our knowledge, this is the first work to study the expressivity of TDL\nfrom a \\emph{topological} perspective. In the second part of the paper, we\ndevelop two new classes of architectures -- multi-cellular networks (MCN) and\nscalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can\nreach full expressivity, but scaling it to large data objects can be\ncomputationally expansive. Designed as a more scalable alternative, SMCN still\nmitigates many of HOMP's expressivity limitations. Finally, we create new\nbenchmarks for evaluating models based on their ability to learn topological\nproperties of complexes. We then evaluate SMCN on these benchmarks and on\nreal-world graph datasets, demonstrating improvements over both HOMP baselines\nand expressive graph methods, highlighting the value of expressively leveraging\ntopological information. Code and data are available at\nhttps://github.com/yoavgelberg/SMCN.\n","authors":["Yam Eitan","Yoav Gelberg","Guy Bar-Shalom","Fabrizio Frasca","Michael Bronstein","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2408.05486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08586v1","updated":"2025-02-12T17:19:36Z","published":"2025-02-12T17:19:36Z","title":"Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks","summary":"  A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.\n","authors":["Ang Li","Yin Zhou","Vethavikashini Chithrra Raghuram","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2502.08586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08585v1","updated":"2025-02-12T17:18:14Z","published":"2025-02-12T17:18:14Z","title":"Scalable Bilevel Loss Balancing for Multi-Task Learning","summary":"  Multi-task learning (MTL) has been widely adopted for its ability to\nsimultaneously learn multiple tasks. While existing gradient manipulation\nmethods often yield more balanced solutions than simple scalarization-based\napproaches, they typically incur a significant computational overhead of\n$\\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In\nthis paper, we propose BiLB4MTL, a simple and scalable loss balancing approach\nfor MTL, formulated from a novel bilevel optimization perspective. Our method\nincorporates three key components: (i) an initial loss normalization, (ii) a\nbilevel loss-balancing formulation, and (iii) a scalable first-order algorithm\nthat requires only $\\mathcal{O}(1)$ time and memory. Theoretically, we prove\nthat BiLB4MTL guarantees convergence not only to a stationary point of the\nbilevel loss balancing problem but also to an $\\epsilon$-accurate Pareto\nstationary point for all $K$ loss functions under mild conditions. Extensive\nexperiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves\nstate-of-the-art performance in both accuracy and efficiency. Code is available\nat https://github.com/OptMN-Lab/-BiLB4MTL.\n","authors":["Peiyao Xiao","Chaosheng Dong","Shaofeng Zou","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2502.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08582v1","updated":"2025-02-12T17:14:07Z","published":"2025-02-12T17:14:07Z","title":"A method for classification of data with uncertainty using hypothesis\n  testing","summary":"  Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.\n","authors":["Shoma Yokura","Akihisa Ichiki"],"pdf_url":"https://arxiv.org/pdf/2502.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08577v1","updated":"2025-02-12T17:10:53Z","published":"2025-02-12T17:10:53Z","title":"FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning","summary":"  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n","authors":["Davide Domini","Gianluca Aguzzi","Lukas Esterle","Mirko Viroli"],"pdf_url":"https://arxiv.org/pdf/2502.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08576v1","updated":"2025-02-12T17:10:34Z","published":"2025-02-12T17:10:34Z","title":"Mapping the Landscape of Generative AI in Network Monitoring and\n  Management","summary":"  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n","authors":["Giampaolo Bovenzi","Francesco Cerasuolo","Domenico Ciuonzo","Davide Di Monda","Idio Guarino","Antonio Montieri","Valerio Persico","Antonio Pescap"],"pdf_url":"https://arxiv.org/pdf/2502.08576v1.pdf","comment":"32 pages, 9 figure, 10 tables"},{"id":"http://arxiv.org/abs/2502.08574v1","updated":"2025-02-12T17:09:13Z","published":"2025-02-12T17:09:13Z","title":"COAST: Intelligent Time-Adaptive Neural Operators","summary":"  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a\nnovel neural operator learning method that leverages a causal language model\n(CLM) framework to dynamically adapt time steps. Our method predicts both the\nevolution of a system and its optimal time step, intelligently balancing\ncomputational efficiency and accuracy. We find that COAST generates variable\nstep sizes that correlate with the underlying system intrinsicities, both\nwithin and across dynamical systems. Within a single trajectory, smaller steps\nare taken in regions of high complexity, while larger steps are employed in\nsimpler regions. Across different systems, more complex dynamics receive more\ngranular time steps. Benchmarked on diverse systems with varied dynamics, COAST\nconsistently outperforms state-of-the-art methods, achieving superior\nperformance in both efficiency and accuracy. This work underscores the\npotential of CLM-based intelligent adaptive solvers for scalable operator\nlearning of dynamical systems.\n","authors":["Zhikai Wu","Shiyang Zhang","Sizhuang He","Sifan Wang","Min Zhu","Anran Jiao","Lu Lu","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.08574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18304v4","updated":"2025-02-12T17:02:06Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v4.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.06914v2","updated":"2025-02-12T16:47:32Z","published":"2025-02-10T09:46:26Z","title":"UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge","summary":"  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n","authors":["Chenao Li","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06914v2.pdf","comment":"18 pages,8 figures"},{"id":"http://arxiv.org/abs/2110.06257v3","updated":"2025-02-12T16:47:01Z","published":"2021-10-12T18:12:57Z","title":"Causal Discovery from Conditionally Stationary Time Series","summary":"  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.\n","authors":["Carles Balsells-Rodas","Xavier Sumba","Tanmayee Narendra","Ruibo Tu","Gabriele Schweikert","Hedvig Kjellstrom","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2110.06257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08555v1","updated":"2025-02-12T16:35:46Z","published":"2025-02-12T16:35:46Z","title":"A Machine Learning-Ready Data Processing Tool for Near Real-Time\n  Forecasting","summary":"  Space weather forecasting is critical for mitigating radiation risks in space\nexploration and protecting Earth-based technologies from geomagnetic\ndisturbances. This paper presents the development of a Machine Learning (ML)-\nready data processing tool for Near Real-Time (NRT) space weather forecasting.\nBy merging data from diverse NRT sources such as solar imagery, magnetic field\nmeasurements, and energetic particle fluxes, the tool addresses key gaps in\ncurrent space weather prediction capabilities. The tool processes and\nstructures the data for machine learning models, focusing on time-series\nforecasting and event detection for extreme solar events. It provides users\nwith a framework to download, process, and label data for ML applications,\nstreamlining the workflow for improved NRT space weather forecasting and\nscientific research.\n","authors":["Maher A Dayeh","Michael J Starkey","Subhamoy Chatterjee","Heather Elliott","Samuel Hart","Kimberly Moreland"],"pdf_url":"https://arxiv.org/pdf/2502.08555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08549v1","updated":"2025-02-12T16:30:39Z","published":"2025-02-12T16:30:39Z","title":"Copula-based mixture model identification for subgroup clustering with\n  imaging applications","summary":"  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n","authors":["Fei Zheng","Nicolas Duchateau"],"pdf_url":"https://arxiv.org/pdf/2502.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08542v1","updated":"2025-02-12T16:27:40Z","published":"2025-02-12T16:27:40Z","title":"Beyond Predictions: A Participatory Framework for Multi-Stakeholder\n  Decision-Making","summary":"  Conventional decision-support systems, primarily based on supervised\nlearning, focus on outcome prediction models to recommend actions. However,\nthey often fail to account for the complexities of multi-actor environments,\nwhere diverse and potentially conflicting stakeholder preferences must be\nbalanced. In this paper, we propose a novel participatory framework that\nredefines decision-making as a multi-stakeholder optimization problem,\ncapturing each actor's preferences through context-dependent reward functions.\nOur framework leverages $k$-fold cross-validation to fine-tune user-provided\noutcome prediction models and evaluate decision strategies, including\ncompromise functions mediating stakeholder trade-offs. We introduce a synthetic\nscoring mechanism that exploits user-defined preferences across multiple\nmetrics to rank decision-making strategies and identify the optimal\ndecision-maker. The selected decision-maker can then be used to generate\nactionable recommendations for new data. We validate our framework using two\nreal-world use cases, demonstrating its ability to deliver recommendations that\neffectively balance multiple metrics, achieving results that are often beyond\nthe scope of purely prediction-based methods. Ablation studies demonstrate that\nour framework, with its modular, model-agnostic, and inherently transparent\ndesign, integrates seamlessly with various predictive models, reward\nstructures, evaluation metrics, and sample sizes, making it particularly suited\nfor complex, high-stakes decision-making contexts.\n","authors":["Vittoria Vineis","Giuseppe Perelli","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08536v1","updated":"2025-02-12T16:21:01Z","published":"2025-02-12T16:21:01Z","title":"Matrix Completion with Graph Information: A Provable Nonconvex\n  Optimization Approach","summary":"  We consider the problem of matrix completion with graphs as side information\ndepicting the interrelations between variables. The key challenge lies in\nleveraging the similarity structure of the graph to enhance matrix recovery.\nExisting approaches, primarily based on graph Laplacian regularization, suffer\nfrom several limitations: (1) they focus only on the similarity between\nneighboring variables, while overlooking long-range correlations; (2) they are\nhighly sensitive to false edges in the graphs and (3) they lack theoretical\nguarantees regarding statistical and computational complexities. To address\nthese issues, we propose in this paper a novel graph regularized matrix\ncompletion algorithm called GSGD, based on preconditioned projected gradient\ndescent approach. We demonstrate that GSGD effectively captures the\nhigher-order correlation information behind the graphs, and achieves superior\nrobustness and stability against the false edges. Theoretically, we prove that\nGSGD achieves linear convergence to the global optimum with near-optimal sample\ncomplexity, providing the first theoretical guarantees for both recovery\naccuracy and efficacy in the perspective of nonconvex optimization. Our\nnumerical experiments on both synthetic and real-world data further validate\nthat GSGD achieves superior recovery accuracy and scalability compared with\nseveral popular alternatives.\n","authors":["Yao Wang","Yiyang Yang","Kaidong Wang","Shanxing Gao","Xiuwu Liao"],"pdf_url":"https://arxiv.org/pdf/2502.08536v1.pdf","comment":"41 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08531v1","updated":"2025-02-12T16:08:48Z","published":"2025-02-12T16:08:48Z","title":"On Different Notions of Redundancy in Conditional-Independence-Based\n  Discovery of Graphical Models","summary":"  The goal of conditional-independence-based discovery of graphical models is\nto find a graph that represents the independence structure of variables in a\ngiven dataset. To learn such a representation, conditional-independence-based\napproaches conduct a set of statistical tests that suffices to identify the\ngraphical representation under some assumptions on the underlying distribution\nof the data. In this work, we highlight that due to the conciseness of the\ngraphical representation, there are often many tests that are not used in the\nconstruction of the graph. These redundant tests have the potential to detect\nor sometimes correct errors in the learned model. We show that not all tests\ncontain this additional information and that such redundant tests have to be\napplied with care. Precisely, we argue that particularly those conditional\n(in)dependence statements are interesting that follow only from graphical\nassumptions but do not hold for every probability distribution.\n","authors":["Philipp M. Faller","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09262v2","updated":"2025-02-12T16:07:40Z","published":"2025-01-16T03:11:50Z","title":"On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement","summary":"  Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.\n","authors":["Jingyi Wang","Haowei Wang","Nai-Yuan Chiang","Cosmin G. Petra"],"pdf_url":"https://arxiv.org/pdf/2501.09262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08518v1","updated":"2025-02-12T15:54:56Z","published":"2025-02-12T15:54:56Z","title":"FedMHO: Heterogeneous One-Shot Federated Learning Towards\n  Resource-Constrained Edge Devices","summary":"  Federated Learning (FL) is increasingly adopted in edge computing scenarios,\nwhere a large number of heterogeneous clients operate under constrained or\nsufficient resources. The iterative training process in conventional FL\nintroduces significant computation and communication overhead, which is\nunfriendly for resource-constrained edge devices. One-shot FL has emerged as a\npromising approach to mitigate communication overhead, and model-heterogeneous\nFL solves the problem of diverse computing resources across clients. However,\nexisting methods face challenges in effectively managing model-heterogeneous\none-shot FL, often leading to unsatisfactory global model performance or\nreliance on auxiliary datasets. To address these challenges, we propose a novel\nFL framework named FedMHO, which leverages deep classification models on\nresource-sufficient clients and lightweight generative models on\nresource-constrained devices. On the server side, FedMHO involves a two-stage\nprocess that includes data generation and knowledge fusion. Furthermore, we\nintroduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem\nduring the knowledge fusion stage, and an unsupervised data optimization\nsolution to improve the quality of synthetic samples. Comprehensive experiments\ndemonstrate the effectiveness of our methods, as they outperform\nstate-of-the-art baselines in various experimental setups.\n","authors":["Dezhong Yao","Yuexin Shi","Tongtong Liu","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08515v1","updated":"2025-02-12T15:47:48Z","published":"2025-02-12T15:47:48Z","title":"The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data","summary":"  This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.\n","authors":["Evgenii Evstafev"],"pdf_url":"https://arxiv.org/pdf/2502.08515v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.05980v3","updated":"2025-02-12T15:40:35Z","published":"2024-02-08T06:48:01Z","title":"Do Large Code Models Understand Programming Concepts? Counterfactual\n  Analysis for Code Predicates","summary":"  Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.\n","authors":["Ashish Hooda","Mihai Christodorescu","Miltiadis Allamanis","Aaron Wilson","Kassem Fawaz","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2402.05980v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08505v1","updated":"2025-02-12T15:36:38Z","published":"2025-02-12T15:36:38Z","title":"Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based\n  Framework for Effective Label Propagation","summary":"  Graph Neural Networks (GNNs) have recently become the predominant tools for\nstudying graph data. Despite state-of-the-art performance on graph\nclassification tasks, GNNs are overwhelmingly trained in a single domain under\nsupervision, thus necessitating a prohibitively high demand for labels and\nresulting in poorly transferable representations. To address this challenge, we\npropose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework\nto bridge the gap between graph data and traditional domain adaptation methods.\nIt extracts graph topological information holistically with a tensor\narchitecture and then reduces domain discrepancy through label propagation. It\nis readily compatible with general GNNs and domain adaptation techniques with\nminimal adjustment through pseudo-labeling. Experiments on various real-world\nbenchmarks show that our LP-TGNN outperforms baselines by a notable margin. We\nalso validate and analyze each component of the proposed framework in the\nablation study.\n","authors":["Tao Wen","Elynn Chen","Yuzhou Chen","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2502.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10229v2","updated":"2025-02-12T15:34:01Z","published":"2024-05-16T16:28:11Z","title":"Random ReLU Neural Networks as Non-Gaussian Processes","summary":"  We consider a large class of shallow neural networks with randomly\ninitialized parameters and rectified linear unit activation functions. We prove\nthat these random neural networks are well-defined non-Gaussian processes. As a\nby-product, we demonstrate that these networks are solutions to stochastic\ndifferential equations driven by impulsive white noise (combinations of random\nDirac measures). These processes are parameterized by the law of the weights\nand biases as well as the density of activation thresholds in each bounded\nregion of the input domain. We prove that these processes are isotropic and\nwide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably\nsimple closed-form expression for their autocovariance function. Our results\nare fundamentally different from prior work in that we consider a\nnon-asymptotic viewpoint: The number of neurons in each bounded region of the\ninput domain (i.e., the width) is itself a random variable with a Poisson law\nwith mean proportional to the density parameter. Finally, we show that, under\nsuitable hypotheses, as the expected width tends to infinity, these processes\ncan converge in law not only to Gaussian processes, but also to non-Gaussian\nprocesses depending on the law of the weights. Our asymptotic results provide a\nnew take on several classical results (wide networks converge to Gaussian\nprocesses) as well as some new ones (wide networks can converge to non-Gaussian\nprocesses).\n","authors":["Rahul Parhi","Pakshal Bohra","Ayoub El Biari","Mehrsa Pourya","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2405.10229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08488v1","updated":"2025-02-12T15:23:29Z","published":"2025-02-12T15:23:29Z","title":"One-Shot Federated Learning with Classifier-Free Diffusion Models","summary":"  Federated learning (FL) enables collaborative learning without data\ncentralization but introduces significant communication costs due to multiple\ncommunication rounds between clients and the server. One-shot federated\nlearning (OSFL) addresses this by forming a global model with a single\ncommunication round, often relying on the server's model distillation or\nauxiliary dataset generation - often through pre-trained diffusion models\n(DMs). Existing DM-assisted OSFL methods, however, typically employ\nclassifier-guided DMs, which require training auxiliary classifier models at\neach client, introducing additional computation overhead. This work introduces\nOSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a\nnovel OSFL approach that eliminates the need for auxiliary models. OSCAR uses\nfoundation models to devise category-specific data representations at each\nclient, seamlessly integrated into a classifier-free diffusion model pipeline\nfor server-side data generation. OSCAR is a simple yet cost-effective OSFL\napproach that outperforms the state-of-the-art on four benchmarking datasets\nwhile reducing the communication load by at least 99%.\n","authors":["Obaidullah Zaland","Shutong Jin","Florian T. Pokorny","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2502.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07746v2","updated":"2025-02-12T15:18:45Z","published":"2024-10-10T09:23:33Z","title":"Benign Overfitting in Single-Head Attention","summary":"  The phenomenon of benign overfitting, where a trained neural network\nperfectly fits noisy training data but still achieves near-optimal test\nperformance, has been extensively studied in recent years for linear models and\nfully-connected/convolutional networks. In this work, we study benign\noverfitting in a single-head softmax attention model, which is the fundamental\nbuilding block of Transformers. We prove that under appropriate conditions, the\nmodel exhibits benign overfitting in a classification setting already after two\nsteps of gradient descent. Moreover, we show conditions where a\nminimum-norm/maximum-margin interpolator exhibits benign overfitting. We study\nhow the overfitting behavior depends on the signal-to-noise ratio (SNR) of the\ndata distribution, namely, the ratio between norms of signal and noise tokens,\nand prove that a sufficiently large SNR is both necessary and sufficient for\nbenign overfitting.\n","authors":["Roey Magen","Shuning Shang","Zhiwei Xu","Spencer Frei","Wei Hu","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.07746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2411.02540v3","updated":"2025-02-12T15:14:01Z","published":"2024-11-04T19:21:06Z","title":"GraphXAIN: Narratives to Explain Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.\n","authors":["Mateusz Cedro","David Martens"],"pdf_url":"https://arxiv.org/pdf/2411.02540v3.pdf","comment":"19 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.11759v4","updated":"2025-02-12T15:07:01Z","published":"2024-10-15T16:28:55Z","title":"LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery","summary":"  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings.\n","authors":["Sujai Hiremath","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2410.11759v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11140v4","updated":"2025-02-12T15:06:14Z","published":"2023-04-21T17:22:08Z","title":"Convergence of Message Passing Graph Neural Networks with Generic\n  Aggregation On Large Random Graphs","summary":"  We study the convergence of message passing graph neural networks on random\ngraph models to their continuous counterpart as the number of nodes tends to\ninfinity. Until now, this convergence was only known for architectures with\naggregation functions in the form of normalized means, or, equivalently, of an\napplication of classical operators like the adjacency matrix or the graph\nLaplacian. We extend such results to a large class of aggregation functions,\nthat encompasses all classically used message passing graph neural networks,\nsuch as attention-based message passing, max convolutional message passing,\n(degree-normalized) convolutional message passing, or moment-based aggregation\nmessage passing. Under mild assumptions, we give non-asymptotic bounds with\nhigh probability to quantify this convergence. Our main result is based on the\nMcDiarmid inequality. Interestingly, this result does not apply to the case\nwhere the aggregation is a coordinate-wise maximum. We treat this case\nseparately and obtain a different convergence rate.\n","authors":["Matthieu Cordonnier","Nicolas Keriven","Nicolas Tremblay","Samuel Vaiter"],"pdf_url":"https://arxiv.org/pdf/2304.11140v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08470v1","updated":"2025-02-12T15:04:23Z","published":"2025-02-12T15:04:23Z","title":"Numerical Schemes for Signature Kernels","summary":"  Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps://github.com/FrancescoPiatti/polysigkernel.\n","authors":["Thomas Cass","Francesco Piatti","Jeffrey Pei"],"pdf_url":"https://arxiv.org/pdf/2502.08470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03667v2","updated":"2025-02-12T14:56:35Z","published":"2024-05-06T17:43:39Z","title":"Fault Detection and Monitoring using a Data-Driven Information-Based\n  Strategy: Method, Theory, and Application","summary":"  The ability to detect when a system undergoes an incipient fault is of\nparamount importance in preventing a critical failure. Classic methods for\nfault detection (including model-based and data-driven approaches) rely on\nthresholding error statistics or simple input-residual dependencies but face\ndifficulties with non-linear or non-Gaussian systems. Behavioral methods (e.g.,\nthose relying on digital twins) address these difficulties but still face\nchallenges when faulty data is scarce, decision guarantees are required, or\nworking with already-deployed models is required. In this work, we propose an\ninformation-driven fault detection method based on a novel concept drift\ndetector, addressing these challenges. The method is tailored to identifying\ndrifts in input-output relationships of additive noise models (i.e., model\ndrifts) and is based on a distribution-free mutual information (MI) estimator.\nOur scheme does not require prior faulty examples and can be applied\ndistribution-free over a large class of system models. Our core contributions\nare twofold. First, we demonstrate the connection between fault detection,\nmodel drift detection, and testing independence between two random variables.\nSecond, we prove several theoretical properties of the proposed MI-based fault\ndetection scheme: (i) strong consistency, (ii) exponentially fast detection of\nthe non-faulty case, and (iii) control of both significance levels and power of\nthe test. To conclude, we validate our theory with synthetic data and the\nbenchmark dataset N-CMAPSS of aircraft turbofan engines. These empirical\nresults support the usefulness of our methodology in many practical and\nrealistic settings, and the theoretical results show performance guarantees\nthat other methods cannot offer.\n","authors":["Camilo Ramrez","Jorge F. Silva","Ferhat Tamssaouet","Toms Rojas","Marcos E. Orchard"],"pdf_url":"https://arxiv.org/pdf/2405.03667v2.pdf","comment":"31 pages, 15 figures. This is the accepted manuscript for publication\n  in Mechanical Systems and Signal Processing. The arXiv version has been\n  updated accordingly"},{"id":"http://arxiv.org/abs/2408.09966v2","updated":"2025-02-12T14:55:29Z","published":"2024-08-19T13:14:02Z","title":"Mask in the Mirror: Implicit Sparsification","summary":"  Continuous sparsification strategies are among the most effective methods for\nreducing the inference costs and memory demands of large-scale neural networks.\nA key factor in their success is the implicit $L_1$ regularization induced by\njointly learning both mask and weight variables, which has been shown\nexperimentally to outperform explicit $L_1$ regularization. We provide a\ntheoretical explanation for this observation by analyzing the learning\ndynamics, revealing that early continuous sparsification is governed by an\nimplicit $L_2$ regularization that gradually transitions to an $L_1$ penalty\nover time. Leveraging this insight, we propose a method to dynamically control\nthe strength of this implicit bias. Through an extension of the mirror flow\nframework, we establish convergence and optimality guarantees in the context of\nunderdetermined linear regression. Our theoretical findings may be of\nindependent interest, as we demonstrate how to enter the rich regime and show\nthat the implicit bias can be controlled via a time-dependent Bregman\npotential. To validate these insights, we introduce PILoT, a continuous\nsparsification approach with novel initialization and dynamic regularization,\nwhich consistently outperforms baselines in standard experiments.\n","authors":["Tom Jacobs","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2408.09966v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08457v1","updated":"2025-02-12T14:52:04Z","published":"2025-02-12T14:52:04Z","title":"Learning Theory for Kernel Bilevel Optimization","summary":"  Bilevel optimization has emerged as a technique for addressing a wide range\nof machine learning problems that involve an outer objective implicitly\ndetermined by the minimizer of an inner problem. In this paper, we investigate\nthe generalization properties for kernel bilevel optimization problems where\nthe inner objective is optimized over a Reproducing Kernel Hilbert Space. This\nsetting enables rich function approximation while providing a foundation for\nrigorous theoretical analysis. In this context, we establish novel\ngeneralization error bounds for the bilevel problem under finite-sample\napproximation. Our approach adopts a functional perspective, inspired by\n(Petrulionyte et al., 2024), and leverages tools from empirical process theory\nand maximal inequalities for degenerate $U$-processes to derive uniform error\nbounds. These generalization error estimates allow to characterize the\nstatistical accuracy of gradient-based methods applied to the empirical\ndiscretization of the bilevel problem.\n","authors":["Fares El Khoury","Edouard Pauwels","Samuel Vaiter","Michael Arbel"],"pdf_url":"https://arxiv.org/pdf/2502.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08649v2","updated":"2025-02-12T14:49:16Z","published":"2024-07-11T16:28:31Z","title":"Confidence-based Estimators for Predictive Performance in Model\n  Monitoring","summary":"  After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.\n","authors":["Juhani Kivimki","Jakub Biaek","Jukka K. Nurminen","Wojtek Kuberski"],"pdf_url":"https://arxiv.org/pdf/2407.08649v2.pdf","comment":"This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]"},{"id":"http://arxiv.org/abs/2311.00055v2","updated":"2025-02-12T14:43:07Z","published":"2023-10-31T18:03:54Z","title":"Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding\n  Perspective","summary":"  Pre-training is prevalent in deep learning for vision and text data,\nleveraging knowledge from other datasets to enhance downstream tasks. However,\nfor tabular data, the inherent heterogeneity in attribute and label spaces\nacross datasets complicates the learning of shareable knowledge. We propose\nTabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train\na general tabular model over diverse datasets. The core idea is to embed data\ninstances into a shared feature space, where each instance is represented by\nits distance to a fixed number of nearest neighbors and their labels. This\n''meta-representation'' transforms heterogeneous tasks into homogeneous local\nprediction problems, enabling the model to infer labels (or scores for each\nlabel) based on neighborhood information. As a result, the pre-trained TabPTM\ncan be applied directly to new datasets, regardless of their diverse attributes\nand labels, without further fine-tuning. Extensive experiments on 101 datasets\nconfirm TabPTM's effectiveness in both classification and regression tasks,\nwith and without fine-tuning.\n","authors":["Han-Jia Ye","Qi-Le Zhou","Huai-Hong Yin","De-Chuan Zhan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2311.00055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08448v1","updated":"2025-02-12T14:40:19Z","published":"2025-02-12T14:40:19Z","title":"Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware\n  Minimization Based on Loss Geometry","summary":"  Recent studies on deep neural networks show that flat minima of the loss\nlandscape correlate with improved generalization. Sharpness-aware minimization\n(SAM) efficiently finds flat regions by updating the parameters according to\nthe gradient at an adversarial perturbation. The perturbation depends on the\nEuclidean metric, making SAM non-invariant under reparametrizations, which\nblurs sharpness and generalization. We propose Monge SAM (M-SAM), a\nreparametrization invariant version of SAM by considering a Riemannian metric\nin the parameter space induced naturally by the loss surface. Compared to\nprevious approaches, M-SAM works under any modeling choice, relies only on mild\nassumptions while being as computationally efficient as SAM. We theoretically\nargue that M-SAM varies between SAM and gradient descent (GD), which increases\nrobustness to hyperparameter selection and reduces attraction to suboptimal\nequilibria like saddle points. We demonstrate this behavior both theoretically\nand empirically on a multi-modal representation alignment task.\n","authors":["Albert Kjller Jacobsen","Georgios Arvanitidis"],"pdf_url":"https://arxiv.org/pdf/2502.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08445v1","updated":"2025-02-12T14:36:25Z","published":"2025-02-12T14:36:25Z","title":"$\\texttt{LucidAtlas}$: Learning Uncertainty-Aware,\n  Covariate-Disentangled, Individualized Atlas Representations","summary":"  The goal of this work is to develop principled techniques to extract\ninformation from high dimensional data sets with complex dependencies in areas\nsuch as medicine that can provide insight into individual as well as population\nlevel variation. We develop $\\texttt{LucidAtlas}$, an approach that can\nrepresent spatially varying information, and can capture the influence of\ncovariates as well as population uncertainty. As a versatile atlas\nrepresentation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate\ninterpretation, individualized prediction, population trend analysis, and\nuncertainty estimation, with the flexibility to incorporate prior knowledge.\nAdditionally, we discuss the trustworthiness and potential risks of neural\nadditive models for analyzing dependent covariates and then introduce a\nmarginalization approach to explain the dependence of an individual predictor\non the models' response (the atlas). To validate our method, we demonstrate its\ngeneralizability on two medical datasets. Our findings underscore the critical\nrole of by-construction interpretable models in advancing scientific discovery.\nOur code will be publicly available upon acceptance.\n","authors":["Yining Jiao","Sreekalyani Bhamidi","Huaizhi Qu","Carlton Zdanski","Julia Kimbell","Andrew Prince","Cameron Worden","Samuel Kirse","Christopher Rutter","Benjamin Shields","William Dunn","Jisan Mahmud","Tianlong Chen","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2502.08445v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v1","updated":"2025-02-12T14:32:17Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.16737v2","updated":"2025-02-12T14:25:30Z","published":"2024-11-23T13:16:06Z","title":"Federated Learning in Chemical Engineering: A Tutorial on a Framework\n  for Privacy-Preserving Collaboration Across Distributed Data Sources","summary":"  Federated Learning (FL) is a decentralized machine learning approach that has\ngained attention for its potential to enable collaborative model training\nacross clients while protecting data privacy, making it an attractive solution\nfor the chemical industry. This work aims to provide the chemical engineering\ncommunity with an accessible introduction to the discipline. Supported by a\nhands-on tutorial and a comprehensive collection of examples, it explores the\napplication of FL in tasks such as manufacturing optimization, multimodal data\nintegration, and drug discovery while addressing the unique challenges of\nprotecting proprietary information and managing distributed datasets. The\ntutorial was built using key frameworks such as $\\texttt{Flower}$ and\n$\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers\nwith the right tools to adopt FL in their specific needs. We compare the\nperformance of FL against centralized learning across three different datasets\nrelevant to chemical engineering applications, demonstrating that FL will often\nmaintain or improve classification performance, particularly for complex and\nheterogeneous data. We conclude with an outlook on the open challenges in\nfederated learning to be tackled and current approaches designed to remediate\nand improve this framework.\n","authors":["Siddhant Dutta","Iago Leal de Freitas","Pedro Maciel Xavier","Claudio Miceli de Farias","David Esteban Bernal Neira"],"pdf_url":"https://arxiv.org/pdf/2411.16737v2.pdf","comment":"53 Pages, 8 figures, Under review in ACS Industrial & Engineering\n  Chemistry Research Journal"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2502.08432v1","updated":"2025-02-12T14:16:45Z","published":"2025-02-12T14:16:45Z","title":"Closer through commonality: Enhancing hypergraph contrastive learning\n  with shared groups","summary":"  Hypergraphs provide a superior modeling framework for representing complex\nmultidimensional relationships in the context of real-world interactions that\noften occur in groups, overcoming the limitations of traditional homogeneous\ngraphs. However, there have been few studies on hypergraphbased contrastive\nlearning, and existing graph-based contrastive learning methods have not been\nable to fully exploit the highorder correlation information in hypergraphs.\nHere, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method\ndesigned to exploit the complex high-dimensional information inherent in\nhypergraphs. While avoiding traditional graph augmentation methods that corrupt\nthe hypergraph topology, the proposed method provides a simple and efficient\nlearning augmentation function by adding noise to node features. Furthermore,\nwe expands beyond the traditional dichotomous relationship between positive and\nnegative samples in contrastive learning by introducing a new relationship of\nweak positives. It demonstrates the importance of fine-graining positive\nsamples in contrastive learning. Therefore, HyFi is able to produce highquality\nembeddings, and outperforms both supervised and unsupervised baselines in\naverage rank on node classification across 10 datasets. Our approach\neffectively exploits high-dimensional hypergraph information, shows significant\nimprovement over existing graph-based contrastive learning methods, and is\nefficient in terms of training speed and GPU memory cost. The source code is\navailable at https://github.com/Noverse0/HyFi.git.\n","authors":["Daeyoung Roh","Donghee Han","Daehee Kim","Keejun Han","Mun Yi"],"pdf_url":"https://arxiv.org/pdf/2502.08432v1.pdf","comment":"11page, 5 figures, 6 tables, 2024 IEEE International Conference on\n  Big Data"},{"id":"http://arxiv.org/abs/2502.08426v1","updated":"2025-02-12T14:09:05Z","published":"2025-02-12T14:09:05Z","title":"Semantic Learning for Molecular Communication in Internet of Bio-Nano\n  Things","summary":"  Molecular communication (MC) provides a foundational framework for\ninformation transmission in the Internet of Bio-Nano Things (IoBNT), where\nefficiency and reliability are crucial. However, the inherent limitations of\nmolecular channels, such as low transmission rates, noise, and inter-symbol\ninterference (ISI), limit their ability to support complex data transmission.\nThis paper proposes an end-to-end semantic learning framework designed to\noptimize task-oriented molecular communication, with a focus on biomedical\ndiagnostic tasks under resource-constrained conditions. The proposed framework\nemploys a deep encoder-decoder architecture to efficiently extract, quantize,\nand decode semantic features, prioritizing task-relevant semantic information\nto enhance diagnostic classification performance. Additionally, a probabilistic\nchannel network is introduced to approximate molecular propagation dynamics,\nenabling gradient-based optimization for end-to-end learning. Experimental\nresults demonstrate that the proposed semantic framework improves diagnostic\naccuracy by at least 25% compared to conventional JPEG compression with LDPC\ncoding methods under resource-constrained communication scenarios.\n","authors":["Hanlin Cai","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2502.08426v1.pdf","comment":"4 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.08416v1","updated":"2025-02-12T13:59:22Z","published":"2025-02-12T13:59:22Z","title":"Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators","summary":"  Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.\n","authors":["Anastasia N. Krouglova","Hayden R. Johnson","Basile Confavreux","Michael Deistler","Pedro J. Gonalves"],"pdf_url":"https://arxiv.org/pdf/2502.08416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08414v1","updated":"2025-02-12T13:57:09Z","published":"2025-02-12T13:57:09Z","title":"Sparse Estimation of Inverse Covariance and Partial Correlation Matrices\n  via Joint Partial Regression","summary":"  We present a new method for estimating high-dimensional sparse partial\ncorrelation and inverse covariance matrices, which exploits the connection\nbetween the inverse covariance matrix and linear regression. The method is a\ntwo-stage estimation method wherein each individual feature is regressed on all\nother features while positive semi-definiteness is enforced simultaneously. We\nprovide statistical rates of convergence for the proposed method which match,\nand improve upon, the state-of-the-art for inverse covariance and partial\ncorrelation matrix estimation, respectively. We also propose an efficient\nproximal splitting algorithm for numerically computing the estimate. The\neffectiveness of the proposed method is demonstrated on both synthetic and\nreal-world data.\n","authors":["Samuel Erickson","Tobias Rydn"],"pdf_url":"https://arxiv.org/pdf/2502.08414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05431v2","updated":"2025-02-12T13:54:01Z","published":"2025-02-08T03:41:16Z","title":"APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding","summary":"  Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.\n","authors":["Xinyu Yang","Tianqi Chen","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.05431v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08397v1","updated":"2025-02-12T13:40:00Z","published":"2025-02-12T13:40:00Z","title":"Strong bounds for large-scale Minimum Sum-of-Squares Clustering","summary":"  Clustering is a fundamental technique in data analysis and machine learning,\nused to group similar data points together. Among various clustering methods,\nthe Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.\nMSSC aims to minimize the total squared Euclidean distance between data points\nand their corresponding cluster centroids. Due to the unsupervised nature of\nclustering, achieving global optimality is crucial, yet computationally\nchallenging. The complexity of finding the global solution increases\nexponentially with the number of data points, making exact methods impractical\nfor large-scale datasets. Even obtaining strong lower bounds on the optimal\nMSSC objective value is computationally prohibitive, making it difficult to\nassess the quality of heuristic solutions. We address this challenge by\nintroducing a novel method to validate heuristic MSSC solutions through\noptimality gaps. Our approach employs a divide-and-conquer strategy,\ndecomposing the problem into smaller instances that can be handled by an exact\nsolver. The decomposition is guided by an auxiliary optimization problem, the\n\"anticlustering problem\", for which we design an efficient heuristic.\nComputational experiments demonstrate the effectiveness of the method for\nlarge-scale instances, achieving optimality gaps below 3% in most cases while\nmaintaining reasonable computational times. These results highlight the\npracticality of our approach in assessing feasible clustering solutions for\nlarge datasets, bridging a critical gap in MSSC evaluation.\n","authors":["Anna Livia Croella","Veronica Piccialli","Antonio M. Sudoso"],"pdf_url":"https://arxiv.org/pdf/2502.08397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08707v2","updated":"2025-02-12T13:29:11Z","published":"2024-08-16T12:40:01Z","title":"Beam Prediction based on Large Language Models","summary":"  In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.\n","authors":["Yucheng Sheng","Kai Huang","Le Liang","Peng Liu","Shi Jin","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.08707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06581v2","updated":"2025-02-12T13:25:22Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15981v2","updated":"2025-02-12T13:22:54Z","published":"2024-10-21T13:06:38Z","title":"Robust Visual Representation Learning with Multi-modal Prior Knowledge\n  for Image Classification Under Distribution Shift","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV) - a distribution-based learning approach\nleveraging multi-modal prior knowledge - to improve generalization under\ndistribution shift. It integrates knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations,\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency across all experiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06862v2","updated":"2025-02-12T13:21:32Z","published":"2025-02-08T00:43:57Z","title":"Poincar Inequality for Local Log-Polyak-Lojasiewicz Measures :\n  Non-asymptotic Analysis in Low-temperature Regime","summary":"  Potential functions in highly pertinent applications, such as deep learning\nin over-parameterized regime, are empirically observed to admit non-isolated\nminima. To understand the convergence behavior of stochastic dynamics in such\nlandscapes, we propose to study the class of \\logPLmeasure\\ measures\n$\\mu_\\epsilon \\propto \\exp(-V/\\epsilon)$, where the potential $V$ satisfies a\nlocal Polyak-{\\L}ojasiewicz (P\\L) inequality, and its set of local minima is\nprovably \\emph{connected}. Notably, potentials in this class can exhibit local\nmaxima and we characterize its optimal set S to be a compact $\\mathcal{C}^2$\n\\emph{embedding submanifold} of $\\mathbb{R}^d$ without boundary. The\n\\emph{non-contractibility} of S distinguishes our function class from the\nclassical convex setting topologically. Moreover, the embedding structure\ninduces a naturally defined Laplacian-Beltrami operator on S, and we show that\nits first non-trivial eigenvalue provides an \\emph{$\\epsilon$-independent}\nlower bound for the \\Poincare\\ constant in the \\Poincare\\ inequality of\n$\\mu_\\epsilon$. As a direct consequence, Langevin dynamics with such non-convex\npotential $V$ and diffusion coefficient $\\epsilon$ converges to its equilibrium\n$\\mu_\\epsilon$ at a rate of $\\tilde{\\mathcal{O}}(1/\\epsilon)$, provided\n$\\epsilon$ is sufficiently small. Here $\\tilde{\\mathcal{O}}$ hides logarithmic\nterms.\n","authors":["Yun Gong","Zebang Shen","Niao He"],"pdf_url":"https://arxiv.org/pdf/2502.06862v2.pdf","comment":"This is a replacement version of arXiv:2501.00429"},{"id":"http://arxiv.org/abs/2402.04059v2","updated":"2025-02-12T13:16:29Z","published":"2024-02-06T15:03:53Z","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","summary":"  Missing values are ubiquitous in multivariate time series (MTS) data, posing\nsignificant challenges for accurate analysis and downstream applications. In\nrecent years, deep learning-based methods have successfully handled missing\ndata by leveraging complex temporal dependencies and learned data\ndistributions. In this survey, we provide a comprehensive summary of deep\nlearning approaches for multivariate time series imputation (MTSI) tasks. We\npropose a novel taxonomy that categorizes existing methods based on two key\nperspectives: imputation uncertainty and neural network architecture.\nFurthermore, we summarize existing MTSI toolkits with a particular emphasis on\nthe PyPOTS Ecosystem, which provides an integrated and standardized foundation\nfor MTSI research. Finally, we discuss key challenges and future research\ndirections, which give insight for further MTSI research. This survey aims to\nserve as a valuable resource for researchers and practitioners in the field of\ntime series analysis and missing data imputation tasks.\n","authors":["Jun Wang","Wenjie Du","Yiyuan Yang","Linglong Qian","Wei Cao","Keli Zhang","Wenjia Wang","Yuxuan Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.04059v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13306v2","updated":"2025-02-12T13:14:53Z","published":"2024-09-20T08:04:12Z","title":"Predicting DNA fragmentation: A non-destructive analogue to chemical\n  assays using machine learning","summary":"  Globally, infertility rates are increasing, with 2.5\\% of all births being\nassisted by in vitro fertilisation (IVF) in 2022. Male infertility is the cause\nfor approximately half of these cases. The quality of sperm DNA has substantial\nimpact on the success of IVF. The assessment of sperm DNA is traditionally done\nthrough chemical assays which render sperm cells ineligible for IVF. Many\ncompounding factors lead to the population crisis, with fertility rates\ndropping globally in recent history. As such assisted reproductive technologies\n(ART) have been the focus of recent research efforts. Simultaneously,\nartificial intelligence has grown ubiquitous and is permeating more aspects of\nmodern life. With the advent of state-of-the-art machine learning and its\nexceptional performance in many sectors, this work builds on these successes\nand proposes a novel framework for the prediction of sperm cell DNA\nfragmentation from images of unstained sperm. Rendering a predictive model\nwhich preserves sperm integrity and allows for optimal selection of sperm for\nIVF.\n","authors":["Byron A Jacobs","Ifthakaar Shaik","Frando Lin"],"pdf_url":"https://arxiv.org/pdf/2409.13306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08378v1","updated":"2025-02-12T13:10:09Z","published":"2025-02-12T13:10:09Z","title":"Learning Humanoid Standing-up Control across Diverse Postures","summary":"  Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/.\n","authors":["Tao Huang","Junli Ren","Huayi Wang","Zirui Wang","Qingwei Ben","Muning Wen","Xiao Chen","Jianan Li","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.08378v1.pdf","comment":"Humanoid Standing-up Control, 12 pages"},{"id":"http://arxiv.org/abs/2502.08376v1","updated":"2025-02-12T13:07:18Z","published":"2025-02-12T13:07:18Z","title":"Enhanced Load Forecasting with GAT-LSTM: Leveraging Grid and Temporal\n  Features","summary":"  Accurate power load forecasting is essential for the efficient operation and\nplanning of electrical grids, particularly given the increased variability and\ncomplexity introduced by renewable energy sources. This paper introduces\nGAT-LSTM, a hybrid model that combines Graph Attention Networks (GAT) and Long\nShort-Term Memory (LSTM) networks. A key innovation of the model is the\nincorporation of edge attributes, such as line capacities and efficiencies,\ninto the attention mechanism, enabling it to dynamically capture spatial\nrelationships grounded in grid-specific physical and operational constraints.\nAdditionally, by employing an early fusion of spatial graph embeddings and\ntemporal sequence features, the model effectively learns and predicts complex\ninteractions between spatial dependencies and temporal patterns, providing a\nrealistic representation of the dynamics of power grids. Experimental\nevaluations on the Brazilian Electricity System dataset demonstrate that the\nGAT-LSTM model significantly outperforms state-of-the-art models, achieving\nreductions of 21. 8% in MAE, 15. 9% in RMSE and 20. 2% in MAPE. These results\nunderscore the robustness and adaptability of the GAT-LSTM model, establishing\nit as a powerful tool for applications in grid management and energy planning.\n","authors":["Ugochukwu Orji","iek Gven","Dan Stowell"],"pdf_url":"https://arxiv.org/pdf/2502.08376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16332v2","updated":"2025-02-12T13:07:07Z","published":"2023-05-22T01:14:46Z","title":"Continual Learning through Human-Robot Interaction: Human Perceptions of\n  a Continual Learning Robot in Repeated Interactions","summary":"  For long-term deployment in dynamic real-world environments, assistive robots\nmust continue to learn and adapt to their environments. Researchers have\ndeveloped various computational models for continual learning (CL) that can\nallow robots to continually learn from limited training data, and avoid\nforgetting previous knowledge. While these CL models can mitigate forgetting on\nstatic, systematically collected datasets, it is unclear how human users might\nperceive a robot that continually learns over multiple interactions with them.\nIn this paper, we developed a system that integrates CL models for object\nrecognition with a Fetch mobile manipulator robot and allows human participants\nto directly teach and test the robot over multiple sessions. We conducted an\nin-person study with 60 participants that interacted with our system in 300\nsessions (5 sessions per participant). We conducted a between-subject study\nwith three different CL models to understand human perceptions of continual\nlearning robots over multiple sessions. Our results suggest that participants'\nperceptions of trust, competence, and usability of a continual learning robot\nsignificantly decrease over multiple sessions if the robot forgets previously\nlearned objects. However, the perceived task load on participants for teaching\nand testing the robot remains the same over multiple sessions even if the robot\nforgets previously learned objects. Our results also indicate that\nstate-of-the-art CL models might perform unreliably when applied on robots\ninteracting with human participants. Further, continual learning robots are not\nperceived as very trustworthy or competent by human participants, regardless of\nthe underlying continual learning model or the session number.\n","authors":["Ali Ayub","Zachary De Francesco","Patrick Holthaus","Chrystopher L. Nehaniv","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2305.16332v2.pdf","comment":"Accepted at the International Journal of Social Robotics (SoRo), 2025"},{"id":"http://arxiv.org/abs/2502.08365v1","updated":"2025-02-12T12:51:36Z","published":"2025-02-12T12:51:36Z","title":"Towards Principled Multi-Agent Task Agnostic Exploration","summary":"  In reinforcement learning, we typically refer to task-agnostic exploration\nwhen we aim to explore the environment without access to the task specification\na priori. In a single-agent setting the problem has been extensively studied\nand mostly understood. A popular approach cast the task-agnostic objective as\nmaximizing the entropy of the state distribution induced by the agent's policy,\nfrom which principles and methods follows. In contrast, little is known about\ntask-agnostic exploration in multi-agent settings, which are ubiquitous in the\nreal world. How should different agents explore in the presence of others? In\nthis paper, we address this question through a generalization to multiple\nagents of the problem of maximizing the state distribution entropy. First, we\ninvestigate alternative formulations, highlighting respective positives and\nnegatives. Then, we present a scalable, decentralized, trust-region policy\nsearch algorithm to address the problem in practical settings. Finally, we\nprovide proof of concept experiments to both corroborate the theoretical\nfindings and pave the way for task-agnostic exploration in challenging\nmulti-agent settings.\n","authors":["Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2502.08365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08364v1","updated":"2025-02-12T12:50:24Z","published":"2025-02-12T12:50:24Z","title":"A Survey on Pre-Trained Diffusion Model Distillations","summary":"  Diffusion Models~(DMs) have emerged as the dominant approach in Generative\nArtificial Intelligence (GenAI), owing to their remarkable performance in tasks\nsuch as text-to-image synthesis. However, practical DMs, such as stable\ndiffusion, are typically trained on massive datasets and thus usually require\nlarge storage. At the same time, many steps may be required, i.e., recursively\nevaluating the trained neural network, to generate a high-quality image, which\nresults in significant computational costs during sample generation. As a\nresult, distillation methods on pre-trained DM have become widely adopted\npractices to develop smaller, more efficient models capable of rapid, few-step\ngeneration in low-resource environment. When these distillation methods are\ndeveloped from different perspectives, there is an urgent need for a systematic\nsurvey, particularly from a methodological perspective. In this survey, we\nreview distillation methods through three aspects: output loss distillation,\ntrajectory distillation and adversarial distillation. We also discuss current\nchallenges and outline future research directions in the conclusion.\n","authors":["Xuhui Fan","Zhangkai Wu","Hongyu Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.07071v2","updated":"2025-02-12T12:38:13Z","published":"2025-01-31T19:43:13Z","title":"TRADES: Generating Realistic Market Simulations with Diffusion Models","summary":"  Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.\n","authors":["Leonardo Berti","Bardh Prenkaj","Paola Velardi"],"pdf_url":"https://arxiv.org/pdf/2502.07071v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.08355v1","updated":"2025-02-12T12:30:49Z","published":"2025-02-12T12:30:49Z","title":"Loss Landscape Analysis for Reliable Quantized ML Models for Scientific\n  Sensing","summary":"  In this paper, we propose a method to perform empirical analysis of the loss\nlandscape of machine learning (ML) models. The method is applied to two ML\nmodels for scientific sensing, which necessitates quantization to be deployed\nand are subject to noise and perturbations due to experimental conditions. Our\nmethod allows assessing the robustness of ML models to such effects as a\nfunction of quantization precision and under different regularization\ntechniques -- two crucial concerns that remained underexplored so far. By\ninvestigating the interplay between performance, efficiency, and robustness by\nmeans of loss landscape analysis, we both established a strong correlation\nbetween gently-shaped landscapes and robustness to input and weight\nperturbations and observed other intriguing and non-obvious phenomena. Our\nmethod allows a systematic exploration of such trade-offs a priori, i.e.,\nwithout training and testing multiple models, leading to more efficient\ndevelopment workflows. This work also highlights the importance of\nincorporating robustness into the Pareto optimization of ML models, enabling\nmore reliable and adaptive scientific sensing systems.\n","authors":["Tommaso Baldi","Javier Campos","Olivia Weng","Caleb Geniesse","Nhan Tran","Ryan Kastner","Alessandro Biondi"],"pdf_url":"https://arxiv.org/pdf/2502.08355v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.08353v1","updated":"2025-02-12T12:28:39Z","published":"2025-02-12T12:28:39Z","title":"Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy","summary":"  With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.\n","authors":["Ruizhan Xue","Huimin Deng","Fang He","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08353v1.pdf","comment":"Submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08340v1","updated":"2025-02-12T12:07:09Z","published":"2025-02-12T12:07:09Z","title":"Hierarchical Learning-based Graph Partition for Large-scale Vehicle\n  Routing Problems","summary":"  Neural solvers based on the divide-and-conquer approach for Vehicle Routing\nProblems (VRPs) in general, and capacitated VRP (CVRP) in particular,\nintegrates the global partition of an instance with local constructions for\neach subproblem to enhance generalization. However, during the global partition\nphase, misclusterings within subgraphs have a tendency to progressively\ncompound throughout the multi-step decoding process of the learning-based\npartition policy. This suboptimal behavior in the global partition phase, in\nturn, may lead to a dramatic deterioration in the performance of the overall\ndecomposition-based system, despite using optimal local constructions. To\naddress these challenges, we propose a versatile Hierarchical Learning-based\nGraph Partition (HLGP) framework, which is tailored to benefit the partition of\nCVRP instances by synergistically integrating global and local partition\npolicies. Specifically, the global partition policy is tasked with creating the\ncoarse multi-way partition to generate the sequence of simpler two-way\npartition subtasks. These subtasks mark the initiation of the subsequent K\nlocal partition levels. At each local partition level, subtasks exclusive for\nthis level are assigned to the local partition policy which benefits from the\ninsensitive local topological features to incrementally alleviate the\ncompounded errors. This framework is versatile in the sense that it optimizes\nthe involved partition policies towards a unified objective harmoniously\ncompatible with both reinforcement learning (RL) and supervised learning (SL).\n(*Due to the notification of arXiv \"The Abstract field cannot be longer than\n1,920 characters\", the appeared Abstract is shortened. For the full Abstract,\nplease download the Article.)\n","authors":["Yuxin Pan","Ruohong Liu","Yize Chen","Zhiguang Cao","Fangzhen Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08340v1.pdf","comment":"Accepted as a Full Paper at AAMAS 2025 (24th International Conference\n  on Autonomous Agents and Multiagent Systems)"}]},"2025-02-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09601v1","updated":"2025-02-13T18:52:36Z","published":"2025-02-13T18:52:36Z","title":"CoT-Valve: Length-Compressible Chain-of-Thought Tuning","summary":"  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n","authors":["Xinyin Ma","Guangnian Wan","Runpeng Yu","Gongfan Fang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09601v1.pdf","comment":"Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09589v1","updated":"2025-02-13T18:46:44Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09567v1","updated":"2025-02-13T18:22:31Z","published":"2025-02-13T18:22:31Z","title":"MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing","summary":"  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n","authors":["Vlad Andrei Negru","Robert Vacareanu","Camelia Lemnaru","Mihai Surdeanu","Rodica Potolea"],"pdf_url":"https://arxiv.org/pdf/2502.09567v1.pdf","comment":"16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2406.05925v2","updated":"2025-02-13T18:02:34Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.06773v2","updated":"2025-02-13T17:50:39Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang","Kai Li"],"pdf_url":"https://arxiv.org/pdf/2406.06773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09532v1","updated":"2025-02-13T17:49:30Z","published":"2025-02-13T17:49:30Z","title":"Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages","summary":"  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n","authors":["Shreyan Biswas","Alexander Erlei","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2502.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08489v2","updated":"2025-02-13T17:33:24Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc Pmies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","Jos Javier Saiz","Ferran Espua","Jaume Prats","Javier Aula-Blasco","Mario Mina","Iigo Pikabea","Adrin Rubio","Alexander Shvets","Anna Salls","Iaki Lacunza","Jorge Palomar","Jlia Falco","Luca Tormo","Luis Vasquez-Reina","Montserrat Marimon","Oriol Pareras","Valle Ruiz-Fernndez","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05331v2","updated":"2025-02-13T17:27:15Z","published":"2025-02-07T21:13:27Z","title":"Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books","summary":"  Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.\n","authors":["Sangmitra Madhusudan","Robert Morabito","Skye Reid","Nikta Gohari Sadr","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2502.05331v2.pdf","comment":"9 pages (excluding references), accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.14792v2","updated":"2025-02-13T17:22:36Z","published":"2024-08-27T05:56:04Z","title":"Measuring Human Contribution in AI-Assisted Content Generation","summary":"  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n","authors":["Yueqi Xie","Tao Qi","Jingwei Yi","Xiyuan Yang","Ryan Whalen","Junming Huang","Qian Ding","Yu Xie","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v2","updated":"2025-02-13T17:12:34Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v2","updated":"2025-02-13T17:11:41Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09497v1","updated":"2025-02-13T17:09:52Z","published":"2025-02-13T17:09:52Z","title":"Improve LLM-based Automatic Essay Scoring with Linguistic Features","summary":"  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n","authors":["Zhaoyi Joey Hou","Alejandro Ciuba","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2502.09497v1.pdf","comment":"To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2502.09457v1","updated":"2025-02-13T16:25:16Z","published":"2025-02-13T16:25:16Z","title":"The Multilingual Mind : A Survey of Multilingual Reasoning in Language\n  Models","summary":"  While reasoning and multilingual capabilities in Language Models (LMs) have\nachieved remarkable progress in recent years, their integration into a unified\nparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning\nrequires language models to handle logical reasoning across languages while\naddressing misalignment, biases, and challenges in low-resource settings. This\nsurvey provides the first in-depth review of multilingual reasoning in LMs. In\nthis survey, we provide a systematic overview of existing methods that leverage\nLMs for multilingual reasoning, specifically outlining the challenges,\nmotivations, and foundational aspects of applying language models to reason\nacross diverse languages. We provide an overview of the standard data resources\nused for training multilingual reasoning in LMs and the evaluation benchmarks\nemployed to assess their multilingual capabilities. Next, we analyze various\nstate-of-the-art methods and their performance on these benchmarks. Finally, we\nexplore future research opportunities to improve multilingual reasoning in LMs,\nfocusing on enhancing their ability to handle diverse languages and complex\nreasoning tasks.\n","authors":["Akash Ghosh","Debayan Datta","Sriparna Saha","Chirag Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.09457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09416v1","updated":"2025-02-13T15:39:07Z","published":"2025-02-13T15:39:07Z","title":"Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use\n  a Different Evaluation Process than Human?","summary":"  One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, $n$-gram based metrics, and sentence-level\nmetrics, and show that resolving the gap improves results for the most of\nmetrics on the SEEDA benchmark. We also found that even BERT-based metrics\nsometimes outperform the metrics of GPT-4. We publish our unified\nimplementation of the metrics and meta-evaluations.\n","authors":["Takumi Goto","Yusuke Sakai","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.09416v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2310.19347v4","updated":"2025-02-13T15:25:02Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization","summary":"  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2412.17395v2","updated":"2025-02-13T15:11:24Z","published":"2024-12-23T08:47:42Z","title":"WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models","summary":"  Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.\n","authors":["Huawen Feng","Pu Zhao","Qingfeng Sun","Can Xu","Fangkai Yang","Lu Wang","Qianli Ma","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09387v1","updated":"2025-02-13T15:04:53Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.15927v2","updated":"2025-02-13T14:55:26Z","published":"2024-11-24T17:32:20Z","title":"Generative Prompt Internalization","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v2.pdf","comment":"NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Kster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08514v2","updated":"2025-02-13T14:34:29Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v3","updated":"2025-02-13T14:13:41Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v3.pdf","comment":"LangFair repository: https://github.com/cvs-health/langfair"},{"id":"http://arxiv.org/abs/2411.05031v2","updated":"2025-02-13T14:02:53Z","published":"2024-11-06T09:52:29Z","title":"On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard","summary":"  Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.\n","authors":["Hossam Amer","Joe Osborne","Michael Zaki","Mohamed Afify"],"pdf_url":"https://arxiv.org/pdf/2411.05031v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.09331v1","updated":"2025-02-13T13:49:30Z","published":"2025-02-13T13:49:30Z","title":"Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs","summary":"  Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.\n","authors":["Itai Mondshine","Tzuf Paz-Argaman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2502.09331v1.pdf","comment":"Accepted for NAACL findings 2025"},{"id":"http://arxiv.org/abs/2502.09316v1","updated":"2025-02-13T13:30:54Z","published":"2025-02-13T13:30:54Z","title":"A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis","summary":"  Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.\n","authors":["Kentaro Imajo","Masanori Hirano","Shuji Suzuki","Hiroaki Mikami"],"pdf_url":"https://arxiv.org/pdf/2502.09316v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.05497v2","updated":"2025-02-13T13:22:40Z","published":"2025-02-08T09:04:16Z","title":"DeepThink: Aligning Language Models with Domain-Specific User Intents","summary":"  Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.\n","authors":["Yang Li","Mingxuan Luo","Yeyun Gong","Chen Lin","Jian Jiao","Yi Liu","Kaili Huang"],"pdf_url":"https://arxiv.org/pdf/2502.05497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09307v1","updated":"2025-02-13T13:19:33Z","published":"2025-02-13T13:19:33Z","title":"When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models","summary":"  Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.\n","authors":["Samuel Joseph Amouyal","Aya Meltzer-Asscher","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2502.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15330v2","updated":"2025-02-13T13:06:00Z","published":"2024-06-21T17:42:52Z","title":"Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection","summary":"  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n","authors":["Haoling Li","Xin Zhang","Xiao Liu","Yeyun Gong","Yifan Wang","Qi Chen","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.15330v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09284v1","updated":"2025-02-13T12:57:15Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v3","updated":"2025-02-13T12:43:59Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Usage?","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17301v2","updated":"2025-02-13T12:25:54Z","published":"2024-11-26T10:48:55Z","title":"ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation","summary":"  Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.\n","authors":["Yunyi Liu","Yingshu Li","Zhanyu Wang","Xinyu Liang","Lingqiao Liu","Lei Wang","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.17301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00008v5","updated":"2025-02-13T12:10:33Z","published":"2023-03-27T18:00:01Z","title":"On the Creativity of Large Language Models","summary":"  Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2304.00008v5.pdf","comment":"Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3"},{"id":"http://arxiv.org/abs/2502.09247v1","updated":"2025-02-13T12:03:36Z","published":"2025-02-13T12:03:36Z","title":"The Joint Entity-Relation Extraction Model Based on Span and Interactive\n  Fusion Representation for Chinese Medical Texts with Complex Semantics","summary":"  Joint entity-relation extraction is a critical task in transforming\nunstructured or semi-structured text into triplets, facilitating the\nconstruction of large-scale knowledge graphs, and supporting various downstream\napplications. Despite its importance, research on Chinese text, particularly\nwith complex semantics in specialized domains like medicine, remains limited.\nTo address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions\ndataset designed to capture the intricacies of medical text. Leveraging the\nstrengths of attention mechanisms in capturing long-range dependencies, we\npropose the SEA module, which enhances the extraction of complex contextual\nsemantic information, thereby improving entity recognition and relation\nextraction. Additionally, to address the inefficiencies of existing methods in\nfacilitating information exchange between entity recognition and relation\nextraction, we present an interactive fusion representation module. This module\nemploys Cross Attention for bidirectional information exchange between the\ntasks and further refines feature extraction through BiLSTM. Experimental\nresults on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that\nour model exhibits strong generalization capabilities. On the CH-DDI dataset,\nour model achieves an F1-score of 96.73% for entity recognition and 78.43% for\nrelation extraction. On the CoNLL04 dataset, it attains an entity recognition\nprecision of 89.54% and a relation extraction accuracy of 71.64%.\n","authors":["Danni Feng","Runzhi Li","Jing Wang","Siyu Yan","Lihong Ma","Yunli Xing"],"pdf_url":"https://arxiv.org/pdf/2502.09247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09237v1","updated":"2025-02-13T11:54:28Z","published":"2025-02-13T11:54:28Z","title":"Reliable Conversational Agents under ASP Control that Understand Natural\n  Language","summary":"  Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable.\n","authors":["Yankai Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.09237v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09231v1","updated":"2025-02-13T11:52:55Z","published":"2025-02-13T11:52:55Z","title":"Answer Set Counting and its Applications","summary":"  We have focused on Answer Set Programming (ASP), more specifically, answer\nset counting, exploring both exact and approximate methodologies. We developed\nan exact ASP counter, sharpASP, which utilizes a compact encoding for\npropositional formulas, significantly enhancing efficiency compared to existing\nmethods that often struggle with inefficient encodings. Our evaluations\nindicate that sharpASP outperforms current ASP counters on several benchmarks.\nIn addition, we proposed an approximate ASP counter, named ApproxASP, a\nhashing-based counter integrating Gauss-Jordan elimination within the ASP\nsolver, clingo. As a practical application, we employed ApproxASP for network\nreliability estimation, demonstrating superior performance over both\ntraditional reliability estimators and #SAT-based methods.\n","authors":["Mohimenul Kabir"],"pdf_url":"https://arxiv.org/pdf/2502.09231v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09216v1","updated":"2025-02-13T11:49:17Z","published":"2025-02-13T11:49:17Z","title":"Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for\n  Autonomous Vehicles","summary":"  In this paper, we present a modular system for representing and reasoning\nwith legal aspects of traffic rules for autonomous vehicles. We focus on a\nsubset of the United Kingdom's Highway Code (HC) related to junctions. As human\ndrivers and automated vehicles (AVs) will interact on the roads, especially in\nurban environments, we claim that an accessible, unitary, high-level\ncomputational model should exist and be applicable to both users. Autonomous\nvehicles introduce a shift in liability that should not bring disadvantages or\nincreased burden on human drivers. We develop a system \"in silico\" of the\nmodel. The proposed system is built of three main components: a natural\nlanguage interface, using Logical English, which encodes the rules; an internal\nrepresentation of the rules in Prolog; and an multi-agent-based simulation\nenvironment, built in NetLogo. The three components interact: Logical English\nis translated into and out of Prolog (along with some support code); Prolog and\nNetLogo interface via predicates. Such a modular approach enables the different\ncomponents to carry different \"burdens\" in the overall system; it also allows\nswapping of modules. Given NetLogo, we can visualize the effect of the modeled\nrules as well as validate the system with a simple dynamic running scenario.\nDesignated agents monitor the behaviour of the vehicles for compliance and\nrecord potential violations where they occur. The information on potential\nviolations is then utilized by Validators, to determine whether the violation\nis punishable, differentiating between exceptions and cases.\n","authors":["Galileo Sartor","Adam Wyner","Giuseppe Contissa"],"pdf_url":"https://arxiv.org/pdf/2502.09216v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09212v1","updated":"2025-02-13T11:48:31Z","published":"2025-02-13T11:48:31Z","title":"LP-LM: No Hallucinations in Question Answering with Logic Programming","summary":"  Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.\n","authors":["Katherine Wu","Yanhong A. Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09212v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2411.16495v3","updated":"2025-02-13T11:46:25Z","published":"2024-11-25T15:35:51Z","title":"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning","summary":"  Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.\n","authors":["Amy Xin","Jinxin Liu","Zijun Yao","Zhicheng Lee","Shulin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2411.16495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10053v2","updated":"2025-02-13T11:43:39Z","published":"2024-08-19T14:48:04Z","title":"Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory","summary":"  Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.\n","authors":["Haoran Li","Wei Fan","Yulin Chen","Jiayang Cheng","Tianshu Chu","Xuebing Zhou","Peizhao Hu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.10053v2.pdf","comment":"To appear at NAACL 25"},{"id":"http://arxiv.org/abs/2412.15151v3","updated":"2025-02-13T11:37:45Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang","Kaisong Song"],"pdf_url":"https://arxiv.org/pdf/2412.15151v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09192v1","updated":"2025-02-13T11:32:09Z","published":"2025-02-13T11:32:09Z","title":"Thinking beyond the anthropomorphic paradigm benefits LLM research","summary":"  Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development.\n","authors":["Lujain Ibrahim","Myra Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08441v2","updated":"2025-02-13T11:30:41Z","published":"2024-07-11T12:30:19Z","title":"Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.\n","authors":["Riccardo Cantini","Giada Cosenza","Alessio Orsino","Domenico Talia"],"pdf_url":"https://arxiv.org/pdf/2407.08441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09188v1","updated":"2025-02-13T11:22:19Z","published":"2025-02-13T11:22:19Z","title":"Matina: A Large-Scale 73B Token Persian Text Corpus","summary":"  Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.\n","authors":["Sara Bourbour Hosseinbeigi","Fatemeh Taherinezhad","Heshaam Faili","Hamed Baghbani","Fatemeh Nadi","Mostafa Amiri"],"pdf_url":"https://arxiv.org/pdf/2502.09188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09183v1","updated":"2025-02-13T11:17:53Z","published":"2025-02-13T11:17:53Z","title":"RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation","summary":"  Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.\n","authors":["Changzhi Zhou","Xinyu Zhang","Dandan Song","Xiancai Chen","Wanli Gu","Huipeng Ma","Yuhang Tian","Mengdi Zhang","Linmei Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09183v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2502.09175v1","updated":"2025-02-13T11:05:55Z","published":"2025-02-13T11:05:55Z","title":"FLAME: Flexible LLM-Assisted Moderation Engine","summary":"  The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.\n","authors":["Ivan Bakulin","Ilia Kopanichuk","Iaroslav Bespalov","Nikita Radchenko","Vladimir Shaposhnikov","Dmitry Dylov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2502.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09168v1","updated":"2025-02-13T10:51:40Z","published":"2025-02-13T10:51:40Z","title":"Musical Heritage Historical Entity Linking","summary":"  Linking named entities occurring in text to their corresponding entity in a\nKnowledge Base (KB) is challenging, especially when dealing with historical\ntexts. In this work, we introduce Musical Heritage named Entities Recognition,\nClassification and Linking (MHERCL), a novel benchmark consisting of manually\nannotated sentences extrapolated from historical periodicals of the music\ndomain. MHERCL contains named entities under-represented or absent in the most\nfamous KBs. We experiment with several State-of-the-Art models on the Entity\nLinking (EL) task and show that MHERCL is a challenging dataset for all of\nthem. We propose a novel unsupervised EL model and a method to extend\nsupervised entity linkers by using Knowledge Graphs (KGs) to tackle the main\ndifficulties posed by historical documents. Our experiments reveal that relying\non unsupervised techniques and improving models with logical constraints based\non KGs and heuristics to predict NIL entities (entities not represented in the\nKB of reference) results in better EL performance on historical documents.\n","authors":["Arianna Graciotti","Nicolas Lazzari","Valentina Presutti","Rocco Tripodi"],"pdf_url":"https://arxiv.org/pdf/2502.09168v1.pdf","comment":"To appear in Artificial Intelligence Review Journal"},{"id":"http://arxiv.org/abs/2308.13916v5","updated":"2025-02-13T10:45:15Z","published":"2023-08-26T16:51:17Z","title":"Exploring Large Language Models for Knowledge Graph Completion","summary":"  Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4.\n","authors":["Liang Yao","Jiazhen Peng","Chengsheng Mao","Yuan Luo"],"pdf_url":"https://arxiv.org/pdf/2308.13916v5.pdf","comment":"Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2502.09156v1","updated":"2025-02-13T10:36:18Z","published":"2025-02-13T10:36:18Z","title":"Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs","summary":"  Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.\n","authors":["Chang Liu","Ying Chang","Jianmin Li","Yiqian Qu","Yu Li","Lingyong Cao","Shuyuan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09120v1","updated":"2025-02-13T09:55:48Z","published":"2025-02-13T09:55:48Z","title":"The influence of visual and linguistic cues on ignorance inference in\n  Vision-Language Models (VLMs)","summary":"  This study explored how Vision-Language Models (VLMs) process ignorance\nimplicatures with visual and linguistic cues. Particularly, we focused on the\neffects of contexts (precise and approximate contexts) and modifier types (bare\nnumerals, superlative, and comparative modifiers), which were considered\npragmatic and semantic factors respectively. Methodologically, we conducted a\ntruth-value judgment task in visually grounded settings using GPT-4o and Gemini\n1.5 Pro. The results indicate that while both models exhibited sensitivity to\nlinguistic cues (modifier), they failed to process ignorance implicatures with\nvisual cues (context) as humans do. Specifically, the influence of context was\nweaker and inconsistent across models, indicating challenges in pragmatic\nreasoning for VLMs. On the other hand, superlative modifiers were more strongly\nassociated with ignorance implicatures as compared to comparative modifiers,\nsupporting the semantic view. These findings highlight the need for further\nadvancements in VLMs to process language-vision information in a\ncontext-dependent way to achieve human-like pragmatic inference.\n","authors":["Ye-eun Cho","Yunho Maeng"],"pdf_url":"https://arxiv.org/pdf/2502.09120v1.pdf","comment":"13 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09100v1","updated":"2025-02-13T09:19:14Z","published":"2025-02-13T09:19:14Z","title":"Logical Reasoning in Large Language Models: A Survey","summary":"  With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.\n","authors":["Hanmeng Liu","Zhizhang Fu","Mengru Ding","Ruoxi Ning","Chaoli Zhang","Xiaozhang Liu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09097v1","updated":"2025-02-13T09:13:23Z","published":"2025-02-13T09:13:23Z","title":"A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian\n  Optimization and Bidirectional Recurrent Unit","summary":"  In this paper, we propose an optimized Transformer model that integrates\nBayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and\napply it to fake news classification for the first time. First, we employ the\nTF-IDF method to extract features from news texts and transform them into\nnumeric representations to facilitate subsequent machine learning tasks. Two\nsets of experiments are then conducted for fake news detection and\nclassification: one using a Transformer model optimized only with BiGRU, and\nthe other incorporating Bayesian algorithms into the BiGRU-based Transformer.\nExperimental results show that the BiGRU-optimized Transformer achieves 100%\naccuracy on the training set and 99.67% on the test set, while the addition of\nthe Bayesian algorithm maintains 100% accuracy on the training set and slightly\nimproves test-set accuracy to 99.73%. This indicates that the Bayesian\nalgorithm boosts model accuracy by 0.06%, further enhancing the detection\ncapability for fake news. Moreover, the proposed algorithm converges rapidly at\naround the 10th training epoch with accuracy nearing 100%, demonstrating both\nits effectiveness and its fast classification ability. Overall, the optimized\nTransformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits\nexcellent continuous learning and detection performance, offering a robust\ntechnical means to combat the spread of fake news in the current era of\ninformation overload.\n","authors":["Tianyi Huang","Zeqiu Xu","Peiyang Yu","Jingyuan Yi","Xiaochuan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09097v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13166v4","updated":"2025-02-13T09:08:42Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v4.pdf","comment":"Published at ICLR 2025. Source code available at\n  https://github.com/SakanaAI/evo-memory"},{"id":"http://arxiv.org/abs/2405.14445v2","updated":"2025-02-13T09:07:10Z","published":"2024-05-23T11:24:23Z","title":"Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study","summary":"  This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM.\n","authors":["Lena Schmidt","Kaitlyn Hair","Sergio Graziosi","Fiona Campbell","Claudia Kapp","Alireza Khanteymoori","Dawn Craig","Mark Engelbert","James Thomas"],"pdf_url":"https://arxiv.org/pdf/2405.14445v2.pdf","comment":"Conference proceedings, peer-reviewed and presented at the 3rd\n  Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems,\n  Glasgow, 2024"},{"id":"http://arxiv.org/abs/2502.09086v1","updated":"2025-02-13T09:00:32Z","published":"2025-02-13T09:00:32Z","title":"A Hybrid Model for Few-Shot Text Classification Using Transfer and\n  Meta-Learning","summary":"  With the continuous development of natural language processing (NLP)\ntechnology, text classification tasks have been widely used in multiple\napplication fields. However, obtaining labeled data is often expensive and\ndifficult, especially in few-shot learning scenarios. To solve this problem,\nthis paper proposes a few-shot text classification model based on transfer\nlearning and meta-learning. The model uses the knowledge of the pre-trained\nmodel for transfer and optimizes the model's rapid adaptability in few-sample\ntasks through a meta-learning mechanism. Through a series of comparative\nexperiments and ablation experiments, we verified the effectiveness of the\nproposed method. The experimental results show that under the conditions of few\nsamples and medium samples, the model based on transfer learning and\nmeta-learning significantly outperforms traditional machine learning and deep\nlearning methods. In addition, ablation experiments further analyzed the\ncontribution of each component to the model performance and confirmed the key\nrole of transfer learning and meta-learning in improving model accuracy.\nFinally, this paper discusses future research directions and looks forward to\nthe potential of this method in practical applications.\n","authors":["Jia Gao","Shuangquan Lyu","Guiran Liu","Binrong Zhu","Hongye Zheng","Xiaoxuan Liao"],"pdf_url":"https://arxiv.org/pdf/2502.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09083v1","updated":"2025-02-13T08:56:25Z","published":"2025-02-13T08:56:25Z","title":"Show Me the Work: Fact-Checkers' Requirements for Explainable Automated\n  Fact-Checking","summary":"  The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.\n","authors":["Greta Warren","Irina Shklovski","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.09083v1.pdf","comment":"Conditionally accepted to CHI'25"},{"id":"http://arxiv.org/abs/2502.09082v1","updated":"2025-02-13T08:55:24Z","published":"2025-02-13T08:55:24Z","title":"CoSER: Coordinating LLM-Based Persona Simulation of Established Roles","summary":"  Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.\n","authors":["Xintao Wang","Heng Wang","Yifei Zhang","Xinfeng Yuan","Rui Xu","Jen-tse Huang","Siyu Yuan","Haoran Guo","Jiangjie Chen","Wei Wang","Yanghua Xiao","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09073v1","updated":"2025-02-13T08:42:29Z","published":"2025-02-13T08:42:29Z","title":"Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables","summary":"  Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics.\n","authors":["Xuzhao Geng","Haozhao Wang","Jun Wang","Wei Liu","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13835v2","updated":"2025-02-13T08:13:52Z","published":"2024-01-24T22:21:04Z","title":"What Large Language Models Know and What People Think They Know","summary":"  As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .\n","authors":["Mark Steyvers","Heliodoro Tejeda","Aakriti Kumar","Catarina Belem","Sheer Karny","Xinyue Hu","Lukas Mayer","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2401.13835v2.pdf","comment":"27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/"},{"id":"http://arxiv.org/abs/2401.11817v2","updated":"2025-02-13T08:11:25Z","published":"2024-01-22T10:26:14Z","title":"Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models","summary":"  Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.\n","authors":["Ziwei Xu","Sanjay Jain","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2401.11817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v1","updated":"2025-02-13T08:10:45Z","published":"2025-02-13T08:10:45Z","title":"An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in\n  One Day via Model Merging","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2405.19778v4","updated":"2025-02-13T08:03:52Z","published":"2024-05-30T07:44:16Z","title":"CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents","summary":"  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n","authors":["Jeiyoon Park","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2405.19778v4.pdf","comment":"NAACL 2025 Industry Track (Oral)"},{"id":"http://arxiv.org/abs/2502.09042v1","updated":"2025-02-13T07:55:54Z","published":"2025-02-13T07:55:54Z","title":"Typhoon T1: An Open Thai Reasoning Model","summary":"  This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.\n","authors":["Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2502.09042v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.00511v2","updated":"2025-02-13T07:35:08Z","published":"2025-02-01T18:09:49Z","title":"Bridging Internal Probability and Self-Consistency for Effective and\n  Efficient LLM Reasoning","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities. However, single-shot inference often yields\nunreliable results for complex reasoning tasks, leading researchers to explore\nmultiple reasoning paths through methods such as perplexity and\nself-consistency. In this paper, we present the first theoretical error\ndecomposition analysis of these techniques, breaking down their error into\nestimation error and model error. Our analysis reveals a fundamental trade-off:\nperplexity methods suffer from substantial model error due to the absence of a\nproper consistency function, while self-consistency exhibits high estimation\nerror due to a slow error convergence rate. To overcome these limitations, we\npropose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines\nPerplexity Consistency, which seamlessly integrates LLM perplexity with\nself-consistency, and Reasoning Pruning, which eliminates low-probability\nreasoning paths to effectively prevent the degeneration of estimation error\nreduction. Theoretical analysis demonstrates that RPC not only accelerates the\nconvergence rate of estimation error to an exponential level but also holds\nstrong potential for further reducing model error. Extensive empirical\nevaluations on seven benchmark datasets confirm that RPC can significantly\nimprove reasoning performance, sample efficiency, and confidence reliability.\n","authors":["Zhi Zhou","Tan Yuhao","Zenan Li","Yuan Yao","Lan-Zhe Guo","Xiaoxing Ma","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.00511v2.pdf","comment":"Preliminary work"},{"id":"http://arxiv.org/abs/2502.06635v2","updated":"2025-02-13T07:31:55Z","published":"2025-02-10T16:31:37Z","title":"Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM","summary":"  Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.\n","authors":["Qingshui Gu","Shu Li","Tianyu Zheng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07367v2","updated":"2025-02-13T07:31:13Z","published":"2024-12-10T10:06:46Z","title":"My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement\n  for Personalized Implicit Emotion Analysis","summary":"  The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA.\n","authors":["Jian Liao","Yu Feng","Yujin Zheng","Jun Zhao","Suge Wang","Jianxing Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.07367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02364v2","updated":"2025-02-13T07:29:46Z","published":"2024-02-04T06:23:05Z","title":"Loss Landscape Degeneracy Drives Stagewise Development in Transformers","summary":"  Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nunderscores the potential of a degeneracy-based perspective for understanding\nmodern deep learning.\n","authors":["Jesse Hoogland","George Wang","Matthew Farrugia-Roberts","Liam Carroll","Susan Wei","Daniel Murfet"],"pdf_url":"https://arxiv.org/pdf/2402.02364v2.pdf","comment":"Material on essential dynamics from v1 of this preprint has been\n  removed from v2 and developed in arXiv:2501.17745"},{"id":"http://arxiv.org/abs/2502.06572v2","updated":"2025-02-13T07:24:46Z","published":"2025-02-10T15:40:35Z","title":"LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM","summary":"  Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .\n","authors":["Zhi Zhou","Kun-Yang Yu","Shi-Yu Tian","Xiao-Wen Yang","Jiang-Xin Shi","Pengxiao Song","Yi-Xuan Jin","Lan-Zhe Guo","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.06572v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09017v1","updated":"2025-02-13T07:11:01Z","published":"2025-02-13T07:11:01Z","title":"Diversity Enhances an LLM's Performance in RAG and Long-context Task","summary":"  The rapid advancements in large language models (LLMs) have highlighted the\nchallenge of context window limitations, primarily due to the quadratic time\ncomplexity of the self-attention mechanism (\\(O(N^2)\\), where \\(N\\) denotes the\ncontext window length). This constraint impacts tasks such as\nretrieval-augmented generation (RAG) in question answering (Q\\&A) and long\ncontext summarization. A common approach involves selecting content with the\nhighest similarity to the query; however, this often leads to redundancy and\nthe exclusion of diverse yet relevant information. Building on principles from\nMaximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we\nintegrate diversity into the content selection process. Our findings reveal\nthat incorporating diversity substantially increases the recall of selecting\nrelevant sentences or chunks before LLM-based Q\\&A and summarization. These\nresults highlight the importance of maintaining diversity in future LLM\napplications to further improve summarization and Q\\&A outcomes.\n","authors":["Zhchao Wang","Bin Bi","Yanqi Luo","Sitaram Asur","Claire Na Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09004v1","updated":"2025-02-13T06:49:14Z","published":"2025-02-13T06:49:14Z","title":"Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content\n  in Mainstream US News Media through the Lens of Hope Speech","summary":"  This paper makes three contributions. First, via a substantial corpus of\n1,419,047 comments posted on 3,161 YouTube news videos of major US cable news\noutlets, we analyze how users engage with LGBTQ+ news content. Our analyses\nfocus both on positive and negative content. In particular, we construct a\nfine-grained hope speech classifier that detects positive (hope speech),\nnegative, neutral, and irrelevant content. Second, in consultation with a\npublic health expert specializing on LGBTQ+ health, we conduct an annotation\nstudy with a balanced and diverse political representation and release a\ndataset of 3,750 instances with fine-grained labels and detailed annotator\ndemographic information. Finally, beyond providing a vital resource for the\nLGBTQ+ community, our annotation study and subsequent in-the-wild assessments\nreveal (1) strong association between rater political beliefs and how they rate\ncontent relevant to a marginalized community; (2) models trained on individual\npolitical beliefs exhibit considerable in-the-wild disagreement; and (3)\nzero-shot large language models (LLMs) align more with liberal raters.\n","authors":["Jonathan Pofcher","Christopher M. Homan","Randall Sell","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2502.09004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13258v2","updated":"2025-02-13T06:34:43Z","published":"2024-10-17T06:30:55Z","title":"How Does Knowledge Selection Help Retrieval Augmented Generation?","summary":"  Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection\nremains less clear. In this paper, we perform a comprehensive analysis of how\nknowledge selection influences downstream generation performance in RAG\nsystems. By simulating different retrieval and selection conditions through a\ncontrolled mixture of gold and distractor knowledge, we assess the impact of\nthese factors on generation outcomes. Our findings indicate that the downstream\ngenerator model's capability, as well as the complexity of the task and\ndataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing a limited additional benefit when a strong generator model\nis used on clear, well-defined tasks. For weaker generator models or more\nambiguous tasks and datasets, the knowledge F1 score becomes a critical factor,\nand the knowledge selector plays a more prominent role in improving overall\nperformance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06876v2","updated":"2025-02-13T06:28:33Z","published":"2025-02-08T11:56:58Z","title":"Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and\n  Harmlessness of Large Language Model via Model Merging","summary":"  Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI, with existing methods like data mixture\nstrategies facing limitations including reliance on expert knowledge and\nconflicting optimization signals. While model merging offers a promising\nalternative by integrating specialized models, its potential for 3H\noptimization remains underexplored. This paper establishes the first\ncomprehensive benchmark for model merging in 3H-aligned LLMs, systematically\nevaluating 15 methods (12 training-free merging and 3 data mixture techniques)\nacross 10 datasets associated with 5 annotation dimensions, 2 LLM families, and\n2 training paradigms. Our analysis reveals three pivotal insights: (i)\npreviously overlooked collaborative/conflicting relationships among 3H\ndimensions, (ii) the consistent superiority of model merging over data mixture\napproaches in balancing alignment trade-offs, and (iii) the critical role of\nparameter-level conflict resolution through redundant component pruning and\noutlier mitigation. Building on these findings, we propose R-TSVM, a\nReweighting-enhanced Task Singular Vector Merging method that incorporates\noutlier-aware parameter weighting and sparsity-adaptive rank selection\nstrategies adapted to the heavy-tailed parameter distribution and sparsity for\nLLMs, further improving LLM alignment across multiple evaluations. We release\nour trained models for further exploration.\n","authors":["Jinluan Yang","Dingnan Jin","Anke Tang","Li Shen","Didi Zhu","Zhengyu Chen","Daixin Wang","Qing Cui","Zhiqiang Zhang","Jun Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2502.06876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06147v2","updated":"2025-02-13T05:37:45Z","published":"2025-02-10T04:25:05Z","title":"LegalViz: Legal Text Visualization by Text To Diagram Generation","summary":"  Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.\n","authors":["Eri Onami","Taiki Miyanishi","Koki Maeda","Shuhei Kurita"],"pdf_url":"https://arxiv.org/pdf/2502.06147v2.pdf","comment":"NAACL2025"},{"id":"http://arxiv.org/abs/2501.19093v2","updated":"2025-02-13T05:32:21Z","published":"2025-01-31T12:39:28Z","title":"Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations","summary":"  Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.\n","authors":["Peichao Lai","Jiaxin Gan","Feiyang Ye","Yilei Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2501.19093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08972v1","updated":"2025-02-13T05:20:21Z","published":"2025-02-13T05:20:21Z","title":"Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning","summary":"  Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning (TICL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.\n","authors":["Hyundong Cho","Karishma Sharma","Nicolaas Jedema","Leonardo F. R. Ribeiro","Alessandro Moschitti","Ravi Krishnan","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2502.08972v1.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.02494v2","updated":"2025-02-13T05:14:49Z","published":"2025-02-04T17:09:44Z","title":"Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining","summary":"  Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.\n","authors":["Dylan Sam","Ayan Chakrabarti","Afshin Rostamizadeh","Srikumar Ramalingam","Gui Citovsky","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.02494v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07058v2","updated":"2025-02-13T04:55:27Z","published":"2025-02-10T21:49:35Z","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","summary":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","authors":["Zixin Tang","Chieh-Yang Huang","Tsung-Chi Li","Ho Yin Sam Ng","Hen-Hsen Huang","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07058v2.pdf","comment":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track"},{"id":"http://arxiv.org/abs/2502.08954v1","updated":"2025-02-13T04:35:55Z","published":"2025-02-13T04:35:55Z","title":"Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs\n  for Clinical Reasoning","summary":"  The deployment of Large Language Models (LLM) on mobile devices offers\nsignificant potential for medical applications, enhancing privacy, security,\nand cost-efficiency by eliminating reliance on cloud-based services and keeping\nsensitive health data local. However, the performance and accuracy of on-device\nLLMs in real-world medical contexts remain underexplored. In this study, we\nbenchmark publicly available on-device LLMs using the AMEGA dataset, evaluating\naccuracy, computational efficiency, and thermal limitation across various\nmobile devices. Our results indicate that compact general-purpose models like\nPhi-3 Mini achieve a strong balance between speed and accuracy, while medically\nfine-tuned models such as Med42 and Aloe attain the highest accuracy. Notably,\ndeploying LLMs on older devices remains feasible, with memory constraints\nposing a greater challenge than raw processing power. Our study underscores the\npotential of on-device LLMs for healthcare while emphasizing the need for more\nefficient inference and models tailored to real-world clinical reasoning.\n","authors":["Leon Nissen","Philipp Zagar","Vishnu Ravi","Aydin Zahedivash","Lara Marie Reimer","Stephan Jonas","Oliver Aalami","Paul Schmiedmayer"],"pdf_url":"https://arxiv.org/pdf/2502.08954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08947v1","updated":"2025-02-13T04:01:54Z","published":"2025-02-13T04:01:54Z","title":"Structured Convergence in Large Language Model Representations via\n  Hierarchical Latent Space Folding","summary":"  Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency.\n","authors":["Fenella Harcourt","Naderdel Piero","Gilbert Sutherland","Daphne Holloway","Harriet Bracknell","Julian Ormsby"],"pdf_url":"https://arxiv.org/pdf/2502.08947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.08943v1","updated":"2025-02-13T03:43:33Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v1.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.08924v1","updated":"2025-02-13T03:20:37Z","published":"2025-02-13T03:20:37Z","title":"Escaping Collapse: The Strength of Weak Data for Large Language Model\n  Training","summary":"  Synthetically-generated data plays an increasingly larger role in training\nlarge language models. However, while synthetic data has been found to be\nuseful, studies have also shown that without proper curation it can cause LLM\nperformance to plateau, or even \"collapse\", after many training iterations. In\nthis paper, we formalize this question and develop a theoretical framework to\ninvestigate how much curation is needed in order to ensure that LLM performance\ncontinually improves. We find that the requirements are nearly minimal. We\ndescribe a training procedure that converges to an optimal LLM even if almost\nall of the non-synthetic training data is of poor quality. Our analysis is\ninspired by boosting, a classic machine learning technique that leverages a\nvery weak learning algorithm to produce an arbitrarily good classifier. Our\ntraining procedure subsumes many recently proposed methods for training LLMs on\nsynthetic data, and thus our analysis sheds light on why they are successful,\nand also suggests opportunities for future improvement. We present experiments\nthat validate our theory, and show that dynamically focusing labeling resources\non the most challenging examples -- in much the same way that boosting focuses\nthe efforts of the weak learner -- leads to improved performance.\n","authors":["Kareem Amin","Sara Babakniya","Alex Bie","Weiwei Kong","Umar Syed","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2502.08924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18997v2","updated":"2025-02-13T23:45:10Z","published":"2024-09-19T06:28:18Z","title":"PropaInsight: Toward Deeper Understanding of Propaganda in Terms of\n  Techniques, Appeals, and Intent","summary":"  Propaganda plays a critical role in shaping public opinion and fueling\ndisinformation. While existing research primarily focuses on identifying\npropaganda techniques, it lacks the ability to capture the broader motives and\nthe impacts of such content. To address these challenges, we introduce\npropainsight, a conceptual framework grounded in foundational social science\nresearch, which systematically dissects propaganda into techniques, arousal\nappeals, and underlying intent. propainsight offers a more granular\nunderstanding of how propaganda operates across different contexts.\nAdditionally, we present propagaze, a novel dataset that combines\nhuman-annotated data with high-quality synthetic data generated through a\nmeticulously designed pipeline. Our experiments show that off-the-shelf LLMs\nstruggle with propaganda analysis, but training with propagaze significantly\nimproves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span\nIoU in technique identification and 66.2% higher BertScore in appeal analysis\ncompared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited\nhuman-annotated data in data-sparse and cross-domain scenarios, showing its\npotential for comprehensive and generalizable propaganda analysis.\n","authors":["Jiateng Liu","Lin Ai","Zizhou Liu","Payam Karisani","Zheng Hui","May Fung","Preslav Nakov","Julia Hirschberg","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2409.18997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18649v2","updated":"2025-02-13T23:32:38Z","published":"2024-05-28T23:20:24Z","title":"LeDex: Training LLMs to Better Self-Debug and Explain Code","summary":"  In the domain of code generation, self-debugging is crucial. It allows LLMs\nto refine their generated code based on execution feedback. This is\nparticularly important because generating correct solutions in one attempt\nproves challenging for complex tasks. Prior works on self-debugging mostly\nfocus on prompting methods by providing LLMs with few-shot examples, which work\npoorly on small open-sourced LLMs. In this work, we propose LeDex, a training\nframework that significantly improves the self-debugging capability of LLMs.\nIntuitively, we observe that a chain of explanations on the wrong code followed\nby code refinement helps LLMs better analyze the wrong code and do refinement.\nWe thus propose an automated pipeline to collect a high-quality dataset for\ncode explanation and refinement by generating a number of explanations and\nrefinement trajectories from the LLM itself or a larger teacher model and\nfiltering via execution verification. We perform supervised fine-tuning (SFT)\nand further reinforcement learning (RL) on both success and failure\ntrajectories with a novel reward design considering code explanation and\nrefinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by\n9.30% over four benchmarks. RL training brings additional up to 3.54%\nimprovement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show\niterative refinement ability and can keep refining code continuously. Lastly,\nour human evaluation shows that the LLMs trained with our framework generate\nmore useful code explanations and help developers better understand bugs in\nsource code.\n","authors":["Nan Jiang","Xiaopeng Li","Shiqi Wang","Qiang Zhou","Soneya Binta Hossain","Baishakhi Ray","Varun Kumar","Xiaofei Ma","Anoop Deoras"],"pdf_url":"https://arxiv.org/pdf/2405.18649v2.pdf","comment":"This paper is accepted by The Thirty-eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2502.09815v1","updated":"2025-02-13T23:24:25Z","published":"2025-02-13T23:24:25Z","title":"Statistical Coherence Alignment for Large Language Model Representation\n  Learning Through Tensor Field Convergence","summary":"  Representation learning plays a central role in structuring internal\nembeddings to capture the statistical properties of language, influencing the\ncoherence and contextual consistency of generated text. Statistical Coherence\nAlignment is introduced as a method to enforce structured token representations\nthrough tensor field convergence, guiding embeddings to reflect statistical\ndependencies inherent in linguistic data. A mathematical framework is\nestablished to quantify coherence alignment, integrating a loss function that\noptimizes representational consistency across training iterations. Empirical\nevaluations demonstrate that applying coherence constraints improves\nperplexity, enhances classification accuracy, and refines rare word embeddings,\ncontributing to a more stable representation space. Comparative analyses with\nbaseline models reveal that the proposed method fosters a more interpretable\ninternal structure, ensuring that embeddings retain contextual dependencies\nwhile mitigating representation collapse. The impact on coherence score\ndistributions suggests that the alignment mechanism strengthens semantic\nintegrity across diverse linguistic constructs, leading to a more balanced\norganization of learned embeddings. Computational assessments indicate that\nwhile the method introduces additional memory and training costs, the\nstructured optimization process justifies the trade-offs in applications\nrequiring heightened contextual fidelity. Experimental results validate the\neffectiveness of coherence alignment in optimizing token representations,\nproviding insights into how statistical dependencies can be leveraged to\nimprove language model training.\n","authors":["Jonathan Gale","Godfrey Aldington","Harriet Thistlewood","Thomas Tattershall","Basil Wentworth","Vincent Enoasmo"],"pdf_url":"https://arxiv.org/pdf/2502.09815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09814v1","updated":"2025-02-13T23:17:10Z","published":"2025-02-13T23:17:10Z","title":"INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for\n  16 African Languages","summary":"  Slot-filling and intent detection are well-established tasks in\nConversational AI. However, current large-scale benchmarks for these tasks\noften exclude evaluations of low-resource languages and rely on translations\nfrom English benchmarks, thereby predominantly reflecting Western-centric\nconcepts. In this paper, we introduce Injongo -- a multicultural, open-source\nbenchmark dataset for 16 African languages with utterances generated by native\nspeakers across diverse domains, including banking, travel, home, and dining.\nThrough extensive experiments, we benchmark the fine-tuning multilingual\ntransformer models and the prompting large language models (LLMs), and show the\nadvantage of leveraging African-cultural utterances over Western-centric\nutterances for improving cross-lingual transfer from the English language.\nExperimental results reveal that current LLMs struggle with the slot-filling\ntask, with GPT-4o achieving an average performance of 26 F1-score. In contrast,\nintent detection performance is notably better, with an average accuracy of\n70.6%, though it still falls behind the fine-tuning baselines. Compared to the\nEnglish language, GPT-4o and fine-tuning baselines perform similarly on intent\ndetection, achieving an accuracy of approximately 81%. Our findings suggest\nthat the performance of LLMs is still behind for many low-resource African\nlanguages, and more work is needed to further improve their downstream\nperformance.\n","authors":["Hao Yu","Jesujoba O. Alabi","Andiswa Bukula","Jian Yun Zhuang","En-Shiun Annie Lee","Tadesse Kebede Guge","Israel Abebe Azime","Happy Buzaaba","Blessing Kudzaishe Sibanda","Godson K. Kalipe","Jonathan Mukiibi","Salomon Kabongo Kabenamualu","Mmasibidi Setaka","Lolwethu Ndolela","Nkiruka Odu","Rooweither Mabuya","Shamsuddeen Hassan Muhammad","Salomey Osei","Sokhar Samb","Juliet W. Murage","Dietrich Klakow","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2502.09814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04237v2","updated":"2025-02-13T22:13:21Z","published":"2024-04-05T17:36:26Z","title":"GroundCocoa: A Benchmark for Evaluating Compositional & Conditional\n  Reasoning in Language Models","summary":"  The rapid progress of large language models (LLMs) has seen them excel and\nfrequently surpass human performance on standard benchmarks. This has enabled\nmany downstream applications, such as LLM agents, to rely on their reasoning to\naddress complex task requirements. However, LLMs are known to unexpectedly\nfalter in simple tasks and under seemingly straightforward circumstances -\nunderscoring the need for better and more diverse evaluation setups to measure\ntheir true capabilities. To this end, we choose to study compositional and\nconditional reasoning, two aspects that are central to human cognition, and\nintroduce GroundCocoa - a lexically diverse benchmark connecting these\nreasoning skills to the real-world problem of flight booking. Our task involves\naligning detailed user preferences with available flight options presented in a\nmultiple-choice format. Results indicate a significant disparity in performance\namong current state-of-the-art LLMs with even the best performing model, GPT-4\nTurbo, not exceeding 67% accuracy despite advanced prompting techniques.\n","authors":["Harsh Kohli","Sachin Kumar","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2404.04237v2.pdf","comment":"16 pages, 17 figures, 3 tables. Accepted to NAACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2410.21236v2","updated":"2025-02-13T21:50:10Z","published":"2024-10-28T17:30:01Z","title":"Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models","summary":"  Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.\n","authors":["Weizhe Chen","Zhicheng Zhang","Guanlin Liu","Renjie Zheng","Wenlei Shi","Chen Dun","Zheng Wu","Xing Jin","Lin Yan"],"pdf_url":"https://arxiv.org/pdf/2410.21236v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11132v2","updated":"2025-02-13T21:38:42Z","published":"2024-06-17T01:23:11Z","title":"RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents","summary":"  In the past year, large language models (LLMs) have had remarkable success in\ndomains outside the traditional natural language processing, and their capacity\nis further expanded into the so-called LLM agents when connected with external\ntools. In all domains, the prompt to the LLMs has been shown to make a big\ndifference in what the LLM would generate and thus affect the performance of\nthe LLM agents. Therefore, automatic prompt engineering (APE) has become an\nimportant question for many researchers and users of LLMs. However, previous\nworks in APE rely on a final checker to evaluate the performance of the given\nprompt -- a requirement that is hard to meet in the case of LLM agents, where\nintermediate feedback is easier to obtain, and the final evaluation could be\nexpensive, inaccurate, or even missing. In this paper, we propose a novel\nmethod, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to\noptimize the step-by-step instructions in the prompts given to LLM agents,\nbased on the chat history obtained from interactions and reflections with LLM\nagents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the\nprompt without the need for a final solution checker. We evaluate our approach\non PDDL generation, TravelPlanner, and Meeting Planning to show that our method\ncould generally improve performance for different reasoning tasks.\n","authors":["Weizhe Chen","Sven Koenig","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2406.11132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v1","updated":"2025-02-13T21:33:57Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09778v1","updated":"2025-02-13T21:23:16Z","published":"2025-02-13T21:23:16Z","title":"Prompt and circumstance: A word-by-word LLM prompting approach to\n  interlinear glossing for low-resource languages","summary":"  Partly automated creation of interlinear glossed text (IGT) has the potential\nto assist in linguistic documentation. We argue that LLMs can make this process\nmore accessible to linguists because of their capacity to follow\nnatural-language instructions. We investigate the effectiveness of a\nretrieval-based LLM prompting approach to glossing, applied to the seven\nlanguages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based\nshared task baseline for every language in the morpheme-level score category,\nand we show that a simple 3-best oracle has higher word-level scores than the\nchallenge winner (a tuned sequence model) in five languages. In a case study on\nTsez, we ask the LLM to automatically create and follow linguistic\ninstructions, reducing errors on a confusing grammatical feature. Our results\nthus demonstrate the potential contributions which LLMs can make in interactive\nsystems for glossing, both in making suggestions to human annotators and\nfollowing directions.\n","authors":["Micha Elsner","David Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15124v4","updated":"2025-02-13T21:18:55Z","published":"2024-11-22T18:44:04Z","title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","summary":"  Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce Tulu 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. Tulu 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the Tulu 3\napproach to more domains.\n","authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.15124v4.pdf","comment":"Added Tulu 3 405B results and additional analyses"},{"id":"http://arxiv.org/abs/2502.09767v1","updated":"2025-02-13T20:51:25Z","published":"2025-02-13T20:51:25Z","title":"Non-Markovian Discrete Diffusion with Causal Language Models","summary":"  Discrete diffusion models have emerged as a flexible and controllable\nparadigm for structured sequence modeling, yet they still lag behind causal\nlanguage models in expressiveness. To bridge the gap between two paradigms, we\nintroduce CaDDi, a causal discrete diffusion model that unifies sequential and\ntemporal modeling within a non-Markovian diffusion framework. Unlike\nconventional diffusion models that operate step by step with no access to prior\nstates, CaDDi integrates the temporal trajectory, enabling more expressive and\ncontrollable generation. Our approach also treats causal language models as a\nspecial case, allowing seamless adoption of pretrained large language models\n(LLMs) for discrete diffusion without the need for architectural modifications.\nEmpirically, we demonstrate that CaDDi outperforms state-of-the-art discrete\ndiffusion models on both natural language and biological sequence tasks,\nnarrowing the gap between diffusion-based methods and large-scale\nautoregressive transformers.\n","authors":["Yangtian Zhang","Sizhuang He","Daniel Levine","Lawrence Zhao","David Zhang","Syed A Rizvi","Emanuele Zappala","Rex Ying","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.09767v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.12543v2","updated":"2025-02-13T20:47:20Z","published":"2024-07-17T13:27:26Z","title":"Abstraction Alignment: Comparing Model-Learned and Human-Encoded\n  Conceptual Relationships","summary":"  While interpretability methods identify a model's learned concepts, they\noverlook the relationships between concepts that make up its abstractions and\ninform its ability to generalize to new data. To assess whether models' have\nlearned human-aligned abstractions, we introduce abstraction alignment, a\nmethodology to compare model behavior against formal human knowledge.\nAbstraction alignment externalizes domain-specific human knowledge as an\nabstraction graph, a set of pertinent concepts spanning levels of abstraction.\nUsing the abstraction graph as a ground truth, abstraction alignment measures\nthe alignment of a model's behavior by determining how much of its uncertainty\nis accounted for by the human abstractions. By aggregating abstraction\nalignment across entire datasets, users can test alignment hypotheses, such as\nwhich human concepts the model has learned and where misalignments recur. In\nevaluations with experts, abstraction alignment differentiates seemingly\nsimilar errors, improves the verbosity of existing model-quality metrics, and\nuncovers improvements to current human abstractions.\n","authors":["Angie Boggust","Hyemin Bang","Hendrik Strobelt","Arvind Satyanarayan"],"pdf_url":"https://arxiv.org/pdf/2407.12543v2.pdf","comment":"20 pages, 7 figures, published in CHI 2025"},{"id":"http://arxiv.org/abs/2502.06648v2","updated":"2025-02-13T20:46:57Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladeni","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16235v2","updated":"2025-02-13T20:14:27Z","published":"2025-01-27T17:33:38Z","title":"Echoes of Discord: Forecasting Hater Reactions to Counterspeech","summary":"  Hate speech (HS) erodes the inclusiveness of online users and propagates\nnegativity and division. Counterspeech has been recognized as a way to mitigate\nthe harmful consequences. While some research has investigated the impact of\nuser-generated counterspeech on social media platforms, few have examined and\nmodeled haters' reactions toward counterspeech, despite the immediate\nalteration of haters' attitudes being an important aspect of counterspeech.\nThis study fills the gap by analyzing the impact of counterspeech from the\nhater's perspective, focusing on whether the counterspeech leads the hater to\nreenter the conversation and if the reentry is hateful. We compile the Reddit\nEchoes of Hate dataset (ReEco), which consists of triple-turn conversations\nfeaturing haters' reactions, to assess the impact of counterspeech. To predict\nhaters' behaviors, we employ two strategies: a two-stage reaction predictor and\na three-way classifier. The linguistic analysis sheds insights on the language\nof counterspeech to hate eliciting different haters' reactions. Experimental\nresults demonstrate that the 3-way classification model outperforms the\ntwo-stage reaction predictor, which first predicts reentry and then determines\nthe reentry type. We conclude the study with an assessment showing the most\ncommon errors identified by the best-performing model.\n","authors":["Xiaoying Song","Sharon Lisseth Perez","Xinchen Yu","Eduardo Blanco","Lingzi Hong"],"pdf_url":"https://arxiv.org/pdf/2501.16235v2.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2407.04841v2","updated":"2025-02-13T20:10:59Z","published":"2024-07-05T19:57:49Z","title":"Associative Recurrent Memory Transformer","summary":"  This paper addresses the challenge of creating a neural architecture for very\nlong sequences that requires constant time for processing new information at\neach time step. Our approach, Associative Recurrent Memory Transformer (ARMT),\nis based on transformer self-attention for local context and segment-level\nrecurrence for storage of task specific information distributed over a long\ncontext. We demonstrate that ARMT outperfors existing alternatives in\nassociative retrieval tasks and sets a new performance record in the recent\nBABILong multi-task long-context benchmark by answering single-fact questions\nover 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.\n","authors":["Ivan Rodkin","Yuri Kuratov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2407.04841v2.pdf","comment":"ICML 2024 Next Generation of Sequence Modeling Architectures Workshop"},{"id":"http://arxiv.org/abs/2502.09747v1","updated":"2025-02-13T20:07:03Z","published":"2025-02-13T20:07:03Z","title":"The Widespread Adoption of Large Language Model-Assisted Writing Across\n  Society","summary":"  The recent advances in large language models (LLMs) attracted significant\npublic and policymaker interest in its adoption patterns. In this paper, we\nsystematically analyze LLM-assisted writing across four domains-consumer\ncomplaints, corporate communications, job postings, and international\norganization press releases-from January 2022 to September 2024. Our dataset\nincludes 687,241 consumer complaints, 537,413 corporate press releases, 304.3\nmillion job postings, and 15,919 United Nations (UN) press releases. Using a\nrobust population-level statistical framework, we find that LLM usage surged\nfollowing the release of ChatGPT in November 2022. By late 2024, roughly 18% of\nfinancial consumer complaint text appears to be LLM-assisted, with adoption\npatterns spread broadly across regions and slightly higher in urban areas. For\ncorporate press releases, up to 24% of the text is attributable to LLMs. In job\npostings, LLM-assisted writing accounts for just below 10% in small firms, and\nis even more common among younger firms. UN press releases also reflect this\ntrend, with nearly 14% of content being generated or modified by LLMs. Although\nadoption climbed rapidly post-ChatGPT, growth appears to have stabilized by\n2024, reflecting either saturation in LLM adoption or increasing subtlety of\nmore advanced models. Our study shows the emergence of a new reality in which\nfirms, consumers and even international organizations substantially rely on\ngenerative AI for communications.\n","authors":["Weixin Liang","Yaohui Zhang","Mihai Codreanu","Jiayu Wang","Hancheng Cao","James Zou"],"pdf_url":"https://arxiv.org/pdf/2502.09747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09615v6","updated":"2025-02-13T20:05:54Z","published":"2024-02-14T23:09:15Z","title":"API Pack: A Massive Multi-Programming Language Dataset for API Call\n  Generation","summary":"  We introduce API Pack, a massive multi-programming language dataset\ncontaining over one million instruction-API calls for improving the API call\ngeneration capabilities of large language models. Our evaluation highlights\nthree key findings: First, fine-tuning on API Pack enables open-source models\nto outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.\nWe show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API\nPack. Second, fine-tuning on a large dataset in one language, combined with\nsmaller datasets from others, improves API generation accuracy across multiple\nlanguages. Third, we confirm the benefits of larger datasets for API\ngeneralization, as increasing fine-tuning data to one million instances\nenhances generalization to new APIs. To support further research, we\nopen-source the API Pack dataset, trained model, and code at\nhttps://github.com/zguo0525/API-Pack.\n","authors":["Zhen Guo","Adriana Meza Soria","Wei Sun","Yikang Shen","Rameswar Panda"],"pdf_url":"https://arxiv.org/pdf/2402.09615v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15661v2","updated":"2025-02-13T19:59:25Z","published":"2024-11-23T22:09:58Z","title":"Improving Next Tokens via Second-to-Last Predictions with Generate and\n  Refine","summary":"  Autoregressive language models like GPT aim to predict next tokens, while\nautoencoding models such as BERT are trained on tasks such as predicting masked\ntokens. We train a decoder-only architecture for predicting the second to last\ntoken for a sequence of tokens. Our approach yields higher computational\ntraining efficiency than BERT-style models by employing a structured\ndeterministic approach to masking tokens. We use our model to improve the next\ntoken predictions of a standard GPT by combining both predictions in a\n``generate-then-refine'' approach. We demonstrate on different variants of\nGPT-2 and different datasets that (not unexpectedly) second to last token\npredictions are much more accurate, i.e., more than 15\\% higher accuracy than\nstandard next token predictions. The ``generate-then-refine'' approach also\ndemonstrates notable improvements in next-token predictions, yielding smaller\nyet consistent and significant gains.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2411.15661v2.pdf","comment":"Accepted at Intelligent Data Analysis (IDA), 2025, held in Konstanz,\n  Germany"},{"id":"http://arxiv.org/abs/2502.09743v1","updated":"2025-02-13T19:58:00Z","published":"2025-02-13T19:58:00Z","title":"Partial Colexifications Improve Concept Embeddings","summary":"  While the embedding of words has revolutionized the field of Natural Language\nProcessing, the embedding of concepts has received much less attention so far.\nA dense and meaningful representation of concepts, however, could prove useful\nfor several tasks in computational linguistics, especially those involving\ncross-linguistic data or sparse data from low resource languages. First methods\nthat have been proposed so far embed concepts from automatically constructed\ncolexification networks. While these approaches depart from automatically\ninferred polysemies, attested across a larger number of languages, they are\nrestricted to the word level, ignoring lexical relations that would only hold\nfor parts of the words in a given language. Building on recently introduced\nmethods for the inference of partial colexifications, we show how they can be\nused to improve concept embeddings in meaningful ways. The learned embeddings\nare evaluated against lexical similarity ratings, recorded instances of\nsemantic shift, and word association data. We show that in all evaluation\ntasks, the inclusion of partial colexifications lead to improved concept\nrepresentations and better results. Our results further show that the learned\nembeddings are able to capture and represent different semantic relationships\nbetween concepts.\n","authors":["Arne Rubehn","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2502.09743v1.pdf","comment":"Submitted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics, Vienna, Austria"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09609v1","updated":"2025-02-13T18:57:20Z","published":"2025-02-13T18:57:20Z","title":"Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple","summary":"  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n","authors":["Tejas Jayashankar","J. Jon Ryu","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2502.09609v1.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09601v1","updated":"2025-02-13T18:52:36Z","published":"2025-02-13T18:52:36Z","title":"CoT-Valve: Length-Compressible Chain-of-Thought Tuning","summary":"  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n","authors":["Xinyin Ma","Guangnian Wan","Runpeng Yu","Gongfan Fang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09601v1.pdf","comment":"Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve"},{"id":"http://arxiv.org/abs/2502.09596v1","updated":"2025-02-13T18:51:12Z","published":"2025-02-13T18:51:12Z","title":"KIMAs: A Configurable Knowledge Integrated Multi-Agent System","summary":"  Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance.\n","authors":["Zitao Li","Fei Wei","Yuexiang Xie","Dawei Gao","Weirui Kuang","Zhijian Ma","Bingchen Qian","Yaliang Li","Bolin Ding"],"pdf_url":"https://arxiv.org/pdf/2502.09596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20092v2","updated":"2025-02-13T18:38:13Z","published":"2024-10-26T06:06:08Z","title":"OGBench: Benchmarking Offline Goal-Conditioned RL","summary":"  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n","authors":["Seohong Park","Kevin Frans","Benjamin Eysenbach","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.20092v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09567v1","updated":"2025-02-13T18:22:31Z","published":"2025-02-13T18:22:31Z","title":"MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing","summary":"  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n","authors":["Vlad Andrei Negru","Robert Vacareanu","Camelia Lemnaru","Mihai Surdeanu","Rodica Potolea"],"pdf_url":"https://arxiv.org/pdf/2502.09567v1.pdf","comment":"16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2411.09101v2","updated":"2025-02-13T18:20:14Z","published":"2024-11-14T00:18:04Z","title":"Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery","summary":"  Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.\n","authors":["Ashim Dahal","Saydul Akbar Murad","Nick Rahimi"],"pdf_url":"https://arxiv.org/pdf/2411.09101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09565v1","updated":"2025-02-13T18:19:20Z","published":"2025-02-13T18:19:20Z","title":"MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models","summary":"  Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.\n","authors":["Quintina Campbell","Sam Cox","Jorge Medina","Brittany Watterson","Andrew D. White"],"pdf_url":"https://arxiv.org/pdf/2502.09565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2502.07864v2","updated":"2025-02-13T18:07:04Z","published":"2025-02-11T18:20:18Z","title":"TransMLA: Multi-Head Latent Attention Is All You Need","summary":"  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n","authors":["Fanxu Meng","Zengwei Yao","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07864v2.pdf","comment":"https://github.com/fxmeng/TransMLA"},{"id":"http://arxiv.org/abs/2406.05925v2","updated":"2025-02-13T18:02:34Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.04103v2","updated":"2025-02-13T17:57:44Z","published":"2025-02-06T14:27:54Z","title":"VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output","summary":"  The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.\n","authors":["Eason Chen","Chenyu Lin","Xinyi Tang","Aprille Xi","Canwen Wang","Jionghao Lin","Kenneth R Koedinger"],"pdf_url":"https://arxiv.org/pdf/2502.04103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v3","updated":"2025-02-13T17:57:28Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel Pduraru","Antonio Brblau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v3.pdf","comment":"8 pages, 4 figures, 6 tables, under review"},{"id":"http://arxiv.org/abs/2406.06773v2","updated":"2025-02-13T17:50:39Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang","Kai Li"],"pdf_url":"https://arxiv.org/pdf/2406.06773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09532v1","updated":"2025-02-13T17:49:30Z","published":"2025-02-13T17:49:30Z","title":"Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages","summary":"  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n","authors":["Shreyan Biswas","Alexander Erlei","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2502.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09511v1","updated":"2025-02-13T17:22:50Z","published":"2025-02-13T17:22:50Z","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","summary":"  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n","authors":["Liang Wang","Chao Song","Zhiyuan Liu","Yu Rong","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14792v2","updated":"2025-02-13T17:22:36Z","published":"2024-08-27T05:56:04Z","title":"Measuring Human Contribution in AI-Assisted Content Generation","summary":"  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n","authors":["Yueqi Xie","Tao Qi","Jingwei Yi","Xiyuan Yang","Ryan Whalen","Junming Huang","Qian Ding","Yu Xie","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09503v1","updated":"2025-02-13T17:15:26Z","published":"2025-02-13T17:15:26Z","title":"AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization","summary":"  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n","authors":["Caleb Cranney","Jesse G. Meyer"],"pdf_url":"https://arxiv.org/pdf/2502.09503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v2","updated":"2025-02-13T17:12:34Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09497v1","updated":"2025-02-13T17:09:52Z","published":"2025-02-13T17:09:52Z","title":"Improve LLM-based Automatic Essay Scoring with Linguistic Features","summary":"  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n","authors":["Zhaoyi Joey Hou","Alejandro Ciuba","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2502.09497v1.pdf","comment":"To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.14346v2","updated":"2025-02-13T17:03:04Z","published":"2025-01-24T09:17:57Z","title":"HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks","summary":"  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n","authors":["Boshko Koloski","Nada Lavra","Bla krlj"],"pdf_url":"https://arxiv.org/pdf/2501.14346v2.pdf","comment":"Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution"},{"id":"http://arxiv.org/abs/2502.09495v1","updated":"2025-02-13T17:01:45Z","published":"2025-02-13T17:01:45Z","title":"Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence","summary":"  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n","authors":["Pierre Beaucoral"],"pdf_url":"https://arxiv.org/pdf/2502.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2502.09484v1","updated":"2025-02-13T16:46:23Z","published":"2025-02-13T16:46:23Z","title":"PenTest++: Elevating Ethical Hacking with AI and Automation","summary":"  Traditional ethical hacking relies on skilled professionals and\ntime-intensive command management, which limits its scalability and efficiency.\nTo address these challenges, we introduce PenTest++, an AI-augmented system\nthat integrates automation with generative AI (GenAI) to optimise ethical\nhacking workflows. Developed in a controlled virtual environment, PenTest++\nstreamlines critical penetration testing tasks, including reconnaissance,\nscanning, enumeration, exploitation, and documentation, while maintaining a\nmodular and adaptable design. The system balances automation with human\noversight, ensuring informed decision-making at key stages, and offers\nsignificant benefits such as enhanced efficiency, scalability, and\nadaptability. However, it also raises ethical considerations, including privacy\nconcerns and the risks of AI-generated inaccuracies (hallucinations). This\nresearch underscores the potential of AI-driven systems like PenTest++ to\ncomplement human expertise in cybersecurity by automating routine tasks,\nenabling professionals to focus on strategic decision-making. By incorporating\nrobust ethical safeguards and promoting ongoing refinement, PenTest++\ndemonstrates how AI can be responsibly harnessed to address operational and\nethical challenges in the evolving cybersecurity landscape.\n","authors":["Haitham S. Al-Sinani","Chris J. Mitchell"],"pdf_url":"https://arxiv.org/pdf/2502.09484v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09471v1","updated":"2025-02-13T16:34:59Z","published":"2025-02-13T16:34:59Z","title":"Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for\n  Weakly-supervised Oriented Object Detection","summary":"  Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).\n","authors":["Yi Yu","Xue Yang","Yansheng Li","Zhenjun Han","Feipeng Da","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2502.09471v1.pdf","comment":"18 pages, 9 figures, 9 tables, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2501.14679v3","updated":"2025-02-13T16:29:16Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09460v1","updated":"2025-02-13T16:27:23Z","published":"2025-02-13T16:27:23Z","title":"Metamorphic Testing for Pose Estimation Systems","summary":"  Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.\n","authors":["Matias Duran","Thomas Laurent","Ellen Rushe","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2502.09460v1.pdf","comment":"Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)"},{"id":"http://arxiv.org/abs/2502.09443v1","updated":"2025-02-13T16:12:17Z","published":"2025-02-13T16:12:17Z","title":"Relational Conformal Prediction for Correlated Time Series","summary":"  We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.\n","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2502.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2502.09436v1","updated":"2025-02-13T16:00:46Z","published":"2025-02-13T16:00:46Z","title":"Variable Stiffness for Robust Locomotion through Reinforcement Learning","summary":"  Reinforcement-learned locomotion enables legged robots to perform highly\ndynamic motions but often accompanies time-consuming manual tuning of joint\nstiffness. This paper introduces a novel control paradigm that integrates\nvariable stiffness into the action space alongside joint positions, enabling\ngrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness\n(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness\npolicies, with grouping in per-leg stiffness (PLS), outperform position-based\ncontrol in velocity tracking and push recovery. In contrast, HJLS excels in\nenergy efficiency. Furthermore, our method showcases robust walking behaviour\non diverse outdoor terrains by sim-to-real transfer, although the policy is\nsorely trained on a flat floor. Our approach simplifies design by eliminating\nper-joint stiffness tuning while keeping competitive results with various\nmetrics.\n","authors":["Dario Spoljaric","Yashuai Yan","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2502.09436v1.pdf","comment":"submitted to IFAC Joint Symposia on Mechatronics & Robotics"},{"id":"http://arxiv.org/abs/2502.09432v1","updated":"2025-02-13T15:55:00Z","published":"2025-02-13T15:55:00Z","title":"Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes","summary":"  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n","authors":["Navdeep Kumar","Adarsh Gupta","Maxence Mohamed Elfatihi","Giorgia Ramponi","Kfir Yehuda Levy","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09423v1","updated":"2025-02-13T15:45:36Z","published":"2025-02-13T15:45:36Z","title":"Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction","summary":"  Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.\n","authors":["Ziyi Chen","Yang Yuan","Siming Zheng","Jialong Guo","Sihan Liang","Yangang Wang","Zongguo Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01706v2","updated":"2025-02-13T15:43:25Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09417v1","updated":"2025-02-13T15:40:39Z","published":"2025-02-13T15:40:39Z","title":"A Survey of Reinforcement Learning for Optimization in Automation","summary":"  Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.\n","authors":["Ahmad Farooq","Kamran Iqbal"],"pdf_url":"https://arxiv.org/pdf/2502.09417v1.pdf","comment":"8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024"},{"id":"http://arxiv.org/abs/2206.12934v4","updated":"2025-02-13T15:36:31Z","published":"2022-06-26T17:55:32Z","title":"Checking Trustworthiness of Probabilistic Computations in a Typed\n  Natural Deduction System","summary":"  In this paper we present the probabilistic typed natural deduction calculus\nTPTND, designed to reason about and derive trustworthiness properties of\nprobabilistic computational processes, like those underlying current AI\napplications. Derivability in TPTND is interpreted as the process of extracting\n$n$ samples of possibly complex outputs with a certain frequency from a given\ncategorical distribution. We formalize trust for such outputs as a form of\nhypothesis testing on the distance between such frequency and the intended\nprobability. The main advantage of the calculus is to render such notion of\ntrustworthiness checkable. We present a computational semantics for the terms\nover which we reason and then the semantics of TPTND, where logical operators\nas well as a Trust operator are defined through introduction and elimination\nrules. We illustrate structural and metatheoretical properties, with particular\nfocus on the ability to establish under which term evolutions and logical rules\napplications the notion of trustworhtiness can be preserved.\n","authors":["Fabio Aurelio D'Asaro","Francesco Genco","Giuseppe Primiero"],"pdf_url":"https://arxiv.org/pdf/2206.12934v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2310.19347v4","updated":"2025-02-13T15:25:02Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization","summary":"  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09389v1","updated":"2025-02-13T15:06:42Z","published":"2025-02-13T15:06:42Z","title":"S$^2$-Diffusion: Generalizing from Instance-level to Category-level\n  Skills in Robot Manipulation","summary":"  Recent advances in skill learning has propelled robot manipulation to new\nheights by enabling it to learn complex manipulation tasks from a practical\nnumber of demonstrations. However, these skills are often limited to the\nparticular action, object, and environment \\textit{instances} that are shown in\nthe training data, and have trouble transferring to other instances of the same\ncategory. In this work we present an open-vocabulary Spatial-Semantic Diffusion\npolicy (S$^2$-Diffusion) which enables generalization from instance-level\ntraining data to category-level, enabling skills to be transferable between\ninstances of the same category. We show that functional aspects of skills can\nbe captured via a promptable semantic module combined with a spatial\nrepresentation. We further propose leveraging depth estimation networks to\nallow the use of only a single RGB camera. Our approach is evaluated and\ncompared on a diverse number of robot manipulation tasks, both in simulation\nand in the real world. Our results show that S$^2$-Diffusion is invariant to\nchanges in category-irrelevant factors as well as enables satisfying\nperformance on other instances within the same category, even if it was not\ntrained on that specific instance. Full videos of all real-world experiments\nare available in the supplementary material.\n","authors":["Quantao Yang","Michael C. Welle","Danica Kragic","Olov Andersson"],"pdf_url":"https://arxiv.org/pdf/2502.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09387v1","updated":"2025-02-13T15:04:53Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.06607v2","updated":"2025-02-13T14:57:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime currently represents the third largest criminal activity\nworldwide while threatening ecosystems as well as human health. Among the\ncrimes related to this activity, improper waste management can nowadays be\ncountered more easily thanks to the increasing availability and decreasing cost\nof Very-High-Resolution Remote Sensing images, which enable semi-automatic\nterritory scanning in search of illegal landfills. This paper proposes a\npipeline, developed in collaboration with professionals from a local\nenvironmental agency, for detecting candidate illegal dumping sites leveraging\na classifier of Remote Sensing images. To identify the best configuration for\nsuch classifier, an extensive set of experiments was conducted and the impact\nof diverse image characteristics and training settings was thoroughly analyzed.\nThe local environmental agency was then involved in an experimental exercise\nwhere outputs from the developed classifier were integrated in the experts'\neveryday work, resulting in time savings with respect to manual\nphoto-interpretation. The classifier was eventually run with valuable results\non a location outside of the training area, highlighting potential for\ncross-border applicability of the proposed pipeline.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15927v2","updated":"2025-02-13T14:55:26Z","published":"2024-11-24T17:32:20Z","title":"Generative Prompt Internalization","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v2.pdf","comment":"NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2410.14682v2","updated":"2025-02-13T14:54:31Z","published":"2024-10-02T19:56:38Z","title":"ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards\n  Spatial-Temporal Cognition with Foundation Models","summary":"  Recent advancements in Large Language Models (LLMs) have spurred numerous\nattempts to apply these technologies to embodied tasks, particularly focusing\non high-level task planning and task decomposition. To further explore this\narea, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which\nspecifically targets embodied task planning using LLMs. It features a\ncontrollable and diverse set of embodied tasks varying in different levels of\ndifficulties and complexities, and is designed to evaluate two critical\ndimensions of LLMs' application in embodied task understanding: spatial\n(relation constraint, occlusion for target objects) and temporal & causal\nunderstanding of the sequence of actions in the environment. By using\nmulti-source simulators as the backend simulator, it can provide immediate\nenvironment feedback to LLMs, which enables LLMs to interact dynamically with\nthe environment and re-plan as necessary. We evaluated the state-of-the-art\nopen source and closed source foundation models, including GPT-4, LLAMA and\nMistral on our proposed benchmark. While they perform adequately well on simple\nnavigation tasks, their performance can significantly deteriorate when faced\nwith tasks that require a deeper understanding of spatial, temporal, and causal\nrelationships. Thus, our benchmark distinguishes itself as a large-scale,\nquantifiable, highly automated, and fine-grained diagnostic framework that\npresents a significant challenge to the latest foundation models. We hope it\ncan spark and drive further research in embodied task planning using foundation\nmodels.\n","authors":["Lingfeng Zhang","Yuening Wang","Hongjian Gu","Atia Hamidizadeh","Zhanguang Zhang","Yuecheng Liu","Yutong Wang","David Gamaliel Arcos Bravo","Junyi Dong","Shunbo Zhou","Tongtong Cao","Xingyue Quan","Yuzheng Zhuang","Yingxue Zhang","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2410.14682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09379v1","updated":"2025-02-13T14:46:40Z","published":"2025-02-13T14:46:40Z","title":"TRIFFID: Autonomous Robotic Aid For Increasing First Responders\n  Efficiency","summary":"  The increasing complexity of natural disaster incidents demands innovative\ntechnological solutions to support first responders in their efforts. This\npaper introduces the TRIFFID system, a comprehensive technical framework that\nintegrates unmanned ground and aerial vehicles with advanced artificial\nintelligence functionalities to enhance disaster response capabilities across\nwildfires, urban floods, and post-earthquake search and rescue missions. By\nleveraging state-of-the-art autonomous navigation, semantic perception, and\nhuman-robot interaction technologies, TRIFFID provides a sophisticated system\ncom- posed of the following key components: hybrid robotic platform,\ncentralized ground station, custom communication infrastructure, and smartphone\napplication. The defined research and development activities demonstrate how\ndeep neural networks, knowledge graphs, and multimodal information fusion can\nenable robots to autonomously navigate and analyze disaster environ- ments,\nreducing personnel risks and accelerating response times. The proposed system\nenhances emergency response teams by providing advanced mission planning,\nsafety monitoring, and adaptive task execution capabilities. Moreover, it\nensures real- time situational awareness and operational support in complex and\nrisky situations, facilitating rapid and precise information collection and\ncoordinated actions.\n","authors":["Jorgen Cani","Panagiotis Koletsis","Konstantinos Foteinos","Ioannis Kefaloukos","Lampros Argyriou","Manolis Falelakis","Ivn Del Pino","Angel Santamaria-Navarro","Martin ech","Ondej Severa","Alessandro Umbrico","Francesca Fracasso","AndreA Orlandini","Dimitrios Drakoulis","Evangelos Markakis","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.09379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09378v1","updated":"2025-02-13T14:46:04Z","published":"2025-02-13T14:46:04Z","title":"A Deep Inverse-Mapping Model for a Flapping Robotic Wing","summary":"  In systems control, the dynamics of a system are governed by modulating its\ninputs to achieve a desired outcome. For example, to control the thrust of a\nquad-copter propeller the controller modulates its rotation rate, relying on a\nstraightforward mapping between the input rotation rate and the resulting\nthrust. This mapping can be inverted to determine the rotation rate needed to\ngenerate a desired thrust. However, in complex systems, such as flapping-wing\nrobots where intricate fluid motions are involved, mapping inputs (wing\nkinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this\nmapping for real-time control is computationally impractical. Here, we report a\nmachine-learning solution for the inverse mapping of a flapping-wing system\nbased on data from an experimental system we have developed. Our model learns\nthe input wing motion required to generate a desired aerodynamic force outcome.\nWe used a sequence-to-sequence model tailored for time-series data and\naugmented it with a novel adaptive-spectrum layer that implements\nrepresentation learning in the frequency domain. To train our model, we\ndeveloped a flapping wing system that simultaneously measures the wing's\naerodynamic force and its 3D motion using high-speed cameras. We demonstrate\nthe performance of our system on an additional open-source dataset of a\nflapping wing in a different flow regime. Results show superior performance\ncompared with more complex state-of-the-art transformer-based models, with 11%\nimprovement on the test datasets median loss. Moreover, our model shows\nsuperior inference time, making it practical for onboard robotic control. Our\nopen-source data and framework may improve modeling and real-time control of\nsystems governed by complex dynamics, from biomimetic robots to biomedical\ndevices.\n","authors":["Hadar Sharvit","Raz Karl","Tsevi Beatus"],"pdf_url":"https://arxiv.org/pdf/2502.09378v1.pdf","comment":"Accepted to ICLR 2025. 10 Pages 5 figures + 2 figures in appendix"},{"id":"http://arxiv.org/abs/2502.07465v2","updated":"2025-02-13T14:38:24Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v2.pdf","comment":"The paper was submitted without the consent of all co-authors. The\n  content of the paper is incomplete and requires substantial additional work\n  before it can be considered a complete and coherent submission"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Kster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08046v2","updated":"2025-02-13T14:34:52Z","published":"2025-01-14T11:53:10Z","title":"Building Symbiotic AI: Reviewing the AI Act for a Human-Centred,\n  Principle-Based Framework","summary":"  Artificial Intelligence (AI) spreads quickly as new technologies and services\ntake over modern society. The need to regulate AI design, development, and use\nis strictly necessary to avoid unethical and potentially dangerous consequences\nto humans. The European Union (EU) has released a new legal framework, the AI\nAct, to regulate AI by undertaking a risk-based approach to safeguard humans\nduring interaction. At the same time, researchers offer a new perspective on AI\nsystems, commonly known as Human-Centred AI (HCAI), highlighting the need for a\nhuman-centred approach to their design. In this context, Symbiotic AI (a\nsubtype of HCAI) promises to enhance human capabilities through a deeper and\ncontinuous collaboration between human intelligence and AI. This article\npresents the results of a Systematic Literature Review (SLR) that aims to\nidentify principles that characterise the design and development of Symbiotic\nAI systems while considering humans as the core of the process. Through content\nanalysis, four principles emerged from the review that must be applied to\ncreate Human-Centred AI systems that can establish a symbiotic relationship\nwith humans. In addition, current trends and challenges were defined to\nindicate open questions that may guide future research for the development of\nSAI systems that comply with the AI Act.\n","authors":["Miriana Calvano","Antonio Curci","Giuseppe Desolda","Andrea Esposito","Rosa Lanzilotti","Antonio Piccinno"],"pdf_url":"https://arxiv.org/pdf/2501.08046v2.pdf","comment":"Second version: 34 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.09365v1","updated":"2025-02-13T14:33:02Z","published":"2025-02-13T14:33:02Z","title":"Simple Path Structural Encoding for Graph Transformers","summary":"  Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.\n","authors":["Louis Airale","Antonio Longa","Mattia Rigon","Andrea Passerini","Roberto Passerone"],"pdf_url":"https://arxiv.org/pdf/2502.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v3","updated":"2025-02-13T14:13:41Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v3.pdf","comment":"LangFair repository: https://github.com/cvs-health/langfair"},{"id":"http://arxiv.org/abs/2502.09341v1","updated":"2025-02-13T14:01:15Z","published":"2025-02-13T14:01:15Z","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","summary":"  Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.\n","authors":["Sumantrak Mukherjee","Mouad Elhamdi","George Mohler","David A. Selby","Yao Xie","Sebastian Vollmer","Gerrit Grossmann"],"pdf_url":"https://arxiv.org/pdf/2502.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13535v3","updated":"2025-02-13T14:01:10Z","published":"2024-07-18T14:07:44Z","title":"Visuospatial navigation without distance, prediction, integration, or\n  maps","summary":"  Navigation is controlled by at least two partially dissociable, concurrently\ndeveloped systems in the brain. The cognitive map informs an organism of its\nlocation and bearing, updated by distance-based prediction and vestibular\nintegration. Response-based systems, on the other hand, directly evaluate\nmovement decisions from immediate percepts. Here we demonstrate the sufficiency\nof visual response-based decision-making in a classic open field navigation\ntask often assumed to require a cognitive map. Three distinct strategies emerge\nto robustly navigate to a hidden goal, each conferring contextual tradeoffs, as\nwell as aligning with behavior observed with rodents, insects, fish, and sperm\ncells. We propose reframing navigation from the bottom-up, without assuming\nonline access to computationally expensive top-down representations, to better\nexplain behavior under energetic or attentional constraints.\n","authors":["Patrick Govoni","Pawel Romanczuk"],"pdf_url":"https://arxiv.org/pdf/2407.13535v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09335v1","updated":"2025-02-13T13:54:58Z","published":"2025-02-13T13:54:58Z","title":"Graph Diffusion Network for Drug-Gene Prediction","summary":"  Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.\n","authors":["Jiayang Wu","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09335v1.pdf","comment":"IEEE/ACM TCBB. 14 pages"},{"id":"http://arxiv.org/abs/2406.10964v2","updated":"2025-02-13T13:43:04Z","published":"2024-06-16T14:49:19Z","title":"Ontology Embedding: A Survey of Methods, Applications and Resources","summary":"  Ontologies are widely used for representing domain knowledge and meta data,\nplaying an increasingly important role in Information Systems, the Semantic\nWeb, Bioinformatics and many other domains. However, logical reasoning that\nontologies can directly support are quite limited in learning, approximation\nand prediction. One straightforward solution is to integrate statistical\nanalysis and machine learning. To this end, automatically learning vector\nrepresentation for knowledge of an ontology i.e., ontology embedding has been\nwidely investigated. Numerous papers have been published on ontology embedding,\nbut a lack of systematic reviews hinders researchers from gaining a\ncomprehensive understanding of this field. To bridge this gap, we write this\nsurvey paper, which first introduces different kinds of semantics of ontologies\nand formally defines ontology embedding as well as its property of\nfaithfulness. Based on this, it systematically categorizes and analyses a\nrelatively complete set of over 80 papers, according to the ontologies they aim\nat and their technical solutions including geometric modeling, sequence\nmodeling and graph propagation. This survey also introduces the applications of\nontology embedding in ontology engineering, machine learning augmentation and\nlife sciences, presents a new library mOWL and discusses the challenges and\nfuture directions.\n","authors":["Jiaoyan Chen","Olga Mashkova","Fernando Zhapa-Camacho","Robert Hoehndorf","Yuan He","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2406.10964v2.pdf","comment":"A comprehensive survey on ontology embedding"},{"id":"http://arxiv.org/abs/2406.03345v3","updated":"2025-02-13T13:25:18Z","published":"2024-06-05T15:04:27Z","title":"Feature contamination: Neural networks learn uncorrelated features and\n  fail to generalize","summary":"  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n","authors":["Tianren Zhang","Chujie Zhao","Guanyu Chen","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03345v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2502.09307v1","updated":"2025-02-13T13:19:33Z","published":"2025-02-13T13:19:33Z","title":"When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models","summary":"  Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.\n","authors":["Samuel Joseph Amouyal","Aya Meltzer-Asscher","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2502.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09305v1","updated":"2025-02-13T13:17:31Z","published":"2025-02-13T13:17:31Z","title":"Predicting Drive Test Results in Mobile Networks Using Optimization\n  Techniques","summary":"  Mobile network operators constantly optimize their networks to ensure\nsuperior service quality and coverage. This optimization is crucial for\nmaintaining an optimal user experience and requires extensive data collection\nand analysis. One of the primary methods for gathering this data is through\ndrive tests, where technical teams use specialized equipment to collect signal\ninformation across various regions. However, drive tests are both costly and\ntime-consuming, and they face challenges such as traffic conditions,\nenvironmental factors, and limited access to certain areas. These constraints\nmake it difficult to replicate drive tests under similar conditions. In this\nstudy, we propose a method that enables operators to predict received signal\nstrength at specific locations using data from other drive test points. By\nreducing the need for widespread drive tests, this approach allows operators to\nsave time and resources while still obtaining the necessary data to optimize\ntheir networks and mitigate the challenges associated with traditional drive\ntests.\n","authors":["MohammadJava Taheri","Abolfazl Diyanat","MortezaAli Ahmadi","Ali Nazari"],"pdf_url":"https://arxiv.org/pdf/2502.09305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09294v1","updated":"2025-02-13T13:08:42Z","published":"2025-02-13T13:08:42Z","title":"Indeterminacy in Affective Computing: Considering Meaning and Context in\n  Data Collection Practices","summary":"  Automatic Affect Prediction (AAP) uses computational analysis of input data\nsuch as text, speech, images, and physiological signals to predict various\naffective phenomena (e.g., emotions or moods). These models are typically\nconstructed using supervised machine-learning algorithms, which rely heavily on\nlabeled training datasets. In this position paper, we posit that all AAP\ntraining data are derived from human Affective Interpretation Processes,\nresulting in a form of Affective Meaning. Research on human affect indicates a\nform of complexity that is fundamental to such meaning: it can possess what we\nrefer to here broadly as Qualities of Indeterminacy (QIs) - encompassing\nSubjectivity (meaning depends on who is interpreting), Uncertainty (lack of\nconfidence regarding meanings' correctness), Ambiguity (meaning contains\nmutually exclusive concepts) and Vagueness (meaning is situated at different\nlevels in a nested hierarchy). Failing to appropriately consider QIs leads to\nresults incapable of meaningful and reliable predictions. Based on this\npremise, we argue that a crucial step in adequately addressing indeterminacy in\nAAP is the development of data collection practices for modeling corpora that\ninvolve the systematic consideration of 1) a relevant set of QIs and 2) context\nfor the associated interpretation processes. To this end, we are 1) outlining a\nconceptual model of AIPs and the QIs associated with the meaning these produce\nand a conceptual structure of relevant context, supporting understanding of its\nrole. Finally, we use our framework for 2) discussing examples of\ncontext-sensitivity-related challenges for addressing QIs in data collection\nsetups. We believe our efforts can stimulate a structured discussion of both\nthe role of aspects of indeterminacy and context in research on AAP, informing\nthe development of better practices for data collection and analysis.\n","authors":["Bernd Dudzik","Tiffany Matej Hrkalovic","Chenxu Hao","Chirag Raman","Masha Tsfasman"],"pdf_url":"https://arxiv.org/pdf/2502.09294v1.pdf","comment":"Accepted at: 12th International Conference on Affective Computing and\n  Intelligent Interaction Workshops and Demos (ACIIW)"},{"id":"http://arxiv.org/abs/2406.15330v2","updated":"2025-02-13T13:06:00Z","published":"2024-06-21T17:42:52Z","title":"Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection","summary":"  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n","authors":["Haoling Li","Xin Zhang","Xiao Liu","Yeyun Gong","Yifan Wang","Qi Chen","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.15330v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.12690v2","updated":"2025-02-13T13:02:07Z","published":"2024-05-23T08:26:25Z","title":"The Dual Imperative: Innovation and Regulation in the AI Era","summary":"  This article addresses the societal costs associated with the lack of\nregulation in Artificial Intelligence and proposes a framework combining\ninnovation and regulation. Over fifty years of AI research, catalyzed by\ndeclining computing costs and the proliferation of data, have propelled AI into\nthe mainstream, promising significant economic benefits. Yet, this rapid\nadoption underscores risks, from bias amplification and labor disruptions to\nexistential threats posed by autonomous systems. The discourse is polarized\nbetween accelerationists, advocating for unfettered technological advancement,\nand doomers, calling for a slowdown to prevent dystopian outcomes. This piece\nadvocates for a middle path that leverages technical innovation and smart\nregulation to maximize the benefits of AI while minimizing its risks, offering\na pragmatic approach to the responsible progress of AI technology. Technical\ninvention beyond the most capable foundation models is needed to contain\ncatastrophic risks. Regulation is required to create incentives for this\nresearch while addressing current issues.\n","authors":["Paulo Carvo"],"pdf_url":"https://arxiv.org/pdf/2407.12690v2.pdf","comment":"This is an original manuscript of an article published by\n  Inderscience in the International Journal of Technology Policy and Law Vol.\n  3, No. 3 on November 28, 2024, available online:\n  https://doi.org/10.1504/IJTPL.2024.142861"},{"id":"http://arxiv.org/abs/2502.09284v1","updated":"2025-02-13T12:57:15Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v2","updated":"2025-02-13T12:54:36Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09795v4","updated":"2025-02-13T12:35:53Z","published":"2024-10-13T10:48:22Z","title":"WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for\n  Molecular Ground-State Conformation Prediction","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09271v1","updated":"2025-02-13T12:33:39Z","published":"2025-02-13T12:33:39Z","title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via\n  Subgraph Injection","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method.\n","authors":["Wenlun Zhang","Enyan Dai","Kentaro Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2502.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17301v2","updated":"2025-02-13T12:25:54Z","published":"2024-11-26T10:48:55Z","title":"ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation","summary":"  Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.\n","authors":["Yunyi Liu","Yingshu Li","Zhanyu Wang","Xinyu Liang","Lingqiao Liu","Lei Wang","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.17301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.09266v5","updated":"2025-02-13T12:15:30Z","published":"2021-05-19T17:22:47Z","title":"Copyright in Generative Deep Learning","summary":"  Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise. In this article, we consider a set of key questions in the area of\ngenerative deep learning for the arts, including the following: is it possible\nto use copyrighted works as training set for generative models? How do we\nlegally store their copies in order to perform the training process? Who (if\nsomeone) will own the copyright on the generated data? We try to answer these\nquestions considering the law in force in both the United States of America and\nthe European Union, and potential future alternatives. We then extend our\nanalysis to code generation, which is an emerging area of generative deep\nlearning. Finally, we also formulate a set of practical guidelines for artists\nand developers working on deep learning generated art, as well as some policy\nsuggestions for policymakers.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2105.09266v5.pdf","comment":"Published in Data & Policy at\n  https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E"},{"id":"http://arxiv.org/abs/2502.09257v1","updated":"2025-02-13T12:13:25Z","published":"2025-02-13T12:13:25Z","title":"Bandit Multiclass List Classification","summary":"  We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels, and the feedback consists of the predicted\nlabels which lie in the set of true labels of the given example. Our main\nresult is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which\nwe design an algorithm that returns an $\\varepsilon$-optimal hypothesis with\nhigh probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm\n/ \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite)\nhypothesis class and $s$ is an upper bound on the number of true labels for a\ngiven example. This bound improves upon known bounds for combinatorial\nsemi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the\nleading terms in our bound match the corresponding full-information rates,\nimplying that bandit feedback essentially comes at no cost. Our PAC learning\nalgorithm is also computationally efficient given access to an ERM oracle for\n$H$. Additionally, we consider the regret minimization setting where data can\nbe generated adversarially, and establish a regret bound of $\\widetilde O(|H| +\n\\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al.\n(2024) who consider the simpler single-label setting corresponding to $s=m=1$,\nand in fact hold for the more general contextual combinatorial semi-bandit\nproblem with $s$-sparse rewards.\n","authors":["Liad Erez","Tomer Koren"],"pdf_url":"https://arxiv.org/pdf/2502.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.02726v7","updated":"2025-02-13T12:12:28Z","published":"2021-04-06T18:00:06Z","title":"Creativity and Machine Learning: A Survey","summary":"  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, key machine learning techniques (including\ngenerative deep learning), and corresponding automatic evaluation methods.\nAfter presenting a critical discussion of the key contributions in this area,\nwe outline the current research challenges and emerging opportunities in this\nfield.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2104.02726v7.pdf","comment":"Published in ACM Computing Surveys at\n  https://dl.acm.org/doi/10.1145/3664595"},{"id":"http://arxiv.org/abs/2502.09256v1","updated":"2025-02-13T12:11:58Z","published":"2025-02-13T12:11:58Z","title":"DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in\n  Segmenting Hemorrhagic Lesions from Fundus Images","summary":"  The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]\n","authors":["Zesheng Li","Minwen Liao","Haoran Chen","Yan Su","Chengchang Pan","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2502.09256v1.pdf","comment":"12 pages,4 figures"},{"id":"http://arxiv.org/abs/2304.00008v5","updated":"2025-02-13T12:10:33Z","published":"2023-03-27T18:00:01Z","title":"On the Creativity of Large Language Models","summary":"  Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2304.00008v5.pdf","comment":"Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3"},{"id":"http://arxiv.org/abs/2502.09254v1","updated":"2025-02-13T12:10:05Z","published":"2025-02-13T12:10:05Z","title":"AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection","summary":"  Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.\n","authors":["Hezhe Qiao","Chaoxi Niu","Ling Chen","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2502.09254v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09247v1","updated":"2025-02-13T12:03:36Z","published":"2025-02-13T12:03:36Z","title":"The Joint Entity-Relation Extraction Model Based on Span and Interactive\n  Fusion Representation for Chinese Medical Texts with Complex Semantics","summary":"  Joint entity-relation extraction is a critical task in transforming\nunstructured or semi-structured text into triplets, facilitating the\nconstruction of large-scale knowledge graphs, and supporting various downstream\napplications. Despite its importance, research on Chinese text, particularly\nwith complex semantics in specialized domains like medicine, remains limited.\nTo address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions\ndataset designed to capture the intricacies of medical text. Leveraging the\nstrengths of attention mechanisms in capturing long-range dependencies, we\npropose the SEA module, which enhances the extraction of complex contextual\nsemantic information, thereby improving entity recognition and relation\nextraction. Additionally, to address the inefficiencies of existing methods in\nfacilitating information exchange between entity recognition and relation\nextraction, we present an interactive fusion representation module. This module\nemploys Cross Attention for bidirectional information exchange between the\ntasks and further refines feature extraction through BiLSTM. Experimental\nresults on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that\nour model exhibits strong generalization capabilities. On the CH-DDI dataset,\nour model achieves an F1-score of 96.73% for entity recognition and 78.43% for\nrelation extraction. On the CoNLL04 dataset, it attains an entity recognition\nprecision of 89.54% and a relation extraction accuracy of 71.64%.\n","authors":["Danni Feng","Runzhi Li","Jing Wang","Siyu Yan","Lihong Ma","Yunli Xing"],"pdf_url":"https://arxiv.org/pdf/2502.09247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09242v1","updated":"2025-02-13T11:57:51Z","published":"2025-02-13T11:57:51Z","title":"From large language models to multimodal AI: A scoping review on the\n  potential of generative AI in medicine","summary":"  Generative artificial intelligence (AI) models, such as diffusion models and\nOpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy\nand automating clinical workflows. The field has advanced rapidly, evolving\nfrom text-only large language models for tasks such as clinical documentation\nand decision support to multimodal AI systems capable of integrating diverse\ndata modalities, including imaging, text, and structured data, within a single\nmodel. The diverse landscape of these technologies, along with rising interest,\nhighlights the need for a comprehensive review of their applications and\npotential. This scoping review explores the evolution of multimodal AI,\nhighlighting its methods, applications, datasets, and evaluation in clinical\nsettings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed,\nIEEE Xplore, and Web of Science, prioritizing recent studies published up to\nthe end of 2024. After rigorous screening, 144 papers were included, revealing\nkey trends and challenges in this dynamic field. Our findings underscore a\nshift from unimodal to multimodal approaches, driving innovations in diagnostic\nsupport, medical report generation, drug discovery, and conversational AI.\nHowever, critical challenges remain, including the integration of heterogeneous\ndata types, improving model interpretability, addressing ethical concerns, and\nvalidating AI systems in real-world clinical settings. This review summarizes\nthe current state of the art, identifies critical gaps, and provides insights\nto guide the development of scalable, trustworthy, and clinically impactful\nmultimodal AI solutions in healthcare.\n","authors":["Lukas Buess","Matthias Keicher","Nassir Navab","Andreas Maier","Soroosh Tayebi Arasteh"],"pdf_url":"https://arxiv.org/pdf/2502.09242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09235v1","updated":"2025-02-13T11:53:57Z","published":"2025-02-13T11:53:57Z","title":"Hybrid Answer Set Programming: Foundations and Applications","summary":"  Answer Set Programming (ASP) is a powerful tool for solving real-world\nproblems. However, many problems involve numeric values and complex constraints\nbeyond the capabilities of standard ASP solvers. Hybrid solvers like CLINGCON\nand CLINGO[DL] address this by using specialized methods for specific\nconstraints. However, these solvers lack a strong theoretical foundation.\n  This issue has first been addressed by introducing the Logic of\nHere-and-There with constraints (HT_c) as an extension of the Logic of\nHere-and-There (HT) and its non-monotone extension Equilibrium Logic. Nowadays,\nHT serves as a logical foundation for ASP and has facilitated a broader\nunderstanding of this paradigm. The idea is that HTC (and other extensions)\nplay an analogous role for hybrid ASP.\n  There remain many open questions about these logics regarding their\nfundamental characteristics as well as their practical use in solvers, ie. how\nthey can guide the implementation.\n  Having a formal understanding of these hybrid logics is also needed to better\nunderstand the inherent structure of the (real-world) problems they are applied\nto and to improve their representations in ASP. As an example of an application\nof ASP we use product configuration.\n","authors":["Nicolas Rhling"],"pdf_url":"https://arxiv.org/pdf/2502.09235v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09233v1","updated":"2025-02-13T11:53:25Z","published":"2025-02-13T11:53:25Z","title":"Commonsense Reasoning-Aided Autonomous Vehicle Systems","summary":"  Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.\n","authors":["Keegan Kimbrell"],"pdf_url":"https://arxiv.org/pdf/2502.09233v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09232v1","updated":"2025-02-13T11:53:10Z","published":"2025-02-13T11:53:10Z","title":"Logical foundations of Smart Contracts","summary":"  Nowadays, sophisticated domains are emerging which require appropriate\nformalisms to be specified accurately in order to reason about them. One such\ndomain is constituted of smart contracts that have emerged in cyber physical\nsystems as a way of enforcing formal agreements between components of these\nsystems. Smart contracts self-execute to run and share business processes\nthrough blockchain, in decentralized systems, with many different participants.\nLegal contracts are in many cases complex documents, with a number of\nexceptions, and many subcontracts. The implementation of smart contracts based\non legal contracts is a long and laborious task, that needs to include all\nactions, procedures, and the effects of actions related to the execution of the\ncontract. An ongoing open problem in this area is to formally account for smart\ncontracts using a uniform and somewhat universal formalism. This thesis\nproposes logical foundations to smart contracts using the Situation Calculus, a\nlogic for reasoning about actions. Situation Calculus is one of the prominent\nlogic-based artificial intelligence approaches that provides enough logical\nmechanism to specify and implement dynamic and complex systems such as\ncontracts. Situation Calculus is suitable to show how worlds dynamically\nchange. Smart contracts are going to be implement with Golog (written en\nProlog), a Situation Calculus-based programming language for modeling complex\nand dynamic behaviors.\n","authors":["Kalonji Kalala"],"pdf_url":"https://arxiv.org/pdf/2502.09232v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09230v1","updated":"2025-02-13T11:52:40Z","published":"2025-02-13T11:52:40Z","title":"Relating Answer Set Programming and Many-sorted Logics for Formal\n  Verification","summary":"  Answer Set Programming (ASP) is an important logic programming paradigm\nwithin the field of Knowledge Representation and Reasoning. As a concise,\nhuman-readable, declarative language, ASP is an excellent tool for developing\ntrustworthy (especially, artificially intelligent) software systems. However,\nformally verifying ASP programs offers some unique challenges, such as\n  1. a lack of modularity (the meanings of rules are difficult to define in\nisolation from the enclosing program),\n  2. the ground-and-solve semantics (the meanings of rules are dependent on the\ninput data with which the program is grounded), and\n  3. limitations of existing tools.\n  My research agenda has been focused on addressing these three issues with the\nintention of making ASP verification an accessible, routine task that is\nregularly performed alongside program development. In this vein, I have\ninvestigated alternative semantics for ASP based on translations into the logic\nof here-and-there and many-sorted first-order logic. These semantics promote a\nmodular understanding of logic programs, bypass grounding, and enable us to use\nautomated theorem provers to automatically verify properties of programs.\n","authors":["Zachary Hansen"],"pdf_url":"https://arxiv.org/pdf/2502.09230v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09228v1","updated":"2025-02-13T11:52:25Z","published":"2025-02-13T11:52:25Z","title":"Computational methods for Dynamic Answer Set Programming","summary":"  In our daily lives and industrial settings, we often encounter dynamic\nproblems that require reasoning over time and metric constraints. These include\ntasks such as scheduling, routing, and production sequencing. Dynamic logics\nhave traditionally addressed these needs but often lack the flexibility and\nintegration required for comprehensive problem modeling. This research aims to\nextend Answer Set Programming (ASP), a powerful declarative problem-solving\napproach, to handle dynamic domains effectively. By integrating concepts from\ndynamic, temporal, and metric logics into ASP, we seek to develop robust\nsystems capable of modeling complex dynamic problems and performing efficient\nreasoning tasks, thereby enhancing ASPs applicability in industrial contexts.\n","authors":["Susana Hahn"],"pdf_url":"https://arxiv.org/pdf/2502.09228v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09226v1","updated":"2025-02-13T11:51:53Z","published":"2025-02-13T11:51:53Z","title":"Generating Causally Compliant Counterfactual Explanations using ASP","summary":"  This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.\n","authors":["Sopam Dasgupta"],"pdf_url":"https://arxiv.org/pdf/2502.09226v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09224v1","updated":"2025-02-13T11:51:22Z","published":"2025-02-13T11:51:22Z","title":"Order-Sorted Intensional Logic: Expressing Subtyping Polymorphism with\n  Typing Assertions and Quantification over Concepts","summary":"  Subtyping, also known as subtype polymorphism, is a concept extensively\nstudied in programming language theory, delineating the substitutability\nrelation among datatypes. This property ensures that programs designed for\nsupertype objects remain compatible with their subtypes.\n  In this paper, we explore the capability of order-sorted logic for utilizing\nthese ideas in the context of Knowledge Representation. We recognize two\nfundamental limitations: First, the inability of this logic to address the\nconcept rather than the value of non-logical symbols, and second, the lack of\nlanguage constructs for constraining the type of terms. Consequently, we\npropose guarded order-sorted intensional logic, where guards are language\nconstructs for annotating typing information and intensional logic provides\nsupport for quantification over concepts.\n","authors":["ore Markovi","Marc Denecker"],"pdf_url":"https://arxiv.org/pdf/2502.09224v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09222v1","updated":"2025-02-13T11:50:51Z","published":"2025-02-13T11:50:51Z","title":"ASP-driven User-interaction with Clinguin","summary":"  We present clinguin, a system for ASP-driven user interface design. Clinguin\nstreamlines the development of user interfaces for ASP developers by letting\nthem build interactive prototypes directly in ASP, eliminating the need for\nseparate frontend languages. To this end, clinguin uses a few dedicated\npredicates to define user interfaces and the treatment of user-triggered\nevents. This simple design greatly facilitates the specification of user\ninteractions with an ASP system, in our case clingo.\n","authors":["Alexander Beiser","Susana Hahn","Torsten Schaub"],"pdf_url":"https://arxiv.org/pdf/2502.09222v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09221v1","updated":"2025-02-13T11:50:36Z","published":"2025-02-13T11:50:36Z","title":"Pearce's Characterisation in an Epistemic Domain","summary":"  Answer-set programming (ASP) is a successful problem-solving approach in\nlogic-based AI. In ASP, problems are represented as declarative logic programs,\nand solutions are identified through their answer sets. Equilibrium logic (EL)\nis a general-purpose nonmonotonic reasoning formalism, based on a monotonic\nlogic called here-and-there logic. EL was basically proposed by Pearce as a\nfoundational framework of ASP. Epistemic specifications (ES) are extensions of\nASP-programs with subjective literals. These new modal constructs in the\nASP-language make it possible to check whether a regular literal of ASP is true\nin every (or some) answer-set of a program. ES-programs are interpreted by\nworld-views, which are essentially collections of answer-sets. (Reflexive)\nautoepistemic logic is a nonmonotonic formalism, modeling self-belief\n(knowledge) of ideally rational agents. A relatively new semantics for ES is\nbased on a combination of EL and (reflexive) autoepistemic logic. In this\npaper, we first propose an overarching framework in the epistemic ASP domain.\nWe then establish a correspondence between existing (reflexive) (auto)epistemic\nequilibrium logics and our easily-adaptable comprehensive framework, building\non Pearce's characterisation of answer-sets as equilibrium models. We achieve\nthis by extending Ferraris' work on answer sets for propositional theories to\nthe epistemic case and reveal the relationship between some ES-semantic\nproposals.\n","authors":["Ezgi Iraz Su"],"pdf_url":"https://arxiv.org/pdf/2502.09221v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09220v1","updated":"2025-02-13T11:50:20Z","published":"2025-02-13T11:50:20Z","title":"Graphical Conditions for the Existence, Unicity and Number of Regular\n  Models","summary":"  The regular models of a normal logic program are a particular type of partial\n(i.e. 3-valued) models which correspond to stable partial models with minimal\nundefinedness. In this paper, we explore graphical conditions on the dependency\ngraph of a finite ground normal logic program to analyze the existence, unicity\nand number of regular models for the program. We show three main results: 1) a\nnecessary condition for the existence of non-trivial (i.e. non-2-valued)\nregular models, 2) a sufficient condition for the unicity of regular models,\nand 3) two upper bounds for the number of regular models based on positive\nfeedback vertex sets. The first two conditions generalize the finite cases of\nthe two existing results obtained by You and Yuan (1994) for normal logic\nprograms with well-founded stratification. The third result is also new to the\nbest of our knowledge. Key to our proofs is a connection that we establish\nbetween finite ground normal logic programs and Boolean network theory.\n","authors":["Van-Giang Trinh","Belaid Benhamou","Sylvain Soliman","Franois Fages"],"pdf_url":"https://arxiv.org/pdf/2502.09220v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09219v1","updated":"2025-02-13T11:50:04Z","published":"2025-02-13T11:50:04Z","title":"Abduction of Domain Relationships from Data for VQA","summary":"  In this paper, we study the problem of visual question answering (VQA) where\nthe image and query are represented by ASP programs that lack domain data. We\nprovide an approach that is orthogonal and complementary to existing knowledge\naugmentation techniques where we abduce domain relationships of image\nconstructs from past examples. After framing the abduction problem, we provide\na baseline approach, and an implementation that significantly improves the\naccuracy of query answering yet requires few examples.\n","authors":["Al Mehdi Saadat Chowdhury","Paulo Shakarian","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2502.09219v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09218v1","updated":"2025-02-13T11:49:48Z","published":"2025-02-13T11:49:48Z","title":"Data2Concept2Text: An Explainable Multilingual Framework for Data\n  Analysis Narration","summary":"  This paper presents a complete explainable system that interprets a set of\ndata, abstracts the underlying features and describes them in a natural\nlanguage of choice. The system relies on two crucial stages: (i) identifying\nemerging properties from data and transforming them into abstract concepts, and\n(ii) converting these concepts into natural language. Despite the impressive\nnatural language generation capabilities demonstrated by Large Language Models,\ntheir statistical nature and the intricacy of their internal mechanism still\nforce us to employ these techniques as black boxes, forgoing trustworthiness.\nDeveloping an explainable pipeline for data interpretation would allow\nfacilitating its use in safety-critical environments like processing medical\ninformation and allowing non-experts and visually impaired people to access\nnarrated information. To this end, we believe that the fields of knowledge\nrepresentation and automated reasoning research could present a valid\nalternative. Expanding on prior research that tackled the first stage (i), we\nfocus on the second stage, named Concept2Text. Being explainable, data\ntranslation is easily modeled through logic-based rules, once again emphasizing\nthe role of declarative programming in achieving AI explainability. This paper\nexplores a Prolog/CLP-based rewriting system to interpret concepts-articulated\nin terms of classes and relations, plus common knowledge-derived from a generic\nontology, generating natural language text. Its main features include\nhierarchical tree rewritings, modular multilingual generation, support for\nequivalent variants across semantic, grammar, and lexical levels, and a\ntransparent rule-based system. We outline the architecture and demonstrate its\nflexibility through some examples capable of generating numerous diverse and\nequivalent rewritings based on the input concept.\n","authors":["Flavio Bertini","Alessandro Dal Pal","Federica Zaglio","Francesco Fabiano","Andrea Formisano"],"pdf_url":"https://arxiv.org/pdf/2502.09218v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09216v1","updated":"2025-02-13T11:49:17Z","published":"2025-02-13T11:49:17Z","title":"Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for\n  Autonomous Vehicles","summary":"  In this paper, we present a modular system for representing and reasoning\nwith legal aspects of traffic rules for autonomous vehicles. We focus on a\nsubset of the United Kingdom's Highway Code (HC) related to junctions. As human\ndrivers and automated vehicles (AVs) will interact on the roads, especially in\nurban environments, we claim that an accessible, unitary, high-level\ncomputational model should exist and be applicable to both users. Autonomous\nvehicles introduce a shift in liability that should not bring disadvantages or\nincreased burden on human drivers. We develop a system \"in silico\" of the\nmodel. The proposed system is built of three main components: a natural\nlanguage interface, using Logical English, which encodes the rules; an internal\nrepresentation of the rules in Prolog; and an multi-agent-based simulation\nenvironment, built in NetLogo. The three components interact: Logical English\nis translated into and out of Prolog (along with some support code); Prolog and\nNetLogo interface via predicates. Such a modular approach enables the different\ncomponents to carry different \"burdens\" in the overall system; it also allows\nswapping of modules. Given NetLogo, we can visualize the effect of the modeled\nrules as well as validate the system with a simple dynamic running scenario.\nDesignated agents monitor the behaviour of the vehicles for compliance and\nrecord potential violations where they occur. The information on potential\nviolations is then utilized by Validators, to determine whether the violation\nis punishable, differentiating between exceptions and cases.\n","authors":["Galileo Sartor","Adam Wyner","Giuseppe Contissa"],"pdf_url":"https://arxiv.org/pdf/2502.09216v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09215v1","updated":"2025-02-13T11:49:02Z","published":"2025-02-13T11:49:02Z","title":"Architecture for Simulating Behavior Mode Changes in Norm-Aware\n  Autonomous Agents","summary":"  This paper presents an architecture for simulating the actions of a\nnorm-aware intelligent agent whose behavior with respect to norm compliance is\nset, and can later be changed, by a human controller. Updating an agent's\nbehavior mode from a norm-abiding to a riskier one may be relevant when the\nagent is involved in time-sensitive rescue operations, for example. We base our\nwork on the Authorization and Obligation Policy Language AOPL designed by\nGelfond and Lobo for the specification of norms. We introduce an architecture\nand a prototype software system that can be used to simulate an agent's plans\nunder different behavior modes that can later be changed by the controller. We\nenvision such software to be useful to policy makers, as they can more readily\nunderstand how agents may act in certain situations based on the agents'\nattitudes towards norm-compliance. Policy makers may then refine their policies\nif simulations show unwanted consequences.\n","authors":["Sean Glaze","Daniela Inclezan"],"pdf_url":"https://arxiv.org/pdf/2502.09215v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09212v1","updated":"2025-02-13T11:48:31Z","published":"2025-02-13T11:48:31Z","title":"LP-LM: No Hallucinations in Question Answering with Logic Programming","summary":"  Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.\n","authors":["Katherine Wu","Yanhong A. Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09212v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2405.12433v3","updated":"2025-02-13T11:48:15Z","published":"2024-05-21T01:16:34Z","title":"LLM+Reasoning+Planning for Supporting Incomplete User Queries in\n  Presence of APIs","summary":"  Recent availability of Large Language Models (LLMs) has led to the\ndevelopment of numerous LLM-based approaches aimed at providing natural\nlanguage interfaces for various end-user tasks. These end-user tasks in turn\ncan typically be accomplished by orchestrating a given set of APIs. In\npractice, natural language task requests (user queries) are often incomplete,\ni.e., they may not contain all the information required by the APIs. While LLMs\nexcel at natural language processing (NLP) tasks, they frequently hallucinate\non missing information or struggle with orchestrating the APIs. The key idea\nbehind our proposed approach is to leverage logical reasoning and classical AI\nplanning along with an LLM for accurately answering user queries including\nidentification and gathering of any missing information in these queries. Our\napproach uses an LLM and ASP (Answer Set Programming) solver to translate a\nuser query to a representation in Planning Domain Definition Language (PDDL)\nvia an intermediate representation in ASP. We introduce a special API\n\"get_info_api\" for gathering missing information. We model all the APIs as PDDL\nactions in a way that supports dataflow between the APIs. Our approach then\nuses a classical AI planner to generate an orchestration of API calls\n(including calls to get_info_api) to answer the user query. Our evaluation\nresults show that our approach significantly outperforms a pure LLM based\napproach by achieving over 95% success rate in most cases on a dataset\ncontaining complete and incomplete single goal and multi-goal queries where the\nmulti-goal queries may or may not require dataflow among the APIs.\n","authors":["Sudhir Agarwal","Anu Sreepathy","David H. Alonso","Prarit Lamba"],"pdf_url":"https://arxiv.org/pdf/2405.12433v3.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09211v1","updated":"2025-02-13T11:47:59Z","published":"2025-02-13T11:47:59Z","title":"Visual Graph Question Answering with ASP and LLMs for Language Parsing","summary":"  Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.\n","authors":["Jakob Johannes Bauer","Thomas Eiter","Nelson Higuera Ruiz","Johannes Oetsch"],"pdf_url":"https://arxiv.org/pdf/2502.09211v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI"},{"id":"http://arxiv.org/abs/2502.09209v1","updated":"2025-02-13T11:47:44Z","published":"2025-02-13T11:47:44Z","title":"On LLM-generated Logic Programs and their Inference Execution Methods","summary":"  Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs.\n","authors":["Paul Tarau"],"pdf_url":"https://arxiv.org/pdf/2502.09209v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.09584v1","updated":"2025-02-13T18:42:20Z","published":"2025-02-13T18:42:20Z","title":"Differentially Private Compression and the Sensitivity of LZ77","summary":"  We initiate the study of differentially private data-compression schemes\nmotivated by the insecurity of the popular \"Compress-Then-Encrypt\" framework.\nData compression is a useful tool which exploits redundancy in data to reduce\nstorage/bandwidth when files are stored or transmitted. However, if the\ncontents of a file are confidential then the length of a compressed file might\nleak confidential information about the content of the file itself. Encrypting\na compressed file does not eliminate this leakage as data encryption schemes\nare only designed to hide the content of confidential message instead of the\nlength of the message. In our proposed Differentially Private\nCompress-Then-Encrypt framework, we add a random positive amount of padding to\nthe compressed file to ensure that any leakage satisfies the rigorous privacy\nguarantee of $(\\epsilon,\\delta)$-differential privacy. The amount of padding\nthat needs to be added depends on the sensitivity of the compression scheme to\nsmall changes in the input, i.e., to what degree can changing a single\ncharacter of the input message impact the length of the compressed file. While\nsome popular compression schemes are highly sensitive to small changes in the\ninput, we argue that effective data compression schemes do not necessarily have\nhigh sensitivity. Our primary technical contribution is analyzing the\nfine-grained sensitivity of the LZ77 compression scheme (IEEE Trans. Inf.\nTheory 1977) which is one of the most common compression schemes used in\npractice. We show that the global sensitivity of the LZ77 compression scheme\nhas the upper bound $\\mathcal{O}(W^{2/3}\\log n)$ where $W\\leq n$ denotes the\nsize of the sliding window. When $W=n$, we show the lower bound\n$\\Omega(n^{2/3}\\log^{1/3}n)$ for the global sensitivity of the LZ77 compression\nscheme which is tight up to a sublogarithmic factor.\n","authors":["Jeremiah Blocki","Seunghoon Lee","Brayan Sebastin Yepes Garcia"],"pdf_url":"https://arxiv.org/pdf/2502.09584v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09553v1","updated":"2025-02-13T18:05:12Z","published":"2025-02-13T18:05:12Z","title":"SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops","summary":"  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n","authors":["Eshaq Jamdar","Amith Kamath Belman"],"pdf_url":"https://arxiv.org/pdf/2502.09553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09549v1","updated":"2025-02-13T18:02:48Z","published":"2025-02-13T18:02:48Z","title":"Registration, Detection, and Deregistration: Analyzing DNS Abuse for\n  Phishing Attacks","summary":"  Phishing continues to pose a significant cybersecurity threat. While\nblocklists currently serve as a primary defense, due to their reactive, passive\nnature, these delayed responses leave phishing websites operational long enough\nto harm potential victims. It is essential to address this fundamental\nchallenge at the root, particularly in phishing domains. Domain registration\npresents a crucial intervention point, as domains serve as the primary gateway\nbetween users and websites. We conduct a comprehensive longitudinal analysis of\n690,502 unique phishing domains, spanning a 39 month period, to examine their\ncharacteristics and behavioral patterns throughout their lifecycle-from initial\nregistration to detection and eventual deregistration. We find that 66.1% of\nthe domains in our dataset are maliciously registered, leveraging\ncost-effective TLDs and targeting brands by mimicking their domain names under\nalternative TLDs (e.g., .top and .tk) instead of the TLDs under which the brand\ndomains are registered (e.g., .com and .ru). We also observe minimal\nimprovements in detection speed for maliciously registered domains compared to\ncompromised domains. Detection times vary widely across blocklists, and\nphishing domains remain accessible for an average of 11.5 days after detection,\nprolonging their potential impact. Our systematic investigation uncovers key\npatterns from registration through detection to deregistration, which could be\nleveraged to enhance anti-phishing active defenses at the DNS level.\n","authors":["Kyungchan Lim","Kiho Lee","Raffaele Sommese","Mattis Jonker","Ricky Mok","kc claffy","Doowon Kim"],"pdf_url":"https://arxiv.org/pdf/2502.09549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09535v1","updated":"2025-02-13T17:50:58Z","published":"2025-02-13T17:50:58Z","title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based\n  Security","summary":"  Mobile sensor data has been proposed for security-critical applications such\nas device pairing, proximity detection, and continuous authentication. However,\nthe foundational assumption that these signals provide sufficient entropy\nremains under-explored. In this work, we systematically analyse the entropy of\nsmartphone sensor data across four diverse datasets spanning multiple\napplication contexts. Our findings reveal pervasive biases, with single-sensor\nmean min-entropy values ranging from 3.408-3.508 bits (S.D.=1.018-1.574), while\nconventional Shannon entropy is several multiples higher. We further\ndemonstrate that correlations between sensor modalities reduce the worst-case\nentropy of using multiple sensors by up to approx. 75% compared to average-case\nShannon entropy. This brings joint min-entropy well below 10 bits in many cases\nand, in the best case, yielding only approx. 24 bits of min-entropy when\ncombining 20 sensor modalities. These results call into question the widely\nheld assumption that adding more sensors inherently yields higher security. We\nultimately caution against relying on raw sensor data as a primary source of\nrandomness.\n","authors":["Carlton Shepherd","Elliot Hurley"],"pdf_url":"https://arxiv.org/pdf/2502.09535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07414v3","updated":"2025-02-13T17:27:11Z","published":"2024-10-09T20:29:04Z","title":"Bayes-Nash Generative Privacy Against Membership Inference Attacks","summary":"  Membership inference attacks (MIAs) expose significant privacy risks by\ndetermining whether an individual's data is in a dataset. While differential\nprivacy (DP) mitigates such risks, it has several limitations in achieving an\noptimal balance between utility and privacy, include limited resolution in\nexpressing this tradeoff in only a few privacy parameters, and intractable\nsensitivity calculations that may be necessary to provide tight privacy\nguarantees. We propose a game-theoretic framework that models privacy\nprotection from MIA as a Bayesian game between a defender and an attacker. In\nthis game, a dataset is the defender's private information, with privacy loss\nto the defender (which is gain to the attacker) captured in terms of the\nattacker's ability to infer membership of individuals in the dataset. To\naddress the strategic complexity of this game, we represent the mixed strategy\nof the defender as a neural network generator which maps a private dataset to\nits public representation (for example, noisy summary statistics), while the\nmixed strategy of the attacker is captured by a discriminator which makes\nmembership inference claims. We refer to the resulting computational approach\nas a general-sum Generative Adversarial Network, which is trained iteratively\nby alternating generator and discriminator updates akin to conventional GANs.\nWe call the defender's data sharing policy thereby obtained Bayes-Nash\nGenerative Privacy (BNGP). The BNGP strategy avoids sensitivity calculations,\nsupports compositions of correlated mechanisms, is robust to the attacker's\nheterogeneous preferences over true and false positives, and yields provable\ndifferential privacy guarantees, albeit in an idealized setting.\n","authors":["Tao Zhang","Rajagopal Venkatesaraman","Rajat K. De","Bradley A. Malin","Yevgeniy Vorobeychik"],"pdf_url":"https://arxiv.org/pdf/2410.07414v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.01811"},{"id":"http://arxiv.org/abs/2502.09484v1","updated":"2025-02-13T16:46:23Z","published":"2025-02-13T16:46:23Z","title":"PenTest++: Elevating Ethical Hacking with AI and Automation","summary":"  Traditional ethical hacking relies on skilled professionals and\ntime-intensive command management, which limits its scalability and efficiency.\nTo address these challenges, we introduce PenTest++, an AI-augmented system\nthat integrates automation with generative AI (GenAI) to optimise ethical\nhacking workflows. Developed in a controlled virtual environment, PenTest++\nstreamlines critical penetration testing tasks, including reconnaissance,\nscanning, enumeration, exploitation, and documentation, while maintaining a\nmodular and adaptable design. The system balances automation with human\noversight, ensuring informed decision-making at key stages, and offers\nsignificant benefits such as enhanced efficiency, scalability, and\nadaptability. However, it also raises ethical considerations, including privacy\nconcerns and the risks of AI-generated inaccuracies (hallucinations). This\nresearch underscores the potential of AI-driven systems like PenTest++ to\ncomplement human expertise in cybersecurity by automating routine tasks,\nenabling professionals to focus on strategic decision-making. By incorporating\nrobust ethical safeguards and promoting ongoing refinement, PenTest++\ndemonstrates how AI can be responsibly harnessed to address operational and\nethical challenges in the evolving cybersecurity landscape.\n","authors":["Haitham S. Al-Sinani","Chris J. Mitchell"],"pdf_url":"https://arxiv.org/pdf/2502.09484v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.03203v3","updated":"2025-02-13T16:28:06Z","published":"2025-02-05T14:23:43Z","title":"FSLH: Flexible Mechanized Speculative Load Hardening","summary":"  The Spectre speculative side-channel attacks pose formidable threats for\ncomputer system security. Research has shown that cryptographic constant-time\ncode can be efficiently protected against Spectre v1 using a selective variant\nof Speculative Load Hardening (SLH). SLH was, however, not strong enough for\nprotecting non-cryptographic code, leading to the introduction of Ultimate SLH,\nwhich provides protection for arbitrary programs, but has too large overhead\nfor general use, since it conservatively assumes that all data is secret. In\nthis paper we introduce a flexible SLH notion that achieves the best of both\nworlds by formally generalizing both Selective and Ultimate SLH. We give a\nsuitable security definition for such transformations protecting arbitrary\nprograms: any transformed program running with speculation should not leak more\nthan what the source program leaks sequentially. We formally prove using the\nRocq prover that two flexible SLH variants enforce this relative security\nguarantee. As easy corollaries we also obtain that Ultimate SLH enforces our\nrelative security notion, and also that the selective variants of value SLH and\naddress SLH enforce speculative constant-time security.\n","authors":["Roberto Blanco","Lon Ducruet","Sebastian Harwig","Catalin Hritcu"],"pdf_url":"https://arxiv.org/pdf/2502.03203v3.pdf","comment":"CSF'25 submission with a few typos fixed"},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2501.16451v2","updated":"2025-02-13T15:48:32Z","published":"2025-01-27T19:16:04Z","title":"Emulating OP_RAND in Bitcoin","summary":"  This paper proposes a method of emulation of \\verb|OP_RAND| opcode on Bitcoin\nthrough a trustless interactive game between transaction counterparties. The\ngame result is probabilistic and doesn't allow any party to cheat, increasing\ntheir chance of winning on any protocol step. The protocol can be organized in\na way unrecognizable to any external party and doesn't require some specific\nscripts or Bitcoin protocol updates. We will show how the protocol works on the\nsimple \\textbf{Thimbles Game} and provide some initial thoughts about\napproaches and applications that can use the mentioned approach.\n","authors":["Oleksandr Kurbatov"],"pdf_url":"https://arxiv.org/pdf/2501.16451v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09396v1","updated":"2025-02-13T15:16:53Z","published":"2025-02-13T15:16:53Z","title":"A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack","summary":"  Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.\n","authors":["Richard J. Preen","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2502.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09385v1","updated":"2025-02-13T15:01:18Z","published":"2025-02-13T15:01:18Z","title":"APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent\n  Threats Using Large Language Models","summary":"  Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due\nto their stealth and ability to mimic normal system behavior, making detection\nparticularly difficult in highly imbalanced datasets. Traditional anomaly\ndetection methods struggle to effectively differentiate APT-related activities\nfrom benign processes, limiting their applicability in real-world scenarios.\nThis paper introduces APT-LLM, a novel embedding-based anomaly detection\nframework that integrates large language models (LLMs) -- BERT, ALBERT,\nDistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.\nUnlike prior approaches, which rely on manually engineered features or\nconventional anomaly detection models, APT-LLM leverages LLMs to encode\nprocess-action provenance traces into semantically rich embeddings, capturing\nnuanced behavioral patterns. These embeddings are analyzed using three\nautoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder\n(VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and\nidentify anomalies. The best-performing model is selected for comparison\nagainst traditional methods. The framework is evaluated on real-world, highly\nimbalanced provenance trace datasets from the DARPA Transparent Computing\nprogram, where APT-like attacks constitute as little as 0.004\\% of the data\nacross multiple operating systems (Android, Linux, BSD, and Windows) and attack\nscenarios. Results demonstrate that APT-LLM significantly improves detection\nperformance under extreme imbalance conditions, outperforming existing anomaly\ndetection methods and highlighting the effectiveness of LLM-based feature\nextraction in cybersecurity.\n","authors":["Sidahmed Benabderrahmane","Petko Valtchev","James Cheney","Talal Rahwan"],"pdf_url":"https://arxiv.org/pdf/2502.09385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09251v1","updated":"2025-02-13T12:04:53Z","published":"2025-02-13T12:04:53Z","title":"Recipe: Hardware-Accelerated Replication Protocols","summary":"  Replication protocols are essential for distributed systems, ensuring\nconsistency, reliability, and fault tolerance. Traditional Crash Fault Tolerant\n(CFT) protocols, which assume a fail-stop model, are inadequate for untrusted\ncloud environments where adversaries or software bugs can cause Byzantine\nbehavior. Byzantine Fault Tolerant (BFT) protocols address these threats but\nface significant performance, resource overheads, and scalability challenges.\nThis paper introduces Recipe, a novel approach to transforming CFT protocols to\noperate securely in Byzantine settings without altering their core logic.\nRecipe rethinks CFT protocols in the context of modern cloud hardware,\nincluding many-core servers, RDMA-capable networks, and Trusted Execution\nEnvironments (TEEs). The approach leverages these advancements to enhance the\nsecurity and performance of replication protocols in untrusted cloud\nenvironments. Recipe implements two practical security mechanisms, i.e.,\ntransferable authentication and non-equivocation, using TEEs and\nhigh-performance networking stacks (e.g., RDMA, DPDK). These mechanisms ensure\nthat any CFT protocol can be transformed into a BFT protocol, guaranteeing\nauthenticity and non-equivocation. The Recipe protocol consists of five key\ncomponents: transferable authentication, initialization, normal operation, view\nchange, and recovery phases. The protocol's correctness is formally verified\nusing Tamarin, a symbolic model checker. Recipe is implemented as a library and\napplied to transform four widely used CFT protocols-Raft, Chain Replication,\nABD, and AllConcur-into Byzantine settings. The results demonstrate up to 24x\nhigher throughput compared to PBFT and 5.9x better performance than\nstate-of-the-art BFT protocols. Additionally, Recipe requires fewer replicas\nand offers confidentiality, a feature absent in traditional BFT protocols.\n","authors":["Dimitra Giantsidi","Emmanouil Giortamis","Julian Pritzi","Maurice Bailleu","Manos Kapritsos","Pramod Bhatotia"],"pdf_url":"https://arxiv.org/pdf/2502.09251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09225v1","updated":"2025-02-13T11:51:37Z","published":"2025-02-13T11:51:37Z","title":"A Coq Formalization of Unification Modulo Exclusive-Or","summary":"  Equational Unification is a critical problem in many areas such as automated\ntheorem proving and security protocol analysis. In this paper, we focus on\nXOR-Unification, that is, unification modulo the theory of exclusive-or. This\ntheory contains an operator with the properties Associativity, Commutativity,\nNilpotency, and the presence of an identity. In the proof assistant Coq, we\nimplement an algorithm that solves XOR unification problems, whose design was\ninspired by Liu and Lynch, and prove it sound, complete, and terminating. Using\nCoq's code extraction capability we obtain an implementation in the programming\nlanguage OCaml.\n","authors":["Yichi Xu","Daniel J. Dougherty","Rose Bohrer"],"pdf_url":"https://arxiv.org/pdf/2502.09225v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2408.10053v2","updated":"2025-02-13T11:43:39Z","published":"2024-08-19T14:48:04Z","title":"Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory","summary":"  Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.\n","authors":["Haoran Li","Wei Fan","Yulin Chen","Jiayang Cheng","Tianshu Chu","Xuebing Zhou","Peizhao Hu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.10053v2.pdf","comment":"To appear at NAACL 25"},{"id":"http://arxiv.org/abs/2502.09201v1","updated":"2025-02-13T11:39:58Z","published":"2025-02-13T11:39:58Z","title":"Commitment Schemes from OWFs with Applications to qOT","summary":"  Commitment schemes are essential to many cryptographic protocols and schemes\nwith applications that include privacy-preserving computation on data,\nprivacy-preserving authentication, and, in particular, oblivious transfer\nprotocols. For quantum oblivious transfer (qOT) protocols, unconditionally\nbinding commitment schemes that do not rely on hardness assumptions from\nstructured mathematical problems are required. These additional constraints\nseverely limit the choice of commitment schemes to random oracle-based\nconstructions or Naor's bit commitment scheme. As these protocols commit to\nindividual bits, the use of such commitment schemes comes at a high bandwidth\nand computational cost.\n  In this work, we investigate improvements to the efficiency of commitment\nschemes used in qOT protocols and propose an extension of Naor's commitment\nscheme requiring the existence of one-way functions (OWF) to reduce\ncommunication complexity for 2-bit strings. Additionally, we provide an\ninteractive string commitment scheme with preprocessing to enable a fast and\nefficient computation of commitments.\n","authors":["Thomas Lornser","Sebastian Ramarcher","Federico Valbusa"],"pdf_url":"https://arxiv.org/pdf/2502.09201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09175v1","updated":"2025-02-13T11:05:55Z","published":"2025-02-13T11:05:55Z","title":"FLAME: Flexible LLM-Assisted Moderation Engine","summary":"  The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.\n","authors":["Ivan Bakulin","Ilia Kopanichuk","Iaroslav Bespalov","Nikita Radchenko","Vladimir Shaposhnikov","Dmitry Dylov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2502.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09139v1","updated":"2025-02-13T10:14:19Z","published":"2025-02-13T10:14:19Z","title":"Zebrafix: Mitigating Memory-Centric Side-Channel Leakage via\n  Interleaving","summary":"  Constant-time code has become the de-facto standard for secure cryptographic\nimplementations. However, some memory-based leakage classes such as ciphertext\nside-channels, silent stores, and data memory-dependent prefetching remain\nunaddressed. In the context of ciphertext side-channel mitigations, the\npracticality of interleaving data with counter values remains to be explored.\nTo close this gap, we define design choices and requirements to leverage\ninterleaving for a generic ciphertext side-channel mitigation. Based on these\nresults, we implement Zebrafix, a compiler-based tool to ensure freshness of\nmemory stores. We evaluate Zebrafix and find that interleaving can perform much\nbetter than other ciphertext side-channel mitigations, at the cost of a high\npractical complexity. We further observe that ciphertext side-channels, silent\nstores and data memory-dependent prefetching belong to a broader attack\ncategory: memory-centric side-channels. Under this unified view, we discuss to\nwhat extent ciphertext side-channel mitigations can be adapted to prevent all\nthree memory-centric side-channel attacks via interleaving.\n","authors":["Anna Ptschke","Jan Wichelmann","Thomas Eisenbarth"],"pdf_url":"https://arxiv.org/pdf/2502.09139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09117v1","updated":"2025-02-13T09:53:00Z","published":"2025-02-13T09:53:00Z","title":"In Specs we Trust? Conformance-Analysis of Implementation to\n  Specifications in Node-RED and Associated Security Risks","summary":"  Low-code development frameworks for IoT platforms offer a simple\ndrag-and-drop mechanism to create applications for the billions of existing IoT\ndevices without the need for extensive programming knowledge. The security of\nsuch software is crucial given the close integration of IoT devices in many\nhighly sensitive areas such as healthcare or home automation. Node-RED is such\na framework, where applications are built from nodes that are contributed by\nopen-source developers. Its reliance on unvetted open-source contributions and\nlack of security checks raises the concern that the applications could be\nvulnerable to attacks, thereby imposing a security risk to end users. The\nlow-code approach suggests, that many users could lack the technical knowledge\nto mitigate, understand, or even realize such security concerns. This paper\nfocuses on \"hidden\" information flows in Node-RED nodes, meaning flows that are\nnot captured by the specifications. They could (unknowingly or with malicious\nintent) cause leaks of sensitive information to unauthorized entities. We\nreport the results of a conformance analysis of all nodes in the Node-RED\nframework, for which we compared the numbers of specified inputs and outputs of\neach node against the number of sources and sinks detected with CodeQL. The\nresults show, that 55% of all nodes exhibit more possible flows than are\nspecified. A risk assessment of a subset of the nodes showed, that 28% of them\nare associated with a high severity and 36% with a medium severity rating.\n","authors":["Simon Schneider","Komal Kashish","Katja Tuma","Riccardo Scandariato"],"pdf_url":"https://arxiv.org/pdf/2502.09117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06099v2","updated":"2025-02-13T09:35:44Z","published":"2024-06-10T08:34:13Z","title":"Sequential Binary Classification for Intrusion Detection","summary":"  Network Intrusion Detection Systems (IDS) have become increasingly important\nas networks become more vulnerable to new and sophisticated attacks. Machine\nLearning (ML)-based IDS are increasingly seen as the most effective approach to\nhandle this issue. However, IDS datasets suffer from high class imbalance,\nwhich impacts the performance of standard ML models. Different from existing\ndata-driven techniques to handling class imbalance, this paper explores a\nstructural approach to handling class imbalance in multi-class classification\n(MCC) problems. The proposed approach - Sequential Binary Classification (SBC),\nis a hierarchical cascade of (regular) binary classifiers. Experiments on\nbenchmark IDS datasets demonstrate that the structural approach to handling\nclass-imbalance, as exemplified by SBC, is a viable approach to handling the\nissue.\n","authors":["Shrihari Vasudevan","Ishan Chokshi","Raaghul Ranganathan","Nachiappan Sundaram"],"pdf_url":"https://arxiv.org/pdf/2406.06099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09084v1","updated":"2025-02-13T08:59:04Z","published":"2025-02-13T08:59:04Z","title":"Application of Tabular Transformer Architectures for Operating System\n  Fingerprinting","summary":"  Operating System (OS) fingerprinting is essential for network management and\ncybersecurity, enabling accurate device identification based on network traffic\nanalysis. Traditional rule-based tools such as Nmap and p0f face challenges in\ndynamic environments due to frequent OS updates and obfuscation techniques.\nWhile Machine Learning (ML) approaches have been explored, Deep Learning (DL)\nmodels, particularly Transformer architectures, remain unexploited in this\ndomain. This study investigates the application of Tabular Transformer\narchitectures-specifically TabTransformer and FT-Transformer-for OS\nfingerprinting, leveraging structured network data from three publicly\navailable datasets. Our experiments demonstrate that FT-Transformer generally\noutperforms traditional ML models, previous approaches and TabTransformer\nacross multiple classification levels (OS family, major, and minor versions).\nThe results establish a strong foundation for DL-based OS fingerprinting,\nimproving accuracy and adaptability in complex network environments.\nFurthermore, we ensure the reproducibility of our research by providing an\nopen-source implementation.\n","authors":["Rubn Prez-Jove","Cristian R. Munteanu","Alejandro Pazos","Jose Vzquez-Naya"],"pdf_url":"https://arxiv.org/pdf/2502.09084v1.pdf","comment":"Submitted as a preprint (not peer reviewed). 22 pages, 9 figures.\n  Code and datasets available at:\n  https://github.com/rubenpjove/tabularT-OS-fingerprinting"},{"id":"http://arxiv.org/abs/2502.08989v1","updated":"2025-02-13T06:01:09Z","published":"2025-02-13T06:01:09Z","title":"RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency\n  Detection in Privacy-Preserving Federated Learning","summary":"  Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security.\n","authors":["Nazatul H. Sultan","Yan Bo","Yansong Gao","Seyit Camtepe","Arash Mahboubi","Hang Thanh Bui","Aufeef Chauhan","Hamed Aboutorab","Michael Bewong","Praveen Gauravaram","Rafiqul Islam","Sharif Abuadbba"],"pdf_url":"https://arxiv.org/pdf/2502.08989v1.pdf","comment":"16 pages, 10 Figures"},{"id":"http://arxiv.org/abs/2502.08970v1","updated":"2025-02-13T05:18:24Z","published":"2025-02-13T05:18:24Z","title":"A Decade of Metric Differential Privacy: Advancements and Applications","summary":"  Metric Differential Privacy (mDP) builds upon the core principles of\nDifferential Privacy (DP) by incorporating various distance metrics, which\noffer adaptable and context-sensitive privacy guarantees for a wide range of\napplications, such as location-based services, text analysis, and image\nprocessing. Since its inception in 2013, mDP has garnered substantial research\nattention, advancing theoretical foundations, algorithm design, and practical\nimplementations. Despite this progress, existing surveys mainly focus on\ntraditional DP and local DP, and they provide limited coverage of mDP. This\npaper provides a comprehensive survey of mDP research from 2013 to 2024,\ntracing its development from the foundations of DP. We categorize essential\nmechanisms, including Laplace, Exponential, and optimization-based approaches,\nand assess their strengths, limitations, and application domains. Additionally,\nwe highlight key challenges and outline future research directions to encourage\ninnovation and real-world adoption of mDP. This survey is designed to be a\nvaluable resource for researchers and practitioners aiming to deepen their\nunderstanding and drive progress in mDP within the broader privacy ecosystem.\n","authors":["Xinpeng Xie","Chenyang Yu","Yan Huang","Yang Cao","Chenxi Qiu"],"pdf_url":"https://arxiv.org/pdf/2502.08970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08966v1","updated":"2025-02-13T05:06:22Z","published":"2025-02-13T05:06:22Z","title":"RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage","summary":"  Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.\n","authors":["Peter Yong Zhong","Siyuan Chen","Ruiqi Wang","McKenna McCall","Ben L. Titzer","Heather Miller"],"pdf_url":"https://arxiv.org/pdf/2502.08966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08921v1","updated":"2025-02-13T03:15:18Z","published":"2025-02-13T03:15:18Z","title":"Detecting Malicious Concepts Without Image Generation in AIGC","summary":"  The task of text-to-image generation has achieved tremendous success in\npractice, with emerging concept generation models capable of producing highly\npersonalized and customized content. Fervor for concept generation is\nincreasing rapidly among users, and platforms for concept sharing have sprung\nup. The concept owners may upload malicious concepts and disguise them with\nnon-malicious text descriptions and example images to deceive users into\ndownloading and generating malicious content. The platform needs a quick method\nto determine whether a concept is malicious to prevent the spread of malicious\nconcepts. However, simply relying on concept image generation to judge whether\na concept is malicious requires time and computational resources. Especially,\nas the number of concepts uploaded and downloaded on the platform continues to\nincrease, this approach becomes impractical and poses a risk of generating\nmalicious content. In this paper, we propose Concept QuickLook, the first\nsystematic work to incorporate malicious concept detection into research, which\nperforms detection based solely on concept files without generating any images.\nWe define malicious concepts and design two work modes for detection: concept\nmatching and fuzzy detection. Extensive experiments demonstrate that the\nproposed Concept QuickLook can detect malicious concepts and demonstrate\npracticality in concept sharing platforms. We also design robustness\nexperiments to further validate the effectiveness of the solution. We hope this\nwork can initiate malicious concept detection tasks and provide some\ninspiration.\n","authors":["Kun Xu","Yushu Zhang","Shuren Qi","Tao Wang","Wenying Wen","Yuming Fang"],"pdf_url":"https://arxiv.org/pdf/2502.08921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00626v3","updated":"2025-02-13T03:11:20Z","published":"2024-02-01T14:41:20Z","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","summary":"  Typographic attacks, adding misleading text to images, can deceive\nvision-language models (LVLMs). The susceptibility of recent large LVLMs like\nGPT4-V to such attacks is understudied, raising concerns about amplified\nmisinformation in personal assistant applications. Previous attacks use simple\nstrategies, such as random misleading words, which don't fully exploit LVLMs'\nlanguage reasoning abilities. We introduce an experimental setup for testing\ntypographic attacks on LVLMs and propose two novel self-generated attacks: (1)\nClass-based attacks, where the model identifies a similar class to deceive\nitself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack\ncombining a deceiving class and description. Our experiments show these attacks\nsignificantly reduce classification performance by up to 60\\% and are effective\nacross different models, including InstructBLIP and MiniGPT4. Code:\nhttps://github.com/mqraitem/Self-Gen-Typo-Attack\n","authors":["Maan Qraitem","Nazia Tasnim","Piotr Teterwak","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2402.00626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19394v2","updated":"2025-02-13T03:00:18Z","published":"2024-12-27T01:00:23Z","title":"An Engorgio Prompt Makes Large Language Model Babble on","summary":"  Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is released at: https://github.com/jianshuod/Engorgio-prompt.\n","authors":["Jianshuo Dong","Ziyuan Zhang","Qingjie Zhang","Tianwei Zhang","Hao Wang","Hewu Li","Qi Li","Chao Zhang","Ke Xu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.19394v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.13252v2","updated":"2025-02-13T02:40:52Z","published":"2025-01-22T22:18:50Z","title":"Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning","summary":"  In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2501.13252v2.pdf","comment":"31 pages, 17 figures"},{"id":"http://arxiv.org/abs/2502.08889v1","updated":"2025-02-13T02:05:45Z","published":"2025-02-13T02:05:45Z","title":"Linear-Time User-Level DP-SCO via Robust Statistics","summary":"  User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.\n","authors":["Badih Ghazi","Ravi Kumar","Daogao Liu","Pasin Manurangsi"],"pdf_url":"https://arxiv.org/pdf/2502.08889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08886v1","updated":"2025-02-13T01:55:43Z","published":"2025-02-13T01:55:43Z","title":"Generative AI for Internet of Things Security: Challenges and\n  Opportunities","summary":"  As Generative AI (GenAI) continues to gain prominence and utility across\nvarious sectors, their integration into the realm of Internet of Things (IoT)\nsecurity evolves rapidly. This work delves into an examination of the\nstate-of-the-art literature and practical applications on how GenAI could\nimprove and be applied in the security landscape of IoT. Our investigation aims\nto map the current state of GenAI implementation within IoT security, exploring\ntheir potential to fortify security measures further. Through the compilation,\nsynthesis, and analysis of the latest advancements in GenAI technologies\napplied to IoT, this paper not only introduces fresh insights into the field,\nbut also lays the groundwork for future research directions. It explains the\nprevailing challenges within IoT security, discusses the effectiveness of GenAI\nin addressing these issues, and identifies significant research gaps through\nMITRE Mitigations. Accompanied with three case studies, we provide a\ncomprehensive overview of the progress and future prospects of GenAI\napplications in IoT security. This study serves as a foundational resource to\nimprove IoT security through the innovative application of GenAI, thus\ncontributing to the broader discourse on IoT security and technology\nintegration.\n","authors":["Yan Lin Aung","Ivan Christian","Ye Dong","Xiaodong Ye","Sudipta Chattopadhyay","Jianying Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08878v1","updated":"2025-02-13T01:27:11Z","published":"2025-02-13T01:27:11Z","title":"Scalable Private Partition Selection via Adaptive Weighting","summary":"  In the differentially private partition selection problem (a.k.a. private set\nunion, private key discovery), users hold subsets of items from an unbounded\nuniverse. The goal is to output as many items as possible from the union of the\nusers' sets while maintaining user-level differential privacy. Solutions to\nthis problem are a core building block for many privacy-preserving ML\napplications including vocabulary extraction in a private corpus, computing\nstatistics over categorical data, and learning embeddings over user-provided\nitems.\n  We propose an algorithm for this problem, MaximumAdaptiveDegree (MAD), which\nadaptively reroutes weight from items with weight far above the threshold\nneeded for privacy to items with smaller weight, thereby increasing the\nprobability that less frequent items are output. Our algorithm can be\nefficiently implemented in massively parallel computation systems allowing\nscalability to very large datasets. We prove that our algorithm stochastically\ndominates the standard parallel algorithm for this problem. We also develop a\ntwo-round version of our algorithm where results of the computation in the\nfirst round are used to bias the weighting in the second round to maximize the\nnumber of items output. In experiments, our algorithms provide the best results\nacross the board among parallel algorithms and scale to datasets with hundreds\nof billions of items, up to three orders of magnitude larger than those\nanalyzed by prior sequential algorithms.\n","authors":["Justin Y. Chen","Vincent Cohen-Addad","Alessandro Epasto","Morteza Zadimoghaddam"],"pdf_url":"https://arxiv.org/pdf/2502.08878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08865v1","updated":"2025-02-13T00:34:21Z","published":"2025-02-13T00:34:21Z","title":"Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic\n  Attacks","summary":"  Extended Reality (XR) experiences involve interactions between users, the\nreal world, and virtual content. A key step to enable these experiences is the\nXR headset sensing and estimating the user's pose in order to accurately place\nand render virtual content in the real world. XR headsets use multiple sensors\n(e.g., cameras, inertial measurement unit) to perform pose estimation and\nimprove its robustness, but this provides an attack surface for adversaries to\ninterfere with the pose estimation process. In this paper, we create and study\nthe effects of acoustic attacks that create false signals in the inertial\nmeasurement unit (IMU) on XR headsets, leading to adverse downstream effects on\nXR applications. We generate resonant acoustic signals on a HoloLens 2 and\nmeasure the resulting perturbations in the IMU readings, and also demonstrate\nboth fine-grained and coarse attacks on the popular ORB-SLAM3 and an\nopen-source XR system (ILLIXR). With the knowledge gleaned from attacking these\nopen-source frameworks, we demonstrate four end-to-end proof-of-concept attacks\non a HoloLens 2: manipulating user input, clickjacking, zone invasion, and\ndenial of user interaction. Our experiments show that current commercial XR\nheadsets are susceptible to acoustic attacks, raising concerns for their\nsecurity.\n","authors":["Zijian Huang","Yicheng Zhang","Sophie Chen","Nael Abu-Ghazaleh","Jiasi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08865v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13904v3","updated":"2025-02-13T18:58:14Z","published":"2025-01-23T18:34:09Z","title":"Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.\n","authors":["Linh Tran","Wei Sun","Stacy Patterson","Ana Milanova"],"pdf_url":"https://arxiv.org/pdf/2501.13904v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09609v1","updated":"2025-02-13T18:57:20Z","published":"2025-02-13T18:57:20Z","title":"Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple","summary":"  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n","authors":["Tejas Jayashankar","J. Jon Ryu","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2502.09609v1.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09591v1","updated":"2025-02-13T18:48:04Z","published":"2025-02-13T18:48:04Z","title":"Censor Dependent Variational Inference","summary":"  This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions.\n","authors":["Chuanhui Liu","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09587v1","updated":"2025-02-13T18:45:56Z","published":"2025-02-13T18:45:56Z","title":"Rolling Ahead Diffusion for Traffic Scene Simulation","summary":"  Realistic driving simulation requires that NPCs not only mimic natural\ndriving behaviors but also react to the behavior of other simulated agents.\nRecent developments in diffusion-based scenario generation focus on creating\ndiverse and realistic traffic scenarios by jointly modelling the motion of all\nthe agents in the scene. However, these traffic scenarios do not react when the\nmotion of agents deviates from their modelled trajectories. For example, the\nego-agent can be controlled by a stand along motion planner. To produce\nreactive scenarios with joint scenario models, the model must regenerate the\nscenario at each timestep based on new observations in a Model Predictive\nControl (MPC) fashion. Although reactive, this method is time-consuming, as one\ncomplete possible future for all NPCs is generated per simulation step.\nAlternatively, one can utilize an autoregressive model (AR) to predict only the\nimmediate next-step future for all NPCs. Although faster, this method lacks the\ncapability for advanced planning. We present a rolling diffusion based traffic\nscene generation model which mixes the benefits of both methods by predicting\nthe next step future and simultaneously predicting partially noised further\nfuture steps at the same time. We show that such model is efficient compared to\ndiffusion model based AR, achieving a beneficial compromise between reactivity\nand computational efficiency.\n","authors":["Yunpeng Liu","Matthew Niedoba","William Harvey","Adam Scibior","Berend Zwartsenberg","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2502.09587v1.pdf","comment":"Accepted to Workshop on Machine Learning for Autonomous Driving at\n  AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09583v1","updated":"2025-02-13T18:41:55Z","published":"2025-02-13T18:41:55Z","title":"Learning to Coordinate with Experts","summary":"  When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.\n","authors":["Mohamad H. Danesh","Tu Trinh","Benjamin Plaut","Nguyen X. Khanh"],"pdf_url":"https://arxiv.org/pdf/2502.09583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20092v2","updated":"2025-02-13T18:38:13Z","published":"2024-10-26T06:06:08Z","title":"OGBench: Benchmarking Offline Goal-Conditioned RL","summary":"  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n","authors":["Seohong Park","Kevin Frans","Benjamin Eysenbach","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.20092v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09571v1","updated":"2025-02-13T18:29:48Z","published":"2025-02-13T18:29:48Z","title":"DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra","summary":"  Mass spectrometry plays a fundamental role in elucidating the structures of\nunknown molecules and subsequent scientific discoveries. One formulation of the\nstructure elucidation task is the conditional $\\textit{de novo}$ generation of\nmolecular structure given a mass spectrum. Toward a more accurate and efficient\nscientific discovery pipeline for small molecules, we present DiffMS, a\nformula-restricted encoder-decoder generative network that achieves\nstate-of-the-art performance on this task. The encoder utilizes a transformer\narchitecture and models mass spectra domain knowledge such as peak formulae and\nneutral losses, and the decoder is a discrete graph diffusion model restricted\nby the heavy-atom composition of a known chemical formula. To develop a robust\ndecoder that bridges latent embeddings and molecular structures, we pretrain\nthe diffusion decoder with fingerprint-structure pairs, which are available in\nvirtually infinite quantities, compared to structure-spectrum pairs that number\nin the tens of thousands. Extensive experiments on established benchmarks show\nthat DiffMS outperforms existing models on $\\textit{de novo}$ molecule\ngeneration. We provide several ablations to demonstrate the effectiveness of\nour diffusion and pretraining approaches and show consistent performance\nscaling with increasing pretraining dataset size. DiffMS code is publicly\navailable at https://github.com/coleygroup/DiffMS.\n","authors":["Montgomery Bohde","Mrunali Manjrekar","Runzhong Wang","Shuiwang Ji","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2502.09571v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09570v1","updated":"2025-02-13T18:28:17Z","published":"2025-02-13T18:28:17Z","title":"Enhancing the Utility of Higher-Order Information in Relational Learning","summary":"  Higher-order information is crucial for relational learning in many domains\nwhere relationships extend beyond pairwise interactions. Hypergraphs provide a\nnatural framework for modeling such relationships, which has motivated recent\nextensions of graph neural net- work architectures to hypergraphs. However,\ncomparisons between hypergraph architectures and standard graph-level models\nremain limited. In this work, we systematically evaluate a selection of\nhypergraph-level and graph-level architectures, to determine their\neffectiveness in leveraging higher-order information in relational learning.\nOur results show that graph-level architectures applied to hypergraph\nexpansions often outperform hypergraph- level ones, even on inputs that are\nnaturally parametrized as hypergraphs. As an alternative approach for\nleveraging higher-order information, we propose hypergraph-level encodings\nbased on classical hypergraph characteristics. While these encodings do not\nsignificantly improve hypergraph architectures, they yield substantial\nperformance gains when combined with graph-level models. Our theoretical\nanalysis shows that hypergraph-level encodings provably increase the\nrepresentational power of message-passing graph neural networks beyond that of\ntheir graph-level counterparts.\n","authors":["Raphael Pellegrin","Lukas Fesser","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2502.09570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v2","updated":"2025-02-13T18:24:40Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Aram Ebtekar","Yuhao Wang","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10238v2","updated":"2025-02-13T18:22:34Z","published":"2024-07-14T15:11:13Z","title":"Asymptotic Normality of Generalized Low-Rank Matrix Sensing via\n  Riemannian Geometry","summary":"  We prove an asymptotic normality guarantee for generalized low-rank matrix\nsensing -- i.e., matrix sensing under a general convex loss $\\bar\\ell(\\langle\nX,M\\rangle,y^*)$, where $M\\in\\mathbb{R}^{d\\times d}$ is the unknown rank-$k$\nmatrix, $X$ is a measurement matrix, and $y^*$ is the corresponding\nmeasurement. Our analysis relies on tools from Riemannian geometry to handle\ndegeneracy of the Hessian of the loss due to rotational symmetry in the\nparameter space. In particular, we parameterize the manifold of low-rank\nmatrices by $\\bar\\theta\\bar\\theta^\\top$, where\n$\\bar\\theta\\in\\mathbb{R}^{d\\times k}$. Then, assuming the minimizer of the\nempirical loss $\\bar\\theta^0\\in\\mathbb{R}^{d\\times k}$ is in a constant size\nball around the true parameters $\\bar\\theta^*$, we prove\n$\\sqrt{n}(\\phi^0-\\phi^*)\\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\\to\\infty$, where\n$\\phi^0$ and $\\phi^*$ are representations of $\\bar\\theta^*$ and $\\bar\\theta^0$\nin the horizontal space of the Riemannian quotient manifold\n$\\mathbb{R}^{d\\times k}/\\text{O}(k)$, and $H^*$ is the Hessian of the true loss\nin the same representation.\n","authors":["Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2407.10238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.07864v2","updated":"2025-02-13T18:07:04Z","published":"2025-02-11T18:20:18Z","title":"TransMLA: Multi-Head Latent Attention Is All You Need","summary":"  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n","authors":["Fanxu Meng","Zengwei Yao","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07864v2.pdf","comment":"https://github.com/fxmeng/TransMLA"},{"id":"http://arxiv.org/abs/2502.09553v1","updated":"2025-02-13T18:05:12Z","published":"2025-02-13T18:05:12Z","title":"SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops","summary":"  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n","authors":["Eshaq Jamdar","Amith Kamath Belman"],"pdf_url":"https://arxiv.org/pdf/2502.09553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v3","updated":"2025-02-13T17:57:28Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel Pduraru","Antonio Brblau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v3.pdf","comment":"8 pages, 4 figures, 6 tables, under review"},{"id":"http://arxiv.org/abs/2502.09534v1","updated":"2025-02-13T17:50:27Z","published":"2025-02-13T17:50:27Z","title":"Fast Tensor Completion via Approximate Richardson Iteration","summary":"  We study tensor completion (TC) through the lens of low-rank tensor\ndecomposition (TD). Many TD algorithms use fast alternating minimization\nmethods, which solve highly structured linear regression problems at each step\n(e.g., for CP, Tucker, and tensor-train decompositions). However, such\nalgebraic structure is lost in TC regression problems, making direct extensions\nunclear. To address this, we propose a lifting approach that approximately\nsolves TC regression problems using structured TD regression algorithms as\nblackbox subroutines, enabling sublinear-time methods. We theoretically analyze\nthe convergence rate of our approximate Richardson iteration based algorithm,\nand we demonstrate on real-world tensors that its running time can be 100x\nfaster than direct methods for CP completion.\n","authors":["Mehrdad Ghadiri","Matthew Fahrbach","Yunbum Kook","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2502.09534v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.09525v1","updated":"2025-02-13T17:37:42Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09511v1","updated":"2025-02-13T17:22:50Z","published":"2025-02-13T17:22:50Z","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","summary":"  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n","authors":["Liang Wang","Chao Song","Zhiyuan Liu","Yu Rong","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09509v1","updated":"2025-02-13T17:21:51Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09503v1","updated":"2025-02-13T17:15:26Z","published":"2025-02-13T17:15:26Z","title":"AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization","summary":"  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n","authors":["Caleb Cranney","Jesse G. Meyer"],"pdf_url":"https://arxiv.org/pdf/2502.09503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09502v1","updated":"2025-02-13T17:14:18Z","published":"2025-02-13T17:14:18Z","title":"Scalable First-order Method for Certifying Optimal k-Sparse GLMs","summary":"  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n","authors":["Jiachang Liu","Soroosh Shafiee","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2502.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09500v1","updated":"2025-02-13T17:10:43Z","published":"2025-02-13T17:10:43Z","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting","summary":"  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.\n","authors":["Nicholas Dronen","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.09500v1.pdf","comment":"16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training"},{"id":"http://arxiv.org/abs/2501.14346v2","updated":"2025-02-13T17:03:04Z","published":"2025-01-24T09:17:57Z","title":"HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks","summary":"  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n","authors":["Boshko Koloski","Nada Lavra","Bla krlj"],"pdf_url":"https://arxiv.org/pdf/2501.14346v2.pdf","comment":"Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution"},{"id":"http://arxiv.org/abs/2502.09496v1","updated":"2025-02-13T17:03:03Z","published":"2025-02-13T17:03:03Z","title":"On Agnostic PAC Learning in the Small Error Regime","summary":"  Binary classification in the classic PAC model exhibits a curious phenomenon:\nEmpirical Risk Minimization (ERM) learners are suboptimal in the realizable\ncase yet optimal in the agnostic case. Roughly speaking, this owes itself to\nthe fact that non-realizable distributions $\\mathcal{D}$ are simply more\ndifficult to learn than realizable distributions -- even when one discounts a\nlearner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best\nhypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners\nare permitted to incur excess error on (easier-to-learn) distributions\n$\\mathcal{D}$ for which $\\tau = \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this\nshortcoming by including $\\tau$ itself as a parameter in the agnostic error\nterm. In this more fine-grained model, they demonstrate tightness of the error\nlower bound $\\tau + \\Omega \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} +\n\\frac{d + \\log(1 / \\delta)}{m} \\right)$ in a regime where $\\tau > d/m$, and\nleave open the question of whether there may be a higher lower bound when $\\tau\n\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we\nresolve this question by exhibiting a learner which achieves error $c \\cdot\n\\tau + O \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} + \\frac{d + \\log(1\n/ \\delta)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower\nbound when $\\tau \\approx d/m$. Further, our learner is computationally\nefficient and is based upon careful aggregations of ERM classifiers, making\nprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS\n`24). We leave open the interesting question of whether our approach can be\nrefined to lower the constant from 2.1 to 1, which would completely settle the\ncomplexity of agnostic learning.\n","authors":["Julian Asilis","Mikael Mller Hgsgaard","Grigoris Velegkas"],"pdf_url":"https://arxiv.org/pdf/2502.09496v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2502.09495v1","updated":"2025-02-13T17:01:45Z","published":"2025-02-13T17:01:45Z","title":"Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence","summary":"  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n","authors":["Pierre Beaucoral"],"pdf_url":"https://arxiv.org/pdf/2502.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09494v1","updated":"2025-02-13T17:00:11Z","published":"2025-02-13T17:00:11Z","title":"Communicating Likelihoods with Normalising Flows","summary":"  We present a machine-learning-based workflow to model an unbinned likelihood\nfrom its samples. A key advancement over existing approaches is the validation\nof the learned likelihood using rigorous statistical tests of the joint\ndistribution, such as the Kolmogorov-Smirnov test of the joint distribution.\nOur method enables the reliable communication of experimental and\nphenomenological likelihoods for subsequent analyses. We demonstrate its\neffectiveness through three case studies in high-energy physics. To support\nbroader adoption, we provide an open-source reference implementation, nabu.\n","authors":["Jack Y. Araz","Anja Beck","Mril Reboud","Michael Spannowsky","Danny van Dyk"],"pdf_url":"https://arxiv.org/pdf/2502.09494v1.pdf","comment":"4 pages + references, 1 figure"},{"id":"http://arxiv.org/abs/2502.09490v1","updated":"2025-02-13T16:57:07Z","published":"2025-02-13T16:57:07Z","title":"Inverse Design with Dynamic Mode Decomposition","summary":"  We introduce a computationally efficient method for the automation of inverse\ndesign in science and engineering. Based on simple least-square regression, the\nunderlying dynamic mode decomposition algorithm can be used to construct a\nlow-rank subspace spanning multiple experiments in parameter space. The\nproposed inverse design dynamic mode composition (ID-DMD) algorithm leverages\nthe computed low-dimensional subspace to enable fast digital design and\noptimization on laptop-level computing, including the potential to prescribe\nthe dynamics themselves. Moreover, the method is robust to noise, physically\ninterpretable, and can provide uncertainty quantification metrics. The\narchitecture can also efficiently scale to large-scale design problems using\nrandomized algorithms in the ID-DMD. The simplicity of the method and its\nimplementation are highly attractive in practice, and the ID-DMD has been\ndemonstrated to be an order of magnitude more accurate than competing methods\nwhile simultaneously being 3-5 orders faster on challenging engineering design\nproblems ranging from structural vibrations to fluid dynamics. Due to its\nspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with\nother leading machine learning methods represents a significant advancement in\ndata-driven methods for inverse design and optimization, promising a paradigm\nshift in how to approach inverse design in practice.\n","authors":["Yunpeng Zhu","Liangliang Cheng","Anping Jing","Hanyu Huo","Ziqiang Lang","Bo Zhang","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2502.09490v1.pdf","comment":"29 pages, 19 figures"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2410.13879v2","updated":"2025-02-13T16:43:47Z","published":"2024-10-03T00:48:07Z","title":"Mixed-curvature decision trees and random forests","summary":"  Decision trees (DTs) and their random forest (RF) extensions are workhorses\nof classification and regression in Euclidean spaces. However, algorithms for\nlearning in non-Euclidean spaces are still limited. We extend DT and RF\nalgorithms to product manifolds: Cartesian products of several hyperbolic,\nhyperspherical, or Euclidean components. Such manifolds handle heterogeneous\ncurvature while still factorizing neatly into simpler components, making them\ncompelling embedding spaces for complex datasets. Our novel angular\nreformulation of DTs respects the geometry of the product manifold, yielding\nsplits that are geodesically convex, maximum-margin, and composable. In the\nspecial cases of single-component manifolds, our method simplifies to its\nEuclidean or hyperbolic counterparts, or introduces hyperspherical DT\nalgorithms, depending on the curvature. We benchmark our method on various\nclassification, regression, and link prediction tasks on synthetic data, graph\nembeddings, mixed-curvature variational autoencoder latent spaces, and\nempirical data. Compared to 7 other classifiers, product RFs ranked first on 25\nout of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights\nthe value of product RFs as straightforward yet powerful new tools for data\nanalysis in product manifolds. Code for our paper is available at\nhttps://github.com/pchlenski/manify.\n","authors":["Philippe Chlenski","Quentin Chu","Raiyan R. Khan","Kaizhu Du","Antonio Khalil Moretti","Itsik Pe'er"],"pdf_url":"https://arxiv.org/pdf/2410.13879v2.pdf","comment":"27 pages, 11 figures. Submitted to ICML 2025"},{"id":"http://arxiv.org/abs/2502.09479v1","updated":"2025-02-13T16:43:32Z","published":"2025-02-13T16:43:32Z","title":"Assessing Generative AI value in a public sector context: evidence from\n  a field experiment","summary":"  The emergence of Generative AI (Gen AI) has motivated an interest in\nunderstanding how it could be used to enhance productivity across various\ntasks. We add to research results for the performance impact of Gen AI on\ncomplex knowledge-based tasks in a public sector setting. In a pre-registered\nexperiment, after establishing a baseline level of performance, we find mixed\nevidence for two types of composite tasks related to document understanding and\ndata analysis. For the Documents task, the treatment group using Gen AI had a\n17% improvement in answer quality scores (as judged by human evaluators) and a\n34% improvement in task completion time compared to a control group. For the\nData task, we find the Gen AI treatment group experienced a 12% reduction in\nquality scores and no significant difference in mean completion time compared\nto the control group. These results suggest that the benefits of Gen AI may be\ntask and potentially respondent dependent. We also discuss field notes and\nlessons learned, as well as supplementary insights from a post-trial survey and\nfeedback workshop with participants.\n","authors":["Trevor Fitzpatrick","Seamus Kelly","Patrick Carey","David Walsh","Ruairi Nugent"],"pdf_url":"https://arxiv.org/pdf/2502.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16333v2","updated":"2025-02-13T16:41:13Z","published":"2024-10-19T15:42:49Z","title":"Conformal Predictive Portfolio Selection","summary":"  This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2410.16333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09473v1","updated":"2025-02-13T16:36:25Z","published":"2025-02-13T16:36:25Z","title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements","summary":"  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n","authors":["Alexander Jenkins","Andrea Cini","Joseph Barker","Alexander Sharp","Arunashis Sau","Varun Valentine","Srushti Valasang","Xinyang Li","Tom Wong","Timothy Betts","Danilo Mandic","Cesare Alippi","Fu Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.09473v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.20440v2","updated":"2025-02-13T16:35:17Z","published":"2024-09-30T16:00:23Z","title":"Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits","summary":"  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.\n","authors":["Mengmeng Li","Daniel Kuhn","Bahar Takesen"],"pdf_url":"https://arxiv.org/pdf/2409.20440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17163v2","updated":"2025-02-13T16:32:55Z","published":"2024-05-27T13:36:50Z","title":"Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep\n  Graph Networks","summary":"  The dynamics of information diffusion within graphs is a critical open issue\nthat heavily influences graph representation learning, especially when\nconsidering long-range propagation. This calls for principled approaches that\ncontrol and regulate the degree of propagation and dissipation of information\nthroughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian\nDeep Graph Networks, a novel framework that models neural information flow in\ngraphs by building on the laws of conservation of Hamiltonian dynamical\nsystems. We reconcile under a single theoretical and practical framework both\nnon-dissipative long-range propagation and non-conservative behaviors,\nintroducing tools from mechanical systems to gauge the equilibrium between the\ntwo components. Our approach can be applied to general message-passing\narchitectures, and it provides theoretical guarantees on information\nconservation in time. Empirical results prove the effectiveness of our\nport-Hamiltonian scheme in pushing simple graph convolutional architectures to\nstate-of-the-art performance in long-range benchmarks.\n","authors":["Simon Heilig","Alessio Gravina","Alessandro Trenta","Claudio Gallicchio","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2405.17163v2.pdf","comment":"Accepted at ICLR 2025 (https://openreview.net/forum?id=03EkqSCKuO)"},{"id":"http://arxiv.org/abs/2411.03263v2","updated":"2025-02-13T16:28:07Z","published":"2024-11-05T17:02:29Z","title":"Proxy-informed Bayesian transfer learning with unknown sources","summary":"  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (cannot \"fine-tune\"), and instead makes use of proxy\n(indirect) information. Our theoretical results show that the threat of\nnegative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n","authors":["Sabina J. Sloman","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.03263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09445v1","updated":"2025-02-13T16:15:43Z","published":"2025-02-13T16:15:43Z","title":"A Differentiable Rank-Based Objective For Better Feature Learning","summary":"  In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.\n","authors":["Krunoslav Lehman Pavasovic","David Lopez-Paz","Giulio Biroli","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2502.09445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19082v2","updated":"2025-02-13T16:14:34Z","published":"2025-01-31T12:15:58Z","title":"A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration","summary":"  Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.\n","authors":["Yuchen Hu","Xi Chen","Weidong Liu","Xiaojun Mao"],"pdf_url":"https://arxiv.org/pdf/2501.19082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09443v1","updated":"2025-02-13T16:12:17Z","published":"2025-02-13T16:12:17Z","title":"Relational Conformal Prediction for Correlated Time Series","summary":"  We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.\n","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2502.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.11415v2","updated":"2025-02-13T16:02:42Z","published":"2024-10-15T09:02:55Z","title":"KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI","summary":"  A popular approach to neurosymbolic AI involves mapping logic formulas to\narithmetic circuits (computation graphs consisting of sums and products) and\npassing the outputs of a neural network through these circuits. This approach\nenforces symbolic constraints onto a neural network in a principled and\nend-to-end differentiable way. Unfortunately, arithmetic circuits are\nchallenging to run on modern AI accelerators as they exhibit a high degree of\nirregular sparsity. To address this limitation, we introduce knowledge layers\n(KLay), a new data structure to represent arithmetic circuits that can be\nefficiently parallelized on GPUs. Moreover, we contribute two algorithms used\nin the translation of traditional circuit representations to KLay and a further\nalgorithm that exploits parallelization opportunities during circuit\nevaluations. We empirically show that KLay achieves speedups of multiple orders\nof magnitude over the state of the art, thereby paving the way towards scaling\nneurosymbolic AI to larger real-world applications.\n","authors":["Jaron Maene","Vincent Derkinderen","Pedro Zuidberg Dos Martires"],"pdf_url":"https://arxiv.org/pdf/2410.11415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09432v1","updated":"2025-02-13T15:55:00Z","published":"2025-02-13T15:55:00Z","title":"Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes","summary":"  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n","authors":["Navdeep Kumar","Adarsh Gupta","Maxence Mohamed Elfatihi","Giorgia Ramponi","Kfir Yehuda Levy","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08097v3","updated":"2025-02-13T15:53:59Z","published":"2024-05-13T18:24:03Z","title":"A Galois theorem for machine learning: Functions on symmetric matrices\n  and point clouds via lightweight invariant features","summary":"  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we provide a general construction of generically\nseparating invariant features using ideas inspired by Galois theory. We\nconstruct $O(n^2)$ invariant features derived from generators for the field of\nrational functions on $n\\times n$ symmetric matrices that are invariant under\njoint permutations of rows and columns. We show that these invariant features\ncan separate all distinct orbits of symmetric matrices except for a measure\nzero set; such features can be used to universally approximate invariant\nfunctions on almost all weighted graphs. For point clouds in a fixed dimension,\nwe prove that the number of invariant features can be reduced, generically\nwithout losing expressivity, to $O(n)$, where $n$ is the number of points. We\ncombine these invariant features with DeepSets to learn functions on symmetric\nmatrices and point clouds with varying sizes. We empirically demonstrate the\nfeasibility of our approach on molecule property regression and point cloud\ndistance prediction.\n","authors":["Ben Blum-Smith","Ningyuan Huang","Marco Cuturi","Soledad Villar"],"pdf_url":"https://arxiv.org/pdf/2405.08097v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01706v2","updated":"2025-02-13T15:43:25Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09417v1","updated":"2025-02-13T15:40:39Z","published":"2025-02-13T15:40:39Z","title":"A Survey of Reinforcement Learning for Optimization in Automation","summary":"  Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.\n","authors":["Ahmad Farooq","Kamran Iqbal"],"pdf_url":"https://arxiv.org/pdf/2502.09417v1.pdf","comment":"8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024"},{"id":"http://arxiv.org/abs/2410.08751v2","updated":"2025-02-13T15:36:19Z","published":"2024-10-11T12:10:51Z","title":"Zero-Shot Offline Imitation Learning via Optimal Transport","summary":"  Zero-shot imitation learning algorithms hold the promise of reproducing\nunseen behavior from as little as a single demonstration at test time. Existing\npractical approaches view the expert demonstration as a sequence of goals,\nenabling imitation with a high-level goal selector, and a low-level\ngoal-conditioned policy. However, this framework can suffer from myopic\nbehavior: the agent's immediate actions towards achieving individual goals may\nundermine long-term objectives. We introduce a novel method that mitigates this\nissue by directly optimizing the occupancy matching objective that is intrinsic\nto imitation learning. We propose to lift a goal-conditioned value function to\na distance between occupancies, which are in turn approximated via a learned\nworld model. The resulting method can learn from offline, suboptimal data, and\nis capable of non-myopic, zero-shot imitation, as we demonstrate in complex,\ncontinuous benchmarks.\n","authors":["Thomas Rupf","Marco Bagatella","Nico Grtler","Jonas Frey","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2410.08751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.09396v1","updated":"2025-02-13T15:16:53Z","published":"2025-02-13T15:16:53Z","title":"A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack","summary":"  Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.\n","authors":["Richard J. Preen","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2502.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09395v1","updated":"2025-02-13T15:16:52Z","published":"2025-02-13T15:16:52Z","title":"Robot Pouring: Identifying Causes of Spillage and Selecting Alternative\n  Action Parameters Using Probabilistic Actual Causation","summary":"  In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a\nlarge variety of objects and goals. When confronted with an unexpected or\nunwanted outcome, we take corrective actions and try again until achieving the\ndesired result. The reasoning performed to identify a cause of the observed\noutcome and to select an appropriate corrective action is a crucial aspect of\nhuman reasoning for successful task execution. Central to this reasoning is the\nassumption that a factor is responsible for producing the observed outcome. In\nthis paper, we investigate the use of probabilistic actual causation to\ndetermine whether a factor is the cause of an observed undesired outcome.\nFurthermore, we show how the actual causation probabilities can be used to find\nalternative actions to change the outcome. We apply the probabilistic actual\ncausation analysis to a robot pouring task. When spillage occurs, the analysis\nindicates whether a task parameter is the cause and how it should be changed to\navoid spillage. The analysis requires a causal graph of the task and the\ncorresponding conditional probability distributions. To fulfill these\nrequirements, we perform a complete causal modeling procedure (i.e., task\nanalysis, definition of variables, determination of the causal graph structure,\nand estimation of conditional probability distributions) using data from a\nrealistic simulation of the robot pouring task, covering a large combinatorial\nspace of task parameters. Based on the results, we discuss the implications of\nthe variables' representation and how the alternative actions suggested by the\nactual causation analysis would compare to the alternative solutions proposed\nby a human observer. The practical use of the analysis of probabilistic actual\ncausation to select alternative action parameters is demonstrated.\n","authors":["Jaime Maldonado","Jonas Krumme","Christoph Zetzsche","Vanessa Didelez","Kerstin Schill"],"pdf_url":"https://arxiv.org/pdf/2502.09395v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.14441v2","updated":"2025-02-13T14:52:43Z","published":"2025-01-24T12:22:18Z","title":"Impact of Batch Normalization on Convolutional Network Representations","summary":"  Batch normalization (BatchNorm) is a popular layer normalization technique\nused when training deep neural networks. It has been shown to enhance the\ntraining speed and accuracy of deep learning models. However, the mechanics by\nwhich BatchNorm achieves these benefits is an active area of research, and\ndifferent perspectives have been proposed. In this paper, we investigate the\neffect of BatchNorm on the resulting hidden representations, that is, the\nvectors of activation values formed as samples are processed at each hidden\nlayer. Specifically, we consider the sparsity of these representations, as well\nas their implicit clustering -- the creation of groups of representations that\nare similar to some extent. We contrast image classification models trained\nwith and without batch normalization and highlight consistent differences\nobserved. These findings highlight that BatchNorm's effect on representational\nsparsity is not a significant factor affecting generalization, while the\nrepresentations of models trained with BatchNorm tend to show more advantageous\nclustering characteristics.\n","authors":["Hermanus L. Potgieter","Coenraad Mouton","Marelie H. Davel"],"pdf_url":"https://arxiv.org/pdf/2501.14441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09376v1","updated":"2025-02-13T14:45:11Z","published":"2025-02-13T14:45:11Z","title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It\n  Fails Loudly (But it Probably Won't Fail)","summary":"  Low-rank adaptation (LoRA) has become a standard approach for fine-tuning\nlarge foundation models. However, our theoretical understanding of LoRA remains\nlimited as prior analyses of LoRA's training dynamics either rely on\nlinearization arguments or consider highly simplified setups. In this work, we\nanalyze the LoRA loss landscape without such restrictive assumptions. We define\ntwo regimes: a ``special regime'', which includes idealized setups where\nlinearization arguments hold, and a ``generic regime'' representing more\nrealistic setups where linearization arguments do not hold. In the generic\nregime, we show that LoRA training converges to a global minimizer with low\nrank and small magnitude, or a qualitatively distinct solution with high rank\nand large magnitude. Finally, we argue that the zero-initialization and weight\ndecay in LoRA training induce an implicit bias toward the low-rank,\nsmall-magnitude region of the parameter space -- where global minima lie --\nthus shedding light on why LoRA training usually succeeds in finding global\nminima.\n","authors":["Junsu Kim","Jaeyeon Kim","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2502.09376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09374v1","updated":"2025-02-13T14:43:22Z","published":"2025-02-13T14:43:22Z","title":"Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training","summary":"  Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3.\n","authors":["Toon Vinck","Nan Jonckers","Gert Dekkers","Jeffrey Prinzie","Peter Karsmakers"],"pdf_url":"https://arxiv.org/pdf/2502.09374v1.pdf","comment":"7 pages, 4 figures, Topical Workshop on Electronics for Particle\n  Physics"},{"id":"http://arxiv.org/abs/2502.07465v2","updated":"2025-02-13T14:38:24Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v2.pdf","comment":"The paper was submitted without the consent of all co-authors. The\n  content of the paper is incomplete and requires substantial additional work\n  before it can be considered a complete and coherent submission"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Kster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09365v1","updated":"2025-02-13T14:33:02Z","published":"2025-02-13T14:33:02Z","title":"Simple Path Structural Encoding for Graph Transformers","summary":"  Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.\n","authors":["Louis Airale","Antonio Longa","Mattia Rigon","Andrea Passerini","Roberto Passerone"],"pdf_url":"https://arxiv.org/pdf/2502.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09363v1","updated":"2025-02-13T14:31:49Z","published":"2025-02-13T14:31:49Z","title":"The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment\n  Weak Labeling for Events in Time","summary":"  Accurate labels are critical for deriving robust machine learning models.\nLabels are used to train supervised learning models and to evaluate most\nmachine learning paradigms. In this paper, we model the accuracy and cost of a\ncommon weak labeling process where annotators assign presence or absence labels\nto fixed-length data segments for a given event class. The annotator labels a\nsegment as \"present\" if it sufficiently covers an event from that class, e.g.,\na birdsong sound event in audio data. We analyze how the segment length affects\nthe label accuracy and the required number of annotations, and compare this\nfixed-length labeling approach with an oracle method that uses the true event\nactivations to construct the segments. Furthermore, we quantify the gap between\nthese methods and verify that in most realistic scenarios the oracle method is\nbetter than the fixed-length labeling method in both accuracy and cost. Our\nfindings provide a theoretical justification for adaptive weak labeling\nstrategies that mimic the oracle process, and a foundation for optimizing weak\nlabeling processes in sequence labeling tasks.\n","authors":["John Martinsson","Olof Mogren","Tuomas Virtanen","Maria Sandsten"],"pdf_url":"https://arxiv.org/pdf/2502.09363v1.pdf","comment":"Submitted to TMLR"},{"id":"http://arxiv.org/abs/2411.17287v2","updated":"2025-02-13T14:31:24Z","published":"2024-11-26T10:19:16Z","title":"Privacy-Preserving Federated Unsupervised Domain Adaptation for\n  Regression on Small-Scale and High-Dimensional Biological Data","summary":"  Machine learning models often struggle with generalization in small,\nheterogeneous datasets due to domain shifts caused by variations in data\ncollection and population differences. This challenge is particularly\npronounced in biological data, where data is high-dimensional, small-scale, and\ndecentralized across institutions. While federated domain adaptation methods\n(FDA) aim to address these challenges, most existing approaches rely on deep\nlearning and focus on classification tasks, making them unsuitable for\nsmall-scale, high-dimensional applications. In this work, we propose freda, a\nprivacy-preserving federated method for unsupervised domain adaptation in\nregression tasks. Unlike deep learning-based FDA approaches, freda is the first\nmethod to enable the federated training of Gaussian Processes to model complex\nfeature relationships while ensuring complete data privacy through randomized\nencoding and secure aggregation. This allows for effective domain adaptation\nwithout direct access to raw data, making it well-suited for applications\ninvolving high-dimensional, heterogeneous datasets. We evaluate freda on the\nchallenging task of age prediction from DNA methylation data, demonstrating\nthat it achieves performance comparable to the centralized state-of-the-art\nmethod while preserving complete data privacy.\n","authors":["Cem Ata Baykara","Ali Burak nal","Nico Pfeifer","Mete Akgn"],"pdf_url":"https://arxiv.org/pdf/2411.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.05366v2","updated":"2025-02-13T14:16:56Z","published":"2024-06-08T06:06:20Z","title":"Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator","summary":"  Risk-sensitive linear quadratic regulator is one of the most fundamental\nproblems in risk-sensitive optimal control. In this paper, we study online\nadaptive control of risk-sensitive linear quadratic regulator in the finite\nhorizon episodic setting. We propose a simple least-squares greedy algorithm\nand show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a\nspecific identifiability assumption, where $N$ is the total number of episodes.\nIf the identifiability assumption is not satisfied, we propose incorporating\nexploration noise into the least-squares-based algorithm, resulting in an\nalgorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. To our best\nknowledge, this is the first set of regret bounds for episodic risk-sensitive\nlinear quadratic regulator. Our proof relies on perturbation analysis of\nless-standard Riccati equations for risk-sensitive linear quadratic control,\nand a delicate analysis of the loss in the risk-sensitive performance criterion\ndue to applying the suboptimal controller in the online learning process.\n","authors":["Wenhao Xu","Xuefeng Gao","Xuedong He"],"pdf_url":"https://arxiv.org/pdf/2406.05366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09346v1","updated":"2025-02-13T14:11:33Z","published":"2025-02-13T14:11:33Z","title":"Machine learning for modelling unstructured grid data in computational\n  physics: a review","summary":"  Unstructured grid data are essential for modelling complex geometries and\ndynamics in computational physics. Yet, their inherent irregularity presents\nsignificant challenges for conventional machine learning (ML) techniques. This\npaper provides a comprehensive review of advanced ML methodologies designed to\nhandle unstructured grid data in high-dimensional dynamical systems. Key\napproaches discussed include graph neural networks, transformer models with\nspatial attention mechanisms, interpolation-integrated ML methods, and meshless\ntechniques such as physics-informed neural networks. These methodologies have\nproven effective across diverse fields, including fluid dynamics and\nenvironmental simulations. This review is intended as a guidebook for\ncomputational scientists seeking to apply ML approaches to unstructured grid\ndata in their domains, as well as for ML researchers looking to address\nchallenges in computational physics. It places special focus on how ML methods\ncan overcome the inherent limitations of traditional numerical techniques and,\nconversely, how insights from computational physics can inform ML development.\nTo support benchmarking, this review also provides a summary of open-access\ndatasets of unstructured grid data in computational physics. Finally, emerging\ndirections such as generative models with unstructured data, reinforcement\nlearning for mesh generation, and hybrid physics-data-driven paradigms are\ndiscussed to inspire future advancements in this evolving field.\n","authors":["Sibo Cheng","Marc Bocquet","Weiping Ding","Tobias Sebastian Finn","Rui Fu","Jinlong Fu","Yike Guo","Eleda Johnson","Siyi Li","Che Liu","Eric Newton Moro","Jie Pan","Matthew Piggott","Cesar Quilodran","Prakhar Sharma","Kun Wang","Dunhui Xiao","Xiao Xue","Yong Zeng","Mingrui Zhang","Hao Zhou","Kewei Zhu","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2502.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04708v2","updated":"2025-02-13T14:06:51Z","published":"2024-11-07T13:45:26Z","title":"Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs","summary":"  Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Jing Li","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2411.04708v2.pdf","comment":"9 pages, 4 tables, 1 figure, paper under review"},{"id":"http://arxiv.org/abs/2502.09341v1","updated":"2025-02-13T14:01:15Z","published":"2025-02-13T14:01:15Z","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","summary":"  Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.\n","authors":["Sumantrak Mukherjee","Mouad Elhamdi","George Mohler","David A. Selby","Yao Xie","Sebastian Vollmer","Gerrit Grossmann"],"pdf_url":"https://arxiv.org/pdf/2502.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09340v1","updated":"2025-02-13T14:00:55Z","published":"2025-02-13T14:00:55Z","title":"This looks like what? Challenges and Future Research Directions for\n  Part-Prototype Models","summary":"  The growing interest in eXplainable Artificial Intelligence (XAI) has\nprompted research into models with built-in interpretability, the most\nprominent of which are part-prototype models. Part-Prototype Models (PPMs) make\ndecisions by comparing an input image to a set of learned prototypes, providing\nhuman-understandable explanations in the form of ``this looks like that''.\nDespite their inherent interpretability, PPMS are not yet considered a valuable\nalternative to post-hoc models. In this survey, we investigate the reasons for\nthis and provide directions for future research. We analyze papers from 2019 to\n2024, and derive a taxonomy of the challenges that current PPMS face. Our\nanalysis shows that the open challenges are quite diverse. The main concern is\nthe quality and quantity of prototypes. Other concerns are the lack of\ngeneralization to a variety of tasks and contexts, and general methodological\nissues, including non-standardized evaluation. We provide ideas for future\nresearch in five broad directions: improving predictive performance, developing\nnovel architectures grounded in theory, establishing frameworks for human-AI\ncollaboration, aligning models with humans, and establishing metrics and\nbenchmarks for evaluation. We hope that this survey will stimulate research and\npromote intrinsically interpretable models for application domains. Our list of\nsurveyed papers is available at https://github.com/aix-group/ppm-survey.\n","authors":["Khawla Elhadri","Tomasz Michalski","Adam Wrbel","Jrg Schltterer","Bartosz Zieliski","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.09340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v2","updated":"2025-02-13T13:55:08Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09335v1","updated":"2025-02-13T13:54:58Z","published":"2025-02-13T13:54:58Z","title":"Graph Diffusion Network for Drug-Gene Prediction","summary":"  Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.\n","authors":["Jiayang Wu","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09335v1.pdf","comment":"IEEE/ACM TCBB. 14 pages"},{"id":"http://arxiv.org/abs/2412.05000v2","updated":"2025-02-13T13:53:58Z","published":"2024-12-06T12:52:24Z","title":"Noise Matters: Diffusion Model-based Urban Mobility Generation with\n  Collaborative Noise Priors","summary":"  With global urbanization, the focus on sustainable cities has largely grown,\ndriving research into equity, resilience, and urban planning, which often\nrelies on mobility data. The rise of web-based apps and mobile devices has\nprovided valuable user data for mobility-related research. However, real-world\nmobility data is costly and raises privacy concerns. To protect privacy while\nretaining key features of real-world movement, the demand for synthetic data\nhas steadily increased. Recent advances in diffusion models have shown great\npotential for mobility trajectory generation due to their ability to model\nrandomness and uncertainty. However, existing approaches often directly apply\nidentically distributed (i.i.d.) noise sampling from image generation\ntechniques, which fail to account for the spatiotemporal correlations and\nsocial interactions that shape urban mobility patterns. In this paper, we\npropose CoDiffMob, a diffusion model for urban mobility generation with\ncollaborative noise priors, we emphasize the critical role of noise in\ndiffusion models for generating mobility data. By leveraging both individual\nmovement characteristics and population-wide dynamics, we construct novel\ncollaborative noise priors that provide richer and more informative guidance\nthroughout the generation process. Extensive experiments demonstrate the\nsuperiority of our method, with generated data accurately capturing both\nindividual preferences and collective patterns, achieving an improvement of\nover 32%. Furthermore, it can effectively replace web-derived mobility data to\nbetter support downstream applications, while safeguarding user privacy and\nfostering a more secure and ethical web. This highlights its tremendous\npotential for applications in sustainable city-related research. The code and\ndata are available at https://github.com/tsinghua-fib-lab/CoDiffMob.\n","authors":["Yuheng Zhang","Yuan Yuan","Jingtao Ding","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2412.05000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09332v1","updated":"2025-02-13T13:49:52Z","published":"2025-02-13T13:49:52Z","title":"Full Swap Regret and Discretized Calibration","summary":"  We study the problem of minimizing swap regret in structured normal-form\ngames. Players have a very large (potentially infinite) number of pure actions,\nbut each action has an embedding into $d$-dimensional space and payoffs are\ngiven by bilinear functions of these embeddings. We provide an efficient\nlearning algorithm for this setting that incurs at most\n$\\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.\n  To achieve this, we introduce a new online learning problem we call\n\\emph{full swap regret minimization}. In this problem, a learner repeatedly\ntakes a (randomized) action in a bounded convex $d$-dimensional action set\n$\\mathcal{K}$ and then receives a loss from the adversary, with the goal of\nminimizing their regret with respect to the \\emph{worst-case} swap function\nmapping $\\mathcal{K}$ to $\\mathcal{K}$. For varied assumptions about the\nconvexity and smoothness of the loss functions, we design algorithms with full\nswap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.\n  Finally, we apply these tools to the problem of online forecasting to\nminimize calibration error, showing that several notions of calibration can be\nviewed as specific instances of full swap regret. In particular, we design\nefficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$\n$\\ell_2$-calibration error and $O(\\max(\\sqrt{\\epsilon T}, T^{1/3}))$\n\\emph{discretized-calibration} error (when the forecaster is restricted to\npredicting multiples of $\\epsilon$).\n","authors":["Maxwell Fishelson","Robert Kleinberg","Princewill Okoroafor","Renato Paes Leme","Jon Schneider","Yifeng Teng"],"pdf_url":"https://arxiv.org/pdf/2502.09332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09329v1","updated":"2025-02-13T13:43:52Z","published":"2025-02-13T13:43:52Z","title":"Bayesian Optimization for Simultaneous Selection of Machine Learning\n  Algorithms and Hyperparameters on Shared Latent Space","summary":"  Selecting the optimal combination of a machine learning (ML) algorithm and\nits hyper-parameters is crucial for the development of high-performance ML\nsystems. However, since the combination of ML algorithms and hyper-parameters\nis enormous, the exhaustive validation requires a significant amount of time.\nMany existing studies use Bayesian optimization (BO) for accelerating the\nsearch. On the other hand, a significant difficulty is that, in general, there\nexists a different hyper-parameter space for each one of candidate ML\nalgorithms. BO-based approaches typically build a surrogate model independently\nfor each hyper-parameter space, by which sufficient observations are required\nfor all candidate ML algorithms. In this study, our proposed method embeds\ndifferent hyper-parameter spaces into a shared latent space, in which a\nsurrogate multi-task model for BO is estimated. This approach can share\ninformation of observations from different ML algorithms by which efficient\noptimization is expected with a smaller number of total observations. We\nfurther propose the pre-training of the latent space embedding with an\nadversarial regularization, and a ranking model for selecting an effective\npre-trained embedding for a given target dataset. Our empirical study\ndemonstrates effectiveness of the proposed method through datasets from OpenML.\n","authors":["Kazuki Ishikawa","Ryota Ozaki","Yohei Kanzaki","Ichiro Takeuchi","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2502.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16839v3","updated":"2025-02-13T13:39:26Z","published":"2025-01-28T10:28:17Z","title":"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","summary":"  Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions.\n","authors":["Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2501.16839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09324v1","updated":"2025-02-13T13:37:52Z","published":"2025-02-13T13:37:52Z","title":"Depth-Bounds for Neural Networks via the Braid Arrangement","summary":"  We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.\n","authors":["Moritz Grillo","Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2502.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09318v1","updated":"2025-02-13T13:33:35Z","published":"2025-02-13T13:33:35Z","title":"SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating\n  Mechanisms","summary":"  In this paper, we propose a novel approach that enhances recurrent neural\nnetworks (RNNs) by incorporating path signatures into their gating mechanisms.\nOur method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit\n(GRU) architectures by replacing their forget and reset gates, respectively,\nwith learnable path signatures. These signatures, which capture the geometric\nfeatures of the entire path history, provide a richer context for controlling\ninformation flow through the network's memory. This modification allows the\nnetworks to make memory decisions based on the full historical context rather\nthan just the current input and state. Through experimental studies, we\ndemonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models\noutperform their traditional counterparts across various sequential learning\ntasks. By leveraging path signatures in recurrent architectures, this method\noffers new opportunities to enhance performance in time series analysis and\nforecasting applications.\n","authors":["Rmi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2502.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04750v2","updated":"2025-02-13T13:33:19Z","published":"2025-02-07T08:33:28Z","title":"Tighter sparse variational Gaussian processes","summary":"  Sparse variational Gaussian process (GP) approximations based on inducing\npoints have become the de facto standard for scaling GPs to large datasets,\nowing to their theoretical elegance, computational efficiency, and ease of\nimplementation. This paper introduces a provably tighter variational\napproximation by relaxing the standard assumption that the conditional\napproximate posterior given the inducing points must match that in the prior.\nThe key innovation is to modify the conditional posterior to have smaller\nvariances than that of the prior at the training points. We derive the\ncollapsed bound for the regression case, describe how to use the proposed\napproximation in large data settings, and discuss its application to handle\northogonally structured inducing points and GP latent variable models.\nExtensive experiments on regression benchmarks, classification, and latent\nvariable models demonstrate that the proposed approximation consistently\nmatches or outperforms standard sparse variational GPs while maintaining the\nsame computational cost. An implementation will be made available in all\npopular GP packages.\n","authors":["Thang D. Bui","Matthew Ashman","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03345v3","updated":"2025-02-13T13:25:18Z","published":"2024-06-05T15:04:27Z","title":"Feature contamination: Neural networks learn uncorrelated features and\n  fail to generalize","summary":"  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n","authors":["Tianren Zhang","Chujie Zhao","Guanyu Chen","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03345v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2502.09306v1","updated":"2025-02-13T13:18:30Z","published":"2025-02-13T13:18:30Z","title":"Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for\n  Generative Modelling","summary":"  We investigate the theoretical properties of general diffusion\n(interpolation) paths and their Langevin Monte Carlo implementation, referred\nto as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on\nthe data distribution. Specifically, we analyse and provide non-asymptotic\nerror bounds for the annealed Langevin dynamics where the path of distributions\nis defined as Gaussian convolutions of the data distribution as in diffusion\nmodels. We then extend our results to recently proposed heavy-tailed (Student's\nt) diffusion paths, demonstrating their theoretical properties for heavy-tailed\ndata distributions for the first time. Our analysis provides theoretical\nguarantees for a class of score-based generative models that interpolate\nbetween a simple distribution (Gaussian or Student's t) and the data\ndistribution in finite time. This approach offers a broader perspective\ncompared to standard score-based diffusion approaches, which are typically\nbased on a forward Ornstein-Uhlenbeck (OU) noising process.\n","authors":["Paula Cordero-Encinar","O. Deniz Akyildiz","Andrew B. Duncan"],"pdf_url":"https://arxiv.org/pdf/2502.09306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09303v1","updated":"2025-02-13T13:16:10Z","published":"2025-02-13T13:16:10Z","title":"Towards Seamless Hierarchical Federated Learning under Intermittent\n  Client Participation: A Stagewise Decision-Making Methodology","summary":"  Federated Learning (FL) offers a pioneering distributed learning paradigm\nthat enables devices/clients to build a shared global model. This global model\nis obtained through frequent model transmissions between clients and a central\nserver, which may cause high latency, energy consumption, and congestion over\nbackhaul links. To overcome these drawbacks, Hierarchical Federated Learning\n(HFL) has emerged, which organizes clients into multiple clusters and utilizes\nedge nodes (e.g., edge servers) for intermediate model aggregations between\nclients and the central server. Current research on HFL mainly focus on\nenhancing model accuracy, latency, and energy consumption in scenarios with a\nstable/fixed set of clients. However, addressing the dynamic availability of\nclients -- a critical aspect of real-world scenarios -- remains underexplored.\nThis study delves into optimizing client selection and client-to-edge\nassociations in HFL under intermittent client participation so as to minimize\noverall system costs (i.e., delay and energy), while achieving fast model\nconvergence. We unveil that achieving this goal involves solving a complex\nNP-hard problem. To tackle this, we propose a stagewise methodology that splits\nthe solution into two stages, referred to as Plan A and Plan B. Plan A focuses\non identifying long-term clients with high chance of participation in\nsubsequent model training rounds. Plan B serves as a backup, selecting\nalternative clients when long-term clients are unavailable during model\ntraining rounds. This stagewise methodology offers a fresh perspective on\nclient selection that can enhance both HFL and conventional FL via enabling\nlow-overhead decision-making processes. Through evaluations on MNIST and\nCIFAR-10 datasets, we show that our methodology outperforms existing benchmarks\nin terms of model accuracy and system costs.\n","authors":["Minghong Wu","Minghui Liwang","Yuhan Su","Li Li","Seyyedali Hosseinalipour","Xianbin Wang","Huaiyu Dai","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2502.09303v1.pdf","comment":"20 pages, 8 figures,5 tables"},{"id":"http://arxiv.org/abs/2108.12113v3","updated":"2025-02-13T13:12:41Z","published":"2021-08-27T04:18:45Z","title":"A method of supervised learning from conflicting data with hidden\n  contexts","summary":"  Conventional supervised learning assumes a stable input-output relationship.\nHowever, this assumption fails in open-ended training settings where the\ninput-output relationship depends on hidden contexts. In this work, we\nformulate a more general supervised learning problem in which training data is\ndrawn from multiple unobservable domains, each potentially exhibiting distinct\ninput-output maps. This inherent conflict in data renders standard empirical\nrisk minimization training ineffective. To address this challenge, we propose a\nmethod LEAF that introduces an allocation function, which learns to assign\nconflicting data to different predictive models. We establish a connection\nbetween LEAF and a variant of the Expectation-Maximization algorithm, allowing\nus to derive an analytical expression for the allocation function. Finally, we\nprovide a theoretical analysis of LEAF and empirically validate its\neffectiveness on both synthetic and real-world tasks involving conflicting\ndata.\n","authors":["Tianren Zhang","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2108.12113v3.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09298v1","updated":"2025-02-13T13:12:16Z","published":"2025-02-13T13:12:16Z","title":"Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep\n  Reinforcement Learning","summary":"  We present a novel method for Deep Reinforcement Learning (DRL),\nincorporating the convex property of the value function over the belief space\nin Partially Observable Markov Decision Processes (POMDPs). We introduce hard-\nand soft-enforced convexity as two different approaches, and compare their\nperformance against standard DRL on two well-known POMDP environments, namely\nthe Tiger and FieldVisionRockSample problems. Our findings show that including\nthe convexity feature can substantially increase performance of the agents, as\nwell as increase robustness over the hyperparameter space, especially when\ntesting on out-of-distribution domains. The source code for this work can be\nfound at https://github.com/Dakout/Convex_DRL.\n","authors":["Daniel Koutas","Daniel Hettegger","Kostas G. Papakonstantinou","Daniel Straub"],"pdf_url":"https://arxiv.org/pdf/2502.09298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09297v1","updated":"2025-02-13T13:11:54Z","published":"2025-02-13T13:11:54Z","title":"When do neural networks learn world models?","summary":"  Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.\n","authors":["Tianren Zhang","Guanyu Chen","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09297v1.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.16106v2","updated":"2025-02-13T13:11:46Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation","summary":"  Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.\n","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.16106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09291v1","updated":"2025-02-13T13:08:11Z","published":"2025-02-13T13:08:11Z","title":"Joint Attention Mechanism Learning to Facilitate Opto-physiological\n  Monitoring during Physical Activity","summary":"  Opto-physiological monitoring is a non-contact technique for measuring\ncardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly\nlead to reliable physiological readings. However, PPG signal acquisition\nprocedures are often accompanied by spurious motion artefacts (MAs), especially\nduring low-to-high-intensity physical activity. This study proposes a practical\nadversarial learning approach for opto-physiological monitoring by using a\ngenerative adversarial network with an attention mechanism (AM-GAN) to model\nmotion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping\nfrom raw and noisy signals to clear PPG signals in an adversarial manner,\nguided by an attention mechanism to directly translate the motion reference of\ntriaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was\nexperimented with three various protocols engaged with 39 subjects in various\nphysical activities. The average absolute error for heart rate (HR) derived\nfrom the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC\ndataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied\nto an in-house LU dataset resulted in average absolute errors for HR and\nrespiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min,\nrespectively. The study demonstrates the robustness and resilience of AM-GAN,\nparticularly during low-to-high-intensity physical activities.\n","authors":["Xiaoyu Zheng","Sijung Hu","Vincent Dwyer","Mahsa Derakhshani","Laura Barrett"],"pdf_url":"https://arxiv.org/pdf/2502.09291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09290v1","updated":"2025-02-13T13:06:56Z","published":"2025-02-13T13:06:56Z","title":"Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value\n  Stacking of Electric Vehicles Under Uncertainties","summary":"  Electric vehicle (EV) coordination can provide significant benefits through\nvehicle-to-everything (V2X) by interacting with the grid, buildings, and other\nEVs. This work aims to develop a V2X value-stacking framework, including\nvehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to\nmaximize economic benefits for residential communities while maintaining\ndistribution voltage. This work also seeks to quantify the impact of prediction\nerrors related to building load, renewable energy, and EV arrivals. A dynamic\nrolling-horizon optimization (RHO) method is employed to leverage multiple\nrevenue streams and maximize the potential of EV coordination. To address\nenergy uncertainties, including hourly local building load, local photovoltaic\n(PV) generation, and EV arrivals, this work develops a Transformer-based\nforecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder\n(GRU-EN-TFD). The simulation results, using real data from Australia's National\nElectricity Market, and the Independent System Operators in New England and New\nYork in the US, reveal that V2X value stacking can significantly reduce energy\ncosts. The proposed GRU-EN-TFD model outperforms the benchmark forecast model.\nUncertainties in EV arrivals have a more substantial impact on value-stacking\nperformance, highlighting the significance of its accurate forecast. This work\nprovides new insights into the dynamic interactions among residential\ncommunities, unlocking the full potential of EV batteries.\n","authors":["Canchen Jiang","Ariel Liebman","Bo Jie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09290v1.pdf","comment":"21 pages, accepted by Renewable Energy"},{"id":"http://arxiv.org/abs/2502.09287v1","updated":"2025-02-13T13:01:46Z","published":"2025-02-13T13:01:46Z","title":"An Uncertainty Principle for Linear Recurrent Neural Networks","summary":"  We consider linear recurrent neural networks, which have become a key\nbuilding block of sequence modeling due to their ability for stable and\neffective long-range modeling. In this paper, we aim at characterizing this\nability on a simple but core copy task, whose goal is to build a linear filter\nof order $S$ that approximates the filter that looks $K$ time steps in the past\n(which we refer to as the shift-$K$ filter), where $K$ is larger than $S$.\nUsing classical signal models and quadratic cost, we fully characterize the\nproblem by providing lower bounds of approximation, as well as explicit filters\nthat achieve this lower bound up to constants. The optimal performance\nhighlights an uncertainty principle: the optimal filter has to average values\naround the $K$-th time step in the past with a range~(width) that is\nproportional to $K/S$.\n","authors":["Alexandre Franois","Antonio Orvieto","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2502.09287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v2","updated":"2025-02-13T12:54:36Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1912.05737v6","updated":"2025-02-13T13:23:14Z","published":"2019-12-12T02:28:13Z","title":"Finite sample properties of parametric MMD estimation: robustness to\n  misspecification and dependence","summary":"  Many works in statistics aim at designing a universal estimation procedure,\nthat is, an estimator that would converge to the best approximation of the\n(unknown) data generating distribution in a model, without any assumption on\nthis distribution. This question is of major interest, in particular because\nthe universality property leads to the robustness of the estimator. In this\npaper, we tackle the problem of universal estimation using a minimum distance\nestimator presented in Briol et al. (2019) based on the Maximum Mean\nDiscrepancy. We show that the estimator is robust to both dependence and to the\npresence of outliers in the dataset. Finally, we provide a theoretical study of\nthe stochastic gradient descent algorithm used to compute the estimator, and we\nsupport our findings with numerical simulations.\n  ** The proof of Proposition 4.4 in the published version contains a mistake.\nThe mistake is fixed here (and the bound is actually improved by a factor 2).\n**\n","authors":["Badr-Eddine Chrief-Abdellatif","Pierre Alquier"],"pdf_url":"https://arxiv.org/pdf/1912.05737v6.pdf","comment":null}]},"2025-02-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.10391v1","updated":"2025-02-14T18:59:51Z","published":"2025-02-14T18:59:51Z","title":"MM-RLHF: The Next Step Forward in Multimodal LLM Alignment","summary":"  Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.\n","authors":["Yi-Fan Zhang","Tao Yu","Haochen Tian","Chaoyou Fu","Peiyan Li","Jianshu Zeng","Wulin Xie","Yang Shi","Huanyu Zhang","Junkang Wu","Xue Wang","Yibo Hu","Bin Wen","Fan Yang","Zhang Zhang","Tingting Gao","Di Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2502.10391v1.pdf","comment":"Project Page: https://mm-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2502.10388v1","updated":"2025-02-14T18:59:28Z","published":"2025-02-14T18:59:28Z","title":"Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction","summary":"  Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome.\n","authors":["WonJin Yoon","Boyu Ren","Spencer Thomas","Chanwhi Kim","Guergana Savova","Mei-Hua Hall","Timothy Miller"],"pdf_url":"https://arxiv.org/pdf/2502.10388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10378v1","updated":"2025-02-14T18:57:04Z","published":"2025-02-14T18:57:04Z","title":"Unknown Word Detection for English as a Second Language (ESL) Learners\n  Using Gaze and Pre-trained Language Models","summary":"  English as a Second Language (ESL) learners often encounter unknown words\nthat hinder their text comprehension. Automatically detecting these words as\nusers read can enable computing systems to provide just-in-time definitions,\nsynonyms, or contextual explanations, thereby helping users learn vocabulary in\na natural and seamless manner. This paper presents EyeLingo, a\ntransformer-based machine learning method that predicts the probability of\nunknown words based on text content and eye gaze trajectory in real time with\nhigh accuracy. A 20-participant user study revealed that our method can achieve\nan accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time\nreading assistance prototype to show the effectiveness of EyeLingo. The user\nstudy shows improvement in willingness to use and usefulness compared to\nbaseline methods.\n","authors":["Jiexin Ding","Bowen Zhao","Yuntao Wang","Xinyun Liu","Rui Hao","Ishan Chatterjee","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2502.10378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10373v1","updated":"2025-02-14T18:51:40Z","published":"2025-02-14T18:51:40Z","title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models","summary":"  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n","authors":["William Chen","Jinchuan Tian","Yifan Peng","Brian Yan","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.10373v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.10361v1","updated":"2025-02-14T18:42:07Z","published":"2025-02-14T18:42:07Z","title":"Enhancing Multilingual LLM Pretraining with Model-Based Data Selection","summary":"  Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.\n","authors":["Bettina Messmer","Vinko Sabolec","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2502.10361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00843v2","updated":"2025-02-14T18:35:03Z","published":"2024-10-30T04:20:10Z","title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation","summary":"  Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.\n","authors":["Reza Moravej","Saurabh Bodhe","Zhanguang Zhang","Didier Chetelat","Dimitrios Tsaras","Yingxue Zhang","Hui-Ling Zhen","Jianye Hao","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.00843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10352v1","updated":"2025-02-14T18:31:39Z","published":"2025-02-14T18:31:39Z","title":"Agentic Verification for Ambiguous Query Disambiguation","summary":"  In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs.\n","authors":["Youngwon Lee","Seung-won Hwang","Ruofan Wu","Feng Yan","Danmei Xu","Moutasem Akkad","Zhewei Yao","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2502.10352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v3","updated":"2025-02-14T18:15:01Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schlkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10341v1","updated":"2025-02-14T18:02:37Z","published":"2025-02-14T18:02:37Z","title":"Organize the Web: Constructing Domains Enhances Pre-Training Data\n  Curation","summary":"  Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation.\n","authors":["Alexander Wettig","Kyle Lo","Sewon Min","Hannaneh Hajishirzi","Danqi Chen","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2502.10341v1.pdf","comment":"Project page: https://weborganizer.allen.ai"},{"id":"http://arxiv.org/abs/2502.10339v1","updated":"2025-02-14T17:59:58Z","published":"2025-02-14T17:59:58Z","title":"STAR: Spectral Truncation and Rescale for Model Merging","summary":"  Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.\n","authors":["Yu-Ang Lee","Ching-Yun Ko","Tejaswini Pedapati","I-Hsin Chung","Mi-Yen Yeh","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10339v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.10338v1","updated":"2025-02-14T17:55:43Z","published":"2025-02-14T17:55:43Z","title":"Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering","summary":"  Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.\n","authors":["Nick Ferguson","Liane Guillou","Alan Bundy","Kwabena Nuamah"],"pdf_url":"https://arxiv.org/pdf/2502.10338v1.pdf","comment":"8 pages. Accepted to the Workshop on Planning in the Era of LLMs\n  (LM4Plan @ AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.10416v2","updated":"2025-02-14T17:40:13Z","published":"2024-12-09T20:03:14Z","title":"SuperMerge: An Approach For Gradient-Based Model Merging","summary":"  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,\nmonolithic, and possess the superpower to simultaneously support thousands of\ntasks. However, high-throughput applications often prefer smaller task-specific\nmodels because of their lower latency and cost. One challenge of using\ntask-specific models is the incremental need for solving newer tasks after the\nmodel is already deployed for existing tasks. A straightforward solution\nrequires fine-tuning the model again for both existing and new tasks, which is\ncomputationally expensive and time-consuming. To address this issue, we propose\na model merging based approach called SUPERMERGE. SUPERMERGE is a\ngradient-based method to systematically merge several fine-tuned models trained\non existing and new tasks. SUPERMERGE is designed to be lightweight and fast,\nand the merged model achieves similar performance to fully fine-tuned models on\nall tasks. Furthermore, we proposed a hierarchical model merging strategy to\nreduce the peak space requirement without sacrificing the performance of the\nmerged model. We experimentally demonstrate that SUPERMERGE outperforms\nexisting model merging methods on common natural language processing and\ncomputer vision tasks.\n","authors":["Haoyu Yang","Zheng Zhang","Saket Sathe"],"pdf_url":"https://arxiv.org/pdf/2412.10416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10291v2","updated":"2025-02-14T17:37:35Z","published":"2024-06-13T03:26:30Z","title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents","summary":"  Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.\n","authors":["Hao Kang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.10291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10297v1","updated":"2025-02-14T16:59:05Z","published":"2025-02-14T16:59:05Z","title":"DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders","summary":"  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers.\n","authors":["Julien Siems","Timur Carstensen","Arber Zela","Frank Hutter","Massimiliano Pontil","Riccardo Grazzi"],"pdf_url":"https://arxiv.org/pdf/2502.10297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17115v2","updated":"2025-02-14T16:44:08Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v2.pdf","comment":"47 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2502.08859v2","updated":"2025-02-14T16:40:15Z","published":"2025-02-13T00:18:34Z","title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","summary":"  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n","authors":["Clinton J. Wang","Dean Lee","Cristina Menghini","Johannes Mols","Jack Doughty","Adam Khoja","Jayson Lynch","Sean Hendryx","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06426v2","updated":"2025-02-14T16:32:54Z","published":"2024-11-10T11:08:28Z","title":"SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains","summary":"  As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.\n","authors":["Bijoy Ahmed Saiem","MD Sadik Hossain Shanto","Rakib Ahsan","Md Rafi ur Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.06426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13610v2","updated":"2025-02-14T16:27:25Z","published":"2024-10-17T14:46:22Z","title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling","summary":"  Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.\n","authors":["Yakun Zhu","Shaohang Wei","Xu Wang","Kui Xue","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13610v2.pdf","comment":"NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.10266v1","updated":"2025-02-14T16:23:39Z","published":"2025-02-14T16:23:39Z","title":"Are Large Language Models the future crowd workers of Linguistics?","summary":"  Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.\n","authors":["Iris Ferrazzo"],"pdf_url":"https://arxiv.org/pdf/2502.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10263v1","updated":"2025-02-14T16:16:02Z","published":"2025-02-14T16:16:02Z","title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers","summary":"  Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.\n","authors":["Aivin V. Solatorio","Rafael Macalaba","James Liounis"],"pdf_url":"https://arxiv.org/pdf/2502.10263v1.pdf","comment":"Project GitHub repository at https://github.com/worldbank/ai4data-use"},{"id":"http://arxiv.org/abs/2502.10250v1","updated":"2025-02-14T15:59:33Z","published":"2025-02-14T15:59:33Z","title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models","summary":"  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n","authors":["Gokul Karthik Kumar","Iheb Chaabane","Kebin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.10250v1.pdf","comment":"Accepted at PAKDD 2025"},{"id":"http://arxiv.org/abs/2502.10248v1","updated":"2025-02-14T15:58:10Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v1.pdf","comment":"35 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.07780v2","updated":"2025-02-14T15:46:51Z","published":"2024-06-12T00:19:40Z","title":"A Critical Look At Tokenwise Reward-Guided Text Generation","summary":"  Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.\n","authors":["Ahmad Rashid","Ruotian Wu","Julia Grosse","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2406.07780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14050v3","updated":"2025-02-14T15:39:29Z","published":"2024-12-18T17:05:08Z","title":"Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation","summary":"  Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.\n","authors":["Vera Neplenbroek","Arianna Bisazza","Raquel Fernndez"],"pdf_url":"https://arxiv.org/pdf/2412.14050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02844v3","updated":"2025-02-14T15:32:00Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19637v2","updated":"2025-02-14T15:20:42Z","published":"2024-10-25T15:39:34Z","title":"A distributional simplicity bias in the learning dynamics of\n  transformers","summary":"  The remarkable capability of over-parameterised neural networks to generalise\neffectively has been explained by invoking a ``simplicity bias'': neural\nnetworks prevent overfitting by initially learning simple classifiers before\nprogressing to more complex, non-linear functions. While simplicity biases have\nbeen described theoretically and experimentally in feed-forward networks for\nsupervised learning, the extent to which they also explain the remarkable\nsuccess of transformers trained with self-supervised techniques remains\nunclear. In our study, we demonstrate that transformers, trained on natural\nlanguage data, also display a simplicity bias. Specifically, they sequentially\nlearn many-body interactions among input tokens, reaching a saturation point in\nthe prediction error for low-degree interactions while continuing to learn\nhigh-degree interactions. To conduct this analysis, we develop a procedure to\ngenerate \\textit{clones} of a given natural language data set, which rigorously\ncapture the interactions between tokens up to a specified order. This approach\nopens up the possibilities of studying how interactions of different orders in\nthe data affect learning, in natural language processing and beyond.\n","authors":["Riccardo Rende","Federica Gerace","Alessandro Laio","Sebastian Goldt"],"pdf_url":"https://arxiv.org/pdf/2410.19637v2.pdf","comment":"10 pages, 5 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.10202v1","updated":"2025-02-14T14:56:19Z","published":"2025-02-14T14:56:19Z","title":"Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?","summary":"  Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance.\n","authors":["Xiliang Zhu","Elena Khasanova","Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10202v1.pdf","comment":"Accepted to NAACL 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.10201v1","updated":"2025-02-14T14:52:41Z","published":"2025-02-14T14:52:41Z","title":"Prediction hubs are context-informed frequent tokens in LLMs","summary":"  Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens.\n","authors":["Beatrix M. G. Nielsen","Iuri Macocco","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2502.10201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01237v2","updated":"2025-02-14T14:47:26Z","published":"2025-01-02T12:55:27Z","title":"Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction","summary":"  Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases.\n","authors":["Alexander Brinkmann","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2501.01237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15451v2","updated":"2025-02-14T14:03:43Z","published":"2025-01-26T08:45:37Z","title":"STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection","summary":"  The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.\n","authors":["Zewen Bai","Yuanyuan Sun","Shengdi Yin","Junyu Lu","Jingjie Zeng","Haohao Zhu","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2501.15451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09078v3","updated":"2025-02-14T13:46:53Z","published":"2024-12-12T09:01:18Z","title":"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning","summary":"  Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.\n","authors":["Zhenni Bi","Kai Han","Chuanjian Liu","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09078v3.pdf","comment":"Code will be available at\n  https://github.com/iamhankai/Forest-of-Thought"},{"id":"http://arxiv.org/abs/2502.10162v1","updated":"2025-02-14T13:46:14Z","published":"2025-02-14T13:46:14Z","title":"Revisiting Generalization Power of a DNN in Terms of Symbolic\n  Interactions","summary":"  This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.\n","authors":["Lei Cheng","Junpeng Zhang","Qihan Ren","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10162v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.19198"},{"id":"http://arxiv.org/abs/2410.14391v2","updated":"2025-02-14T13:15:13Z","published":"2024-10-18T11:52:10Z","title":"Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation","summary":"  Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation.\n","authors":["Wafaa Mohammed","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.14391v2.pdf","comment":"9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.10347v2","updated":"2025-02-14T13:13:33Z","published":"2024-10-14T10:00:49Z","title":"A Unified Approach to Routing and Cascading for LLMs","summary":"  The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.\n","authors":["Jasper Dekoninck","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.10347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10140v1","updated":"2025-02-14T13:10:39Z","published":"2025-02-14T13:10:39Z","title":"Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages","summary":"  Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.\n","authors":["Daniil Gurgurov","Ivan Vykopal","Josef van Genabith","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2502.10140v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2411.12118v2","updated":"2025-02-14T12:56:45Z","published":"2024-11-18T23:12:13Z","title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers","summary":"  In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.\n","authors":["Tiberiu Musat"],"pdf_url":"https://arxiv.org/pdf/2411.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14497v2","updated":"2025-02-14T12:38:15Z","published":"2025-01-24T13:53:54Z","title":"Evaluating and Improving Graph to Text Generation with Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.\n","authors":["Jie He","Yijun Yang","Wanqiu Long","Deyi Xiong","Victor Gutierrez-Basulto","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2501.14497v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.04348v2","updated":"2025-02-14T11:46:43Z","published":"2025-02-04T15:16:17Z","title":"Prompt-based Depth Pruning of Large Language Models","summary":"  Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines.\n","authors":["Juyun Wee","Minjae Park","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2502.04348v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07016v3","updated":"2025-02-14T11:01:27Z","published":"2024-06-11T07:16:34Z","title":"Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary","summary":"  Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.\n","authors":["Dmitry Kobak","Rita Gonzlez-Mrquez","Emke-gnes Horvt","Jan Lause"],"pdf_url":"https://arxiv.org/pdf/2406.07016v3.pdf","comment":"v3: Updating the manuscript to include all PubMed abstracts until the\n  end of 2024"},{"id":"http://arxiv.org/abs/2502.10064v1","updated":"2025-02-14T10:41:42Z","published":"2025-02-14T10:41:42Z","title":"Hands-off Image Editing: Language-guided Editing without any\n  Task-specific Labeling, Masking or even Training","summary":"  Instruction-guided image editing consists in taking an image and an\ninstruction and deliverring that image altered according to that instruction.\nState-of-the-art approaches to this task suffer from the typical scaling up and\ndomain adaptation hindrances related to supervision as they eventually resort\nto some kind of task-specific labelling, masking or training. We propose a\nnovel approach that does without any such task-specific supervision and offers\nthus a better potential for improvement. Its assessment demonstrates that it is\nhighly effective, achieving very competitive performance.\n","authors":["Rodrigo Santos","Antnio Branco","Joo Silva","Joo Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2502.10064v1.pdf","comment":"Published in COLING 2025"},{"id":"http://arxiv.org/abs/2502.10061v1","updated":"2025-02-14T10:32:29Z","published":"2025-02-14T10:32:29Z","title":"Annotating Compositionality Scores for Irish Noun Compounds is Hard Work","summary":"  Noun compounds constitute a challenging construction for NLP applications,\ngiven their variability in idiomaticity and interpretation. In this paper, we\npresent an analysis of compound nouns identified in Irish text of varied\ndomains by expert annotators, focusing on compositionality as a key feature,\nbut also domain specificity, as well as familiarity and confidence of the\nannotator giving the ratings. Our findings and the discussion that ensued\ncontributes towards a greater understanding of how these constructions appear\nin Irish language, and how they might be treated separately from English noun\ncompounds.\n","authors":["Abigail Walsh","Teresa Clifford","Emma Daly","Jane Dunne","Brian Davis","Gearid  Cleircn"],"pdf_url":"https://arxiv.org/pdf/2502.10061v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.10058v1","updated":"2025-02-14T10:21:10Z","published":"2025-02-14T10:21:10Z","title":"MTLM: an Innovative Language Model Training Paradigm for ASR","summary":"  Pre-training Transformer-based language models (LMs) on a large amount of\ntext has proven crucial for improving automatic speech recognition (ASR)\nperformance. Generally, traditional LMs are unidirectional and unable to access\nthe context on the right. This paper proposes a method for training LMs that\nenable traditional unidirectional LMs to fully utilize left and right contexts.\nCompared with the unidirectional LMs, our LM facilitates ASR to transcribe\nhypotheses more consistently and in a more semantically unambiguous way, as it\nincorporates richer contextual representations. Finally, our experimental\nresults on the LibriSpeech corpus demonstrate that our model outperforms\ntraditional unidirectional LMs, whether n-best rescoring or shallow fusion is\nused as the decoding algorithm.\n","authors":["Qingliang Meng","Pengju Ren","Tian Li","Changsong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.10058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02370v4","updated":"2025-02-14T10:04:55Z","published":"2024-09-04T01:40:20Z","title":"Do Large Language Models Possess Sensitive to Sentiment?","summary":"  Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.\n","authors":["Yang Liu","Xichou Zhu","Zhou Shen","Yi Liu","Min Li","Yujun Chen","Benzi John","Zhenzhen Ma","Tao Hu","Zhi Li","Zhiyang Xu","Wei Luo","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02370v4.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.02375v4","updated":"2025-02-14T10:02:14Z","published":"2024-09-04T01:51:37Z","title":"How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review","summary":"  The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.\n","authors":["Yang Liu","Xichou Zhu","Zhou Shen","Yi Liu","Min Li","Yujun Chen","Benzi John","Zhenzhen Ma","Tao Hu","Zhi Li","Bolong Yang","Manman Wang","Zongxing Xie","Peng Liu","Dan Cai","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02375v4.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.10051v1","updated":"2025-02-14T10:00:20Z","published":"2025-02-14T10:00:20Z","title":"ORI: O Routing Intelligence","summary":"  Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.\n","authors":["Ahmad Shadid","Rahul Kumar","Mohit Mayank"],"pdf_url":"https://arxiv.org/pdf/2502.10051v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.01584v2","updated":"2025-02-14T09:56:31Z","published":"2024-09-03T03:42:56Z","title":"Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models","summary":"  As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt\n","authors":["Shintaro Ozaki","Kazuki Hayashi","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2409.01584v2.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2406.04344v3","updated":"2025-02-14T09:51:46Z","published":"2024-06-06T17:59:56Z","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models","summary":"  Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.\n","authors":["Tim Z. Xiao","Robert Bamler","Bernhard Schlkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.04344v3.pdf","comment":"Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)"},{"id":"http://arxiv.org/abs/2502.03930v2","updated":"2025-02-14T09:49:57Z","published":"2025-02-06T10:09:49Z","title":"DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation","summary":"  Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.\n","authors":["Dongya Jia","Zhuo Chen","Jiawei Chen","Chenpeng Du","Jian Wu","Jian Cong","Xiaobin Zhuang","Chumin Li","Zhen Wei","Yuping Wang","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03930v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.16345v2","updated":"2025-02-14T09:32:11Z","published":"2024-11-25T12:44:02Z","title":"Preference Optimization for Reasoning with Pseudo Feedback","summary":"  Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.\n","authors":["Fangkai Jiao","Geyang Guo","Xingxing Zhang","Nancy F. Chen","Shafiq Joty","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16345v2.pdf","comment":"28 pages, 11 figures. ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10013v1","updated":"2025-02-14T08:47:10Z","published":"2025-02-14T08:47:10Z","title":"Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation","summary":"  Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment.\n","authors":["Clive Pendleton","Ewan Harrington","Giles Fairbrother","Jasper Arkwright","Nigel Fenwick","Richard Katrix"],"pdf_url":"https://arxiv.org/pdf/2502.10013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16466v4","updated":"2025-02-14T08:44:16Z","published":"2023-11-28T04:07:34Z","title":"The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry","summary":"  Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. We analyzed over 1\nmillion complaints and identified a significant increase in LLM usage following\nthe release of ChatGPT. We find that LLM usage is associated with an increased\nlikelihood of obtaining relief from financial firms. To investigate this\nrelationship, we employ an instrumental variable approach to mitigate\nendogeneity concerns around LLM adoption. Although instrumental variables\nsuggest a potential causal link, they cannot fully capture all unobserved\nheterogeneity. To further establish this causal relationship, we conducted\ncontrolled experiments, which support that LLMs can enhance the clarity and\npersuasiveness of consumer narratives, thereby increasing the likelihood of\nobtaining relief. Our findings suggest that facilitating access to LLMs can\nhelp firms better understand consumer concerns and level the playing field\namong consumers. This underscores the importance of policies promoting\ntechnological accessibility, enabling all consumers to effectively voice their\nconcerns.\n","authors":["Minkyu Shin","Jin Kim","Jiwoong Shin"],"pdf_url":"https://arxiv.org/pdf/2311.16466v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12475v2","updated":"2025-02-14T08:40:39Z","published":"2024-12-17T02:22:24Z","title":"RareAgents: Advancing Rare Disease Care through LLM-Empowered\n  Multi-disciplinary Team","summary":"  Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field.\n","authors":["Xuanzhong Chen","Ye Jin","Xiaohao Mao","Lun Wang","Shuyang Zhang","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00675v2","updated":"2025-02-14T08:38:16Z","published":"2025-02-02T05:25:03Z","title":"ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction,\n  and Column Exploration","summary":"  Text-to-SQL systems have unlocked easier access to critical data insights by\nenabling natural language queries over structured databases. However, deploying\nsuch systems in enterprise environments remains challenging due to factors such\nas large, complex schemas (> 3000 columns), diverse SQL dialects (e.g.,\nBigQuery, Snowflake) and sophisticated query requirements (e.g.,\ntransformation, analytics). Current state-of-the-art performance on the Spider\n2.0 dataset -- a benchmark built to mimic such complex environments -- remains\nlimited at 20%. Key limitations include inadequate instruction-following, poor\nlong-context comprehension, weak self-refinement, and insufficient\ndialect-specific knowledge. To address these gaps, we propose ReFoRCE\n(Self-Refinement Agent with Format Restriction and Column Exploration) which\nintroduces (1) table compression to mitigate long-context limitations (2)\nformat restriction to ensure accurate answer format, and (3) iterative column\nexploration for enhanced schema understanding. Additionally, it employs\nself-refinement pipeline consisting of (1) parallelized workflows with voting\nmechanisms and (2) a Common Table Expression (CTE) based refinement approach to\nhandle unresolved cases. ReFoRCE achieves state-of-the-art results scoring\n31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.\n","authors":["Minghang Deng","Ashwin Ramachandran","Canwen Xu","Lanxiang Hu","Zhewei Yao","Anupam Datta","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00675v2.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.10003v1","updated":"2025-02-14T08:34:26Z","published":"2025-02-14T08:34:26Z","title":"SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim\n  Verification","summary":"  Verifying scientific claims presents a significantly greater challenge than\nverifying political or news-related claims. Unlike the relatively broad\naudience for political claims, the users of scientific claim verification\nsystems can vary widely, ranging from researchers testing specific hypotheses\nto everyday users seeking information on a medication. Additionally, the\nevidence for scientific claims is often highly complex, involving technical\nterminology and intricate domain-specific concepts that require specialized\nmodels for accurate verification. Despite considerable interest from the\nresearch community, there is a noticeable lack of large-scale scientific claim\nverification datasets to benchmark and train effective models. To bridge this\ngap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num,\nderived from scientific research papers. We propose several baseline models\ntailored for scientific claim verification to assess the effectiveness of these\ndatasets. Additionally, we evaluate models trained on SciClaimHunt and\nSciClaimHunt_Num against existing scientific claim verification datasets to\ngauge their quality and reliability. Furthermore, we conduct human evaluations\nof the claims in proposed datasets and perform error analysis to assess the\neffectiveness of the proposed baseline models. Our findings indicate that\nSciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for\ntraining models in scientific claim verification.\n","authors":["Sujit Kumar","Anshul Sharma","Siddharth Hemant Khincha","Gargi Shroff","Sanasam Ranbir Singh","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.10003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10001v1","updated":"2025-02-14T08:33:31Z","published":"2025-02-14T08:33:31Z","title":"EmbBERT-Q: Breaking Memory Barriers in Embedded NLP","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.\n","authors":["Riccardo Bravin","Massimo Pavan","Hazem Hesham Yousef Shalby","Fabrizio Pittorino","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2502.10001v1.pdf","comment":"24 pages, 4 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.09992v1","updated":"2025-02-14T08:23:51Z","published":"2025-02-14T08:23:51Z","title":"Large Language Diffusion Models","summary":"  Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.\n","authors":["Shen Nie","Fengqi Zhu","Zebin You","Xiaolu Zhang","Jingyang Ou","Jun Hu","Jun Zhou","Yankai Lin","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09990v1","updated":"2025-02-14T08:22:51Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09977v1","updated":"2025-02-14T08:04:22Z","published":"2025-02-14T08:04:22Z","title":"LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  - No Silver Bullet for LC or RAG Routing","summary":"  Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2,326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.\n","authors":["Kuan Li","Liwen Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Shuai Wang","Minhao Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09977v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.09969v1","updated":"2025-02-14T07:55:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2502.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19324v2","updated":"2025-02-14T07:30:00Z","published":"2025-01-31T17:19:57Z","title":"Reward-Guided Speculative Decoding for Efficient LLM Reasoning","summary":"  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.\n","authors":["Baohao Liao","Yuhui Xu","Hanze Dong","Junnan Li","Christof Monz","Silvio Savarese","Doyen Sahoo","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2501.19324v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2502.09956v1","updated":"2025-02-14T07:28:08Z","published":"2025-02-14T07:28:08Z","title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","summary":"  Recent interest in building foundation models for KGs has highlighted a\nfundamental challenge: knowledge-graph data is relatively scarce. The\nbest-known KGs are primarily human-labeled, created by pattern-matching, or\nextracted using early NLP techniques. While human-generated KGs are in short\nsupply, automatically extracted KGs are of questionable quality. We present a\nsolution to this data scarcity problem in the form of a text-to-KG generator\n(KGGen), a package that uses language models to create high-quality graphs from\nplaintext. Unlike other KG extractors, KGGen clusters related entities to\nreduce sparsity in extracted KGs. KGGen is available as a Python library\n(\\texttt{pip install kg-gen}), making it accessible to everyone. Along with\nKGGen, we release the first benchmark, Measure of of Information in Nodes and\nEdges (MINE), that tests an extractor's ability to produce a useful KG from\nplain text. We benchmark our new tool against existing extractors and\ndemonstrate far superior performance.\n","authors":["Belinda Mo","Kyssen Yu","Joshua Kazdan","Proud Mpala","Lisa Yu","Chris Cundy","Charilaos Kanatsoulis","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2502.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09944v1","updated":"2025-02-14T06:47:37Z","published":"2025-02-14T06:47:37Z","title":"Self-Supervised Learning for Neural Topic Models with\n  Variance-Invariance-Covariance Regularization","summary":"  In our study, we propose a self-supervised neural topic model (NTM) that\ncombines the power of NTMs and regularized self-supervised learning methods to\nimprove performance. NTMs use neural networks to learn latent topics hidden\nbehind the words in documents, enabling greater flexibility and the ability to\nestimate more coherent topics compared to traditional topic models. On the\nother hand, some self-supervised learning methods use a joint embedding\narchitecture with two identical networks that produce similar representations\nfor two augmented versions of the same input. Regularizations are applied to\nthese representations to prevent collapse, which would otherwise result in the\nnetworks outputting constant or redundant representations for all inputs. Our\nmodel enhances topic quality by explicitly regularizing latent topic\nrepresentations of anchor and positive samples. We also introduced an\nadversarial data augmentation method to replace the heuristic sampling method.\nWe further developed several variation models including those on the basis of\nan NTM that incorporates contrastive learning with both positive and negative\nsamples. Experimental results on three datasets showed that our models\noutperformed baselines and state-of-the-art models both quantitatively and\nqualitatively.\n","authors":["Weiran Xu","Kengo Hirami","Koji Eguchi"],"pdf_url":"https://arxiv.org/pdf/2502.09944v1.pdf","comment":"Preprint accepted in Springer Knowledge and Information Systems\n  (KAIS), in press"},{"id":"http://arxiv.org/abs/2502.09940v1","updated":"2025-02-14T06:34:08Z","published":"2025-02-14T06:34:08Z","title":"A Preliminary Exploration with GPT-4o Voice Mode","summary":"  With the rise of multimodal large language models, GPT-4o stands out as a\npioneering model, driving us to evaluate its capabilities. This report assesses\nGPT-4o across various tasks to analyze its audio processing and reasoning\nabilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and\nmusic understanding, performing well in tasks like intent classification,\nspoken command classification, semantic and grammatical reasoning.,\nmultilingual speech recognition, and singing analysis. It also shows greater\nrobustness against hallucinations than other large audio-language models\n(LALMs). However, it struggles with tasks such as audio duration prediction and\ninstrument classification. Additionally, GPT-4o's safety mechanisms cause it to\ndecline tasks like speaker identification, age classification, MOS prediction,\nand audio deepfake detection. Notably, the model exhibits a significantly\ndifferent refusal rate when responding to speaker verification tasks on\ndifferent datasets. This is likely due to variations in the accompanying\ninstructions or the quality of the input audio, suggesting the sensitivity of\nits built-in safeguards. Finally, we acknowledge that model performance varies\nwith evaluation protocols. This report only serves as a preliminary exploration\nof the current state of LALMs.\n","authors":["Yu-Xiang Lin","Chih-Kai Yang","Wei-Chih Chen","Chen-An Li","Chien-yu Huang","Xuanjun Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2502.09940v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.08943v2","updated":"2025-02-14T06:10:00Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v2.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.09933v1","updated":"2025-02-14T06:05:12Z","published":"2025-02-14T06:05:12Z","title":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot\n  In-Context Inductive Reasoning","summary":"  Inductive Reasoning (IR), the ability to summarize rules from examples and\napply on new ones, has long been viewed as a primal ability for general\nintelligence and widely studied by cognitive science and AI researchers. Many\nbenchmarks have been proposed to measure such ability for Large Language Models\n(LLMs); however, they focus on few-shot (usually $<$10) setting and lack\nevaluation for aggregating many pieces of information from long contexts. On\nthe other hand, the ever-growing context length of LLMs have brought forth the\nnovel paradigm of many-shot In-Context Learning (ICL), which addresses new\ntasks with hundreds to thousands of examples without expensive and inefficient\nfine-tuning. However, many-shot evaluations are mostly focused on\nclassification (a very limited aspect of IR), and popular long-context LLM\ntasks such as Needle-In-A-Haystack (NIAH) seldom require complicated\nintelligence for integrating many pieces of information. To fix the issues from\nboth worlds, we propose MIR-Bench, the first many-shot in-context inductive\nreasoning benchmark that asks LLM to induce output via input-output examples\nfrom underlying functions with diverse data format. Based on MIR-Bench, we\nstudy many novel problems for inductive reasoning and many-shot ICL, including\nrobustness against erroneous shots and the effect of Chain-of-Thought (CoT),\nand acquired insightful findings.\n","authors":["Kai Yan","Zhan Ling","Kang Liu","Yifan Yang","Ting-Han Fan","Lingfeng Shen","Zhengyin Du","Jiecao Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09933v1.pdf","comment":"32 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.18279v9","updated":"2025-02-14T05:42:16Z","published":"2024-11-27T12:13:39Z","title":"Large Language Model-Brained GUI Agents: A Survey","summary":"  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n","authors":["Chaoyun Zhang","Shilin He","Jiaxu Qian","Bowen Li","Liqun Li","Si Qin","Yu Kang","Minghua Ma","Guyue Liu","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18279v9.pdf","comment":"The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration"},{"id":"http://arxiv.org/abs/2407.08952v2","updated":"2025-02-14T04:56:16Z","published":"2024-07-12T03:15:01Z","title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection","summary":"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.\n","authors":["Ye Liu","Jiajun Zhu","Xukai Liu","Haoyu Tang","Yanghai Zhang","Kai Zhang","Xiaofang Zhou","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02619v9","updated":"2025-02-14T04:43:31Z","published":"2024-02-04T21:33:18Z","title":"Arithmetic in Transformers Explained","summary":"  While recent work has shown transformers can learn addition, previous models\nexhibit poor prediction accuracy and are limited to small numbers. Furthermore,\nthe relationship between single-task and multitask arithmetic capabilities\nremains unexplored. In this work, we analyze 44 autoregressive transformer\nmodels trained on addition, subtraction, or both. These include 16\naddition-only models, 2 subtraction-only models, 8 \"mixed\" models trained to\nperform addition and subtraction, and 14 mixed models initialized with\nparameters from an addition-only model. The models span 5- to 15-digit\nquestions, 2 to 4 attention heads, and 2 to 3 layers. We show that the addition\nmodels converge on a common logical algorithm, with most models achieving\n>99.999% prediction accuracy. We provide a detailed mechanistic explanation of\nhow this algorithm is implemented within the network architecture.\nSubtraction-only models have lower accuracy. With the initialized mixed models,\nthrough parameter transfer experiments, we explore how multitask learning\ndynamics evolve, revealing that some features originally specialized for\naddition become polysemantic, serving both operations, and boosting subtraction\naccuracy. We explain the mixed algorithm mechanically. Finally, we introduce a\nreusable library of mechanistic interpretability tools to define, locate, and\nvisualize these algorithmic circuits across multiple models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v9.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.18472v3","updated":"2025-02-14T04:30:10Z","published":"2024-09-27T06:18:55Z","title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base","summary":"  URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.\n","authors":["Aditya Khan","Mason Shipton","David Anugraha","Kaiyao Duan","Phuong H. Hoang","Eric Khiu","A. Seza Doruz","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2409.18472v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2304.00228v3","updated":"2025-02-14T04:00:08Z","published":"2023-04-01T05:04:06Z","title":"Accuracy and Political Bias of News Source Credibility Ratings by Large\n  Language Models","summary":"  Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information.\n","authors":["Kai-Cheng Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2304.00228v3.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09573v2","updated":"2025-02-14T03:31:39Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15140v2","updated":"2025-02-14T02:57:30Z","published":"2025-01-25T08:52:43Z","title":"Analyzing and Boosting the Power of Fine-Grained Visual Recognition for\n  Multi-modal Large Language Models","summary":"  Multi-modal large language models (MLLMs) have shown remarkable abilities in\nvarious visual understanding tasks. However, MLLMs still struggle with\nfine-grained visual recognition (FGVR), which aims to identify\nsubordinate-level categories from images. This can negatively impact more\nadvanced capabilities of MLLMs, such as object-centric visual question\nanswering and reasoning. In our study, we revisit three quintessential\ncapabilities of MLLMs for FGVR, including object information extraction,\ncategory knowledge reserve, object-category alignment, and position of the root\ncause as a misalignment problem. To address this issue, we present Finedefics,\nan MLLM that enhances the model's FGVR capability by incorporating informative\nattribute descriptions of objects into the training phase. We employ\ncontrastive learning on object-attribute pairs and attribute-category pairs\nsimultaneously and use examples from similar but incorrect categories as hard\nnegatives, naturally bringing representations of visual objects and category\nnames closer. Extensive evaluations across multiple popular FGVR datasets\ndemonstrate that Finedefics outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code is available at\nhttps://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.\n","authors":["Hulingxiao He","Geng Li","Zijun Geng","Jinglin Xu","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2501.15140v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09870v1","updated":"2025-02-14T02:43:46Z","published":"2025-02-14T02:43:46Z","title":"A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism\n  of Language Technologies","summary":"  Recent attention to anthropomorphism -- the attribution of human-like\nqualities to non-human objects or entities -- of language technologies like\nLLMs has sparked renewed discussions about potential negative impacts of\nanthropomorphism. To productively discuss the impacts of this anthropomorphism\nand in what contexts it is appropriate, we need a shared vocabulary for the\nvast variety of ways that language can be anthropomorphic. In this work, we\ndraw on existing literature and analyze empirical cases of user interactions\nwith language technologies to develop a taxonomy of textual expressions that\ncan contribute to anthropomorphism. We highlight challenges and tensions\ninvolved in understanding linguistic anthropomorphism, such as how all language\nis fundamentally human and how efforts to characterize and shift perceptions of\nhumanness in machines can also dehumanize certain humans. We discuss ways that\nour taxonomy supports more precise and effective discussions of and decisions\nabout anthropomorphism of language technologies.\n","authors":["Alicia DeVrio","Myra Cheng","Lisa Egede","Alexandra Olteanu","Su Lin Blodgett"],"pdf_url":"https://arxiv.org/pdf/2502.09870v1.pdf","comment":"18 pages, 1 figure, to appear at CHI 2025"},{"id":"http://arxiv.org/abs/2409.19471v2","updated":"2025-02-14T02:40:55Z","published":"2024-09-28T22:33:44Z","title":"SELP: Generating Safe and Efficient Task Plans for Robot Agents with\n  Large Language Models","summary":"  Despite significant advancements in large language models (LLMs) that enhance\nrobot agents' understanding and execution of natural language (NL) commands,\nensuring the agents adhere to user-specified constraints remains challenging,\nparticularly for complex commands and long-horizon tasks. To address this\nchallenge, we present three key insights, equivalence voting, constrained\ndecoding, and domain-specific fine-tuning, which significantly enhance LLM\nplanners' capability in handling complex tasks. Equivalence voting ensures\nconsistency by generating and sampling multiple Linear Temporal Logic (LTL)\nformulas from NL commands, grouping equivalent LTL formulas, and selecting the\nmajority group of formulas as the final LTL formula. Constrained decoding then\nuses the generated LTL formula to enforce the autoregressive inference of\nplans, ensuring the generated plans conform to the LTL. Domain-specific\nfine-tuning customizes LLMs to produce safe and efficient plans within specific\ntask domains. Our approach, Safe Efficient LLM Planner (SELP), combines these\ninsights to create LLM planners to generate plans adhering to user commands\nwith high confidence. We demonstrate the effectiveness and generalizability of\nSELP across different robot agents and tasks, including drone navigation and\nrobot manipulation. For drone navigation tasks, SELP outperforms\nstate-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks\nconforming to NL commands) and by 19.8% in plan efficiency. For robot\nmanipulation tasks, SELP achieves 20.4% improvement in safety rate. Our\ndatasets for evaluating NL-to-LTL and robot task planning will be released in\ngithub.com/lt-asset/selp.\n","authors":["Yi Wu","Zikang Xiong","Yiran Hu","Shreyash S. Iyengar","Nan Jiang","Aniket Bera","Lin Tan","Suresh Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2409.19471v2.pdf","comment":"This paper has been accepted for presentation at the 2025 IEEE\n  International Conference on Robotics and Automation (ICRA), May 19-23, 2025,\n  Atlanta, USA, and for inclusion in the conference proceeding"},{"id":"http://arxiv.org/abs/2502.09863v1","updated":"2025-02-14T02:16:48Z","published":"2025-02-14T02:16:48Z","title":"Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning","summary":"  The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.\n","authors":["Dhruva Karkada","James B. Simon","Yasaman Bahri","Michael R. DeWeese"],"pdf_url":"https://arxiv.org/pdf/2502.09863v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.09858v1","updated":"2025-02-14T01:46:00Z","published":"2025-02-14T01:46:00Z","title":"Automated Hypothesis Validation with Agentic Sequential Falsifications","summary":"  Hypotheses are central to information acquisition, decision-making, and\ndiscovery. However, many real-world hypotheses are abstract, high-level\nstatements that are difficult to validate directly. This challenge is further\nintensified by the rise of hypothesis generation from Large Language Models\n(LLMs), which are prone to hallucination and produce hypotheses in volumes that\nmake manual validation impractical. Here we propose Popper, an agentic\nframework for rigorous automated validation of free-form hypotheses. Guided by\nKarl Popper's principle of falsification, Popper validates a hypothesis using\nLLM agents that design and execute falsification experiments targeting its\nmeasurable implications. A novel sequential testing framework ensures strict\nType-I error control while actively gathering evidence from diverse\nobservations, whether drawn from existing data or newly conducted procedures.\nWe demonstrate Popper on six domains including biology, economics, and\nsociology. Popper delivers robust error control, high power, and scalability.\nFurthermore, compared to human scientists, Popper achieved comparable\nperformance in validating complex biological hypotheses while reducing time by\n10 folds, providing a scalable, rigorous solution for hypothesis validation.\n","authors":["Kexin Huang","Ying Jin","Ryan Li","Michael Y. Li","Emmanuel Cands","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2502.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09854v1","updated":"2025-02-14T01:39:45Z","published":"2025-02-14T01:39:45Z","title":"Efficient Multitask Learning in Small Language Models Through\n  Upside-Down Reinforcement Learning","summary":"  In this work, we demonstrate that small language models (SLMs), specifically\na 100M parameter GPT-2 model, can achieve competitive performance in multitask\nprompt generation tasks while requiring only a fraction of the computational\nresources needed by large language models (LLMs). Through a novel combination\nof upside-down reinforcement learning and synthetic data distillation from a\npowerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5%\nof state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite\nbeing up to 80 times smaller, making it highly suitable for\nresource-constrained and real-time applications. This study highlights the\npotential of SLMs as efficient multitask learners in multimodal settings,\nproviding a promising alternative to LLMs for scalable, low-latency\ndeployments.\n","authors":["Yu-Chen Lin","Sanat Sharma","Hari Manikandan","Jayant Kumar","Tracy Holloway King","Jing Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.09854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05584v5","updated":"2025-02-14T01:21:57Z","published":"2024-10-08T00:52:03Z","title":"Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?","summary":"  Reward Models (RMs) are crucial for aligning language models with human\npreferences. Currently, the evaluation of RMs depends on measuring accuracy\nagainst a validation set of manually annotated preference data. Although this\nmethod is straightforward and widely adopted, the relationship between RM\naccuracy and downstream policy performance remains under-explored. In this\nwork, we conduct experiments in a synthetic setting to investigate how\ndifferences in RM measured by accuracy translate into gaps in optimized policy\nperformance. Our findings reveal that while there is a weak positive\ncorrelation between accuracy and downstream performance, policies optimized\ntowards RMs with similar accuracy can exhibit quite different performance.\nMoreover, we discover that the way of measuring accuracy significantly impacts\nits ability to predict the final policy performance. Through the lens of the\nRegressional Goodhart effect, we recognize that accuracy, when used for\nmeasuring RM quality, can fail to fully capture the potential RM\noveroptimization. This underscores the inadequacy of relying solely on accuracy\nto reflect their impact on policy optimization.\n","authors":["Xueru Wen","Jie Lou","Yaojie Lu","Hongyu Lin","Xing Yu","Xinyu Lu","Ben He","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05584v5.pdf","comment":"Accepted at ICLR2025 Spotlight"},{"id":"http://arxiv.org/abs/2502.03824v3","updated":"2025-02-14T01:05:29Z","published":"2025-02-06T07:19:59Z","title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","summary":"  LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.\n","authors":["Minsang Kim","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2502.03824v3.pdf","comment":"the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted"},{"id":"http://arxiv.org/abs/2501.12746v4","updated":"2025-02-14T01:02:04Z","published":"2025-01-22T09:27:11Z","title":"EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering","summary":"  When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted analysis to provide high-quality answers. Current LLM-based\nquestion answering methods lack a detailed definition and learning process for\nevidence analysis, leading to the risk of error propagation and hallucinations\nwhile using evidence. Although increasing the parameter size of LLMs can\nalleviate these issues, it also presents challenges in training and deployment\nwith limited resources. In this study, we propose EvidenceMap, which aims to\nenable a tiny pre-trained language model to explicitly learn multiple aspects\nof biomedical evidence, including supportive evaluation, logical correlation\nand content summarization, thereby latently guiding a small generative model\n(around 3B parameters) to provide textual responses. Experimental results\ndemonstrate that our method, learning evidence analysis by fine-tuning a model\nwith only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and\n5.7% in reference-based quality and accuracy, respectively.\n","authors":["Chang Zong","Jian Wan","Siliang Tang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.12746v4.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08648v2","updated":"2025-02-14T00:32:56Z","published":"2025-01-15T08:24:03Z","title":"MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities","summary":"  While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning). This separation overlooks the opportunity for\ndeveloping a more versatile language model and for these objectives to\ncomplement each other. In this work, we propose MAGNET, a method for adapting\ndecoder-only LLMs to generate robust representations and infill missing text\nspans. MAGNET employs three self-supervised training objectives and introduces\nan attention mechanism that combines bidirectional and causal attention,\nenabling unified training across all objectives. Our results demonstrate that\nLLMs adapted with MAGNET (1) surpass strong text encoders on token-level and\nsentence-level representation learning tasks, (2) generate contextually\nappropriate text infills by leveraging past and future contexts, (3) perform\nopen-ended text generation without excessive repetition of words or phrases,\nand (4) preserve the knowledge and reasoning capability gained by the LLM\nduring pretraining.\n","authors":["Savya Khosla","Aditi Tiwari","Kushal Kafle","Simon Jenni","Handong Zhao","John Collomosse","Jing Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08648v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.10389v1","updated":"2025-02-14T18:59:36Z","published":"2025-02-14T18:59:36Z","title":"Region-Adaptive Sampling for Diffusion Transformers","summary":"  Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.\n","authors":["Ziming Liu","Yifan Yang","Chengruidong Zhang","Yiqi Zhang","Lili Qiu","Yang You","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10385v1","updated":"2025-02-14T18:58:04Z","published":"2025-02-14T18:58:04Z","title":"Simplifying DINO via Coding Rate Regularization","summary":"  DINO and DINOv2 are two model families being widely used to learn\nrepresentations from unlabeled imagery data at large scales. Their learned\nrepresentations often enable state-of-the-art performance for downstream tasks,\nsuch as image classification and segmentation. However, they employ many\nempirically motivated design choices and their training pipelines are highly\ncomplex and unstable -- many hyperparameters need to be carefully tuned to\nensure that the representations do not collapse -- which poses considerable\ndifficulty to improving them or adapting them to new domains. In this work, we\nposit that we can remove most such-motivated idiosyncrasies in the pre-training\npipelines, and only need to add an explicit coding rate term in the loss\nfunction to avoid collapse of the representations. As a result, we obtain\nhighly simplified variants of the DINO and DINOv2 which we call SimDINO and\nSimDINOv2, respectively. Remarkably, these simplified models are more robust to\ndifferent design choices, such as network architecture and hyperparameters, and\nthey learn even higher-quality representations, measured by performance on\ndownstream tasks, offering a Pareto improvement over the corresponding DINO and\nDINOv2 models. This work highlights the potential of using simplifying design\nprinciples to improve the empirical practice of deep learning.\n","authors":["Ziyang Wu","Jingyuan Zhang","Druv Pai","XuDong Wang","Chandan Singh","Jianwei Yang","Jianfeng Gao","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2502.10385v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.10383v1","updated":"2025-02-14T18:57:29Z","published":"2025-02-14T18:57:29Z","title":"Representation and Interpretation in Artificial and Natural Computing","summary":"  Artificial computing machinery transforms representations through an\nobjective process, to be interpreted subjectively by humans, so the machine and\nthe interpreter are different entities, but in the putative natural computing\nboth processes are performed by the same agent. The method or process that\ntransforms a representation is called here \\emph{the mode of computing}. The\nmode used by digital computers is the algorithmic one, but there are others,\nsuch as quantum computers and diverse forms of non-conventional computing, and\nthere is an open-ended set of representational formats and modes that could be\nused in artificial and natural computing. A mode based on a notion of computing\ndifferent from Turing's may perform feats beyond what the Turing Machine does\nbut the modes would not be of the same kind and could not be compared. For a\nmode of computing to be more powerful than the algorithmic one, it ought to\ncompute functions lacking an effective algorithm, and Church Thesis would not\nhold. Here, a thought experiment including a computational demon using a\nhypothetical mode for such an effect is presented. If there is natural\ncomputing, there is a mode of natural computing whose properties may be causal\nto the phenomenological experience. Discovering it would come with solving the\nhard problem of consciousness; but if it turns out that such a mode does not\nexist, there is no such thing as natural computing, and the mind is not a\ncomputational process.\n","authors":["Luis A. Pineda"],"pdf_url":"https://arxiv.org/pdf/2502.10383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10373v1","updated":"2025-02-14T18:51:40Z","published":"2025-02-14T18:51:40Z","title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models","summary":"  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n","authors":["William Chen","Jinchuan Tian","Yifan Peng","Brian Yan","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.10373v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.10363v1","updated":"2025-02-14T18:42:42Z","published":"2025-02-14T18:42:42Z","title":"BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds","summary":"  Traversing risky terrains with sparse footholds poses a significant challenge\nfor humanoid robots, requiring precise foot placements and stable locomotion.\nExisting approaches designed for quadrupedal robots often fail to generalize to\nhumanoid robots due to differences in foot geometry and unstable morphology,\nwhile learning-based approaches for humanoid locomotion still face great\nchallenges on complex terrains due to sparse foothold reward signals and\ninefficient learning processes. To address these challenges, we introduce\nBeamDojo, a reinforcement learning (RL) framework designed for enabling agile\nhumanoid locomotion on sparse footholds. BeamDojo begins by introducing a\nsampling-based foothold reward tailored for polygonal feet, along with a double\ncritic to balancing the learning process between dense locomotion rewards and\nsparse foothold rewards. To encourage sufficient trail-and-error exploration,\nBeamDojo incorporates a two-stage RL approach: the first stage relaxes the\nterrain dynamics by training the humanoid on flat terrain while providing it\nwith task terrain perceptive observations, and the second stage fine-tunes the\npolicy on the actual task terrain. Moreover, we implement a onboard LiDAR-based\nelevation map to enable real-world deployment. Extensive simulation and\nreal-world experiments demonstrate that BeamDojo achieves efficient learning in\nsimulation and enables agile locomotion with precise foot placement on sparse\nfootholds in the real world, maintaining a high success rate even under\nsignificant external disturbances.\n","authors":["Huayi Wang","Zirui Wang","Junli Ren","Qingwei Ben","Tao Huang","Weinan Zhang","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.10363v1.pdf","comment":"Project website: https://why618188.github.io/beamdojo"},{"id":"http://arxiv.org/abs/2411.00843v2","updated":"2025-02-14T18:35:03Z","published":"2024-10-30T04:20:10Z","title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation","summary":"  Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.\n","authors":["Reza Moravej","Saurabh Bodhe","Zhanguang Zhang","Didier Chetelat","Dimitrios Tsaras","Yingxue Zhang","Hui-Ling Zhen","Jianye Hao","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.00843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v3","updated":"2025-02-14T18:15:01Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schlkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10339v1","updated":"2025-02-14T17:59:58Z","published":"2025-02-14T17:59:58Z","title":"STAR: Spectral Truncation and Rescale for Model Merging","summary":"  Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.\n","authors":["Yu-Ang Lee","Ching-Yun Ko","Tejaswini Pedapati","I-Hsin Chung","Mi-Yen Yeh","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10339v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.10338v1","updated":"2025-02-14T17:55:43Z","published":"2025-02-14T17:55:43Z","title":"Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering","summary":"  Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.\n","authors":["Nick Ferguson","Liane Guillou","Alan Bundy","Kwabena Nuamah"],"pdf_url":"https://arxiv.org/pdf/2502.10338v1.pdf","comment":"8 pages. Accepted to the Workshop on Planning in the Era of LLMs\n  (LM4Plan @ AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.10416v2","updated":"2025-02-14T17:40:13Z","published":"2024-12-09T20:03:14Z","title":"SuperMerge: An Approach For Gradient-Based Model Merging","summary":"  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,\nmonolithic, and possess the superpower to simultaneously support thousands of\ntasks. However, high-throughput applications often prefer smaller task-specific\nmodels because of their lower latency and cost. One challenge of using\ntask-specific models is the incremental need for solving newer tasks after the\nmodel is already deployed for existing tasks. A straightforward solution\nrequires fine-tuning the model again for both existing and new tasks, which is\ncomputationally expensive and time-consuming. To address this issue, we propose\na model merging based approach called SUPERMERGE. SUPERMERGE is a\ngradient-based method to systematically merge several fine-tuned models trained\non existing and new tasks. SUPERMERGE is designed to be lightweight and fast,\nand the merged model achieves similar performance to fully fine-tuned models on\nall tasks. Furthermore, we proposed a hierarchical model merging strategy to\nreduce the peak space requirement without sacrificing the performance of the\nmerged model. We experimentally demonstrate that SUPERMERGE outperforms\nexisting model merging methods on common natural language processing and\ncomputer vision tasks.\n","authors":["Haoyu Yang","Zheng Zhang","Saket Sathe"],"pdf_url":"https://arxiv.org/pdf/2412.10416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10291v2","updated":"2025-02-14T17:37:35Z","published":"2024-06-13T03:26:30Z","title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents","summary":"  Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.\n","authors":["Hao Kang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.10291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10325v1","updated":"2025-02-14T17:34:28Z","published":"2025-02-14T17:34:28Z","title":"Process Reward Models for LLM Agents: Practical Framework and Directions","summary":"  We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.\n","authors":["Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.10325v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07516v2","updated":"2025-02-14T17:24:56Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10311v1","updated":"2025-02-14T17:14:02Z","published":"2025-02-14T17:14:02Z","title":"ExplainReduce: Summarising local explanations via proxies","summary":"  Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.\n","authors":["Lauri Sepplinen","Mudong Guo","Kai Puolamki"],"pdf_url":"https://arxiv.org/pdf/2502.10311v1.pdf","comment":"22 pages with a 7 page appendix, 7 + 5 figures, 2 tables. The\n  datasets and source code used in the paper are available at\n  https://github.com/edahelsinki/explainreduce"},{"id":"http://arxiv.org/abs/2502.10308v1","updated":"2025-02-14T17:12:20Z","published":"2025-02-14T17:12:20Z","title":"LLM-Powered Preference Elicitation in Combinatorial Assignment","summary":"  We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.\n","authors":["Ermis Soumalias","Yanchen Jiang","Kehang Zhu","Michael Curry","Sven Seuken","David C. Parkes"],"pdf_url":"https://arxiv.org/pdf/2502.10308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10303v1","updated":"2025-02-14T17:06:34Z","published":"2025-02-14T17:06:34Z","title":"Reinforcement Learning in Strategy-Based and Atari Games: A Review of\n  Google DeepMinds Innovations","summary":"  Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.\n","authors":["Abdelrhman Shaheen","Anas Badr","Ali Abohendy","Hatem Alsaadawy","Nadine Alsayad"],"pdf_url":"https://arxiv.org/pdf/2502.10303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17115v2","updated":"2025-02-14T16:44:08Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v2.pdf","comment":"47 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2502.10284v1","updated":"2025-02-14T16:42:54Z","published":"2025-02-14T16:42:54Z","title":"A Hybrid Cross-Stage Coordination Pre-ranking Model for Online\n  Recommendation Systems","summary":"  Large-scale recommendation systems often adopt cascading architecture\nconsisting of retrieval, pre-ranking, ranking, and re-ranking stages. With\nstrict latency requirements, pre-ranking utilizes lightweight models to perform\na preliminary selection from massive retrieved candidates. However, recent\nworks focus solely on improving consistency with ranking, relying exclusively\non downstream stages. Since downstream input is derived from the pre-ranking\noutput, they will exacerbate the sample selection bias (SSB) issue and Matthew\neffect, leading to sub-optimal results. To address the limitation, we propose a\nnovel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate\ninformation from upstream (retrieval) and downstream (ranking, re-ranking)\nstages. Specifically, cross-stage coordination refers to the pre-ranking's\nadaptability to the entire stream and the role of serving as a more effective\nbridge between upstream and downstream. HCCP consists of Hybrid Sample\nConstruction and Hybrid Objective Optimization. Hybrid sample construction\ncaptures multi-level unexposed data from the entire stream and rearranges them\nto become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid\nobjective optimization contains the joint optimization of consistency and\nlong-tail precision through our proposed Margin InfoNCE loss. It is\nspecifically designed to learn from such hybrid unexposed samples, improving\nthe overall performance and mitigating the SSB issue. The appendix describes a\nproof of the efficacy of the proposed loss in selecting potential positives.\nExtensive offline and online experiments indicate that HCCP outperforms SOTA\nmethods by improving cross-stage coordination. It contributes up to 14.9% UCVR\nand 1.3% UCTR in the JD E-commerce recommendation system. Concerning code\nprivacy, we provide a pseudocode for reference.\n","authors":["Binglei Zhao","Houying Qi","Guang Xu","Mian Ma","Xiwei Zhao","Feng Mei","Sulong Xu","Jinghe Hu"],"pdf_url":"https://arxiv.org/pdf/2502.10284v1.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.08859v2","updated":"2025-02-14T16:40:15Z","published":"2025-02-13T00:18:34Z","title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","summary":"  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n","authors":["Clinton J. Wang","Dean Lee","Cristina Menghini","Johannes Mols","Jack Doughty","Adam Khoja","Jayson Lynch","Sean Hendryx","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06426v2","updated":"2025-02-14T16:32:54Z","published":"2024-11-10T11:08:28Z","title":"SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains","summary":"  As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.\n","authors":["Bijoy Ahmed Saiem","MD Sadik Hossain Shanto","Rakib Ahsan","Md Rafi ur Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.06426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10273v1","updated":"2025-02-14T16:31:43Z","published":"2025-02-14T16:31:43Z","title":"Probing Perceptual Constancy in Large Vision Language Models","summary":"  Perceptual constancy is the ability to maintain stable perceptions of objects\ndespite changes in sensory input, such as variations in distance, angle, or\nlighting. This ability is crucial for recognizing visual information in a\ndynamic world, making it essential for Vision-Language Models (VLMs). However,\nwhether VLMs are currently and theoretically capable of mastering this ability\nremains underexplored. In this study, we evaluated 33 VLMs using 253\nexperiments across three domains: color, size, and shape constancy. The\nexperiments included single-image and video adaptations of classic cognitive\ntasks, along with novel tasks in in-the-wild conditions, to evaluate the\nmodels' recognition of object properties under varying conditions. We found\nsignificant variability in VLM performance, with models performance in shape\nconstancy clearly dissociated from that of color and size constancy.\n","authors":["Haoran Sun","Suyang Yu","Yijiang Li","Qingying Gao","Haiyun Lyu","Hokin Deng","Dezhi Luo"],"pdf_url":"https://arxiv.org/pdf/2502.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13610v2","updated":"2025-02-14T16:27:25Z","published":"2024-10-17T14:46:22Z","title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling","summary":"  Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.\n","authors":["Yakun Zhu","Shaohang Wei","Xu Wang","Kui Xue","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13610v2.pdf","comment":"NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.10266v1","updated":"2025-02-14T16:23:39Z","published":"2025-02-14T16:23:39Z","title":"Are Large Language Models the future crowd workers of Linguistics?","summary":"  Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.\n","authors":["Iris Ferrazzo"],"pdf_url":"https://arxiv.org/pdf/2502.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10263v1","updated":"2025-02-14T16:16:02Z","published":"2025-02-14T16:16:02Z","title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers","summary":"  Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.\n","authors":["Aivin V. Solatorio","Rafael Macalaba","James Liounis"],"pdf_url":"https://arxiv.org/pdf/2502.10263v1.pdf","comment":"Project GitHub repository at https://github.com/worldbank/ai4data-use"},{"id":"http://arxiv.org/abs/2410.20856v2","updated":"2025-02-14T16:09:49Z","published":"2024-10-28T09:19:29Z","title":"Strada-LLM: Graph LLM for traffic prediction","summary":"  Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.\n","authors":["Seyed Mohamad Moghadas","Yangxintong Lyu","Bruno Cornelis","Alexandre Alahi","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2410.20856v2.pdf","comment":"The reviewers decided to reject it. After getting the reviews, we\n  wanted to study more."},{"id":"http://arxiv.org/abs/2502.10239v1","updated":"2025-02-14T15:49:02Z","published":"2025-02-14T15:49:02Z","title":"Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices","summary":"  Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning.\n","authors":["Mohamed Aboelenien Ahmed","Kilian Pfeiffer","Ramin Khalili","Heba Khdr","Jrg Henkel"],"pdf_url":"https://arxiv.org/pdf/2502.10239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10236v1","updated":"2025-02-14T15:46:37Z","published":"2025-02-14T15:46:37Z","title":"Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise\n  Control","summary":"  Diffusion Probabilistic Models (DPMs) are powerful generative models that\nhave achieved unparalleled success in a number of generative tasks. In this\nwork, we aim to build inductive biases into the training and sampling of\ndiffusion models to better accommodate the target distribution of the data to\nmodel. For topologically structured data, we devise a frequency-based noising\noperator to purposefully manipulate, and set, these inductive biases. We first\nshow that appropriate manipulations of the noising forward process can lead\nDPMs to focus on particular aspects of the distribution to learn. We show that\ndifferent datasets necessitate different inductive biases, and that appropriate\nfrequency-based noise control induces increased generative performance compared\nto standard diffusion. Finally, we demonstrate the possibility of ignoring\ninformation at particular frequencies while learning. We show this in an image\ncorruption and recovery task, where we train a DPM to recover the original\ntarget distribution after severe noise corruption.\n","authors":["Thomas Jiralerspong","Berton Earnshaw","Jason Hartford","Yoshua Bengio","Luca Scimeca"],"pdf_url":"https://arxiv.org/pdf/2502.10236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00677v5","updated":"2025-02-14T15:34:58Z","published":"2023-07-02T22:30:08Z","title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary\n  Directed Differential with Normalized Density and Self-Adaption","summary":"  Density-based clustering is the most popular clustering algorithm since it\ncan identify clusters of arbitrary shape as long as they are separated by\nlow-density regions. However, a high-density region that is not separated by\nlow-density ones might also have different structures belonging to multiple\nclusters. As far as we know, all previous density-based clustering algorithms\nfail to detect such structures. In this paper, we provide a novel density-based\nclustering scheme to address this problem. It is the rst clustering algorithm\nthat can detect meticulous structures in a high-density region that is not\nseparated by low-density ones and thus extends the range of applications of\nclustering. The algorithm employs secondary directed differential, hierarchy,\nnormalized density, as well as the self-adaption coefficient, called Structure\nDetecting Cluster by Hierarchical Secondary Directed Differential with\nNormalized Density and Self-Adaption, dubbed SDC-HSDD-NDSA. Experiments on\nsynthetic and real datasets are implemented to verify the effectiveness,\nrobustness, and granularity independence of the algorithm, and the scheme is\ncompared to unsupervised schemes in the Python package Scikit-learn. Results\ndemonstrate that our algorithm outperforms previous ones in many situations,\nespecially significantly when clusters have regular internal structures. For\nexample, averaging over the eight noiseless synthetic datasets with structures\nemploying ARI and NMI criteria, previous algorithms obtain scores below 0.6 and\n0.7, while the presented algorithm obtains scores higher than 0.9 and 0.95,\nrespectively.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2307.00677v5.pdf","comment":"18 pages"}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.10390v1","updated":"2025-02-14T18:59:40Z","published":"2025-02-14T18:59:40Z","title":"(How) Can Transformers Predict Pseudo-Random Numbers?","summary":"  Transformers excel at discovering patterns in sequential data, yet their\nfundamental limitations and learning mechanisms remain crucial topics of\ninvestigation. In this paper, we study the ability of Transformers to learn\npseudo-random number sequences from linear congruential generators (LCGs),\ndefined by the recurrence relation $x_{t+1} = a x_t + c \\;\\mathrm{mod}\\; m$.\nOur analysis reveals that with sufficient architectural capacity and training\ndata variety, Transformers can perform in-context prediction of LCG sequences\nwith unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding\nlayers and attention patterns, we uncover how Transformers develop algorithmic\nstructures to learn these sequences in two scenarios of increasing complexity.\nFirst, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but\nfixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our\nanalysis reveals that models learn to factorize the modulus and utilize\ndigit-wise number representations to make sequential predictions. In the\nsecond, more challenging scenario of unseen moduli, we show that Transformers\ncan generalize to unseen moduli up to $m_{\\text{test}} = 2^{16}$. In this case,\nthe model employs a two-step strategy: first estimating the unknown modulus\nfrom the context, then utilizing prime factorizations to generate predictions.\nFor this task, we observe a sharp transition in the accuracy at a critical\ndepth $=3$. We also find that the number of in-context sequence elements needed\nto reach high accuracy scales sublinearly with the modulus.\n","authors":["Tao Tao","Darshil Doshi","Dayal Singh Kalra","Tianyu He","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2502.10390v1.pdf","comment":"10+16 pages, 12+20 figures"},{"id":"http://arxiv.org/abs/2502.08008v2","updated":"2025-02-14T18:52:34Z","published":"2025-02-11T23:07:14Z","title":"An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models","summary":"  Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.\n","authors":["Kasra Ahmadi","Rouzbeh Behnia","Reza Ebrahimi","Mehran Mozaffari Kermani","Jeremiah Birrell","Jason Pacheco","Attila A Yavuz"],"pdf_url":"https://arxiv.org/pdf/2502.08008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19272v4","updated":"2025-02-14T18:24:55Z","published":"2024-05-29T17:03:31Z","title":"Differentially Private Clustered Federated Learning","summary":"  Federated learning (FL), which is a decentralized machine learning (ML)\napproach, often incorporates differential privacy (DP) to provide rigorous data\nprivacy guarantees. Previous works attempted to address high structured data\nheterogeneity in vanilla FL settings through clustering clients (a.k.a\nclustered FL), but these methods remain sensitive and prone to errors, further\nexacerbated by the DP noise. This vulnerability makes the previous methods\ninappropriate for differentially private FL (DPFL) settings with structured\ndata heterogeneity. To address this gap, we propose an algorithm for\ndifferentially private clustered FL, which is robust to the DP noise in the\nsystem and identifies the underlying clients' clusters correctly. To this end,\nwe propose to cluster clients based on both their model updates and training\nloss values. Furthermore, for clustering clients' model updates at the end of\nthe first round, our proposed approach addresses the server's uncertainties by\nemploying large batch sizes as well as Gaussian Mixture Models (GMM) to reduce\nthe impact of DP and stochastic noise and avoid potential clustering errors.\nThis idea is efficient especially in privacy-sensitive scenarios with more DP\nnoise. We provide theoretical analysis to justify our approach and evaluate it\nacross diverse data distributions and privacy budgets. Our experimental results\nshow its effectiveness in addressing large structured data heterogeneity in\nDPFL.\n","authors":["Saber Malekmohammadi","Afaf Taik","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2405.19272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03519v4","updated":"2025-02-14T18:16:23Z","published":"2024-06-05T17:41:42Z","title":"Noise-Aware Algorithm for Heterogeneous Differentially Private Federated\n  Learning","summary":"  High utility and rigorous data privacy are of the main goals of a federated\nlearning (FL) system, which learns a model from the data distributed among some\nclients. The latter has been tried to achieve by using differential privacy in\nFL (DPFL). There is often heterogeneity in clients privacy requirements, and\nexisting DPFL works either assume uniform privacy requirements for clients or\nare not applicable when server is not fully trusted (our setting). Furthermore,\nthere is often heterogeneity in batch and/or dataset size of clients, which as\nshown, results in extra variation in the DP noise level across clients model\nupdates. With these sources of heterogeneity, straightforward aggregation\nstrategies, e.g., assigning clients aggregation weights proportional to their\nprivacy parameters will lead to lower utility. We propose Robust-HDP, which\nefficiently estimates the true noise level in clients model updates and reduces\nthe noise-level in the aggregated model updates considerably. Robust-HDP\nimproves utility and convergence speed, while being safe to the clients that\nmay maliciously send falsified privacy parameter to server. Extensive\nexperimental results on multiple datasets and our theoretical analysis confirm\nthe effectiveness of Robust-HDP. Our code can be found here.\n","authors":["Saber Malekmohammadi","Yaoliang Yu","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2406.03519v4.pdf","comment":"Proceedings of the 41 st International Conference on Machine\n  Learning, Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2502.10329v1","updated":"2025-02-14T17:43:01Z","published":"2025-02-14T17:43:01Z","title":"VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking\n  Effect","summary":"  The rapid advancements in AI voice cloning, fueled by machine learning, have\nsignificantly impacted text-to-speech (TTS) and voice conversion (VC) fields.\nWhile these developments have led to notable progress, they have also raised\nconcerns about the misuse of AI VC technology, causing economic losses and\nnegative public perceptions. To address this challenge, this study focuses on\ncreating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds\npseudo-timbre (jamming information) based on SFS into audio segments that are\nimperceptible to the human ear, thereby forming systematic fragments to prevent\nvoice cloning. This approach protects the voice without compromising its\nquality. In comparison to existing methods, such as adversarial noise\nincorporation, VocalCrypt significantly enhances robustness and real-time\nperformance, achieving a 500\\% increase in generation speed while maintaining\ninterference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our\nmethod offers preemptive defense, reducing implementation costs and enhancing\nfeasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets\nshow that our AI-cloned speech defense system performs excellently in automatic\nspeaker verification (ASV) tests while preserving the integrity of the\nprotected audio.\n","authors":["Qingyuan Fei","Wenjie Hou","Xuan Hai","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.10329v1.pdf","comment":"9 pages, four figures"},{"id":"http://arxiv.org/abs/2502.10321v1","updated":"2025-02-14T17:27:40Z","published":"2025-02-14T17:27:40Z","title":"Dynamic Fraud Proof","summary":"  In this paper, we present a novel fraud-proof mechanism that achieves fast\nfinality and, when combined with optimistic execution, enables real-time\ntransaction processing. State-of-the-art optimistic rollups typically adopt a\n7-day challenge window, during which any honest party can raise a challenge in\ncase of fraud. We propose a new assert/challenge construction called \"Dynamic\nFraud Proofs\" that achieves sub-second finality in ideal scenarios, while\ndynamically delaying settlement in the event of fraud detection and challenge\nresolution. The system relies on 1) a dynamic challenge period and 2) a\nconfigurable number of randomly selected verifier nodes who must interactively\napprove a state commitment without raising a challenge. If these conditions are\nnot met, the state is not finalized, and the challenge period and approval\ncriteria are dynamically adjusted. We provide a detailed analysis of the\nsystem's design, explaining how it maintains the assumption of a single honest\nnode and addresses censorship attacks by inverting the traditional challenge\nprocess. Additionally, we formalize the system's probabilistic security model\nand discuss how bonding, incentives, and slashing mechanisms can encourage\nhonest behavior, thereby increasing the likelihood of fast settlement in ideal\nscenarios.\n","authors":["Gabriele Picco","Andrea Fortugno"],"pdf_url":"https://arxiv.org/pdf/2502.10321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08832v2","updated":"2025-02-14T23:25:25Z","published":"2025-02-12T22:45:46Z","title":"LSM Trees in Adversarial Environments","summary":"  The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated.\n","authors":["Hayder Tirmazi"],"pdf_url":"https://arxiv.org/pdf/2502.08832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10599v1","updated":"2025-02-14T23:11:51Z","published":"2025-02-14T23:11:51Z","title":"Federated Learning-Driven Cybersecurity Framework for IoT Networks with\n  Privacy-Preserving and Real-Time Threat Detection Capabilities","summary":"  The rapid expansion of the Internet of Things (IoT) ecosystem has transformed\nvarious sectors but has also introduced significant cybersecurity challenges.\nTraditional centralized security methods often struggle to balance privacy\npreservation and real-time threat detection in IoT networks. To address these\nissues, this study proposes a Federated Learning-Driven Cybersecurity Framework\ndesigned specifically for IoT environments. The framework enables decentralized\ndata processing by training models locally on edge devices, ensuring data\nprivacy. Secure aggregation of these locally trained models is achieved using\nhomomorphic encryption, allowing collaborative learning without exposing\nsensitive information.\n  The proposed framework utilizes recurrent neural networks (RNNs) for anomaly\ndetection, optimized for resource-constrained IoT networks. Experimental\nresults demonstrate that the system effectively detects complex cyber threats,\nincluding distributed denial-of-service (DDoS) attacks, with over 98% accuracy.\nAdditionally, it improves energy efficiency by reducing resource consumption by\n20% compared to centralized approaches.\n  This research addresses critical gaps in IoT cybersecurity by integrating\nfederated learning with advanced threat detection techniques. The framework\noffers a scalable and privacy-preserving solution adaptable to various IoT\napplications. Future work will explore the integration of blockchain for\ntransparent model aggregation and quantum-resistant cryptographic methods to\nfurther enhance security in evolving technological landscapes.\n","authors":["Milad Rahmati"],"pdf_url":"https://arxiv.org/pdf/2502.10599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10556v1","updated":"2025-02-14T21:10:03Z","published":"2025-02-14T21:10:03Z","title":"Recent Advances in Malware Detection: Graph Learning and Explainability","summary":"  The rapid evolution of malware has necessitated the development of\nsophisticated detection methods that go beyond traditional signature-based\napproaches. Graph learning techniques have emerged as powerful tools for\nmodeling and analyzing the complex relationships inherent in malware behavior,\nleveraging advancements in Graph Neural Networks (GNNs) and related methods.\nThis survey provides a comprehensive exploration of recent advances in malware\ndetection, focusing on the interplay between graph learning and explainability.\nIt begins by reviewing malware analysis techniques and datasets, emphasizing\ntheir foundational role in understanding malware behavior and supporting\ndetection strategies. The survey then discusses feature engineering, graph\nreduction, and graph embedding methods, highlighting their significance in\ntransforming raw data into actionable insights, while ensuring scalability and\nefficiency. Furthermore, this survey focuses on explainability techniques and\ntheir applications in malware detection, ensuring transparency and\ntrustworthiness. By integrating these components, this survey demonstrates how\ngraph learning and explainability contribute to building robust, interpretable,\nand scalable malware detection systems. Future research directions are outlined\nto address existing challenges and unlock new opportunities in this critical\narea of cybersecurity.\n","authors":["Hossein Shokouhinejad","Roozbeh Razavi-Far","Hesamodin Mohammadian","Mahdi Rabbani","Samuel Ansong","Griffin Higgins","Ali A Ghorbani"],"pdf_url":"https://arxiv.org/pdf/2502.10556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08254v2","updated":"2025-02-14T20:16:35Z","published":"2023-05-14T21:33:19Z","title":"HighGuard: Cross-Chain Business Logic Monitoring of Smart Contracts","summary":"  Logical flaws in smart contracts are often exploited, leading to significant\nfinancial losses. Our tool, HighGuard, detects transactions that violate\nbusiness logic specifications of smart contracts. HighGuard employs dynamic\ncondition response (DCR) graph models as formal specifications to verify\ncontract execution against these models. It is capable of operating in a\ncross-chain environment for detecting business logic flaws across different\nblockchain platforms. We demonstrate HighGuard's effectiveness in identifying\ndeviations from specified behaviors in smart contracts without requiring code\ninstrumentation or incurring additional gas costs. By using precise\nspecifications in the monitor, HighGuard achieves detection without false\npositives. Our evaluation, involving 54 exploits, confirms HighGuard's\neffectiveness in detecting business logic vulnerabilities.\n  Our open-source implementation of HighGuard and a screencast of its usage are\navailable at: https://github.com/mojtaba-eshghie/HighGuard\nhttps://www.youtube.com/watch?v=sZYVV-slDaY\n","authors":["Mojtaba Eshghie","Wolfgang Ahrendt","Cyrille Artho","Thomas Troels Hildebrandt","Gerardo Schneider"],"pdf_url":"https://arxiv.org/pdf/2305.08254v2.pdf","comment":"The most version of the paper was accepted and presented at ASE 2024\n  conference:\n  https://conf.researchr.org/details/ase-2024/ase-2024-tool-demonstrations/12/HighGuard-Cross-Chain-Business-Logic-Monitoring-of-Smart-Contracts"},{"id":"http://arxiv.org/abs/2502.10538v1","updated":"2025-02-14T20:10:14Z","published":"2025-02-14T20:10:14Z","title":"Amortized Locally Decodable Codes","summary":"  Locally Decodable Codes (LDCs) are error correcting codes that admit\nefficient decoding of individual message symbols without decoding the entire\nmessage. Unfortunately, known LDC constructions offer a sub-optimal trade-off\nbetween rate, error tolerance and locality, the number of queries that the\ndecoder must make to the received codeword $\\tilde {y}$ to recovered a\nparticular symbol from the original message $x$, even in relaxed settings where\nthe encoder/decoder share randomness or where the channel is resource bounded.\nWe initiate the study of Amortized Locally Decodable Codes where the local\ndecoder wants to recover multiple symbols of the original message $x$ and the\ntotal number of queries to the received codeword $\\tilde{y}$ can be amortized\nby the total number of message symbols recovered. We demonstrate that\namortization allows us to overcome prior barriers and impossibility results. We\nfirst demonstrate that the Hadamard code achieves amortized locality below $2$\n-- a result that is known to be impossible without amortization. Second, we\nstudy amortized locally decodable codes in cryptographic settings where the\nsender and receiver share a secret key or where the channel is resource-bounded\nand where the decoder wants to recover a consecutive subset of message symbols\n$[L,R]$. In these settings we show that it is possible to achieve a trifecta:\nconstant rate, error tolerance and constant amortized locality.\n","authors":["Jeremiah Blocki","Justin Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10525v1","updated":"2025-02-14T19:41:23Z","published":"2025-02-14T19:41:23Z","title":"Towards Watermarking of Open-Source LLMs","summary":"  While watermarks for closed LLMs have matured and have been included in\nlarge-scale deployments, these methods are not applicable to open-source\nmodels, which allow users full control over the decoding process. This setting\nis understudied yet critical, given the rising performance of open-source\nmodels. In this work, we lay the foundation for systematic study of open-source\nLLM watermarking. For the first time, we explicitly formulate key requirements,\nincluding durability against common model modifications such as model merging,\nquantization, or finetuning, and propose a concrete evaluation setup. Given the\nprevalence of these modifications, durability is crucial for an open-source\nwatermark to be effective. We survey and evaluate existing methods, showing\nthat they are not durable. We also discuss potential ways to improve their\ndurability and highlight remaining challenges. We hope our work enables future\nprogress on this important problem.\n","authors":["Thibaud Gloaguen","Nikola Jovanovi","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2502.10525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10512v1","updated":"2025-02-14T19:18:39Z","published":"2025-02-14T19:18:39Z","title":"A Sea of Coins: The Proliferation of Cryptocurrencies in UniswapV2","summary":"  Blockchain technology has revolutionized financial markets by enabling\ndecentralized exchanges (DEXs) that operate without intermediaries. Uniswap V2,\na leading DEX, facilitates the rapid creation and trading of new tokens,\noffering high return potential but exposing investors to significant risks. In\nthis work, we analyze the financial impact of newly created tokens, assessing\ntheir market dynamics, profitability and liquidity manipulations. Our findings\nreveal that a significant portion of market liquidity is trapped in honeypots,\nreducing market efficiency and misleading investors. Applying a simple\nbuy-and-hold strategy, we are able to uncover some major risks associated with\ninvesting in newly created tokens, including the widespread presence of rug\npulls and sandwich attacks. We extract the optimal sandwich amount, revealing\nthat their proliferation in new tokens stems from higher profitability in\nlow-liquidity pools. Furthermore, we analyze the fundamental differences\nbetween token price evolution in swap time and physical time. Using clustering\ntechniques, we highlight these differences and identify typical patterns of\nhoneypot and sellable tokens. Our study provides insights into the risks and\nfinancial dynamics of decentralized markets and their challenges for investors.\n","authors":["Manuel Naviglio","Francesco Tarantelli","Fabrizio Lillo"],"pdf_url":"https://arxiv.org/pdf/2502.10512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16763v2","updated":"2025-02-14T18:57:34Z","published":"2025-01-28T07:37:43Z","title":"Utilizing Transaction Prioritization to Enhance Confirmation Speed in\n  the IOTA Network","summary":"  With the rapid advancement of blockchain technology, a significant trend is\nthe adoption of Directed Acyclic Graphs (DAGs) as an alternative to traditional\nchain-based architectures for organizing ledger records. Systems like IOTA,\nwhich are specially designed for the Internet of Things (IoT), leverage\nDAG-based architectures to achieve greater scalability by enabling multiple\nattachment points in the ledger for new transactions while allowing these\ntransactions to be added to the network without incurring any fees. To\ndetermine these attachment points, many tip selection algorithms commonly\nemploy specific strategies on the DAG ledger. Transaction prioritization is not\nconsidered in the IOTA network, which becomes especially important when network\nbandwidth is limited. In this paper, we propose an optimization framework\ndesigned to integrate a priority level for critical or high-priority IoT\ntransactions within the IOTA network. We evaluate our system using fully based\non the official IOTA GitHub repository, which employs the currently operational\nIOTA node software (Hornet version), as part of the Chrysalis update (1.5). The\nexperimental results show that higher-priority transactions in the proposed\nalgorithm reach final confirmation in less time compared to the original IOTA\nsystem.\n","authors":["Seyyed Ali Aghamiri","Reza Sharifnia","Ahmad Khonsari"],"pdf_url":"https://arxiv.org/pdf/2501.16763v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2412.09057v2","updated":"2025-02-14T17:17:20Z","published":"2024-12-12T08:33:39Z","title":"PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection","summary":"  Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.\n","authors":["Yuexin Li","Hiok Kuek Tan","Qiaoran Meng","Mei Lin Lock","Tri Cao","Shumin Deng","Nay Oo","Hoon Wei Lim","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2412.09057v2.pdf","comment":"Accepted by WWW 2025 (Demo Track)"},{"id":"http://arxiv.org/abs/2502.10495v1","updated":"2025-02-14T16:55:45Z","published":"2025-02-14T16:55:45Z","title":"SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models","summary":"  In the rapidly evolving landscape of image generation, Latent Diffusion\nModels (LDMs) have emerged as powerful tools, enabling the creation of highly\nrealistic images. However, this advancement raises significant concerns\nregarding copyright infringement and the potential misuse of generated content.\nCurrent watermarking techniques employed in LDMs often embed constant signals\nto the generated images that compromise their stealthiness, making them\nvulnerable to detection by malicious attackers. In this paper, we introduce\nSWA-LDM, a novel approach that enhances watermarking by randomizing the\nembedding process, effectively eliminating detectable patterns while preserving\nimage quality and robustness. Our proposed watermark presence attack reveals\nthe inherent vulnerabilities of existing latent-based watermarking methods,\ndemonstrating how easily these can be exposed. Through comprehensive\nexperiments, we validate that SWA-LDM not only fortifies watermark stealthiness\nbut also maintains competitive performance in watermark robustness and visual\nfidelity. This work represents a pivotal step towards securing LDM-generated\nimages against unauthorized use, ensuring both copyright protection and content\nintegrity in an era where digital image authenticity is paramount.\n","authors":["Zhonghao Yang","Linye Lyu","Xuanhang Chang","Daojing He","YU LI"],"pdf_url":"https://arxiv.org/pdf/2502.10495v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.10293v1","updated":"2025-02-14T16:55:13Z","published":"2025-02-14T16:55:13Z","title":"A Roadmap to Address Burnout in the Cybersecurity Profession: Outcomes\n  from a Multifaceted Workshop","summary":"  This paper addresses the critical issue of burnout among cybersecurity\nprofessionals, a growing concern that threatens the effectiveness of digital\ndefense systems. As the industry faces a significant attrition crisis, with\nnearly 46% of cybersecurity leaders contemplating departure from their roles,\nit is imperative to explore the causes and consequences of burnout through a\nsocio-technical lens. These challenges were discussed by experts from academia\nand industry in a multi-disciplinary workshop at the 26th International\nConference on Human-Computer Interaction to address broad antecedents of\nburnout, manifestation and its consequences among cybersecurity professionals,\nas well as programs to mitigate impacts from burnout. Central to the analysis\nis an empirical study of former National Security Agency (NSA) tactical cyber\noperators. This paper presents key insights in the following areas based on\ndiscussions in the workshop: lessons for public and private sectors from the\nNSA study, a comparative review of addressing burnout in the healthcare\nprofession. It also outlines a roadmap for future collaborative research,\nthereby informing interdisciplinary studies.\n","authors":["Ann Rangarajan","Calvin Nobles","Josiah Dykstra","Margaret Cunningham","Nikki Robinson","Tammie Hollis","Celeste Lyn Paul","Charles Gulotta"],"pdf_url":"https://arxiv.org/pdf/2502.10293v1.pdf","comment":"Accepted to HCI International 2025. 19 pages"},{"id":"http://arxiv.org/abs/2502.10283v1","updated":"2025-02-14T16:38:51Z","published":"2025-02-14T16:38:51Z","title":"Anomaly Detection with LWE Encrypted Control","summary":"  Detecting attacks using encrypted signals is challenging since encryption\nhides its information content. We present a novel mechanism for anomaly\ndetection over Learning with Errors (LWE) encrypted signals without using\ndecryption, secure channels, nor complex communication schemes. Instead, the\ndetector exploits the homomorphic property of LWE encryption to perform\nhypothesis tests on transformations of the encrypted samples. The specific\ntransformations are determined by solutions to a hard lattice-based\nminimization problem. While the test's sensitivity deteriorates with suboptimal\nsolutions, similar to the exponential deterioration of the (related) test that\nbreaks the cryptosystem, we show that the deterioration is polynomial for our\ntest. This rate gap can be exploited to pick parameters that lead to somewhat\nweaker encryption but large gains in detection capability. Finally, we conclude\nthe paper by presenting a numerical example that simulates anomaly detection,\ndemonstrating the effectiveness of our method in identifying attacks.\n","authors":["Rijad Alisic","Junsoo Kim","Henrik Sandberg"],"pdf_url":"https://arxiv.org/pdf/2502.10283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06426v2","updated":"2025-02-14T16:32:54Z","published":"2024-11-10T11:08:28Z","title":"SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains","summary":"  As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.\n","authors":["Bijoy Ahmed Saiem","MD Sadik Hossain Shanto","Rakib Ahsan","Md Rafi ur Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.06426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10194v1","updated":"2025-02-14T14:36:47Z","published":"2025-02-14T14:36:47Z","title":"Translating Common Security Assertions Across Processor Designs: A\n  RISC-V Case Study","summary":"  RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches.\n","authors":["Sharjeel Imtiaz","Uljana Reinsalu","Tara Ghasempouri"],"pdf_url":"https://arxiv.org/pdf/2502.10194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10166v1","updated":"2025-02-14T13:54:40Z","published":"2025-02-14T13:54:40Z","title":"\"It's Like Not Being Able to Read and Write\": Narrowing the Digital\n  Divide for Older Adults and Leveraging the Role of Digital Educators in\n  Ireland","summary":"  As digital services increasingly replace traditional analogue systems,\nensuring that older adults are not left behind is critical to fostering\ninclusive access. This study explores how digital educators support older\nadults in developing essential digital skills, drawing insights from interviews\nwith $34$ educators in Ireland. These educators, both professional and\nvolunteer, offer instruction through a range of formats, including workshops,\nremote calls, and in-person sessions. Our findings highlight the importance of\npersonalized, step-by-step guidance tailored to older adults' learning needs,\nas well as fostering confidence through hands-on engagement with technology.\nKey challenges identified include limited transportation options, poor internet\nconnectivity, outdated devices, and a lack of familial support for learning. To\naddress these barriers, we propose enhanced public funding, expanded access to\nresources, and sustainable strategies such as providing relevant and practical\ncourse materials. Additionally, innovative tools like simulated online\nplatforms for practicing digital transactions can help reduce anxiety and\nenhance digital literacy among older adults. This study underscores the vital\nrole that digital educators play in bridging the digital divide, creating a\nmore inclusive, human-centered approach to digital learning for older adults.\n","authors":["Melanie Gruben","Ashley Sheil","Sanchari Das","Michelle O Keeffe","Jacob Camilleri","Moya Cronin","Hazel Murray"],"pdf_url":"https://arxiv.org/pdf/2502.10166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02844v2","updated":"2025-02-14T13:27:24Z","published":"2025-02-05T02:59:23Z","title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement\n  Learning","summary":"  Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystem-wide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL.\n","authors":["Sunwoo Lee","Jaebak Hwang","Yonghyeon Jo","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02844v2.pdf","comment":"8 pages main, 21 pages appendix with reference. Submitted to ICML\n  2025"},{"id":"http://arxiv.org/abs/2502.10490v1","updated":"2025-02-14T13:15:13Z","published":"2025-02-14T13:15:13Z","title":"A Robust Attack: Displacement Backdoor Attack","summary":"  As artificial intelligence becomes more prevalent in our lives, people are\nenjoying the convenience it brings, but they are also facing hidden threats,\nsuch as data poisoning and ad- versarial attacks. These threats can have\ndisastrous consequences for the application of artificial intelligence,\nespecially for some applications that take effect immediately, such as\nautonomous driving and medical fields. Among these threats, backdoor attacks\nhave left a deep impression on people with their concealment and simple\ndeployment, making them a threat that cannot be ignored, however, in the\nprocess of deploying the backdoor model, the backdoor attack often has some\nreasons that make it unsatisfactory in real-world applications, such as jitter\nand brightness changes. Based on this, we propose a highly robust backdoor\nattack that shifts the target sample and combines it with itself to form a\nbackdoor sample, the Displacement Backdoor Attack(DBA). Experimental results\nshow that the DBA attack can resist data augmentation that simulates real-world\ndifferences, such as rotation and cropping.\n","authors":["Yong Li","Han Gao"],"pdf_url":"https://arxiv.org/pdf/2502.10490v1.pdf","comment":"6 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2405.16488"},{"id":"http://arxiv.org/abs/2502.09201v2","updated":"2025-02-14T12:35:38Z","published":"2025-02-13T11:39:58Z","title":"Commitment Schemes from OWFs with Applications to Quantum Oblivious\n  Transfer","summary":"  Commitment schemes are essential to many cryptographic protocols and schemes\nwith applications that include privacy-preserving computation on data,\nprivacy-preserving authentication, and, in particular, oblivious transfer\nprotocols. For quantum oblivious transfer (qOT) protocols, unconditionally\nbinding commitment schemes that do not rely on hardness assumptions from\nstructured mathematical problems are required. These additional constraints\nseverely limit the choice of commitment schemes to random oracle-based\nconstructions or Naor's bit commitment scheme. As these protocols commit to\nindividual bits, the use of such commitment schemes comes at a high bandwidth\nand computational cost. In this work, we investigate improvements to the\nefficiency of commitment schemes used in qOT protocols and propose an extension\nof Naor's commitment scheme requiring the existence of one-way functions (OWF)\nto reduce communication complexity for 2-bit strings. Additionally, we provide\nan interactive string commitment scheme with preprocessing to enable a fast and\nefficient computation of commitments.\n","authors":["Thomas Lornser","Sebastian Ramacher","Federico Valbusa"],"pdf_url":"https://arxiv.org/pdf/2502.09201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10110v1","updated":"2025-02-14T12:16:38Z","published":"2025-02-14T12:16:38Z","title":"ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models","summary":"  With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites.\n","authors":["Hiroki Nakano","Takashi Koide","Daiki Chiba"],"pdf_url":"https://arxiv.org/pdf/2502.10110v1.pdf","comment":"Accepted for publication at DIMVA 2025"},{"id":"http://arxiv.org/abs/2412.16264v3","updated":"2025-02-14T12:15:37Z","published":"2024-12-20T09:22:07Z","title":"Continual Learning with Strategic Selection and Forgetting for Network\n  Intrusion Detection","summary":"  Intrusion Detection Systems (IDS) are crucial for safeguarding digital\ninfrastructure. In dynamic network environments, both threat landscapes and\nnormal operational behaviors are constantly changing, resulting in concept\ndrift. While continuous learning mitigates the adverse effects of concept\ndrift, insufficient attention to drift patterns and excessive preservation of\noutdated knowledge can still hinder the IDS's adaptability. In this paper, we\npropose SSF (Strategic Selection and Forgetting), a novel continual learning\nmethod for IDS, providing continuous model updates with a constantly refreshed\nmemory buffer. Our approach features a strategic sample selection algorithm to\nselect representative new samples and a strategic forgetting mechanism to drop\noutdated samples. The proposed strategic sample selection algorithm prioritizes\nnew samples that cause the `drifted' pattern, enabling the model to better\nunderstand the evolving landscape. Additionally, we introduce strategic\nforgetting upon detecting significant drift by discarding outdated samples to\nfree up memory, allowing the incorporation of more recent data. SSF captures\nevolving patterns effectively and ensures the model is aligned with the change\nof data patterns, significantly enhancing the IDS's adaptability to concept\ndrift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15\ndatasets demonstrates its superior adaptability to concept drift for network\nintrusion detection. The code is released at\nhttps://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.\n","authors":["Xinchen Zhang","Running Zhao","Zhihan Jiang","Handi Chen","Yulong Ding","Edith C. H. Ngai","Shuang-Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.16264v3.pdf","comment":"Accepted by IEEE International Conference on Computer Communications\n  (INFOCOM) 2025"},{"id":"http://arxiv.org/abs/2502.10487v1","updated":"2025-02-14T11:15:27Z","published":"2025-02-14T11:15:27Z","title":"Fast Proxies for LLM Robustness Evaluation","summary":"  Evaluating the robustness of LLMs to adversarial attacks is crucial for safe\ndeployment, yet current red-teaming methods are often prohibitively expensive.\nWe compare the ability of fast proxy metrics to predict the real-world\nrobustness of an LLM against a simulated attacker ensemble. This allows us to\nestimate a model's robustness to computationally expensive attacks without\nrequiring runs of the attacks themselves. Specifically, we consider\ngradient-descent-based embedding-space attacks, prefilling attacks, and direct\nprompting. Even though direct prompting in particular does not achieve high\nASR, we find that it and embedding-space attacks can predict attack success\nrates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank)\ncorrelations with the full attack ensemble while reducing computational cost by\nthree orders of magnitude.\n","authors":["Tim Beyer","Jan Schuchardt","Leo Schwinn","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2502.10487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09535v2","updated":"2025-02-14T10:43:47Z","published":"2025-02-13T17:50:58Z","title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based\n  Security","summary":"  Mobile sensor data has been proposed for security-critical applications such\nas device pairing, proximity detection, and continuous authentication. However,\nthe foundational assumption that these signals provide sufficient entropy\nremains under-explored. In this work, we systematically analyse the entropy of\nmobile sensor data across four diverse datasets spanning multiple application\ncontexts. Our findings reveal pervasive biases, with single-sensor mean\nmin-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite\nShannon entropy being several multiples higher. We further demonstrate that\ncorrelations between sensor modalities reduce the worst-case entropy of using\nmultiple sensors by up to approx. 75% compared to average-case Shannon entropy.\nThis brings joint min-entropy well below 10 bits in many cases and, in the best\ncase, yielding only approx. 24 bits of min-entropy when combining 20 sensor\nmodalities. These results call into question the widely held assumption that\nadding more sensors inherently yields higher security. We ultimately caution\nagainst relying on raw sensor data as a primary source of randomness.\n","authors":["Carlton Shepherd","Elliot Hurley"],"pdf_url":"https://arxiv.org/pdf/2502.09535v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.10392v1","updated":"2025-02-14T18:59:59Z","published":"2025-02-14T18:59:59Z","title":"Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding","summary":"  In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.\n","authors":["Wenxuan Guo","Xiuwei Xu","Ziwei Wang","Jianjiang Feng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2502.10392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10390v1","updated":"2025-02-14T18:59:40Z","published":"2025-02-14T18:59:40Z","title":"(How) Can Transformers Predict Pseudo-Random Numbers?","summary":"  Transformers excel at discovering patterns in sequential data, yet their\nfundamental limitations and learning mechanisms remain crucial topics of\ninvestigation. In this paper, we study the ability of Transformers to learn\npseudo-random number sequences from linear congruential generators (LCGs),\ndefined by the recurrence relation $x_{t+1} = a x_t + c \\;\\mathrm{mod}\\; m$.\nOur analysis reveals that with sufficient architectural capacity and training\ndata variety, Transformers can perform in-context prediction of LCG sequences\nwith unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding\nlayers and attention patterns, we uncover how Transformers develop algorithmic\nstructures to learn these sequences in two scenarios of increasing complexity.\nFirst, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but\nfixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our\nanalysis reveals that models learn to factorize the modulus and utilize\ndigit-wise number representations to make sequential predictions. In the\nsecond, more challenging scenario of unseen moduli, we show that Transformers\ncan generalize to unseen moduli up to $m_{\\text{test}} = 2^{16}$. In this case,\nthe model employs a two-step strategy: first estimating the unknown modulus\nfrom the context, then utilizing prime factorizations to generate predictions.\nFor this task, we observe a sharp transition in the accuracy at a critical\ndepth $=3$. We also find that the number of in-context sequence elements needed\nto reach high accuracy scales sublinearly with the modulus.\n","authors":["Tao Tao","Darshil Doshi","Dayal Singh Kalra","Tianyu He","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2502.10390v1.pdf","comment":"10+16 pages, 12+20 figures"},{"id":"http://arxiv.org/abs/2502.10381v1","updated":"2025-02-14T18:57:16Z","published":"2025-02-14T18:57:16Z","title":"Balancing the Scales: A Theoretical and Algorithmic Framework for\n  Learning from Imbalanced Data","summary":"  Class imbalance remains a major challenge in machine learning, especially in\nmulti-class problems with long-tailed distributions. Existing methods, such as\ndata resampling, cost-sensitive techniques, and logistic loss modifications,\nthough popular and often effective, lack solid theoretical foundations. As an\nexample, we demonstrate that cost-sensitive methods are not Bayes consistent.\nThis paper introduces a novel theoretical framework for analyzing\ngeneralization in imbalanced classification. We propose a new class-imbalanced\nmargin loss function for both binary and multi-class settings, prove its strong\n$H$-consistency, and derive corresponding learning guarantees based on\nempirical loss and a new notion of class-sensitive Rademacher complexity.\nLeveraging these theoretical results, we devise novel and general learning\nalgorithms, IMMAX (Imbalanced Margin Maximization), which incorporate\nconfidence margins and are applicable to various hypothesis sets. While our\nfocus is theoretical, we also present extensive empirical results demonstrating\nthe effectiveness of our algorithms compared to existing baselines.\n","authors":["Corinna Cortes","Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.10381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08008v2","updated":"2025-02-14T18:52:34Z","published":"2025-02-11T23:07:14Z","title":"An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models","summary":"  Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.\n","authors":["Kasra Ahmadi","Rouzbeh Behnia","Reza Ebrahimi","Mehran Mozaffari Kermani","Jeremiah Birrell","Jason Pacheco","Attila A Yavuz"],"pdf_url":"https://arxiv.org/pdf/2502.08008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10373v1","updated":"2025-02-14T18:51:40Z","published":"2025-02-14T18:51:40Z","title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models","summary":"  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n","authors":["William Chen","Jinchuan Tian","Yifan Peng","Brian Yan","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.10373v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.10365v1","updated":"2025-02-14T18:43:22Z","published":"2025-02-14T18:43:22Z","title":"AffinityFlow: Guided Flows for Antibody Affinity Maturation","summary":"  Antibodies are widely used as therapeutics, but their development requires\ncostly affinity maturation, involving iterative mutations to enhance binding\naffinity.This paper explores a sequence-only scenario for affinity maturation,\nusing solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold\nwithin flow matching to generate diverse protein structures, enabling a\nsequence-conditioned generative model of structure. Building on this, we\npropose an alternating optimization framework that (1) fixes the sequence to\nguide structure generation toward high binding affinity using a structure-based\naffinity predictor, then (2) applies inverse folding to create sequence\nmutations, refined by a sequence-based affinity predictor for post selection.\nTo address this, we develop a co-teaching module that incorporates valuable\ninformation from noisy biophysical energies into predictor refinement. The\nsequence-based predictor selects consensus samples to teach the structure-based\npredictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art\nperformance in affinity maturation experiments. We plan to open-source our code\nafter acceptance.\n","authors":["Can Chen","Karla-Luise Herpoldt","Chenchao Zhao","Zichen Wang","Marcus Collins","Shang Shang","Ron Benson"],"pdf_url":"https://arxiv.org/pdf/2502.10365v1.pdf","comment":"14 pages, 5 figures"}]},"2025-02-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.12150v1","updated":"2025-02-17T18:59:02Z","published":"2025-02-17T18:59:02Z","title":"Idiosyncrasies in Large Language Models","summary":"  In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning existing text embedding models on LLM-generated texts\nyields excellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, particularly for training on synthetic\ndata and inferring model similarity. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.\n","authors":["Mingjie Sun","Yida Yin","Zhiqiu Xu","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12150v1.pdf","comment":"Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html"},{"id":"http://arxiv.org/abs/2502.12149v1","updated":"2025-02-17T18:58:36Z","published":"2025-02-17T18:58:36Z","title":"HARBOR: Exploring Persona Dynamics in Multi-Agent Competition","summary":"  We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.\n","authors":["Kenan Jiang","Li Xiong","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09589v2","updated":"2025-02-17T18:56:30Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as important factors. In addition, we show\nsimilarities and discrepancies between the logical reasoning performances of\nhumans and LLMs by collecting and comparing behavioral data from both.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.07381v4","updated":"2025-02-17T18:54:59Z","published":"2024-11-11T21:32:06Z","title":"MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement --\n  LLaMA3.1 and GPT-4o for Complete Abstract Adaptation","summary":"  This report is the system description of the MaLei team (Manchester and\nLeiden) for the shared task Plain Language Adaptation of Biomedical Abstracts\n(PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated\nwith TREC2024 (33rd Text REtrieval Conference\nhttps://ir.nist.gov/evalbase/conf/trec-2024). This report contains two sections\ncorresponding to the two sub-tasks in PLABA-2024. In task one (term\nreplacement), we applied fine-tuned ReBERTa-Base models to identify and\nclassify the difficult terms, jargon, and acronyms in the biomedical abstracts\nand reported the F1 score (Task 1A and 1B). In task two (complete abstract\nadaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot\nprompts to complete the abstract adaptation and reported the scores in BLEU,\nSARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024\non Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd\nand 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores\nacross the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed\nmodel achieved the highest Completeness score for Task 2. We share our source\ncodes, fine-tuned models, and related resources at\nhttps://github.com/HECTA-UoM/PLABA2024\n","authors":["Zhidong Ling","Zihao Li","Pablo Romero","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2411.07381v4.pdf","comment":"ongoing work - system report for PLABA2024 with TREC-2024"},{"id":"http://arxiv.org/abs/2502.12137v1","updated":"2025-02-17T18:53:42Z","published":"2025-02-17T18:53:42Z","title":"REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives","summary":"  Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment\n","authors":["Sayantan Adak","Pauras Mangesh Meher","Paramita Das","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12137v1.pdf","comment":"Accepted at COLING2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.12134v1","updated":"2025-02-17T18:52:29Z","published":"2025-02-17T18:52:29Z","title":"SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs","summary":"  Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nsuffer from catastrophic forgetting, limiting their applicability to\nstate-of-the-art LLMs that already perform well in zero-shot settings with a\nproper instruction. To address this challenge, we propose a novel approach for\ncontinuous-space reasoning that does not require modifying the underlying LLM.\nSpecifically, we employ a lightweight assistant model to generate\ninstance-specific soft thought tokens speculatively as the initial chain of\nthoughts, which are then mapped into the LLM's representation space via a\nprojection module. Experimental results on five reasoning benchmarks\ndemonstrate that our method enhances LLM reasoning performance through\nsupervised, parameter-efficient fine-tuning.\n","authors":["Yige Xu","Xu Guo","Zhiwei Zeng","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2502.12134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13018v2","updated":"2025-02-17T18:51:33Z","published":"2024-12-17T15:38:42Z","title":"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain","summary":"  As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.\n","authors":["Shuting Wang","Jiejun Tan","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.13018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09606v2","updated":"2025-02-17T18:48:26Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12124v1","updated":"2025-02-17T18:46:46Z","published":"2025-02-17T18:46:46Z","title":"RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for\n  Inspirational Quote Extraction from Long Documents","summary":"  Inspirational quotes from famous individuals are often used to convey\nthoughts in news articles, essays, and everyday conversations. In this paper,\nwe propose a novel context-based quote extraction system that aims to extract\nthe most relevant quote from a long text. We formulate this quote extraction as\nan open domain question answering problem first by employing a vector-store\nbased retriever and then applying a multi-task reader. We curate three\ncontext-based quote extraction datasets and introduce a novel multi-task\nframework RA-MTR that improves the state-of-the-art performance, achieving a\nmaximum improvement of 5.08% in BoW F1-score.\n","authors":["Sayantan Adak","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12124v1.pdf","comment":"Accepted at COLING2025-MAIN"},{"id":"http://arxiv.org/abs/2502.12123v1","updated":"2025-02-17T18:46:32Z","published":"2025-02-17T18:46:32Z","title":"On the Query Complexity of Verifier-Assisted Language Generation","summary":"  Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.\n","authors":["Edoardo Botta","Yuchen Li","Aashay Mehta","Jordan T. Ash","Cyril Zhang","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2502.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12120v1","updated":"2025-02-17T18:45:25Z","published":"2025-02-17T18:45:25Z","title":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws","summary":"  Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.\n","authors":["Prasanna Mayilvahanan","Thaddus Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2502.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12119v1","updated":"2025-02-17T18:43:41Z","published":"2025-02-17T18:43:41Z","title":"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection","summary":"  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n","authors":["Jinhe Bi","Yifan Wang","Danqi Yan","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12118v1","updated":"2025-02-17T18:43:24Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v2","updated":"2025-02-17T18:42:31Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v2.pdf","comment":"We will reflect comments from the reviewers and re-submit"},{"id":"http://arxiv.org/abs/2406.11785v3","updated":"2025-02-17T18:37:13Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12110v1","updated":"2025-02-17T18:36:14Z","published":"2025-02-17T18:36:14Z","title":"A-MEM: Agentic Memory for LLM Agents","summary":"  While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code is available at https://github.com/WujiangXu/AgenticMemory.\n","authors":["Wujiang Xu","Zujie Liang","Kai Mei","Hang Gao","Juntao Tan","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12109v1","updated":"2025-02-17T18:31:57Z","published":"2025-02-17T18:31:57Z","title":"Personality Structured Interview for Large Language Model Simulation in\n  Personality Research","summary":"  Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.\n","authors":["Pengda Wang","Huiqi Zou","Hanjie Chen","Tianjun Sun","Ziang Xiao","Frederick L. Oswald"],"pdf_url":"https://arxiv.org/pdf/2502.12109v1.pdf","comment":"41 Pages, 30 Tables, 5 Figures"},{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2412.18367v5","updated":"2025-02-17T18:13:38Z","published":"2024-12-24T11:50:18Z","title":"Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset (GIST)","summary":"  The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduce GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality is benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST is integrated into translation workflows using\npost-translation refinement methods that require no retraining, where LLM\nprompting consistently improves BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.\n","authors":["Jiarui Liu","Iman Ouzzani","Wenkai Li","Lechen Zhang","Tianyue Ou","Houda Bouamor","Zhijing Jin","Mona Diab"],"pdf_url":"https://arxiv.org/pdf/2412.18367v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12094v1","updated":"2025-02-17T18:12:36Z","published":"2025-02-17T18:12:36Z","title":"A Study on Leveraging Search and Self-Feedback for Agent Reasoning","summary":"  Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.\n","authors":["Karthikeyan K","Michelle Yuan","Elman Mansimov","Katerina Margatina","Anurag Pratik","Daniele Bonadiman","Monica Sunkara","Yi Zhang","Yassine Benajiba"],"pdf_url":"https://arxiv.org/pdf/2502.12094v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.03035v2","updated":"2025-02-17T18:11:20Z","published":"2025-01-06T14:23:02Z","title":"Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning","summary":"  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n","authors":["Zhen Li","Yupeng Su","Runming Yang","Congkai Xie","Zheng Wang","Zhongwei Xie","Ngai Wong","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.03035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18057v3","updated":"2025-02-17T18:08:17Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for removing private or hazardous\ninformation from deep learning models. While MU has advanced significantly in\nunimodal (text or vision) settings, multimodal unlearning (MMU) remains\nunderexplored due to the lack of open benchmarks for evaluating cross-modal\ndata removal. To address this gap, we introduce CLEAR, the first open-source\nbenchmark designed specifically for MMU. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We conduct a comprehensive\nanalysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four\nevaluation sets, demonstrating that jointly unlearning both modalities\noutperforms single-modality approaches. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16491v2","updated":"2025-02-17T18:05:21Z","published":"2024-10-21T20:32:27Z","title":"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded\n  Data","summary":"  In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in language. Leveraging this dataset, we explore Supervised\nFine-Tuning and Direct Preference Optimization as training-based methods to\nalign LLMs more naturally with human personality patterns. Our methods\noutperform prompting on personality assessments such as BFI and IPIP-NEO, with\ntrait correlations more closely matching human data. Furthermore, our\nexperiments reveal that models trained to exhibit higher conscientiousness,\nhigher agreeableness, lower extraversion, and lower neuroticism display better\nperformance on reasoning tasks, aligning with psychological findings on how\nthese traits impact human cognitive performance. To our knowledge, this work is\nthe first comprehensive study to demonstrate how training-based methods can\nshape LLM personalities through learning from real human behaviors.\n","authors":["Wenkai Li","Jiarui Liu","Andy Liu","Xuhui Zhou","Mona Diab","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2410.16491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12085v1","updated":"2025-02-17T17:59:56Z","published":"2025-02-17T17:59:56Z","title":"APB: Accelerating Distributed Long-Context Inference by Passing\n  Compressed Context Blocks across GPUs","summary":"  While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB.\n","authors":["Yuxiang Huang","Mingye Li","Xu Han","Chaojun Xiao","Weilin Zhao","Sun Ao","Hao Zhou","Jie Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12085v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12084v1","updated":"2025-02-17T17:57:50Z","published":"2025-02-17T17:57:50Z","title":"VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues","summary":"  Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\nfurther analysis of various language-side and vision-side prompting methods,\nleads to a total of eight key findings. We identify critical challenges in\nmodels' ability to link visual cues, highlighting a significant performance gap\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\nintegrating language-based reasoning in vision-centric tasks to prevent\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\nfostering models' ability to independently structure and infer relationships\namong visual cues.\n","authors":["Jianshu Zhang","Dongyu Yao","Renjie Pi","Paul Pu Liang","Yi R."," Fung"],"pdf_url":"https://arxiv.org/pdf/2502.12084v1.pdf","comment":"Project Page: https://vlm2-bench.github.io/"},{"id":"http://arxiv.org/abs/2502.12082v1","updated":"2025-02-17T17:56:23Z","published":"2025-02-17T17:56:23Z","title":"AdaSplash: Adaptive Sparse Flash Attention","summary":"  The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.\n","authors":["Nuno Gonalves","Marcos Treviso","Andr F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2502.12082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12081v1","updated":"2025-02-17T17:55:55Z","published":"2025-02-17T17:55:55Z","title":"Unhackable Temporal Rewarding for Scalable Video MLLMs","summary":"  In the pursuit of superior video-processing MLLMs, we have encountered a\nperplexing paradox: the \"anti-scaling law\", where more data and larger models\nlead to worse performance. This study unmasks the culprit: \"temporal hacking\",\na phenomenon where models shortcut by fixating on select frames, missing the\nfull video narrative. In this work, we systematically establish a comprehensive\ntheory of temporal hacking, defining it from a reinforcement learning\nperspective, introducing the Temporal Perplexity (TPL) score to assess this\nmisalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework\nto mitigate the temporal hacking. Both theoretically and empirically, TPL\nproves to be a reliable indicator of temporal modeling quality, correlating\nstrongly with frame activation patterns. Extensive experiments reveal that UTR\nnot only counters temporal hacking but significantly elevates video\ncomprehension capabilities. This work not only advances video-AI systems but\nalso illuminates the critical importance of aligning proxy rewards with true\nobjectives in MLLM development.\n","authors":["En Yu","Kangheng Lin","Liang Zhao","Yana Wei","Zining Zhu","Haoran Wei","Jianjian Sun","Zheng Ge","Xiangyu Zhang","Jingyu Wang","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2502.12081v1.pdf","comment":"Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/"},{"id":"http://arxiv.org/abs/2501.11613v6","updated":"2025-02-17T17:55:47Z","published":"2025-01-20T17:19:02Z","title":"Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems","summary":"  This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.\n","authors":["Giorgio Robino"],"pdf_url":"https://arxiv.org/pdf/2501.11613v6.pdf","comment":"Added Experimental Results sections"},{"id":"http://arxiv.org/abs/2502.12073v1","updated":"2025-02-17T17:43:08Z","published":"2025-02-17T17:43:08Z","title":"Can LLMs Simulate Social Media Engagement? A Study on Action-Guided\n  Response Generation","summary":"  Social media enables dynamic user engagement with trending topics, and recent\nresearch has explored the potential of large language models (LLMs) for\nresponse generation. While some studies investigate LLMs as agents for\nsimulating user behavior on social media, their focus remains on practical\nviability and scalability rather than a deeper understanding of how well LLM\naligns with human behavior. This paper analyzes LLMs' ability to simulate\nsocial media engagement through action guided response generation, where a\nmodel first predicts a user's most likely engagement action-retweet, quote, or\nrewrite-towards a trending post before generating a personalized response\nconditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and\nDeepSeek-R1 in social media engagement simulation regarding a major societal\nevent discussed on X. Our findings reveal that zero-shot LLMs underperform BERT\nin action prediction, while few-shot prompting initially degrades the\nprediction accuracy of LLMs with limited examples. However, in response\ngeneration, few-shot LLMs achieve stronger semantic alignment with ground truth\nposts.\n","authors":["Zhongyi Qiu","Hanjia Lyu","Wei Xiong","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12067v1","updated":"2025-02-17T17:37:26Z","published":"2025-02-17T17:37:26Z","title":"TokenSkip: Controllable Chain-of-Thought Compression in LLMs","summary":"  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.\n","authors":["Heming Xia","Yongqi Li","Chak Tou Leong","Wenjie Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.12067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12065v1","updated":"2025-02-17T17:34:48Z","published":"2025-02-17T17:34:48Z","title":"Formalizing Complex Mathematical Statements with LLMs: A Study on\n  Mathematical Definitions","summary":"  Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.\n","authors":["Lan Zhang","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2502.12065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11427v2","updated":"2025-02-17T17:34:45Z","published":"2024-06-17T11:25:57Z","title":"DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors","summary":"  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n","authors":["Keon Lee","Dong Won Kim","Jaehyeon Kim","Seungjun Chung","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2406.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12064v1","updated":"2025-02-17T17:32:55Z","published":"2025-02-17T17:32:55Z","title":"AI-generated Text Detection with a GLTR-based Approach","summary":"  The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.\n","authors":["Luca Yan Wu","Isabel Segura-Bedmar"],"pdf_url":"https://arxiv.org/pdf/2502.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12057v1","updated":"2025-02-17T17:25:11Z","published":"2025-02-17T17:25:11Z","title":"Culture is Not Trivia: Sociocultural Theory for Cultural NLP","summary":"  The field of cultural NLP has recently experienced rapid growth, driven by a\npressing need to ensure that language technologies are effective and safe\nacross a pluralistic user base. This work has largely progressed without a\nshared conception of culture, instead choosing to rely on a wide array of\ncultural proxies. However, this leads to a number of recurring limitations:\ncoarse national boundaries fail to capture nuanced differences that lay within\nthem, limited coverage restricts datasets to only a subset of usually\nhighly-represented cultures, and a lack of dynamicity results in static\ncultural benchmarks that do not change as culture evolves. In this position\npaper, we argue that these methodological limitations are symptomatic of a\ntheoretical gap. We draw on a well-developed theory of culture from\nsociocultural linguistics to fill this gap by 1) demonstrating in a case study\nhow it can clarify methodological constraints and affordances, 2) offering\ntheoretically-motivated paths forward to achieving cultural competence, and 3)\narguing that localization is a more useful framing for the goals of much\ncurrent work in cultural NLP.\n","authors":["Naitian Zhou","David Bamman","Isaac L. Bleaman"],"pdf_url":"https://arxiv.org/pdf/2502.12057v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2405.01474v3","updated":"2025-02-17T17:24:42Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v3.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.12055v1","updated":"2025-02-17T17:24:37Z","published":"2025-02-17T17:24:37Z","title":"Designing Role Vectors to Improve LLM Inference Behaviour","summary":"  The influence of personas on Large Language Models (LLMs) has been widely\nstudied, yet their direct impact on performance remains uncertain. This work\nexplores a novel approach to guiding LLM behaviour through role vectors, an\nalternative to persona-based prompting. We construct 29 role vectors derived\nfrom model activations and evaluate their impact on benchmark performance\nacross multiple domains. Our analysis investigates whether these vectors can\neffectively steer models toward domain-specific expertise. We measure two key\ninterventions: (i) activation addition, which reinforces role-specific\ndirections, and (ii) directional ablation, which removes them. Results on\nwell-established benchmarks indicate that role vectors do, in fact, influence\nmodel behaviour, improving task performance in relevant domains while\nmarginally affecting unrelated tasks. This, in turn, suggests that manipulating\ninternal model representations has a greater impact on outcomes than\npersona-based prompting.\n","authors":["Daniele Potert","Andrea Seveso","Fabio Mercorio"],"pdf_url":"https://arxiv.org/pdf/2502.12055v1.pdf","comment":"Submitted to ARR 2025 February cycle"},{"id":"http://arxiv.org/abs/2502.12052v1","updated":"2025-02-17T17:22:49Z","published":"2025-02-17T17:22:49Z","title":"A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability","summary":"  In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.\n","authors":["Xinyu Hu","Mingqi Gao","Li Lin","Zhenghan Yu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2502.12052v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.12051v1","updated":"2025-02-17T17:20:41Z","published":"2025-02-17T17:20:41Z","title":"How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines","summary":"  Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.\n","authors":["Ayan Sengupta","Yash Goel","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.12051v1.pdf","comment":"20 pages, 8 tables, 4 figures"},{"id":"http://arxiv.org/abs/2502.12050v1","updated":"2025-02-17T17:18:39Z","published":"2025-02-17T17:18:39Z","title":"SpeechT: Findings of the First Mentorship in Speech Translation","summary":"  This work presents the details and findings of the first mentorship in speech\ntranslation (SpeechT), which took place in December 2024 and January 2025. To\nfulfil the requirements of the mentorship, the participants engaged in key\nactivities, including data preparation, modelling, and advanced research.\n","authors":["Yasmin Moslem","Juan Julin Cea Morn","Mariano Gonzalez-Gomez","Muhammad Hazim Al Farouq","Farah Abdou","Satarupa Deb"],"pdf_url":"https://arxiv.org/pdf/2502.12050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05171v2","updated":"2025-02-17T17:14:04Z","published":"2025-02-07T18:55:02Z","title":"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach","summary":"  We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.\n","authors":["Jonas Geiping","Sean McLeish","Neel Jain","John Kirchenbauer","Siddharth Singh","Brian R. Bartoldson","Bhavya Kailkhura","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.05171v2.pdf","comment":"The model is available at\n  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can\n  be found at https://github.com/seal-rg/recurrent-pretraining"},{"id":"http://arxiv.org/abs/2502.12025v1","updated":"2025-02-17T16:57:56Z","published":"2025-02-17T16:57:56Z","title":"SafeChain: Safety of Language Models with Long Chain-of-Thought\n  Reasoning Capabilities","summary":"  Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.\n","authors":["Fengqing Jiang","Zhangchen Xu","Yuetai Li","Luyao Niu","Zhen Xiang","Bo Li","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2502.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12022v1","updated":"2025-02-17T16:56:23Z","published":"2025-02-17T16:56:23Z","title":"Teaching LLMs According to Their Aptitude: Adaptive Reasoning for\n  Mathematical Problem Solving","summary":"  Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.\n","authors":["Xin Xu","Yan Xu","Tianhao Chen","Yuchen Yan","Chengwu Liu","Zaoyu Chen","Yufei Wang","Yichun Yin","Yasheng Wang","Lifeng Shang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12022v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.12018v1","updated":"2025-02-17T16:52:42Z","published":"2025-02-17T16:52:42Z","title":"Atom of Thoughts for Markov LLM Test-Time Scaling","summary":"  Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.\n","authors":["Fengwei Teng","Zhaoyang Yu","Quan Shi","Jiayi Zhang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00997v3","updated":"2025-02-17T16:51:23Z","published":"2025-02-03T02:34:46Z","title":"MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs","summary":"  The recent success of specialized Large Language Models (LLMs) in domains\nsuch as mathematical reasoning and coding has led to growing interest in\nmethods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)\nmodel, with the goal of enhancing performance in each domain while retaining\neffectiveness on general tasks. However, the effective merging of expert models\nremains an open challenge, especially for models with highly divergent weight\nparameters or different architectures. State-of-the-art MoE merging methods\nonly work with homogeneous model architectures and rely on simple unweighted\naveraging to merge expert layers, which does not address parameter interference\nand requires extensive fine-tuning of the merged MoE to restore performance. To\naddress these limitations, this paper introduces new MoE merging techniques,\nincluding strategies to mitigate parameter interference, routing heuristics to\nreduce the need for MoE fine-tuning, and a novel method for merging experts\nwith different architectures. Extensive experiments across multiple domains\ndemonstrate the effectiveness of our proposed methods, reducing fine-tuning\ncosts, improving performance over state-of-the-art methods, and expanding the\napplicability of MoE merging.\n","authors":["Yuhang Zhou","Giannis Karamanolakis","Victor Soto","Anna Rumshisky","Mayank Kulkarni","Furong Huang","Wei Ai","Jianhua Lu"],"pdf_url":"https://arxiv.org/pdf/2502.00997v3.pdf","comment":"Accepted by NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.12007v1","updated":"2025-02-17T16:43:47Z","published":"2025-02-17T16:43:47Z","title":"Demographic Attributes Prediction from Speech Using WavLM Embeddings","summary":"  This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.\n","authors":["Yuchen Yang","Thomas Thebaud","Najim Dehak"],"pdf_url":"https://arxiv.org/pdf/2502.12007v1.pdf","comment":"6 pages, accepted by The Conference on Information Sciences and\n  Systems (CISS)"},{"id":"http://arxiv.org/abs/2502.12001v1","updated":"2025-02-17T16:39:28Z","published":"2025-02-17T16:39:28Z","title":"Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition","summary":"  This paper investigates the integration of technical vocabulary in merged\nlanguage models. We explore the knowledge transfer mechanisms involved when\ncombining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.\n","authors":["Thibault Rousset","Taisei Kakibuchi","Yusuke Sasaki","Yoshihide Nomura"],"pdf_url":"https://arxiv.org/pdf/2502.12001v1.pdf","comment":"Presented at the 263rd IPSJ-NL Workshop"},{"id":"http://arxiv.org/abs/2502.11995v1","updated":"2025-02-17T16:35:15Z","published":"2025-02-17T16:35:15Z","title":"Presumed Cultural Identity: How Names Shape LLM Responses","summary":"  Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.\n","authors":["Siddhesh Pawar","Arnav Arora","Lucie-Aime Kaffee","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.11995v1.pdf","comment":"23 Pages, 13 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2502.09969v2","updated":"2025-02-17T16:26:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tr"],"pdf_url":"https://arxiv.org/pdf/2502.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14483v2","updated":"2025-02-17T16:21:10Z","published":"2024-11-19T20:16:26Z","title":"Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat","summary":"  Deciding which large language model (LLM) to use is a complex challenge.\nPairwise ranking has emerged as a new method for evaluating human preferences\nfor LLMs. This approach entails humans evaluating pairs of model outputs based\non a predefined criterion. By collecting these comparisons, a ranking can be\nconstructed using methods such as Elo. However, applying these algorithms as\nconstructed in the context of LLM evaluation introduces several challenges. In\nthis paper, we explore the effectiveness of ranking systems for head-to-head\ncomparisons of LLMs. We formally define a set of fundamental principles for\neffective ranking and conduct a series of extensive evaluations on the\nrobustness of several ranking algorithms in the context of LLMs. Our analysis\nuncovers key insights into the factors that affect ranking accuracy and\nefficiency, offering guidelines for selecting the most appropriate methods\nbased on specific evaluation contexts and resource constraints.\n","authors":["Roland Daynauth","Christopher Clarke","Krisztian Flautner","Lingjia Tang","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2411.14483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11973v1","updated":"2025-02-17T16:20:22Z","published":"2025-02-17T16:20:22Z","title":"Generating Text from Uniform Meaning Representation","summary":"  Uniform Meaning Representation (UMR) is a recently developed graph-based\nsemantic representation, which expands on Abstract Meaning Representation (AMR)\nin a number of ways, in particular through the inclusion of document-level\ninformation and multilingual flexibility. In order to effectively adopt and\nleverage UMR for downstream tasks, efforts must be placed toward developing a\nUMR technological ecosystem. Though still limited amounts of UMR annotations\nhave been produced to date, in this work, we investigate the first approaches\nto producing text from multilingual UMR graphs: (1) a pipeline conversion of\nUMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large\nlanguage models with UMR data, and (3) fine-tuning existing AMR-to-text\ngeneration models with UMR data. Our best performing model achieves a\nmultilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared\nto the reference, which is a promising indication of the effectiveness of\nfine-tuning approaches for UMR-to-text generation with even limited amounts of\nUMR data.\n","authors":["Emma Markle","Reihaneh Iranmanesh","Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2502.11973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v2","updated":"2025-02-17T16:11:44Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v2.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.11962v1","updated":"2025-02-17T16:10:30Z","published":"2025-02-17T16:10:30Z","title":"Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware\n  Instruction Fine-Tuning","summary":"  Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.\n","authors":["Tianyi Wu","Jingwei Ni","Bryan Hooi","Jiaheng Zhang","Elliott Ash","See-Kiong Ng","Mrinmaya Sachan","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2502.11962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11948v1","updated":"2025-02-17T16:01:41Z","published":"2025-02-17T16:01:41Z","title":"Can Your Uncertainty Scores Detect Hallucinated Entity?","summary":"  To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research.\n","authors":["Min-Hsuan Yeh","Max Kamachee","Seongheon Park","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.11948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v5","updated":"2025-02-17T16:01:40Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v5.pdf","comment":"13 pages, under review"},{"id":"http://arxiv.org/abs/2502.11946v1","updated":"2025-02-17T15:58:56Z","published":"2025-02-17T15:58:56Z","title":"Step-Audio: Unified Understanding and Generation in Intelligent Speech\n  Interaction","summary":"  Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.\n","authors":["Ailin Huang","Boyong Wu","Bruce Wang","Chao Yan","Chen Hu","Chengli Feng","Fei Tian","Feiyu Shen","Jingbei Li","Mingrui Chen","Peng Liu","Ruihang Miao","Wang You","Xi Chen","Xuerui Yang","Yechang Huang","Yuxiang Zhang","Zheng Gong","Zixin Zhang","Brian Li","Changyi Wan","Hanpeng Hu","Ranchen Ming","Song Yuan","Xuelin Zhang","Yu Zhou","Bingxin Li","Buyun Ma","Kang An","Wei Ji","Wen Li","Xuan Wen","Yuankai Ma","Yuanwei Liang","Yun Mou","Bahtiyar Ahmidi","Bin Wang","Bo Li","Changxin Miao","Chen Xu","Chengting Feng","Chenrun Wang","Dapeng Shi","Deshan Sun","Dingyuan Hu","Dula Sai","Enle Liu","Guanzhe Huang","Gulin Yan","Heng Wang","Haonan Jia","Haoyang Zhang","Jiahao Gong","Jianchang Wu","Jiahong Liu","Jianjian Sun","Jiangjie Zhen","Jie Feng","Jie Wu","Jiaoren Wu","Jie Yang","Jinguo Wang","Jingyang Zhang","Junzhe Lin","Kaixiang Li","Lei Xia","Li Zhou","Longlong Gu","Mei Chen","Menglin Wu","Ming Li","Mingxiao Li","Mingyao Liang","Na Wang","Nie Hao","Qiling Wu","Qinyuan Tan","Shaoliang Pang","Shiliang Yang","Shuli Gao","Siqi Liu","Sitong Liu","Tiancheng Cao","Tianyu Wang","Wenjin Deng","Wenqing He","Wen Sun","Xin Han","Xiaomin Deng","Xiaojia Liu","Xu Zhao","Yanan Wei","Yanbo Yu","Yang Cao","Yangguang Li","Yangzhen Ma","Yanming Xu","Yaqiang Shi","Yilei Wang","Yinmin Zhong","Yu Luo","Yuanwei Lu","Yuhe Yin","Yuting Yan","Yuxiang Yang","Zhe Xie","Zheng Ge","Zheng Sun","Zhewei Huang","Zhichao Chang","Zidong Yang","Zili Zhang","Binxing Jiao","Daxin Jiang","Heung-Yeung Shum","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18547v4","updated":"2025-02-17T15:55:08Z","published":"2024-12-24T16:55:45Z","title":"Token-Budget-Aware LLM Reasoning","summary":"  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n","authors":["Tingxu Han","Zhenting Wang","Chunrong Fang","Shiyu Zhao","Shiqing Ma","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18547v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11932v1","updated":"2025-02-17T15:42:01Z","published":"2025-02-17T15:42:01Z","title":"On Representational Dissociation of Language and Arithmetic in Large\n  Language Models","summary":"  The association between language and (non-linguistic) thinking ability in\nhumans has long been debated, and recently, neuroscientific evidence of brain\nactivity patterns has been considered. Such a scientific context naturally\nraises an interdisciplinary question -- what about such a language-thought\ndissociation in large language models (LLMs)? In this paper, as an initial\nforay, we explore this question by focusing on simple arithmetic skills (e.g.,\n$1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in\nLLMs' representation space. Our experiments with linear classifiers and cluster\nseparability tests demonstrate that simple arithmetic equations and general\nlanguage input are encoded in completely separated regions in LLMs' internal\nrepresentation space across all the layers, which is also supported with more\ncontrolled stimuli (e.g., spelled-out equations). These tentatively suggest\nthat arithmetic reasoning is mapped into a distinct region from general\nlanguage input, which is in line with the neuroscientific observations of human\nbrain activations, while we also point out their somewhat cognitively\nimplausible geometric properties.\n","authors":["Riku Kisako","Tatsuki Kuribayashi","Ryohei Sasano"],"pdf_url":"https://arxiv.org/pdf/2502.11932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11926v1","updated":"2025-02-17T15:39:50Z","published":"2025-02-17T15:39:50Z","title":"BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages","summary":"  People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER-- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.\n","authors":["Shamsuddeen Hassan Muhammad","Nedjma Ousidhoum","Idris Abdulmumin","Jan Philip Wahle","Terry Ruas","Meriem Beloucif","Christine de Kock","Nirmal Surange","Daniela Teodorescu","Ibrahim Said Ahmad","David Ifeoluwa Adelani","Alham Fikri Aji","Felermino D. M. A. Ali","Ilseyar Alimova","Vladimir Araujo","Nikolay Babakov","Naomi Baes","Ana-Maria Bucur","Andiswa Bukula","Guanqun Cao","Rodrigo Tufino Cardenas","Rendi Chevi","Chiamaka Ijeoma Chukwuneke","Alexandra Ciobotaru","Daryna Dementieva","Murja Sani Gadanya","Robert Geislinger","Bela Gipp","Oumaima Hourrane","Oana Ignat","Falalu Ibrahim Lawan","Rooweither Mabuya","Rahmad Mahendra","Vukosi Marivate","Andrew Piper","Alexander Panchenko","Charles Henrique Porto Ferreira","Vitaly Protasov","Samuel Rutunda","Manish Shrivastava","Aura Cristina Udrea","Lilian Diana Awuor Wanzare","Sophie Wu","Florian Valentin Wunderlich","Hanif Muhammad Zhafran","Tianhui Zhang","Yi Zhou","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2502.11926v1.pdf","comment":"20 pages, under review"},{"id":"http://arxiv.org/abs/2502.11919v1","updated":"2025-02-17T15:32:54Z","published":"2025-02-17T15:32:54Z","title":"From Text to Trust: Empowering AI-assisted Decision Making with Adaptive\n  LLM-powered Analysis","summary":"  AI-assisted decision making becomes increasingly prevalent, yet individuals\noften fail to utilize AI-based decision aids appropriately especially when the\nAI explanations are absent, potentially as they do not %understand reflect on\nAI's decision recommendations critically. Large language models (LLMs), with\ntheir exceptional conversational and analytical capabilities, present great\nopportunities to enhance AI-assisted decision making in the absence of AI\nexplanations by providing natural-language-based analysis of AI's decision\nrecommendation, e.g., how each feature of a decision making task might\ncontribute to the AI recommendation. In this paper, via a randomized\nexperiment, we first show that presenting LLM-powered analysis of each task\nfeature, either sequentially or concurrently, does not significantly improve\npeople's AI-assisted decision performance. To enable decision makers to better\nleverage LLM-powered analysis, we then propose an algorithmic framework to\ncharacterize the effects of LLM-powered analysis on human decisions and\ndynamically decide which analysis to present. Our evaluation with human\nsubjects shows that this approach effectively improves decision makers'\nappropriate reliance on AI in AI-assisted decision making.\n","authors":["Zhuoyan Li","Hangxiao Zhu","Zhuoran Lu","Ziang Xiao","Ming Yin"],"pdf_url":"https://arxiv.org/pdf/2502.11919v1.pdf","comment":"CHI 2025"},{"id":"http://arxiv.org/abs/2502.11916v1","updated":"2025-02-17T15:31:59Z","published":"2025-02-17T15:31:59Z","title":"EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models","summary":"  Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.\n","authors":["Jiamin Su","Yibo Yan","Fangteng Fu","Han Zhang","Jingheng Ye","Xiang Liu","Jiahao Huo","Huiyu Zhou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11916v1.pdf","comment":"JS and YY are co-first authors. XH is the corresponding author"},{"id":"http://arxiv.org/abs/2502.10051v2","updated":"2025-02-17T15:30:22Z","published":"2025-02-14T10:00:20Z","title":"ORI: O Routing Intelligence","summary":"  Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.\n","authors":["Ahmad Shadid","Rahul Kumar","Mohit Mayank"],"pdf_url":"https://arxiv.org/pdf/2502.10051v2.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.11903v1","updated":"2025-02-17T15:24:49Z","published":"2025-02-17T15:24:49Z","title":"MMRC: A Large-Scale Benchmark for Understanding Multimodal Large\n  Language Model in Real-World Conversation","summary":"  Recent multimodal large language models (MLLMs) have demonstrated significant\npotential in open-ended conversation, generating more accurate and personalized\nresponses. However, their abilities to memorize, recall, and reason in\nsustained interactions within real-world scenarios remain underexplored. This\npaper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for\nevaluating six core open-ended abilities of MLLMs: information extraction,\nmulti-turn reasoning, information update, image management, memory recall, and\nanswer refusal. With data collected from real-world scenarios, MMRC comprises\n5,120 conversations and 28,720 corresponding manually labeled questions, posing\na significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC\nindicate an accuracy drop during open-ended interactions. We identify four\ncommon failure patterns: long-term memory degradation, inadequacies in updating\nfactual knowledge, accumulated assumption of error propagation, and reluctance\nto say no. To mitigate these issues, we propose a simple yet effective\nNOTE-TAKING strategy, which can record key information from the conversation\nand remind the model during its responses, enhancing conversational\ncapabilities. Experiments across six MLLMs demonstrate significant performance\nimprovements.\n","authors":["Haochen Xue","Feilong Tang","Ming Hu","Yexin Liu","Qidong Huang","Yulong Li","Chengzhi Liu","Zhongxing Xu","Chong Zhang","Chun-Mei Feng","Yutong Xie","Imran Razzak","Zongyuan Ge","Jionglong Su","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2502.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11901v1","updated":"2025-02-17T15:24:11Z","published":"2025-02-17T15:24:11Z","title":"Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o\n  Under Data Scarsity","summary":"  Existing LMs struggle with proof-oriented programming due to data scarcity,\nwhich manifest in two key ways: (1) a lack of sufficient corpora for\nproof-oriented programming languages such as F*, and (2) the absence of\nlarge-scale, project-level proof-oriented implementations that can teach the\nmodel the intricate reasoning process when performing proof-oriented\nprogramming. We present the first on synthetic data augmentation for project\nlevel proof oriented programming for both generation and repair. Our method\naddresses data scarcity by synthesizing basic proof-oriented programming\nproblems for proficiency in that language; incorporating diverse coding data\nfor reasoning capability elicitation and creating new proofs and repair data\nwithin existing repositories. This approach enables language models to both\nsynthesize and repair proofs for function- and repository-level code. We show\nthat our fine-tuned 14B parameter model, PoPilot, can exceed the performance of\nthe models that outperforms GPT-4o in project-level proof-oriented programming\nby 64% relative margin, and can improve GPT-4o's performance by 54% by\nrepairing its outputs over GPT-4o's self-repair.\n","authors":["Dylan Zhang","Justin Wang","Tianran Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06600v2","updated":"2025-02-17T15:22:32Z","published":"2025-02-10T16:00:00Z","title":"Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?","summary":"  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n","authors":["Gonalo Gomes","Chrysoula Zerva","Bruno Martins"],"pdf_url":"https://arxiv.org/pdf/2502.06600v2.pdf","comment":"Accepted in Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11890v1","updated":"2025-02-17T15:16:44Z","published":"2025-02-17T15:16:44Z","title":"Revisiting Classification Taxonomy for Grammatical Errors","summary":"  Grammatical error classification plays a crucial role in language learning\nsystems, but existing classification taxonomies often lack rigorous validation,\nleading to inconsistencies and unreliable feedback. In this paper, we revisit\nprevious classification taxonomies for grammatical errors by introducing a\nsystematic and qualitative evaluation framework. Our approach examines four\naspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability.\nThen, we construct a high-quality grammatical error classification dataset\nannotated with multiple classification taxonomies and evaluate them grounding\non our proposed evaluation framework. Our experiments reveal the drawbacks of\nexisting taxonomies. Our contributions aim to improve the precision and\neffectiveness of error analysis, providing more understandable and actionable\nfeedback for language learners.\n","authors":["Deqing Zou","Jingheng Ye","Yulu Liu","Yu Wu","Zishan Xu","Yinghui Li","Hai-Tao Zheng","Bingxu An","Zhao Wei","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11890v1.pdf","comment":"30 pages, 4 figures and 5 tables"},{"id":"http://arxiv.org/abs/2502.11886v1","updated":"2025-02-17T15:13:29Z","published":"2025-02-17T15:13:29Z","title":"LIMR: Less is More for RL Scaling","summary":"  In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.\n","authors":["Xuefeng Li","Haoyang Zou","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11886v1.pdf","comment":"6pages"},{"id":"http://arxiv.org/abs/2502.11882v1","updated":"2025-02-17T15:09:45Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2502.11881v1","updated":"2025-02-17T15:08:50Z","published":"2025-02-17T15:08:50Z","title":"Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models","summary":"  Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.\n","authors":["Hyunwoo Kim","Melanie Sclar","Tan Zhi-Xuan","Lance Ying","Sydney Levine","Yang Liu","Joshua B. Tenenbaum","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2502.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11880v1","updated":"2025-02-17T15:06:28Z","published":"2025-02-17T15:06:28Z","title":"Bitnet.cpp: Efficient Edge Inference for Ternary LLMs","summary":"  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shijie Cao","Yan Xia","Ting Cao","Jianyu Wei","Shuming Ma","Hongyu Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11880v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.02039v2","updated":"2025-02-17T15:05:40Z","published":"2024-07-02T08:11:18Z","title":"Prompt Stability Scoring for Text Annotation with Large Language Models","summary":"  Researchers are increasingly using language models (LMs) for text annotation.\nThese approaches rely only on a prompt telling the model to return a given\noutput according to a set of instructions. The reproducibility of LM outputs\nmay nonetheless be vulnerable to small changes in the prompt design. This calls\ninto question the replicability of classification routines. To tackle this\nproblem, researchers have typically tested a variety of semantically similar\nprompts to determine what we call ``prompt stability.\" These approaches remain\nad-hoc and task specific. In this article, we propose a general framework for\ndiagnosing prompt stability by adapting traditional approaches to intra- and\ninter-coder reliability scoring. We call the resulting metric the Prompt\nStability Score (PSS) and provide a Python package \\texttt{promptstability} for\nits estimation. Using six different datasets and twelve outcomes, we classify\n$\\sim$3.1m rows of data and $\\sim$300m input tokens to: a) diagnose when prompt\nstability is low; and b) demonstrate the functionality of the package. We\nconclude by providing best practice recommendations for applied researchers.\n","authors":["Christopher Barrie","Elli Palaiologou","Petter Trnberg"],"pdf_url":"https://arxiv.org/pdf/2407.02039v2.pdf","comment":"39 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.02079v2","updated":"2025-02-17T15:05:06Z","published":"2024-05-03T13:12:28Z","title":"Argumentative Large Language Models for Explainable and Contestable\n  Decision-Making","summary":"  The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.\n","authors":["Gabriel Freedman","Adam Dejl","Deniz Gorur","Xiang Yin","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2405.02079v2.pdf","comment":"18 pages, 18 figures, Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.11874v1","updated":"2025-02-17T15:02:09Z","published":"2025-02-17T15:02:09Z","title":"VAQUUM: Are Vague Quantifiers Grounded in Visual Data?","summary":"  Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.\n","authors":["Hugh Mee Wong","Rick Nouwen","Albert Gatt"],"pdf_url":"https://arxiv.org/pdf/2502.11874v1.pdf","comment":"Submitted to ARR ACL 2025, 12 pages for main paper (5 figures), 15\n  pages including appendix (2 figures)"},{"id":"http://arxiv.org/abs/2502.11866v1","updated":"2025-02-17T14:57:47Z","published":"2025-02-17T14:57:47Z","title":"Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire\n  Articles Beyond the Front Page","summary":"  I introduce a new large-scale dataset of historical wire articles from U.S.\nSouthern newspapers, spanning 1960-1975 and covering multiple wire services:\nThe Associated Press, United Press International, Newspaper Enterprise\nAssociation. Unlike prior work focusing on front-page content, this dataset\ncaptures articles across the entire newspaper, offering broader insight into\nmid-century Southern coverage. The dataset includes a version that has\nundergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its\nsuitability for quantitative text analysis. Additionally, duplicate versions of\narticles are retained to enable analysis of editorial differences in language\nand framing across newspapers. Each article is tagged by wire service,\nfacilitating comparative studies of editorial patterns across agencies. This\nresource opens new avenues for research in computational social science,\ndigital humanities, and historical linguistics, providing a detailed\nperspective on how Southern newspapers relayed national and international news\nduring a transformative period in American history. The dataset will be made\navailable upon publication or request for research purposes.\n","authors":["Michael McRae"],"pdf_url":"https://arxiv.org/pdf/2502.11866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11862v1","updated":"2025-02-17T14:53:49Z","published":"2025-02-17T14:53:49Z","title":"Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu","summary":"  In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.\n","authors":["Renhao Pei","Yihong Liu","Peiqin Lin","Franois Yvon","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2502.11862v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.11861v1","updated":"2025-02-17T14:53:23Z","published":"2025-02-17T14:53:23Z","title":"Exploring Large Language Models in Healthcare: Insights into Corpora\n  Sources, Customization Strategies, and Evaluation Metrics","summary":"  This study reviewed the use of Large Language Models (LLMs) in healthcare,\nfocusing on their training corpora, customization techniques, and evaluation\nmetrics. A systematic search of studies from 2021 to 2024 identified 61\narticles. Four types of corpora were used: clinical resources, literature,\nopen-source datasets, and web-crawled data. Common construction techniques\nincluded pre-training, prompt engineering, and retrieval-augmented generation,\nwith 44 studies combining multiple methods. Evaluation metrics were categorized\ninto process, usability, and outcome metrics, with outcome metrics divided into\nmodel-based and expert-assessed outcomes. The study identified critical gaps in\ncorpus fairness, which contributed to biases from geographic, cultural, and\nsocio-economic factors. The reliance on unverified or unstructured data\nhighlighted the need for better integration of evidence-based clinical\nguidelines. Future research should focus on developing a tiered corpus\narchitecture with vetted sources and dynamic weighting, while ensuring model\ntransparency. Additionally, the lack of standardized evaluation frameworks for\ndomain-specific models called for comprehensive validation of LLMs in\nreal-world healthcare settings.\n","authors":["Shuqi Yang","Mingrui Jing","Shuai Wang","Jiaxin Kou","Manfei Shi","Weijie Xing","Yan Hu","Zheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11861v1.pdf","comment":"45 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2502.11859v1","updated":"2025-02-17T14:50:53Z","published":"2025-02-17T14:50:53Z","title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics","summary":"  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n","authors":["Wenrui Xu","Dalin Lyu","Weihang Wang","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11856v1","updated":"2025-02-17T14:48:18Z","published":"2025-02-17T14:48:18Z","title":"LLMs as a synthesis between symbolic and continuous approaches to\n  language","summary":"  Since the middle of the 20th century, a fierce battle is being fought between\nsymbolic and continuous approaches to language and cognition. The success of\ndeep learning models, and LLMs in particular, has been alternatively taken as\nshowing that the continuous camp has won, or dismissed as an irrelevant\nengineering development. However, in this position paper I argue that deep\nlearning models for language actually represent a synthesis between the two\ntraditions. This is because 1) deep learning architectures allow for both\ncontinuous/distributed and symbolic/discrete-like representations and\ncomputations; 2) models trained on language make use this flexibility. In\nparticular, I review recent research in mechanistic interpretability that\nshowcases how a substantial part of morphosyntactic knowledge is encoded in a\nnear-discrete fashion in LLMs. This line of research suggests that different\nbehaviors arise in an emergent fashion, and models flexibly alternate between\nthe two modes (and everything in between) as needed. This is possibly one of\nthe main reasons for their wild success; and it is also what makes them\nparticularly interesting for the study of language and cognition. Is it time\nfor peace?\n","authors":["Gemma Boleda"],"pdf_url":"https://arxiv.org/pdf/2502.11856v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.11843v1","updated":"2025-02-17T14:36:39Z","published":"2025-02-17T14:36:39Z","title":"Can LLM Agents Maintain a Persona in Discourse?","summary":"  Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.\n","authors":["Pranav Bhandari","Nicolas Fay","Michael Wise","Amitava Datta","Stephanie Meek","Usman Naseem","Mehwish Nasim"],"pdf_url":"https://arxiv.org/pdf/2502.11843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14838v2","updated":"2025-02-17T14:34:58Z","published":"2024-12-19T13:28:42Z","title":"DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs","summary":"  Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.\n","authors":["Xiabin Zhou","Wenbin Wang","Minyan Zeng","Jiaxian Guo","Xuebo Liu","Li Shen","Min Zhang","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2412.14838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08356v2","updated":"2025-02-17T14:29:48Z","published":"2025-02-12T12:39:51Z","title":"Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.\n","authors":["Kushagra Bhushan","Yatin Nandwani","Dinesh Khandelwal","Sonam Gupta","Gaurav Pandey","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08356v2.pdf","comment":"22 pages, 14 tables, to be published in NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11830v1","updated":"2025-02-17T14:25:54Z","published":"2025-02-17T14:25:54Z","title":"Text Classification in the LLM Era - Where do we stand?","summary":"  Large Language Models revolutionized NLP and showed dramatic performance\nimprovements across several tasks. In this paper, we investigated the role of\nsuch language models in text classification and how they compare with other\napproaches relying on smaller pre-trained language models. Considering 32\ndatasets spanning 8 languages, we compared zero-shot classification, few-shot\nfine-tuning and synthetic data based classifiers with classifiers built using\nthe complete human labeled dataset. Our results show that zero-shot approaches\ndo well for sentiment classification, but are outperformed by other approaches\nfor the rest of the tasks, and synthetic data sourced from multiple LLMs can\nbuild better classifiers than zero-shot open LLMs. We also see wide performance\ndisparities across languages in all the classification scenarios. We expect\nthat these findings would guide practitioners working on developing text\nclassification systems across languages.\n","authors":["Sowmya Vajjala","Shwetali Shimangaud"],"pdf_url":"https://arxiv.org/pdf/2502.11830v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2502.11829v1","updated":"2025-02-17T14:25:45Z","published":"2025-02-17T14:25:45Z","title":"Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code\n  Generation Capabilities","summary":"  This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.\n","authors":["Hanbin Wang","Xiaoxuan Zhou","Zhipeng Xu","Keyuan Cheng","Yuxin Zuo","Kai Tian","Jingwei Song","Junting Lu","Wenhui Hu","Xueyang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11829v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.11824v1","updated":"2025-02-17T14:16:01Z","published":"2025-02-17T14:16:01Z","title":"M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research.\n","authors":["Chengyan Wu","Bolei Ma","Yihong Liu","Zheyu Zhang","Ningyuan Deng","Yanshu Li","Baolan Chen","Yi Zhang","Barbara Plank","Yun Xue"],"pdf_url":"https://arxiv.org/pdf/2502.11824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v1","updated":"2025-02-17T13:59:41Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.11811v1","updated":"2025-02-17T13:55:42Z","published":"2025-02-17T13:55:42Z","title":"FineFilter: A Fine-grained Noise Filtering Mechanism for\n  Retrieval-Augmented Large Language Models","summary":"  Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy.Existing methods use re-ranking or summarization\nto identify the most relevant sentences, but directly and accurately locating\nanswer clues from these large-scale and complex documents remains challenging.\nUnlike these document-level operations, we treat noise filtering as a\nsentence-level MinMax optimization problem: first identifying the potential\nclues from multiple documents using contextual information, then ranking them\nby relevance, and finally retaining the least clues through truncation. In this\npaper, we propose FineFilter, a novel fine-grained noise filtering mechanism\nfor RAG consisting of a clue extractor, a re-ranker, and a truncator. We\noptimize each module to tackle complex reasoning challenges: (1) Clue extractor\nfirstly uses sentences containing the answer and similar ones as fine-tuned\ntargets, aiming at extracting sufficient potential clues; (2) Re-ranker is\ntrained to prioritize effective clues based on the real feedback from\ngeneration module, with clues capable of generating correct answer as positive\nsamples and others as negative; (3) Truncator takes the minimum clues needed to\nanswer the question (truncation point) as fine-tuned targets, and performs\ntruncation on the re-ranked clues to achieve fine-grained noise filtering.\nExperiments on three QA datasets demonstrate that FineFilter significantly\noutperforms baselines in terms of performance and inference cost. Further\nanalysis on each module shows the effectiveness of our optimizations for\ncomplex reasoning.\n","authors":["Qianchi Zhang","Hainan Zhang","Liang Pang","Hongwei Zheng","Yongxin Tong","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11806v1","updated":"2025-02-17T13:50:29Z","published":"2025-02-17T13:50:29Z","title":"Exploring Translation Mechanism of Large Language Models","summary":"  Large language models (LLMs) have succeeded remarkably in multilingual\ntranslation tasks. However, the inherent translation mechanisms of LLMs remain\npoorly understood, largely due to sophisticated architectures and vast\nparameter scales. In response to this issue, this study explores the\ntranslation mechanism of LLM from the perspective of computational components\n(e.g., attention heads and MLPs). Path patching is utilized to explore causal\nrelationships between components, detecting those crucial for translation tasks\nand subsequently analyzing their behavioral patterns in human-interpretable\nterms. Comprehensive analysis reveals that translation is predominantly\nfacilitated by a sparse subset of specialized attention heads (less than 5\\%),\nwhich extract source language, indicator, and positional features. MLPs\nsubsequently integrate and process these features by transiting towards\nEnglish-centric latent representations. Notably, building on the above\nfindings, targeted fine-tuning of only 64 heads achieves translation\nimprovement comparable to full-parameter tuning while preserving general\ncapabilities.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Xiucheng Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.10711v3","updated":"2025-02-17T13:49:45Z","published":"2025-01-18T09:51:57Z","title":"How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks\n  For LLMs","summary":"  Various benchmarks have been proposed to assess the performance of large\nlanguage models (LLMs) in different coding scenarios. We refer to them as\ncode-related benchmarks. However, there are no systematic guidelines by which\nsuch a benchmark should be developed to ensure its quality, reliability, and\nreproducibility. We propose How2Bench, which is comprised of a 55-criteria\nchecklist as a set of guidelines to govern the development of code-related\nbenchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks\nreleased within the past decade and found concerning issues. Nearly 70% of the\nbenchmarks did not take measures for data quality assurance; over 10% did not\neven open source or only partially open source. Many highly cited benchmarks\nhave loopholes, including duplicated samples, incorrect reference\ncodes/tests/prompts, and unremoved sensitive/confidential information. Finally,\nwe conducted a human study involving 49 participants, which revealed\nsignificant gaps in awareness of the importance of data quality,\nreproducibility, and transparency.\n","authors":["Jialun Cao","Yuk-Kit Chan","Zixuan Ling","Wenxuan Wang","Shuqing Li","Mingwei Liu","Ruixi Qiao","Yuting Han","Chaozheng Wang","Boxi Yu","Pinjia He","Shuai Wang","Zibin Zheng","Michael R. Lyu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2501.10711v3.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2403.19318v3","updated":"2025-02-17T13:45:00Z","published":"2024-03-28T11:21:12Z","title":"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office\n  Usage Scenarios","summary":"  We introduce TableLLM, a robust large language model (LLM) with 8 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted\nbenchmarks tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction. Our codes and data are publicly available at\nhttps://github.com/TableLLM/TableLLM.\n","authors":["Xiaokang Zhang","Sijia Luo","Bohan Zhang","Zeyao Ma","Jing Zhang","Yang Li","Guanlin Li","Zijun Yao","Kangli Xu","Jinchang Zhou","Daniel Zhang-Li","Jifan Yu","Shu Zhao","Juanzi Li","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.19318v3.pdf","comment":"https://tablellm.github.io/"},{"id":"http://arxiv.org/abs/2502.11799v1","updated":"2025-02-17T13:42:12Z","published":"2025-02-17T13:42:12Z","title":"Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.\n","authors":["Peiying Yu","Guoxin Chen","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v2","updated":"2025-02-17T13:30:15Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions through\nsubnetworks that can be composed to perform more complex tasks. Recent advances\nin mechanistic interpretability have made progress in identifying\n$\\textit{circuits}$, which represent the minimal computational subgraphs\nresponsible for a model's behavior on specific tasks. However, most studies\nfocus on identifying circuits for individual tasks without investigating how\nfunctionally similar circuits $\\textit{relate}$ to each other. To address this\ngap, we study the modularity of neural networks by analyzing circuits for\nhighly compositional subtasks within a transformer-based language model.\nSpecifically, given a probabilistic context-free grammar, we identify and\ncompare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through set operations to represent more\ncomplex functional model capabilities.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v2.pdf","comment":"22 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.11789v1","updated":"2025-02-17T13:28:14Z","published":"2025-02-17T13:28:14Z","title":"Personality Editing for Language Models through Relevant Knowledge\n  Editing","summary":"  Large Language Models (LLMs) play a vital role in applications like\nconversational agents and content creation, where controlling a model's\npersonality is crucial for maintaining tone, consistency, and engagement.\nHowever, traditional prompt-based techniques for controlling personality often\nfall short, as they do not effectively mitigate the model's inherent biases. In\nthis paper, we introduce a novel method PALETTE that enhances personality\ncontrol through knowledge editing. By generating adjustment queries inspired by\npsychological assessments, our approach systematically adjusts responses to\npersonality-related queries similar to modifying factual knowledge, thereby\nachieving controlled shifts in personality traits. Experimental results from\nboth automatic and human evaluations demonstrate that our method enables more\nstable and well-balanced personality control in LLMs.\n","authors":["Seojin Hwang","Yumin Kim","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11789v1.pdf","comment":"15 pages, 3 figures, 16 tables"},{"id":"http://arxiv.org/abs/2501.07824v2","updated":"2025-02-17T13:26:52Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07173v2","updated":"2025-02-17T13:25:17Z","published":"2024-10-09T17:59:33Z","title":"Better Language Models Exhibit Higher Visual Alignment","summary":"  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01220v2","updated":"2025-02-17T13:20:37Z","published":"2025-02-03T10:24:55Z","title":"Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts","summary":"  Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.\n","authors":["Hichem Ammar Khodja","Frdric Bchet","Quentin Brabant","Alexis Nasr","Gwnol Lecorv"],"pdf_url":"https://arxiv.org/pdf/2502.01220v2.pdf","comment":"preprint v2"},{"id":"http://arxiv.org/abs/2412.12499v2","updated":"2025-02-17T13:20:15Z","published":"2024-12-17T03:03:17Z","title":"LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for\n  Low-Resource Language Reasoning","summary":"  Large language models (LLMs) have exhibited impressive multilingual reasoning\ncapabilities, driven by extensive multilingual pre-training corpora and\ninstruction fine-tuning data. However, a performance gap exists between high-\nand low-resource language reasoning tasks due to the language imbalance in the\npre-training corpus, which is exacerbated by evaluation bias in existing\nreasoning benchmarks lacking low-resource language coverage. To alleviate this\nissue, we propose LinguaLIFT, a two-stage instruction tuning framework for\nadvancing low-resource language reasoning. LinguaLIFT employs a language\nalignment layer to capture multilingual alignment in a code-switched tuning way\nwithout requiring multilingual instruction or parallel data, thereby\ntransferring the cross-lingual reasoning capabilities to low-resource languages\nthrough English-only instruction tuning data. To comprehensively evaluate the\nmultilingual reasoning capabilities, we introduce the Multilingual Math World\nProblem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and\n10 high-resource languages. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and four widely used\nbenchmarks.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v2","updated":"2025-02-17T13:16:00Z","published":"2025-02-13T08:10:45Z","title":"Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging - An Open Recipe","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.11779v1","updated":"2025-02-17T13:14:11Z","published":"2025-02-17T13:14:11Z","title":"Efficient Response Generation Method Selection for Fine-Tuning Large\n  Language Models","summary":"  The training data for fine-tuning large language models (LLMs) is typically\nstructured as input-output pairs. However, for many tasks, there can be\nmultiple equally valid output variations for the same input. Recent studies\nhave observed that the choice of output variation used in training can affect\nthe model's performance. This raises an important question: how can we generate\nthe most effective output from the many possible response generation strategy\noptions? Rather than relying on the traditional but resource-intensive\ntrain-and-evaluate approach, this paper proposes a scalable, approximate method\nfor estimating the quality of a small subset of generated training data derived\nfrom the same input. We then evaluate how well this small subset of generated\noutput fits the target model we are trying to train. We present a large-scale\nbenchmark covering diverse reasoning-based datasets to support our study.\n  The central idea is that a good output should closely resemble the output\ngenerated by the target LLM. We formalize this 'closeness' as the expected\nalignment score between a candidate output and the output sampled from the\ntarget LLM. We connect this measurement to the perplexity metric used in\nprevious literature and demonstrate that leveraging an alignment-based metric\ncan provide better predictions of model performance. Using this strategy, we\ncan evaluate a small subset of the generated output from each response\ngeneration strategy option, then select the most effective strategy. We show\nthat an LLM trained on data generated by the selected strategy could lead to a\nsignificant performance gain in many cases.\n","authors":["Xuan Ren","Qi Chen","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13669v2","updated":"2025-02-17T13:10:33Z","published":"2025-01-23T13:54:53Z","title":"How to Alleviate Catastrophic Forgetting in LLMs Finetuning?\n  Hierarchical Layer-Wise and Element-Wise Regularization","summary":"  Large Language Models (LLMs) exhibit strong general language capabilities.\nHowever, fine-tuning these models on domain-specific tasks often leads to\ncatastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss based on\nelement-wise parameter importance, which constrains the updates to parameters\ncrucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10-15% of the storage, highlighting the practical efficiency. The\ncode will be released.\n","authors":["Shezheng Song","Hao Xu","Jun Ma","Shasha Li","Long Peng","Qian Wan","Xiaodong Liu","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2501.13669v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.11771v1","updated":"2025-02-17T13:00:44Z","published":"2025-02-17T13:00:44Z","title":"The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It","summary":"  The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why current LLMs struggle to detect even simple arithmetic errors.\n","authors":["Leonardo Bertolazzi","Philipp Mondorf","Barbara Plank","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2502.11771v1.pdf","comment":"34 pages, 31 figures"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.12154v1","updated":"2025-02-17T18:59:50Z","published":"2025-02-17T18:59:50Z","title":"Diffusion Models without Classifier-free Guidance","summary":"  This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.\n","authors":["Zhicong Tang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2502.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12149v1","updated":"2025-02-17T18:58:36Z","published":"2025-02-17T18:58:36Z","title":"HARBOR: Exploring Persona Dynamics in Multi-Agent Competition","summary":"  We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.\n","authors":["Kenan Jiang","Li Xiong","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12145v1","updated":"2025-02-17T18:56:20Z","published":"2025-02-17T18:56:20Z","title":"Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented\n  Generation with Flexible User Control","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.\n","authors":["Jinyan Su","Jennifer Healey","Preslav Nakov","Claire Cardie"],"pdf_url":"https://arxiv.org/pdf/2502.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12143v1","updated":"2025-02-17T18:56:15Z","published":"2025-02-17T18:56:15Z","title":"Small Models Struggle to Learn from Strong Reasoners","summary":"  Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.\n","authors":["Yuetai Li","Xiang Yue","Zhangchen Xu","Fengqing Jiang","Luyao Niu","Bill Yuchen Lin","Bhaskar Ramasubramanian","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2502.12143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12131v1","updated":"2025-02-17T18:49:40Z","published":"2025-02-17T18:49:40Z","title":"Transformer Dynamics: A neuroscientific approach to interpretability of\n  large language models","summary":"  As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.\n","authors":["Jesseba Fernando","Grigori Guitchounts"],"pdf_url":"https://arxiv.org/pdf/2502.12131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12130v1","updated":"2025-02-17T18:49:25Z","published":"2025-02-17T18:49:25Z","title":"Scaling Autonomous Agents via Automatic Reward Modeling And Planning","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making.\n","authors":["Zhenfang Chen","Delin Chen","Rui Sun","Wenjun Liu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2502.12130v1.pdf","comment":"ICLR2025, Project page: https://armap-agent.github.io"},{"id":"http://arxiv.org/abs/2502.12128v1","updated":"2025-02-17T18:49:13Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. (Code is available at\nhttps://github.com/ml-jku/LaM-SLidE)\n","authors":["Florian Sestak","Artur Toshev","Andreas Frst","Gnter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v1.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2502.09606v2","updated":"2025-02-17T18:48:26Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12125v1","updated":"2025-02-17T18:47:01Z","published":"2025-02-17T18:47:01Z","title":"Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the\n  Lens of Class Hierarchy","summary":"  We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.\n","authors":["Roman Malashin","Valeria Yachnaya","Alexander Mullin"],"pdf_url":"https://arxiv.org/pdf/2502.12125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12120v1","updated":"2025-02-17T18:45:25Z","published":"2025-02-17T18:45:25Z","title":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws","summary":"  Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.\n","authors":["Prasanna Mayilvahanan","Thaddus Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2502.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12119v1","updated":"2025-02-17T18:43:41Z","published":"2025-02-17T18:43:41Z","title":"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection","summary":"  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n","authors":["Jinhe Bi","Yifan Wang","Danqi Yan","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v2","updated":"2025-02-17T18:42:31Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v2.pdf","comment":"We will reflect comments from the reviewers and re-submit"},{"id":"http://arxiv.org/abs/2406.11785v3","updated":"2025-02-17T18:37:13Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12109v1","updated":"2025-02-17T18:31:57Z","published":"2025-02-17T18:31:57Z","title":"Personality Structured Interview for Large Language Model Simulation in\n  Personality Research","summary":"  Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.\n","authors":["Pengda Wang","Huiqi Zou","Hanjie Chen","Tianjun Sun","Ziang Xiao","Frederick L. Oswald"],"pdf_url":"https://arxiv.org/pdf/2502.12109v1.pdf","comment":"41 Pages, 30 Tables, 5 Figures"},{"id":"http://arxiv.org/abs/2502.12108v1","updated":"2025-02-17T18:29:24Z","published":"2025-02-17T18:29:24Z","title":"Using the Path of Least Resistance to Explain Deep Networks","summary":"  Integrated Gradients (IG), a widely used axiomatic path-based attribution\nmethod, assigns importance scores to input features by integrating model\ngradients along a straight path from a baseline to the input. While effective\nin some cases, we show that straight paths can lead to flawed attributions. In\nthis paper, we identify the cause of these misattributions and propose an\nalternative approach that treats the input space as a Riemannian manifold,\ncomputing attributions by integrating gradients along geodesics. We call this\nmethod Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we\nintroduce two techniques: a k-Nearest Neighbours-based approach for smaller\nmodels and a Stochastic Variational Inference-based method for larger ones.\nAdditionally, we propose a new axiom, Strong Completeness, extending the axioms\nsatisfied by IG. We show that this property is desirable for attribution\nmethods and that GIG is the only method that satisfies it. Through experiments\non both synthetic and real-world data, we demonstrate that GIG outperforms\nexisting explainability methods, including IG.\n","authors":["Sina Salek","Joseph Enguehard"],"pdf_url":"https://arxiv.org/pdf/2502.12108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2502.12102v1","updated":"2025-02-17T18:23:29Z","published":"2025-02-17T18:23:29Z","title":"Relational Norms for Human-AI Cooperation","summary":"  How we should design and interact with social artificial intelligence depends\non the socio-relational role the AI is meant to emulate or occupy. In human\nsociety, relationships such as teacher-student, parent-child, neighbors,\nsiblings, or employer-employee are governed by specific norms that prescribe or\nproscribe cooperative functions including hierarchy, care, transaction, and\nmating. These norms shape our judgments of what is appropriate for each\npartner. For example, workplace norms may allow a boss to give orders to an\nemployee, but not vice versa, reflecting hierarchical and transactional\nexpectations. As AI agents and chatbots powered by large language models are\nincreasingly designed to serve roles analogous to human positions - such as\nassistant, mental health provider, tutor, or romantic partner - it is\nimperative to examine whether and how human relational norms should extend to\nhuman-AI interactions. Our analysis explores how differences between AI systems\nand humans, such as the absence of conscious experience and immunity to\nfatigue, may affect an AI's capacity to fulfill relationship-specific functions\nand adhere to corresponding norms. This analysis, which is a collaborative\neffort by philosophers, psychologists, relationship scientists, ethicists,\nlegal experts, and AI researchers, carries important implications for AI\nsystems design, user behavior, and regulation. While we accept that AI systems\ncan offer significant benefits such as increased availability and consistency\nin certain socio-relational roles, they also risk fostering unhealthy\ndependencies or unrealistic expectations that could spill over into human-human\nrelationships. We propose that understanding and thoughtfully shaping (or\nimplementing) suitable human-AI relational norms will be crucial for ensuring\nthat human-AI interactions are ethical, trustworthy, and favorable to human\nwell-being.\n","authors":["Brian D. Earp","Sebastian Porsdam Mann","Mateo Aboy","Edmond Awad","Monika Betzler","Marietjie Botes","Rachel Calcott","Mina Caraccio","Nick Chater","Mark Coeckelbergh","Mihaela Constantinescu","Hossein Dabbagh","Kate Devlin","Xiaojun Ding","Vilius Dranseika","Jim A. C. Everett","Ruiping Fan","Faisal Feroz","Kathryn B. Francis","Cindy Friedman","Orsolya Friedrich","Iason Gabriel","Ivar Hannikainen","Julie Hellmann","Arasj Khodadade Jahrome","Niranjan S. Janardhanan","Paul Jurcys","Andreas Kappes","Maryam Ali Khan","Gordon Kraft-Todd","Maximilian Kroner Dale","Simon M. Laham","Benjamin Lange","Muriel Leuenberger","Jonathan Lewis","Peng Liu","David M. Lyreskog","Matthijs Maas","John McMillan","Emilian Mihailov","Timo Minssen","Joshua Teperowski Monrad","Kathryn Muyskens","Simon Myers","Sven Nyholm","Alexa M. Owen","Anna Puzio","Christopher Register","Madeline G. Reinecke","Adam Safron","Henry Shevlin","Hayate Shimizu","Peter V. Treit","Cristina Voinea","Karen Yan","Anda Zahiu","Renwen Zhang","Hazem Zohny","Walter Sinnott-Armstrong","Ilina Singh","Julian Savulescu","Margaret S. Clark"],"pdf_url":"https://arxiv.org/pdf/2502.12102v1.pdf","comment":"76 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.18989v3","updated":"2025-02-17T18:19:41Z","published":"2024-07-25T08:47:11Z","title":"Machine Learning for Equitable Load Shedding: Real-time Solution via\n  Learning Binding Constraints","summary":"  Timely and effective load shedding in power systems is critical for\nmaintaining supply-demand balance and preventing cascading blackouts. To\neliminate load shedding bias against specific regions in the system,\noptimization-based methods are uniquely positioned to help balance between\neconomical and equity considerations. However, the resulting optimization\nproblem involves complex constraints, which can be time-consuming to solve and\nthus cannot meet the real-time requirements of load shedding. To tackle this\nchallenge, in this paper we present an efficient machine learning algorithm to\nenable millisecond-level computation for the optimization-based load shedding\nproblem. Numerical studies on both a 3-bus toy example and a realistic RTS-GMLC\nsystem have demonstrated the validity and efficiency of the proposed algorithm\nfor delivering equitable and real-time load shedding decisions.\n","authors":["Yuqi Zhou","Joseph Severino","Sanjana Vijayshankar","Juliette Ugirumurera","Jibo Sanyal"],"pdf_url":"https://arxiv.org/pdf/2407.18989v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12094v1","updated":"2025-02-17T18:12:36Z","published":"2025-02-17T18:12:36Z","title":"A Study on Leveraging Search and Self-Feedback for Agent Reasoning","summary":"  Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.\n","authors":["Karthikeyan K","Michelle Yuan","Elman Mansimov","Katerina Margatina","Anurag Pratik","Daniele Bonadiman","Monica Sunkara","Yi Zhang","Yassine Benajiba"],"pdf_url":"https://arxiv.org/pdf/2502.12094v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.03035v2","updated":"2025-02-17T18:11:20Z","published":"2025-01-06T14:23:02Z","title":"Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning","summary":"  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n","authors":["Zhen Li","Yupeng Su","Runming Yang","Congkai Xie","Zheng Wang","Zhongwei Xie","Ngai Wong","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.03035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12088v1","updated":"2025-02-17T18:04:39Z","published":"2025-02-17T18:04:39Z","title":"Meta-Statistical Learning: Supervised Learning of Statistical Inference","summary":"  This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.\n","authors":["Maxime Peyrard","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.12088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11613v6","updated":"2025-02-17T17:55:47Z","published":"2025-01-20T17:19:02Z","title":"Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems","summary":"  This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.\n","authors":["Giorgio Robino"],"pdf_url":"https://arxiv.org/pdf/2501.11613v6.pdf","comment":"Added Experimental Results sections"},{"id":"http://arxiv.org/abs/2408.17401v2","updated":"2025-02-17T17:49:36Z","published":"2024-08-30T16:36:53Z","title":"Exploring the Effect of Explanation Content and Format on User\n  Comprehension and Trust in Healthcare","summary":"  AI-driven tools for healthcare are widely acknowledged as potentially\nbeneficial to health practitioners and patients, e.g. the QCancer regression\ntool for cancer risk prediction. However, for these tools to be trusted, they\nneed to be supplemented with explanations. We examine how explanations' content\nand format affect user comprehension and trust when explaining QCancer's\npredictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding\nformat, we present SHAP explanations, conventionally, as charts (SC) and\nOcclusion-1 explanations as charts (OC) as well as text (OT), to which their\nsimpler nature lends itself. We conduct experiments with two sets of\nstakeholders: the general public (representing patients) and medical students\n(representing healthcare practitioners). Our experiments showed higher\nsubjective comprehension and trust for Occlusion-1 over SHAP explanations based\non content. However, when controlling for format, only OT outperformed SC,\nsuggesting this trend is driven by preferences for text. Other findings\ncorroborated that explanation format, rather than content, is often the\ncritical factor.\n","authors":["Antonio Rago","Bence Palfi","Purin Sukpanichnant","Hannibal Nabli","Kavyesh Vivek","Olga Kostopoulou","James Kinross","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2408.17401v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.12067v1","updated":"2025-02-17T17:37:26Z","published":"2025-02-17T17:37:26Z","title":"TokenSkip: Controllable Chain-of-Thought Compression in LLMs","summary":"  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.\n","authors":["Heming Xia","Yongqi Li","Chak Tou Leong","Wenjie Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.12067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12066v1","updated":"2025-02-17T17:35:42Z","published":"2025-02-17T17:35:42Z","title":"CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication\n  Facilities with Large Language Models","summary":"  Automating planning with LLMs presents transformative opportunities for\ntraditional industries, yet remains underexplored. In commercial construction,\nthe complexity of automated scheduling often requires manual intervention to\nensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to\noptimize construction schedules in complex projects like semiconductor\nfabrication. CONSTRUCTA addresses key challenges by: (1) integrating\nconstruction-specific knowledge through static RAG; (2) employing\ncontext-sampling techniques inspired by architectural expertise to provide\nrelevant input; and (3) deploying Construction DPO to align schedules with\nexpert preferences using RLHF. Experiments on proprietary data demonstrate\nperformance improvements of +42.3% in missing value prediction, +79.1% in\ndependency analysis, and +28.9% in automated planning compared to baseline\nmethods, showcasing its potential to revolutionize construction workflows and\ninspire domain-specific LLM advancements.\n","authors":["Yifan Zhang","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11427v2","updated":"2025-02-17T17:34:45Z","published":"2024-06-17T11:25:57Z","title":"DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors","summary":"  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n","authors":["Keon Lee","Dong Won Kim","Jaehyeon Kim","Seungjun Chung","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2406.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12064v1","updated":"2025-02-17T17:32:55Z","published":"2025-02-17T17:32:55Z","title":"AI-generated Text Detection with a GLTR-based Approach","summary":"  The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.\n","authors":["Luca Yan Wu","Isabel Segura-Bedmar"],"pdf_url":"https://arxiv.org/pdf/2502.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01474v3","updated":"2025-02-17T17:24:42Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v3.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.12054v1","updated":"2025-02-17T17:24:14Z","published":"2025-02-17T17:24:14Z","title":"PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning","summary":"  Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:/dxzxy12138.github.io/PhysReason.\n","authors":["Xinyu Zhang","Yuxuan Dong","Yanrui Wu","Jiaxing Huang","Chengyou Jia","Basura Fernando","Mike Zheng Shou","Lingling Zhang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09838v2","updated":"2025-02-17T17:17:44Z","published":"2025-02-14T00:42:36Z","title":"HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation","summary":"  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n","authors":["Tianwei Lin","Wenqiao Zhang","Sijing Li","Yuqian Yuan","Binhe Yu","Haoyuan Li","Wanggui He","Hao Jiang","Mengze Li","Xiaohui Song","Siliang Tang","Jun Xiao","Hui Lin","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.09838v2.pdf","comment":"Comments: added project page"},{"id":"http://arxiv.org/abs/2502.12048v1","updated":"2025-02-17T17:16:41Z","published":"2025-02-17T17:16:41Z","title":"A Survey on Bridging EEG Signals and Generative AI: From Image and Text\n  to Beyond","summary":"  Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.\n","authors":["Shreya Shukla","Jose Torres","Abhijit Mishra","Jacek Gwizdka","Shounak Roychowdhury"],"pdf_url":"https://arxiv.org/pdf/2502.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12031v1","updated":"2025-02-17T17:02:26Z","published":"2025-02-17T17:02:26Z","title":"Masked Latent Prediction and Classification for Self-Supervised Audio\n  Representation Learning","summary":"  Recently, self-supervised learning methods based on masked latent prediction\nhave proven to encode input data into powerful representations. However, during\ntraining, the learned latent space can be further transformed to extract\nhigher-level information that could be more suited for downstream\nclassification tasks. Therefore, we propose a new method: MAsked latenT\nPrediction And Classification (MATPAC), which is trained with two pretext tasks\nsolved jointly. As in previous work, the first pretext task is a masked latent\nprediction task, ensuring a robust input representation in the latent space.\nThe second one is unsupervised classification, which utilises the latent\nrepresentations of the first pretext task to match probability distributions\nbetween a teacher and a student. We validate the MATPAC method by comparing it\nto other state-of-the-art proposals and conducting ablations studies. MATPAC\nreaches state-of-the-art self-supervised learning results on reference audio\nclassification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms\ncomparable supervised methods results for musical auto-tagging on\nMagna-tag-a-tune.\n","authors":["Aurian Quelennec","Pierre Chouteau","Geoffroy Peeters","Slim Essid"],"pdf_url":"https://arxiv.org/pdf/2502.12031v1.pdf","comment":"Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2502.12029v1","updated":"2025-02-17T17:02:01Z","published":"2025-02-17T17:02:01Z","title":"KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths\n  over Knowledge Graphs","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.\n","authors":["Qi Zhao","Hongyu Yang","Qi Song","Xinwei Yao","Xiangyang Li"],"pdf_url":"https://arxiv.org/pdf/2502.12029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12025v1","updated":"2025-02-17T16:57:56Z","published":"2025-02-17T16:57:56Z","title":"SafeChain: Safety of Language Models with Long Chain-of-Thought\n  Reasoning Capabilities","summary":"  Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.\n","authors":["Fengqing Jiang","Zhangchen Xu","Yuetai Li","Luyao Niu","Zhen Xiang","Bo Li","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2502.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12022v1","updated":"2025-02-17T16:56:23Z","published":"2025-02-17T16:56:23Z","title":"Teaching LLMs According to Their Aptitude: Adaptive Reasoning for\n  Mathematical Problem Solving","summary":"  Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.\n","authors":["Xin Xu","Yan Xu","Tianhao Chen","Yuchen Yan","Chengwu Liu","Zaoyu Chen","Yufei Wang","Yichun Yin","Yasheng Wang","Lifeng Shang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12022v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2501.18592v3","updated":"2025-02-17T16:54:39Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v3.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2502.12018v1","updated":"2025-02-17T16:52:42Z","published":"2025-02-17T16:52:42Z","title":"Atom of Thoughts for Markov LLM Test-Time Scaling","summary":"  Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.\n","authors":["Fengwei Teng","Zhaoyang Yu","Quan Shi","Jiayi Zhang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00997v3","updated":"2025-02-17T16:51:23Z","published":"2025-02-03T02:34:46Z","title":"MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs","summary":"  The recent success of specialized Large Language Models (LLMs) in domains\nsuch as mathematical reasoning and coding has led to growing interest in\nmethods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)\nmodel, with the goal of enhancing performance in each domain while retaining\neffectiveness on general tasks. However, the effective merging of expert models\nremains an open challenge, especially for models with highly divergent weight\nparameters or different architectures. State-of-the-art MoE merging methods\nonly work with homogeneous model architectures and rely on simple unweighted\naveraging to merge expert layers, which does not address parameter interference\nand requires extensive fine-tuning of the merged MoE to restore performance. To\naddress these limitations, this paper introduces new MoE merging techniques,\nincluding strategies to mitigate parameter interference, routing heuristics to\nreduce the need for MoE fine-tuning, and a novel method for merging experts\nwith different architectures. Extensive experiments across multiple domains\ndemonstrate the effectiveness of our proposed methods, reducing fine-tuning\ncosts, improving performance over state-of-the-art methods, and expanding the\napplicability of MoE merging.\n","authors":["Yuhang Zhou","Giannis Karamanolakis","Victor Soto","Anna Rumshisky","Mayank Kulkarni","Furong Huang","Wei Ai","Jianhua Lu"],"pdf_url":"https://arxiv.org/pdf/2502.00997v3.pdf","comment":"Accepted by NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.12007v1","updated":"2025-02-17T16:43:47Z","published":"2025-02-17T16:43:47Z","title":"Demographic Attributes Prediction from Speech Using WavLM Embeddings","summary":"  This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.\n","authors":["Yuchen Yang","Thomas Thebaud","Najim Dehak"],"pdf_url":"https://arxiv.org/pdf/2502.12007v1.pdf","comment":"6 pages, accepted by The Conference on Information Sciences and\n  Systems (CISS)"},{"id":"http://arxiv.org/abs/2407.02025v4","updated":"2025-02-17T16:36:37Z","published":"2024-07-02T07:48:22Z","title":"On the Expressive Power of Sparse Geometric MPNNs","summary":"  Motivated by applications in chemistry and other sciences, we study the\nexpressive power of message-passing neural networks for geometric graphs, whose\nnode features correspond to 3-dimensional positions. Recent work has shown that\nsuch models can separate generic pairs of non-isomorphic geometric graphs,\nthough they may fail to separate some rare and complicated instances. However,\nthese results assume a fully connected graph, where each node possesses\ncomplete knowledge of all other nodes. In contrast, often, in application,\nevery node only possesses knowledge of a small number of nearest neighbors.\n  This paper shows that generic pairs of non-isomorphic geometric graphs can be\nseparated by message-passing networks with rotation equivariant features as\nlong as the underlying graph is connected. When only invariant intermediate\nfeatures are allowed, generic separation is guaranteed for generically globally\nrigid graphs. We introduce a simple architecture, EGENNET, which achieves our\ntheoretical guarantees and compares favorably with alternative architecture on\nsynthetic and chemical benchmarks. Our code is available at\nhttps://github.com/yonatansverdlov/E-GenNet.\n","authors":["Yonatan Sverdlov","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2407.02025v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11995v1","updated":"2025-02-17T16:35:15Z","published":"2025-02-17T16:35:15Z","title":"Presumed Cultural Identity: How Names Shape LLM Responses","summary":"  Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.\n","authors":["Siddhesh Pawar","Arnav Arora","Lucie-Aime Kaffee","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.11995v1.pdf","comment":"23 Pages, 13 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2410.06665v2","updated":"2025-02-17T16:34:29Z","published":"2024-10-09T08:19:31Z","title":"Revisiting Multi-Permutation Equivariance through the Lens of\n  Irreducible Representations","summary":"  This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.\n","authors":["Yonatan Sverdlov","Ido Springer","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2410.06665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11989v1","updated":"2025-02-17T16:28:15Z","published":"2025-02-17T16:28:15Z","title":"Characterizing Photorealism and Artifacts in Diffusion Model-Generated\n  Images","summary":"  Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.\n","authors":["Negar Kamali","Karyn Nakamura","Aakriti Kumar","Angelos Chatzimparmpas","Jessica Hullman","Matthew Groh"],"pdf_url":"https://arxiv.org/pdf/2502.11989v1.pdf","comment":"26 pages, 24 Figures, Accepted by ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.09969v2","updated":"2025-02-17T16:26:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tr"],"pdf_url":"https://arxiv.org/pdf/2502.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11981v1","updated":"2025-02-17T16:22:46Z","published":"2025-02-17T16:22:46Z","title":"Machine Learning Should Maximize Welfare, Not (Only) Accuracy","summary":"  Decades of research in machine learning have given us powerful tools for\nmaking accurate predictions. But when used in social settings and on human\ninputs, better accuracy does not immediately translate to better social\noutcomes. This may not be surprising given that conventional learning\nframeworks are not designed to express societal preferences -- let alone\npromote them. This position paper argues that machine learning is currently\nmissing, and can gain much from incorporating, a proper notion of social\nwelfare. The field of welfare economics asks: how should we allocate limited\nresources to self-interested agents in a way that maximizes social benefit? We\nargue that this perspective applies to many modern applications of machine\nlearning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail\napplications and use-cases for which our framework can be effective, identify\ntechnical challenges and practical opportunities, and highlight future avenues\nworth pursuing.\n","authors":["Nir Rosenfeld","Haifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14483v2","updated":"2025-02-17T16:21:10Z","published":"2024-11-19T20:16:26Z","title":"Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat","summary":"  Deciding which large language model (LLM) to use is a complex challenge.\nPairwise ranking has emerged as a new method for evaluating human preferences\nfor LLMs. This approach entails humans evaluating pairs of model outputs based\non a predefined criterion. By collecting these comparisons, a ranking can be\nconstructed using methods such as Elo. However, applying these algorithms as\nconstructed in the context of LLM evaluation introduces several challenges. In\nthis paper, we explore the effectiveness of ranking systems for head-to-head\ncomparisons of LLMs. We formally define a set of fundamental principles for\neffective ranking and conduct a series of extensive evaluations on the\nrobustness of several ranking algorithms in the context of LLMs. Our analysis\nuncovers key insights into the factors that affect ranking accuracy and\nefficiency, offering guidelines for selecting the most appropriate methods\nbased on specific evaluation contexts and resource constraints.\n","authors":["Roland Daynauth","Christopher Clarke","Krisztian Flautner","Lingjia Tang","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2411.14483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11969v1","updated":"2025-02-17T16:18:07Z","published":"2025-02-17T16:18:07Z","title":"Learning Generalizable Prompt for CLIP with Class Similarity Knowledge","summary":"  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n","authors":["Sehun Jung","Hyang-won Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11968v1","updated":"2025-02-17T16:18:00Z","published":"2025-02-17T16:18:00Z","title":"Theoretical Barriers in Bellman-Based Reinforcement Learning","summary":"  Reinforcement Learning algorithms designed for high-dimensional spaces often\nenforce the Bellman equation on a sampled subset of states, relying on\ngeneralization to propagate knowledge across the state space. In this paper, we\nidentify and formalize a fundamental limitation of this common approach.\nSpecifically, we construct counterexample problems with a simple structure that\nthis approach fails to exploit. Our findings reveal that such algorithms can\nneglect critical information about the problems, leading to inefficiencies.\nFurthermore, we extend this negative result to another approach from the\nliterature: Hindsight Experience Replay learning state-to-state reachability.\n","authors":["Brieuc Pinon","Raphal Jungers","Jean-Charles Delvenne"],"pdf_url":"https://arxiv.org/pdf/2502.11968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11965v1","updated":"2025-02-17T16:13:40Z","published":"2025-02-17T16:13:40Z","title":"A MIMO Wireless Channel Foundation Model via CIR-CSI Consistency","summary":"  In the field of artificial intelligence, self-supervised learning has\ndemonstrated superior generalization capabilities by leveraging large-scale\nunlabeled datasets for pretraining, which is especially critical for wireless\ncommunication models to adapt to a variety of scenarios. This paper\ninnovatively treats Channel State Information (CSI) and Channel Impulse\nResponse (CIR) as naturally aligned multi-modal data and proposes the first\nMIMO wireless channel foundation model, named CSI-CLIP. By effectively\ncapturing the joint representations of both CIR and CSI, CSI-CLIP exhibits\nremarkable adaptability across scenarios and robust feature extraction\ncapabilities. Experimental results show that in positioning task, CSI-CLIP\nreduces the mean error distance by 22%; in beam management task, it increases\naccuracy by 1% compared to traditional supervised methods, as well as in the\nchannel identification task. These improvements not only highlight the\npotential and value of CSI-CLIP in integrating sensing and communication but\nalso demonstrate its significant advantages over existing techniques. Moreover,\nviewing CSI and CIR as multi-modal pairs and contrastive learning for wireless\nchannel foundation model open up new research directions in the domain of MIMO\nwireless communications.\n","authors":["Jun Jiang","Wenjun Yu","Yunfan Li","Yuan Gao","Shugong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11965v1.pdf","comment":"6 pages, 2025 ICMLCN accepted"},{"id":"http://arxiv.org/abs/2501.19353v2","updated":"2025-02-17T16:11:44Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v2.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.11962v1","updated":"2025-02-17T16:10:30Z","published":"2025-02-17T16:10:30Z","title":"Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware\n  Instruction Fine-Tuning","summary":"  Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.\n","authors":["Tianyi Wu","Jingwei Ni","Bryan Hooi","Jiaheng Zhang","Elliott Ash","See-Kiong Ng","Mrinmaya Sachan","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2502.11962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03540v3","updated":"2025-02-17T16:07:09Z","published":"2025-02-05T19:00:52Z","title":"Path Planning for Masked Diffusion Model Sampling","summary":"  In this paper, we explore how token unmasking order influences generative\nquality in masked diffusion models (MDMs). We derive an expanded evidence lower\nbound (ELBO) that introduces a planner to select which tokens to unmask at each\nstep. Our analysis reveals that alternative unmasking strategies can enhance\ngeneration performance. Building on this, we propose Path Planning (P2), a\nsampling framework that uses a pre-trained BERT model or the denoiser itself to\nguide unmasking decisions. P2 generalizes all known MDM sampling strategies and\nsignificantly improves performance across diverse domains, including language\ngeneration (in-context learning, code generation, story infilling, mathematical\nreasoning, reverse curse correction) and biological sequence generation\n(protein and RNA sequences).\n","authors":["Fred Zhangzhi Peng","Zachary Bezemek","Sawan Patel","Jarrid Rector-Brooks","Sherwood Yao","Alexander Tong","Pranam Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2502.03540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11959v1","updated":"2025-02-17T16:07:07Z","published":"2025-02-17T16:07:07Z","title":"STRIVE: Structured Reasoning for Self-Improvement in Claim Verification","summary":"  Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.\n","authors":["Haisong Gong","Jing Li","Junfei Wu","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11949v1","updated":"2025-02-17T16:02:54Z","published":"2025-02-17T16:02:54Z","title":"Massively Scaling Explicit Policy-conditioned Value Functions","summary":"  We introduce a scaling strategy for Explicit Policy-Conditioned Value\nFunctions (EPVFs) that significantly improves performance on challenging\ncontinuous-control tasks. EPVFs learn a value function V({\\theta}) that is\nexplicitly conditioned on the policy parameters, enabling direct gradient-based\nupdates to the parameters of any policy. However, EPVFs at scale struggle with\nunrestricted parameter growth and efficient exploration in the policy parameter\nspace. To address these issues, we utilize massive parallelization with\nGPU-based simulators, big batch sizes, weight clipping and scaled peturbations.\nOur results show that EPVFs can be scaled to solve complex tasks, such as a\ncustom Ant environment, and can compete with state-of-the-art Deep\nReinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)\nand Soft Actor-Critic (SAC). We further explore action-based policy parameter\nrepresentations from previous work and specialized neural network architectures\nto efficiently handle weight-space features, which have not been used in the\ncontext of DRL before.\n","authors":["Nico Bohlinger","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.11949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11946v1","updated":"2025-02-17T15:58:56Z","published":"2025-02-17T15:58:56Z","title":"Step-Audio: Unified Understanding and Generation in Intelligent Speech\n  Interaction","summary":"  Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.\n","authors":["Ailin Huang","Boyong Wu","Bruce Wang","Chao Yan","Chen Hu","Chengli Feng","Fei Tian","Feiyu Shen","Jingbei Li","Mingrui Chen","Peng Liu","Ruihang Miao","Wang You","Xi Chen","Xuerui Yang","Yechang Huang","Yuxiang Zhang","Zheng Gong","Zixin Zhang","Brian Li","Changyi Wan","Hanpeng Hu","Ranchen Ming","Song Yuan","Xuelin Zhang","Yu Zhou","Bingxin Li","Buyun Ma","Kang An","Wei Ji","Wen Li","Xuan Wen","Yuankai Ma","Yuanwei Liang","Yun Mou","Bahtiyar Ahmidi","Bin Wang","Bo Li","Changxin Miao","Chen Xu","Chengting Feng","Chenrun Wang","Dapeng Shi","Deshan Sun","Dingyuan Hu","Dula Sai","Enle Liu","Guanzhe Huang","Gulin Yan","Heng Wang","Haonan Jia","Haoyang Zhang","Jiahao Gong","Jianchang Wu","Jiahong Liu","Jianjian Sun","Jiangjie Zhen","Jie Feng","Jie Wu","Jiaoren Wu","Jie Yang","Jinguo Wang","Jingyang Zhang","Junzhe Lin","Kaixiang Li","Lei Xia","Li Zhou","Longlong Gu","Mei Chen","Menglin Wu","Ming Li","Mingxiao Li","Mingyao Liang","Na Wang","Nie Hao","Qiling Wu","Qinyuan Tan","Shaoliang Pang","Shiliang Yang","Shuli Gao","Siqi Liu","Sitong Liu","Tiancheng Cao","Tianyu Wang","Wenjin Deng","Wenqing He","Wen Sun","Xin Han","Xiaomin Deng","Xiaojia Liu","Xu Zhao","Yanan Wei","Yanbo Yu","Yang Cao","Yangguang Li","Yangzhen Ma","Yanming Xu","Yaqiang Shi","Yilei Wang","Yinmin Zhong","Yu Luo","Yuanwei Lu","Yuhe Yin","Yuting Yan","Yuxiang Yang","Zhe Xie","Zheng Ge","Zheng Sun","Zhewei Huang","Zhichao Chang","Zidong Yang","Zili Zhang","Binxing Jiao","Daxin Jiang","Heung-Yeung Shum","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18547v4","updated":"2025-02-17T15:55:08Z","published":"2024-12-24T16:55:45Z","title":"Token-Budget-Aware LLM Reasoning","summary":"  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n","authors":["Tingxu Han","Zhenting Wang","Chunrong Fang","Shiyu Zhao","Shiqing Ma","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18547v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11941v1","updated":"2025-02-17T15:52:22Z","published":"2025-02-17T15:52:22Z","title":"Deep Spatio-Temporal Neural Network for Air Quality Reanalysis","summary":"  Air quality prediction is key to mitigating health impacts and guiding\ndecisions, yet existing models tend to focus on temporal trends while\noverlooking spatial generalization. We propose AQ-Net, a spatiotemporal\nreanalysis model for both observed and unobserved stations in the near future.\nAQ-Net utilizes the LSTM and multi-head attention for the temporal regression.\nWe also propose a cyclic encoding technique to ensure continuous time\nrepresentation. To learn fine-grained spatial air quality estimation, we\nincorporate AQ-Net with the neural kNN to explore feature-based interpolation,\nsuch that we can fill the spatial gaps given coarse observation stations. To\ndemonstrate the efficiency of our model for spatiotemporal reanalysis, we use\ndata from 2013-2017 collected in northern China for PM2.5 analysis. Extensive\nexperiments show that AQ-Net excels in air quality reanalysis, highlighting the\npotential of hybrid spatio-temporal models to better capture environmental\ndynamics, especially in urban areas where both spatial and temporal variability\nare critical.\n","authors":["Ammar Kheder","Benjamin Foreback","Lili Wang","Zhi-Song Liu","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2502.11941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11937v1","updated":"2025-02-17T15:48:46Z","published":"2025-02-17T15:48:46Z","title":"FitLight: Federated Imitation Learning for Plug-and-Play Autonomous\n  Traffic Signal Control","summary":"  Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)\nmethods have been extensively studied, their practical applications still raise\nsome serious issues such as high learning cost and poor generalizability. This\nis because the ``trial-and-error'' training style makes RL agents extremely\ndependent on the specific traffic environment, which also requires a long\nconvergence time. To address these issues, we propose a novel Federated\nImitation Learning (FIL)-based framework for multi-intersection TSC, named\nFitLight, which allows RL agents to plug-and-play for any traffic environment\nwithout additional pre-training cost. Unlike existing imitation learning\napproaches that rely on pre-training RL agents with demonstrations, FitLight\nallows real-time imitation learning and seamless transition to reinforcement\nlearning. Due to our proposed knowledge-sharing mechanism and novel hybrid\npressure-based agent design, RL agents can quickly find a best control policy\nwith only a few episodes. Moreover, for resource-constrained TSC scenarios,\nFitLight supports model pruning and heterogeneous model aggregation, such that\nRL agents can work on a micro-controller with merely 16{\\it KB} RAM and 32{\\it\nKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art\nmethods, FitLight not only provides a superior starting point but also\nconverges to a better final solution on both real-world and synthetic datasets,\neven under extreme resource limitations.\n","authors":["Yutong Ye","Yingbo Zhou","Zhusen Liu","Xiao Du","Hao Zhou","Xiang Lian","Mingsong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11925v1","updated":"2025-02-17T15:35:36Z","published":"2025-02-17T15:35:36Z","title":"GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.\n","authors":["Yi Fang","Bowen Jin","Jiacheng Shen","Sirui Ding","Qiaoyu Tan","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2502.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19684v2","updated":"2025-02-17T15:32:50Z","published":"2024-12-27T15:21:17Z","title":"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,\n  Adaptive, Universal Prompt Optimization Framework","summary":"  Efficient multimodal large language models (EMLLMs), in contrast to\nmultimodal large language models (MLLMs), reduce model size and computational\ncosts and are often deployed on resource-constrained devices. However, due to\ndata privacy concerns, existing open-source EMLLMs rarely have access to\nprivate domain-specific data during the pre-training process, making them\ndifficult to directly apply in device-specific domains, such as certain\nbusiness scenarios. To address this weakness, this paper focuses on the\nefficient adaptation of EMLLMs to private domains, specifically in two areas:\n1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.\nSpecifically, we propose a tun\\textbf{\\underline{I}}ng-free,\na\\textbf{\\underline{D}}aptiv\\textbf{\\underline{E}},\nunivers\\textbf{\\underline{AL}} \\textbf{\\underline{Prompt}} Optimization\nFramework, abbreviated as \\textit{\\textbf{\\ourmethod{}}} which consists of two\nstages: 1) Predefined Prompt, based on the reinforcement searching strategy,\ngenerate a prompt optimization strategy tree to acquire optimization priors; 2)\nPrompt Reflection initializes the prompt based on optimization priors, followed\nby self-reflection to further search and refine the prompt. By doing so,\n\\ourmethod{} elegantly generates the ``ideal prompts'' for processing private\ndomain-specific data. Note that our method requires no parameter fine-tuning\nand only a small amount of data to quickly adapt to the data distribution of\nprivate data. Extensive experiments across multiple tasks demonstrate that our\nproposed \\ourmethod{} significantly improves both efficiency and performance\ncompared to baselines.\n","authors":["Jiang Liu","Bolin Li","Haoyuan Li","Tianwei Lin","Wenqiao Zhang","Tao Zhong","Zhelun Yu","Jinghao Wei","Hao Cheng","Wanggui He","Fangxun Shu","Hao Jiang","Zheqi Lv","Juncheng Li","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.19684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11916v1","updated":"2025-02-17T15:31:59Z","published":"2025-02-17T15:31:59Z","title":"EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models","summary":"  Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.\n","authors":["Jiamin Su","Yibo Yan","Fangteng Fu","Han Zhang","Jingheng Ye","Xiang Liu","Jiahao Huo","Huiyu Zhou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11916v1.pdf","comment":"JS and YY are co-first authors. XH is the corresponding author"},{"id":"http://arxiv.org/abs/2502.11915v1","updated":"2025-02-17T15:31:27Z","published":"2025-02-17T15:31:27Z","title":"On the robustness of ChatGPT in teaching Korean Mathematics","summary":"  ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education.\n","authors":["Phuong-Nam Nguyen","Quang Nguyen-The","An Vu-Minh","Diep-Anh Nguyen","Xuan-Lam Pham"],"pdf_url":"https://arxiv.org/pdf/2502.11915v1.pdf","comment":"21 pages, 12 figures, includes statistical analysis of ChatGPT's\n  robustness in solving and rating multilingual mathematics questions. Focus on\n  Korean CSAT Mathematics. Evaluates AI accuracy, rating effectiveness, and\n  topic analysis"},{"id":"http://arxiv.org/abs/2502.06600v2","updated":"2025-02-17T15:22:32Z","published":"2025-02-10T16:00:00Z","title":"Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?","summary":"  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n","authors":["Gonalo Gomes","Chrysoula Zerva","Bruno Martins"],"pdf_url":"https://arxiv.org/pdf/2502.06600v2.pdf","comment":"Accepted in Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11897v1","updated":"2025-02-17T15:22:31Z","published":"2025-02-17T15:22:31Z","title":"DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation","summary":"  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.\n","authors":["Zhihang Yuan","Siyuan Wang","Rui Xie","Hanling Zhang","Tongcheng Fang","Yuzhang Shang","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11896v1","updated":"2025-02-17T15:22:19Z","published":"2025-02-17T15:22:19Z","title":"CAMEL: Continuous Action Masking Enabled by Large Language Models for\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.\n","authors":["Yanxiao Zhao","Yangge Qian","Jingyang Shan","Xiaolin Qin"],"pdf_url":"https://arxiv.org/pdf/2502.11896v1.pdf","comment":"Accepted at RLDM 2025"},{"id":"http://arxiv.org/abs/2502.11895v1","updated":"2025-02-17T15:21:11Z","published":"2025-02-17T15:21:11Z","title":"Continual Quantization-Aware Pre-Training: When to transition from\n  16-bit to 1.58-bit pre-training for BitNet language models?","summary":"  Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.\n","authors":["Jacob Nielsen","Peter Schneider-Kamp","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2502.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11887v1","updated":"2025-02-17T15:13:41Z","published":"2025-02-17T15:13:41Z","title":"Stonefish: Supporting Machine Learning Research in Marine Robotics","summary":"  Simulations are highly valuable in marine robotics, offering a cost-effective\nand controlled environment for testing in the challenging conditions of\nunderwater and surface operations. Given the high costs and logistical\ndifficulties of real-world trials, simulators capable of capturing the\noperational conditions of subsea environments have become key in developing and\nrefining algorithms for remotely-operated and autonomous underwater vehicles.\nThis paper highlights recent enhancements to the Stonefish simulator, an\nadvanced open-source platform supporting development and testing of marine\nrobotics solutions. Key updates include a suite of additional sensors, such as\nan event-based camera, a thermal camera, and an optical flow camera, as well\nas, visual light communication, support for tethered operations, improved\nthruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.\nThese developments and an automated annotation tool significantly bolster\nStonefish's role in marine robotics research, especially in the field of\nmachine learning, where training data with a known ground truth is hard or\nimpossible to collect.\n","authors":["Michele Grimaldi","Patryk Cieslak","Eduardo Ochoa","Vibhav Bharti","Hayat Rajani","Ignacio Carlucho","Maria Koskinopoulou","Yvan R. Petillot","Nuno Gracias"],"pdf_url":"https://arxiv.org/pdf/2502.11887v1.pdf","comment":"Accepted as full paper at ICRA 2025"},{"id":"http://arxiv.org/abs/2502.11886v1","updated":"2025-02-17T15:13:29Z","published":"2025-02-17T15:13:29Z","title":"LIMR: Less is More for RL Scaling","summary":"  In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.\n","authors":["Xuefeng Li","Haoyang Zou","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11886v1.pdf","comment":"6pages"},{"id":"http://arxiv.org/abs/2502.11882v1","updated":"2025-02-17T15:09:45Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2501.15369v2","updated":"2025-02-17T15:09:31Z","published":"2025-01-26T02:34:58Z","title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","summary":"  We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.\n","authors":["Chuanyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.15369v2.pdf","comment":"Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer"},{"id":"http://arxiv.org/abs/2502.11881v1","updated":"2025-02-17T15:08:50Z","published":"2025-02-17T15:08:50Z","title":"Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models","summary":"  Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.\n","authors":["Hyunwoo Kim","Melanie Sclar","Tan Zhi-Xuan","Lance Ying","Sydney Levine","Yang Liu","Joshua B. Tenenbaum","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2502.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11880v1","updated":"2025-02-17T15:06:28Z","published":"2025-02-17T15:06:28Z","title":"Bitnet.cpp: Efficient Edge Inference for Ternary LLMs","summary":"  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shijie Cao","Yan Xia","Ting Cao","Jianyu Wei","Shuming Ma","Hongyu Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11880v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2405.02079v2","updated":"2025-02-17T15:05:06Z","published":"2024-05-03T13:12:28Z","title":"Argumentative Large Language Models for Explainable and Contestable\n  Decision-Making","summary":"  The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.\n","authors":["Gabriel Freedman","Adam Dejl","Deniz Gorur","Xiang Yin","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2405.02079v2.pdf","comment":"18 pages, 18 figures, Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.11863v1","updated":"2025-02-17T14:55:46Z","published":"2025-02-17T14:55:46Z","title":"FedEAT: A Robustness Optimization Framework for Federated LLMs","summary":"  Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.\n","authors":["Yahao Pang","Xingyuan Wu","Xiaojin Zhang","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2502.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11850v1","updated":"2025-02-17T14:44:12Z","published":"2025-02-17T14:44:12Z","title":"Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif\n  Discovery","summary":"  Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.\n","authors":["Aras Yurtman","Daan Van Wesenbeeck","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2502.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11844v1","updated":"2025-02-17T14:37:47Z","published":"2025-02-17T14:37:47Z","title":"BaxBench: Can LLMs Generate Correct and Secure Backends?","summary":"  The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.\n","authors":["Mark Vero","Niels Mndler","Victor Chibotaru","Veselin Raychev","Maximilian Baader","Nikola Jovanovi","Jingxuan He","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2502.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11843v1","updated":"2025-02-17T14:36:39Z","published":"2025-02-17T14:36:39Z","title":"Can LLM Agents Maintain a Persona in Discourse?","summary":"  Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.\n","authors":["Pranav Bhandari","Nicolas Fay","Michael Wise","Amitava Datta","Stephanie Meek","Usman Naseem","Mehwish Nasim"],"pdf_url":"https://arxiv.org/pdf/2502.11843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.08726v2","updated":"2025-02-17T14:32:21Z","published":"2024-05-14T16:12:27Z","title":"I-CTRL: Imitation to Control Humanoid Robots Through Constrained\n  Reinforcement Learning","summary":"  Humanoid robots have the potential to mimic human motions with high visual\nfidelity, yet translating these motions into practical, physical execution\nremains a significant challenge. Existing techniques in the graphics community\noften prioritize visual fidelity over physics-based feasibility, posing a\nsignificant challenge for deploying bipedal systems in practical applications.\nThis paper addresses these issues through bounded residual reinforcement\nlearning to produce physics-based high-quality motion imitation onto legged\nhumanoid robots that enhance motion resemblance while successfully following\nthe reference human trajectory. Our framework, Imitation to Control Humanoid\nRobots Through Bounded Residual Reinforcement Learning (I-CTRL), reformulates\nmotion imitation as a constrained refinement over non-physics-based retargeted\nmotions. I-CTRL excels in motion imitation with simple and unique rewards that\ngeneralize across five robots. Moreover, our framework introduces an automatic\npriority scheduler to manage large-scale motion datasets when efficiently\ntraining a unified RL policy across diverse motions. The proposed approach\nsignifies a crucial step forward in advancing the control of bipedal robots,\nemphasizing the importance of aligning visual and physical realism for\nsuccessful motion imitation.\n","authors":["Yashuai Yan","Esteve Valls Mascaro","Tobias Egle","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2405.08726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11831v1","updated":"2025-02-17T14:27:14Z","published":"2025-02-17T14:27:14Z","title":"Intuitive physics understanding emerges from self-supervised pretraining\n  on natural videos","summary":"  We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.\n","authors":["Quentin Garrido","Nicolas Ballas","Mahmoud Assran","Adrien Bardes","Laurent Najman","Michael Rabbat","Emmanuel Dupoux","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2502.11831v1.pdf","comment":"24 pages,14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.11829v1","updated":"2025-02-17T14:25:45Z","published":"2025-02-17T14:25:45Z","title":"Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code\n  Generation Capabilities","summary":"  This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.\n","authors":["Hanbin Wang","Xiaoxuan Zhou","Zhipeng Xu","Keyuan Cheng","Yuxin Zuo","Kai Tian","Jingwei Song","Junting Lu","Wenhui Hu","Xueyang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11829v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.11817v1","updated":"2025-02-17T14:09:51Z","published":"2025-02-17T14:09:51Z","title":"AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling","summary":"  Knowledge Tracing (KT) aims to predict students' future performances based on\ntheir former exercises and additional information in educational settings. KT\nhas received significant attention since it facilitates personalized\nexperiences in educational situations. Simultaneously, the autoregressive\nmodeling on the sequence of former exercises has been proven effective for this\ntask. One of the primary challenges in autoregressive modeling for Knowledge\nTracing is effectively representing the anterior (pre-response) and posterior\n(post-response) states of learners across exercises. Existing methods often\nemploy complex model architectures to update learner states using question and\nresponse records. In this study, we propose a novel perspective on knowledge\ntracing task by treating it as a generative process, consistent with the\nprinciples of autoregressive models. We demonstrate that knowledge states can\nbe directly represented through autoregressive encodings on a question-response\nalternate sequence, where model generate the most probable representation in\nhidden state space by analyzing history interactions. This approach underpins\nour framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).\nAdditionally, we incorporate supplementary educational information, such as\nquestion-related skills, into our framework through an auxiliary task, and\ninclude extra exercise details, like response time, as additional inputs. Our\nproposed framework is implemented using advanced autoregressive technologies\nfrom Natural Language Generation (NLG) for both training and prediction.\nEmpirical evaluations on four real-world KT datasets indicate that AAKT\nconsistently outperforms all baseline models in terms of AUC, ACC, and RMSE.\nFurthermore, extensive ablation studies and visualized analysis validate the\neffectiveness of key components in AAKT.\n","authors":["Hao Zhou","Wenge Rong","Jianfei Zhang","Qing Sun","Yuanxin Ouyang","Zhang Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v1","updated":"2025-02-17T13:59:41Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.11809v1","updated":"2025-02-17T13:54:02Z","published":"2025-02-17T13:54:02Z","title":"Revealing Bias Formation in Deep Neural Networks Through the Geometric\n  Mechanisms of Human Visual Decoupling","summary":"  Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.\n","authors":["Yanbiao Ma","Bowei Liu","Wei Dai","Jiayi Chen","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2502.11809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.10711v3","updated":"2025-02-17T13:49:45Z","published":"2025-01-18T09:51:57Z","title":"How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks\n  For LLMs","summary":"  Various benchmarks have been proposed to assess the performance of large\nlanguage models (LLMs) in different coding scenarios. We refer to them as\ncode-related benchmarks. However, there are no systematic guidelines by which\nsuch a benchmark should be developed to ensure its quality, reliability, and\nreproducibility. We propose How2Bench, which is comprised of a 55-criteria\nchecklist as a set of guidelines to govern the development of code-related\nbenchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks\nreleased within the past decade and found concerning issues. Nearly 70% of the\nbenchmarks did not take measures for data quality assurance; over 10% did not\neven open source or only partially open source. Many highly cited benchmarks\nhave loopholes, including duplicated samples, incorrect reference\ncodes/tests/prompts, and unremoved sensitive/confidential information. Finally,\nwe conducted a human study involving 49 participants, which revealed\nsignificant gaps in awareness of the importance of data quality,\nreproducibility, and transparency.\n","authors":["Jialun Cao","Yuk-Kit Chan","Zixuan Ling","Wenxuan Wang","Shuqing Li","Mingwei Liu","Ruixi Qiao","Yuting Han","Chaozheng Wang","Boxi Yu","Pinjia He","Shuai Wang","Zibin Zheng","Michael R. Lyu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2501.10711v3.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2502.11799v1","updated":"2025-02-17T13:42:12Z","published":"2025-02-17T13:42:12Z","title":"Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.\n","authors":["Peiying Yu","Guoxin Chen","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07824v2","updated":"2025-02-17T13:26:52Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07173v2","updated":"2025-02-17T13:25:17Z","published":"2024-10-09T17:59:33Z","title":"Better Language Models Exhibit Higher Visual Alignment","summary":"  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12499v2","updated":"2025-02-17T13:20:15Z","published":"2024-12-17T03:03:17Z","title":"LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for\n  Low-Resource Language Reasoning","summary":"  Large language models (LLMs) have exhibited impressive multilingual reasoning\ncapabilities, driven by extensive multilingual pre-training corpora and\ninstruction fine-tuning data. However, a performance gap exists between high-\nand low-resource language reasoning tasks due to the language imbalance in the\npre-training corpus, which is exacerbated by evaluation bias in existing\nreasoning benchmarks lacking low-resource language coverage. To alleviate this\nissue, we propose LinguaLIFT, a two-stage instruction tuning framework for\nadvancing low-resource language reasoning. LinguaLIFT employs a language\nalignment layer to capture multilingual alignment in a code-switched tuning way\nwithout requiring multilingual instruction or parallel data, thereby\ntransferring the cross-lingual reasoning capabilities to low-resource languages\nthrough English-only instruction tuning data. To comprehensively evaluate the\nmultilingual reasoning capabilities, we introduce the Multilingual Math World\nProblem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and\n10 high-resource languages. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and four widely used\nbenchmarks.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v2","updated":"2025-02-17T13:16:00Z","published":"2025-02-13T08:10:45Z","title":"Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging - An Open Recipe","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.11777v1","updated":"2025-02-17T13:11:35Z","published":"2025-02-17T13:11:35Z","title":"Deep Neural Networks for Accurate Depth Estimation with Latent Space\n  Features","summary":"  Depth estimation plays a pivotal role in advancing human-robot interactions,\nespecially in indoor environments where accurate 3D scene reconstruction is\nessential for tasks like navigation and object handling. Monocular depth\nestimation, which relies on a single RGB camera, offers a more affordable\nsolution compared to traditional methods that use stereo cameras or LiDAR.\nHowever, despite recent progress, many monocular approaches struggle with\naccurately defining depth boundaries, leading to less precise reconstructions.\nIn response to these challenges, this study introduces a novel depth estimation\nframework that leverages latent space features within a deep convolutional\nneural network to enhance the precision of monocular depth maps. The proposed\nmodel features dual encoder-decoder architecture, enabling both color-to-depth\nand depth-to-depth transformations. This structure allows for refined depth\nestimation through latent space encoding. To further improve the accuracy of\ndepth boundaries and local features, a new loss function is introduced. This\nfunction combines latent loss with gradient loss, helping the model maintain\nthe integrity of depth boundaries. The framework is thoroughly tested using the\nNYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in\ncomplex indoor scenarios. The results clearly show that this approach\neffectively reduces depth ambiguities and blurring, making it a promising\nsolution for applications in human-robot interaction and 3D scene\nreconstruction.\n","authors":["Siddiqui Muhammad Yasir","Hyunsik Ahn"],"pdf_url":"https://arxiv.org/pdf/2502.11777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13669v2","updated":"2025-02-17T13:10:33Z","published":"2025-01-23T13:54:53Z","title":"How to Alleviate Catastrophic Forgetting in LLMs Finetuning?\n  Hierarchical Layer-Wise and Element-Wise Regularization","summary":"  Large Language Models (LLMs) exhibit strong general language capabilities.\nHowever, fine-tuning these models on domain-specific tasks often leads to\ncatastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss based on\nelement-wise parameter importance, which constrains the updates to parameters\ncrucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10-15% of the storage, highlighting the practical efficiency. The\ncode will be released.\n","authors":["Shezheng Song","Hao Xu","Jun Ma","Shasha Li","Long Peng","Qian Wan","Xiaodong Liu","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2501.13669v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.04064v2","updated":"2025-02-17T13:04:24Z","published":"2024-10-05T07:25:56Z","title":"Text2Chart31: Instruction Tuning for Chart Generation with Automatic\n  Feedback","summary":"  Large language models (LLMs) have demonstrated strong capabilities across\nvarious language tasks, notably through instruction-tuning methods. However,\nLLMs face challenges in visualizing complex, real-world data through charts and\nplots. Firstly, existing datasets rarely cover a full range of chart types,\nsuch as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning\nmethods do not fully leverage the intricate relationships within rich datasets,\nincluding text, code, and figures. To address these challenges, we propose a\nhierarchical pipeline and a new dataset for chart generation. Our dataset,\nText2Chart31, includes 31 unique plot types referring to the Matplotlib\nlibrary, with 11.1K tuples of descriptions, code, data tables, and plots.\nMoreover, we introduce a reinforcement learning-based instruction tuning\ntechnique for chart generation tasks without requiring human feedback. Our\nexperiments show that this approach significantly enhances the model\nperformance, enabling smaller models to outperform larger open-source models\nand be comparable to state-of-the-art proprietary models in data visualization\ntasks. We make the code and dataset available at\nhttps://github.com/fatemehpesaran310/Text2Chart31.\n","authors":["Fatemeh Pesaran Zadeh","Juyeon Kim","Jin-Hwa Kim","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04064v2.pdf","comment":"EMNLP 2024 Main Oral. Code and dataset are released at\n  https://github.com/fatemehpesaran310/Text2Chart31"},{"id":"http://arxiv.org/abs/2502.11771v1","updated":"2025-02-17T13:00:44Z","published":"2025-02-17T13:00:44Z","title":"The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It","summary":"  The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why current LLMs struggle to detect even simple arithmetic errors.\n","authors":["Leonardo Bertolazzi","Philipp Mondorf","Barbara Plank","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2502.11771v1.pdf","comment":"34 pages, 31 figures"},{"id":"http://arxiv.org/abs/2501.16207v2","updated":"2025-02-17T13:00:34Z","published":"2025-01-27T17:00:56Z","title":"From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs","summary":"  The research in AI-based formal mathematical reasoning has shown an unstop-\npable growth trend. These studies have excelled in mathematical competitions\nlike IMO and have made significant progress. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and breaks\nit down into sub-tasks. We constructed 18k high-quality instruction-response\npairs across five formal specification languages (Coq, Lean4, Dafny, ACSL, and\nTLA+) by distilling gpt-4o and evaluated against ten open-sourced LLMs,\nincluding recent popular DeepSeek-R1. We also fine-tuned several 7~8B small\nmodels to achieve comparable performance with Deepseek-R1-671B. Interestingly,\nwe observed that fine-tuning with formal data also enhances mathematics,\nreasoning, and coding capabilities. Fine-tuned models are released at https:\n//huggingface.co/fm-universe.\n","authors":["Jialun Cao","Yaojie Lu","Meiziniu Li","Haoyang Ma","Haokun Li","Mengda He","Cheng Wen","Le Sun","Hongyu Zhang","Shengchao Qin","Shing-Chi Cheung","Cong Tian"],"pdf_url":"https://arxiv.org/pdf/2501.16207v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2502.11770v1","updated":"2025-02-17T13:00:15Z","published":"2025-02-17T13:00:15Z","title":"Cognitive-Aligned Document Selection for Retrieval-augmented Generation","summary":"  Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance.\n","authors":["Bingyu Wan","Fuxi Zhang","Zhongpeng Qi","Jiayi Ding","Jijun Li","Baoshi Fan","Yijia Zhang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11763v1","updated":"2025-02-17T12:55:41Z","published":"2025-02-17T12:55:41Z","title":"Lightweight Deepfake Detection Based on Multi-Feature Fusion","summary":"  Deepfake technology utilizes deep learning based face manipulation techniques\nto seamlessly replace faces in videos creating highly realistic but\nartificially generated content. Although this technology has beneficial\napplications in media and entertainment misuse of its capabilities may lead to\nserious risks including identity theft cyberbullying and false information. The\nintegration of DL with visual cognition has resulted in important technological\nimprovements particularly in addressing privacy risks caused by artificially\ngenerated deepfake images on digital media platforms. In this study we propose\nan efficient and lightweight method for detecting deepfake images and videos\nmaking it suitable for devices with limited computational resources. In order\nto reduce the computational burden usually associated with DL models our method\nintegrates machine learning classifiers in combination with keyframing\napproaches and texture analysis. Moreover the features extracted with a\nhistogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands\nwere integrated to evaluate using random forest extreme gradient boosting extra\ntrees and support vector classifier algorithms. Our findings show a\nfeature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and\n96% on FaceForensics++ and Celeb-DFv2 respectively.\n","authors":["Siddiqui Muhammad Yasir","Hyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11756v1","updated":"2025-02-17T12:52:10Z","published":"2025-02-17T12:52:10Z","title":"On the Computation of the Fisher Information in Continual Learning","summary":"  One of the most popular methods for continual learning with deep neural\nnetworks is Elastic Weight Consolidation (EWC), which involves computing the\nFisher Information. The exact way in which the Fisher Information is computed\nis however rarely described, and multiple different implementations for it can\nbe found online. This blog post discusses and empirically compares several\noften-used implementations, which highlights that many currently reported\nresults for EWC could likely be improved by changing the way the Fisher\nInformation is computed.\n","authors":["Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2502.11756v1.pdf","comment":"To appear in the blogpost track at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01053v4","updated":"2025-02-17T12:50:31Z","published":"2024-05-02T07:15:23Z","title":"On the Universality of Self-Supervised Representation Learning","summary":"  In this paper, we investigate the characteristics that define a good\nrepresentation or model. We propose that such a representation or model should\npossess universality, characterized by: (i) discriminability: performing well\non training samples; (ii) generalization: performing well on unseen datasets;\nand (iii) transferability: performing well on unseen tasks with distribution\nshifts. Despite its importance, current self-supervised learning (SSL) methods\nlack explicit modeling of universality, and theoretical analysis remains\nunderexplored. To address these issues, we aim to explore and incorporate\nuniversality into SSL. Specifically, we first revisit SSL from a task\nperspective and find that each mini-batch can be viewed as a multi-class\nclassification task. We then propose that a universal SSL model should achieve:\n(i) learning universality by minimizing loss across all training samples, and\n(ii) evaluation universality by learning causally invariant representations\nthat generalize well to unseen tasks. To quantify this, we introduce a\n$\\sigma$-measurement that assesses the gap between the performance of SSL model\nand optimal task-specific models. Furthermore, to model universality, we\npropose the GeSSL framework. It first learns task-specific models by minimizing\nSSL loss, then incorporates future updates to enhance discriminability, and\nfinally integrates these models to learn from multiple tasks. Theoretical and\nempirical evidence supports the effectiveness of GeSSL.\n","authors":["Wenwen Qiang","Jingyao Wang","Lingyu Si","Chuxiong Sun","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2405.01053v4.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.12103v1","updated":"2025-02-17T18:24:48Z","published":"2025-02-17T18:24:48Z","title":"CriteoPrivateAd: A Real-World Bidding Dataset to Design Private\n  Advertising Systems","summary":"  In the past years, many proposals have emerged in order to address online\nadvertising use-cases without access to third-party cookies. All these\nproposals leverage some privacy-enhancing technologies such as aggregation or\ndifferential privacy. Yet, no public and rich-enough ground truth is currently\navailable to assess the relevancy of aforementioned private advertising\nframeworks. We are releasing the largest, in terms of number of features,\nbidding dataset specifically built in alignment with the design of major\nbrowser vendors proposals such as Chrome Privacy Sandbox. This dataset, coined\nCriteoPrivateAd, stands for an anonymised version of Criteo production logs and\nprovides sufficient data to learn bidding models commonly used in online\nadvertising under many privacy constraints (delayed reports, display and\nuser-level differential privacy, user signal quantisation or aggregated\nreports). We ensured that this dataset, while being anonymised, is able to\nprovide offline results close to production performance of adtech companies\nincluding Criteo - making it a relevant ground truth to design private\nadvertising systems. The dataset is available in Hugging Face:\nhttps://huggingface.co/datasets/criteo/CriteoPrivateAd.\n","authors":["Mehdi Sebbar","Corentin Odic","Mathieu Lchine","Alos Bissuel","Nicolas Chrysanthos","Anthony D'Amato","Alexandre Gilotte","Fabian Hring","Sarah Nogueira","Maxime Vono"],"pdf_url":"https://arxiv.org/pdf/2502.12103v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.10281v2","updated":"2025-02-17T16:21:11Z","published":"2025-02-14T16:38:08Z","title":"TrustZero - open, verifiable and scalable zero-trust","summary":"  We present a passport-level trust token for Europe. In an era of escalating\ncyber threats fueled by global competition in economic, military, and\ntechnological domains, traditional security models are proving inadequate. The\nrise of advanced attacks exploiting zero-day vulnerabilities, supply chain\ninfiltration, and system interdependencies underscores the need for a paradigm\nshift in cybersecurity. Zero Trust Architecture (ZTA) emerges as a\ntransformative framework that replaces implicit trust with continuous\nverification of identity and granular access control. This thesis introduces\nTrustZero, a scalable layer of zero-trust security built around a universal\n\"trust token\" - a non-revocable self-sovereign identity with cryptographic\nsignatures to enable robust, mathematically grounded trust attestations. By\nintegrating ZTA principles with cryptography, TrustZero establishes a secure\nweb-of-trust framework adaptable to legacy systems and inter-organisational\ncommunication.\n","authors":["Adrian-Tudor Dumitrescu","Johan Pouwelse"],"pdf_url":"https://arxiv.org/pdf/2502.10281v2.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.11920v1","updated":"2025-02-17T15:33:07Z","published":"2025-02-17T15:33:07Z","title":"A limited technical background is sufficient for attack-defense tree\n  acceptability","summary":"  Attack-defense trees (ADTs) are a prominent graphical threat modeling method\nthat is highly recommended for analyzing and communicating security-related\ninformation. Despite this, existing empirical studies of attack trees have\nestablished their acceptability only for users with highly technical (computer\nscience) backgrounds while raising questions about their suitability for threat\nmodeling stakeholders with a limited technical background. Our research\naddresses this gap by investigating the impact of the users' technical\nbackground on ADT acceptability in an empirical study.\n  Our Method Evaluation Model-based study consisted of n = 102 participants (53\nwith a strong computer science background and 49 with a limited computer\nscience background) who were asked to complete a series of ADT-related tasks.\nBy analyzing their responses and comparing the results, we reveal that a very\nlimited technical background is sufficient for ADT acceptability. This finding\nunderscores attack trees' viability as a threat modeling method.\n","authors":["Nathan Daniel Schiele","Olga Gadyatskaya"],"pdf_url":"https://arxiv.org/pdf/2502.11920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11854v1","updated":"2025-02-17T14:46:58Z","published":"2025-02-17T14:46:58Z","title":"Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on\n  the CICIoMT2024 Dataset","summary":"  The rapid proliferation of Internet of Medical Things (IoMT) devices in\nhealthcare has introduced unique cybersecurity challenges, primarily due to the\ndiverse communication protocols and critical nature of these devices This\nresearch aims to develop an advanced, real-time anomaly detection framework\ntailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024\ndataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS,\nDDoS), time-series (active/idle states), and device-specific (Bluetooth) data,\nour study captures a comprehensive range of IoMT interactions As part of our\ndata analysis, various machine learning techniques are employed which include\nan ensemble model using XGBoost for improved performance against specific\nattack types, sequential models comprised of LSTM and CNN-LSTM that leverage\ntime dependencies, and unsupervised models such as Autoencoders and Isolation\nForest that are good in general anomaly detection The results of the experiment\nprove with an ensemble model lowers false positive rates and reduced\ndetections.\n","authors":["Prathamesh Chandekar","Mansi Mehta","Swet Chandan"],"pdf_url":"https://arxiv.org/pdf/2502.11854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11844v1","updated":"2025-02-17T14:37:47Z","published":"2025-02-17T14:37:47Z","title":"BaxBench: Can LLMs Generate Correct and Secure Backends?","summary":"  The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.\n","authors":["Mark Vero","Niels Mndler","Victor Chibotaru","Veselin Raychev","Maximilian Baader","Nikola Jovanovi","Jingxuan He","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2502.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08133v3","updated":"2025-02-17T13:50:54Z","published":"2024-11-12T19:24:42Z","title":"Impactful Bit-Flip Search on Full-precision Models","summary":"  Neural networks have shown remarkable performance in various tasks, yet they\nremain susceptible to subtle changes in their input or model parameters. One\nparticularly impactful vulnerability arises through the Bit-Flip Attack (BFA),\nwhere flipping a small number of critical bits in a model's parameters can\nseverely degrade its performance. A common technique for inducing bit flips in\nDRAM is the Row-Hammer attack, which exploits frequent uncached memory accesses\nto alter data. Identifying susceptible bits can be achieved through exhaustive\nsearch or progressive layer-by-layer analysis, especially in quantized\nnetworks. In this work, we introduce Impactful Bit-Flip Search (IBS), a novel\nmethod for efficiently pinpointing and flipping critical bits in full-precision\nnetworks. Additionally, we propose a Weight-Stealth technique that\nstrategically modifies the model's parameters in a way that maintains the float\nvalues within the original distribution, thereby bypassing simple range checks\noften used in tamper detection.\n","authors":["Nadav Benedek","Matan Levy","Mahmood Sharif"],"pdf_url":"https://arxiv.org/pdf/2411.08133v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11798v1","updated":"2025-02-17T13:39:05Z","published":"2025-02-17T13:39:05Z","title":"BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion\n  Model","summary":"  Backdoor learning is a critical research topic for understanding the\nvulnerabilities of deep neural networks. While it has been extensively studied\nin discriminative models over the past few years, backdoor learning in\ndiffusion models (DMs) has recently attracted increasing attention, becoming a\nnew research hotspot. Although many different backdoor attack and defense\nmethods have been proposed for DMs, a comprehensive benchmark for backdoor\nlearning in DMs is still lacking. This absence makes it difficult to conduct\nfair comparisons and thoroughly evaluate existing approaches, thus hindering\nfuture research progress. To address this issue, we propose BackdoorDM, the\nfirst comprehensive benchmark designed for backdoor learning in DMs. It\ncomprises nine state-of-the-art (SOTA) attack methods, four SOTA defense\nstrategies, and two helpful visualization analysis tools. We first\nsystematically classify and formulate the existing literature in a unified\nframework, focusing on three different backdoor attack types and five backdoor\ntarget types, which are restricted to a single type in discriminative models.\nThen, we systematically summarize the evaluation metrics for each type and\npropose a unified backdoor evaluation method based on GPT-4o. Finally, we\nconduct a comprehensive evaluation and highlight several important conclusions.\nWe believe that BackdoorDM will help overcome current barriers and contribute\nto building a trustworthy DMs community. The codes are released in\nhttps://github.com/linweiii/BackdoorDM.\n","authors":["Weilin Lin","Nanjun Zhou","Yanyun Wang","Jianze Li","Hui Xiong","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05109v3","updated":"2025-02-17T13:20:15Z","published":"2024-10-07T15:04:37Z","title":"Secure Software/Hardware Hybrid In-Field Testing for System-on-Chip","summary":"  Modern Systems-on-Chip (SoCs) incorporate built-in self-test (BIST) modules\ndeeply integrated into the device's intellectual property (IP) blocks. Such\nmodules handle hardware faults and defects during device operation. As such,\nBIST results potentially reveal the internal structure and state of the device\nunder test (DUT) and hence open attack vectors. So-called result compaction can\novercome this vulnerability by hiding the BIST chain structure but introduces\nthe issues of aliasing and invalid signatures. Software-BIST provides a\nflexible solution, that can tackle these issues, but suffers from limited\nobservability and fault coverage. In this paper, we hence introduce a\nlow-overhead software/hardware hybrid approach that overcomes the mentioned\nlimitations. It relies on (a) keyed-hash message authentication code (KMAC)\navailable on the SoC providing device-specific secure and valid signatures with\nzero aliasing and (b) the SoC processor for test scheduling hence increasing\nDUT availability. The proposed approach offers both on-chip- and remote-testing\ncapabilities. We showcase a RISC-V-based SoC to demonstrate our approach,\ndiscussing system overhead and resulting compaction rates.\n","authors":["Saleh Mulhem","Christian Ewert","Andrija Neskovic","Amrit Sharma Poudel","Christoph Hbner","Mladen Berekovic","Rainer Buchty"],"pdf_url":"https://arxiv.org/pdf/2410.05109v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11745v1","updated":"2025-02-17T12:39:03Z","published":"2025-02-17T12:39:03Z","title":"Understanding RowHammer Under Reduced Refresh Latency: Experimental\n  Analysis of Real DRAM Chips and Implications on Future Solutions","summary":"  RowHammer is a major read disturbance mechanism in DRAM where repeatedly\naccessing (hammering) a row of DRAM cells (DRAM row) induces bitflips in\nphysically nearby DRAM rows (victim rows). To ensure robust DRAM operation,\nstate-of-the-art mitigation mechanisms restore the charge in potential victim\nrows (i.e., they perform preventive refresh or charge restoration). With newer\nDRAM chip generations, these mechanisms perform preventive refresh more\naggressively and cause larger performance, energy, or area overheads.\nTherefore, it is essential to develop a better understanding and in-depth\ninsights into the preventive refresh to secure real DRAM chips at low cost. In\nthis paper, our goal is to mitigate RowHammer at low cost by understanding the\nimpact of reduced preventive refresh latency on RowHammer. To this end, we\npresent the first rigorous experimental study on the interactions between\nrefresh latency and RowHammer characteristics in real DRAM chips. Our\nexperimental characterization using 388 real DDR4 DRAM chips from three major\nmanufacturers demonstrates that a preventive refresh latency can be\nsignificantly reduced (by 64%). To investigate the impact of reduced preventive\nrefresh latency on system performance and energy efficiency, we reduce the\npreventive refresh latency and adjust the aggressiveness of existing RowHammer\nsolutions by developing a new mechanism, Partial Charge Restoration for\nAggressive Mitigation (PaCRAM). Our results show that PaCRAM reduces the\nperformance and energy overheads induced by five state-of-the-art RowHammer\nmitigation mechanisms with small additional area overhead. Thus, PaCRAM\nintroduces a novel perspective into addressing RowHammer vulnerability at low\ncost by leveraging our experimental observations. To aid future research, we\nopen-source our PaCRAM implementation at https://github.com/CMU-SAFARI/PaCRAM.\n","authors":["Yahya Can Turul","A. Giray Yalk","smail Emir Yksel","Ataberk Olgun","Ouzhan Canpolat","Nisa Bostanc","Mohammad Sadrosadati","Ouz Ergin","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2502.11745v1.pdf","comment":"To appear in HPCA'25"},{"id":"http://arxiv.org/abs/2502.11737v1","updated":"2025-02-17T12:23:53Z","published":"2025-02-17T12:23:53Z","title":"2FA: Navigating the Challenges and Solutions for Inclusive Access","summary":"  The digital age requires strong security measures to protect online\nactivities. Two-Factor Authentication (2FA) has emerged as a critical solution.\nHowever, its implementation presents significant challenges, particularly in\nterms of accessibility for people with disabilities. This paper examines the\nintricacies of deploying 2FA in a way that is secure and accessible to all\nusers by outlining the concrete challenges for people who are affected by\nvarious types of impairments. This research investigates the implications of\n2FA on digital inclusivity and proposes solutions to enhance accessibility. An\nanalysis was conducted to examine the implementation and availability of\nvarious 2FA methods across popular online platforms. The results reveal a\ndiverse landscape of authentication strategies. While 2FA significantly\nimproves account security, its current adoption is hampered by inconsistencies\nacross platforms and a lack of standardised, accessible options for users with\ndisabilities. Future advancements in 2FA technologies, including but not\nlimited to autofill capabilities and the adoption of Fast IDentity Onlines\n(FIDO) protocols, offer possible directions for more inclusive authentication\nmechanisms. However, ongoing research is necessary to address the evolving\nneeds of users with disabilities and to mitigate new security challenges. This\npaper proposes a collaborative approach among stakeholders to ensure that\nsecurity improvements do not compromise accessibility. It promotes a digital\nenvironment where security and inclusivity mutually reinforce each other.\n","authors":["Alexander Lengert"],"pdf_url":"https://arxiv.org/pdf/2502.11737v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.11687v1","updated":"2025-02-17T11:25:28Z","published":"2025-02-17T11:25:28Z","title":"ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning","summary":"  Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.\n","authors":["Manaar Alam","Hithem Lamri","Michail Maniatakos"],"pdf_url":"https://arxiv.org/pdf/2502.11687v1.pdf","comment":"This paper is accepted at 62nd Design Automation Conference (DAC)\n  2025"},{"id":"http://arxiv.org/abs/2405.06368v4","updated":"2025-02-17T11:23:32Z","published":"2024-05-10T10:10:37Z","title":"DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under\n  Differentially Private Federated Learning using Dynamic Low-Rank Adaptation","summary":"  Federated learning (FL) allows clients to collaboratively train a global\nmodel without sharing their local data with a server. However, clients'\ncontributions to the server can still leak sensitive information. Differential\nprivacy (DP) addresses such leakage by providing formal privacy guarantees,\nwith mechanisms that add randomness to the clients' contributions. The\nrandomness makes it infeasible to train large transformer-based models, common\nin modern federated learning systems. In this work, we empirically evaluate the\npracticality of fine-tuning large scale on-device transformer-based models with\ndifferential privacy in a federated learning system. We conduct comprehensive\nexperiments on various system properties for tasks spanning a multitude of\ndomains: speech recognition, computer vision (CV) and natural language\nunderstanding (NLU). Our results show that full fine-tuning under\ndifferentially private federated learning (DP-FL) generally leads to huge\nperformance degradation which can be alleviated by reducing the dimensionality\nof contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks\nof existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA)\nconsistently outperforms other methods. An even more promising approach,\nDyLoRA, which makes the low rank variable, when naively combined with FL would\nstraightforwardly break differential privacy. We therefore propose an\nadaptation method that can be combined with differential privacy and call it\nDP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word\nerror rate (WER) increase due to DP to less than 2% and 7% respectively with 1\nmillion clients and a stringent privacy budget of $\\epsilon=2$.\n","authors":["Jie Xu","Karthikeyan Saravanan","Rogier van Dalen","Haaris Mehmood","David Tuckey","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2405.06368v4.pdf","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.11658v1","updated":"2025-02-17T10:49:23Z","published":"2025-02-17T10:49:23Z","title":"\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data","summary":"  Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.\n","authors":["Antoine Boutet","Victor Morel"],"pdf_url":"https://arxiv.org/pdf/2502.11658v1.pdf","comment":"Submitted to ICWSM on January 15, 2025"},{"id":"http://arxiv.org/abs/2502.11650v1","updated":"2025-02-17T10:42:13Z","published":"2025-02-17T10:42:13Z","title":"\"I'm 73, you can't expect me to have multiple passwords\": Password\n  Management Concerns and Solutions of Irish Older Adults","summary":"  Based on Irish older adult's perceptions, practices, and challenges regarding\npassword management, the goal of this study was to compile suitable advice that\ncan benefit this demographic. To achieve this, we first conducted semi\nstructured interviews (n=37), we then collated advice based on best practice\nand what we learned from these interviews. We facilitated two independent focus\ngroups (n=31) to evaluate and adjust this advice and tested the finalized\nadvice through an observational study (n=15). The participants were aged\nbetween 59 and 86 and came from various counties in Ireland, both rural and\nurban. The findings revealed that managing multiple passwords was a significant\nsource of frustration, leading some participants to adopt novel and informal\nstrategies for storing them. A notable hesitation to adopt digital password\nmanagers and passphrases was also observed. Participants appreciated guidance\non improving their password practices, with many affirming that securely\nwriting down passwords was a practical strategy. Irish older adults\ndemonstrated strong intuition regarding cybersecurity, notably expressing\nconcerns over knowledge-based security checks used by banks and government\ninstitutions. This study aims to contribute to the aggregation of practical\npassword advice suited to older adults, making password security more\nmanageable and less burdensome for this demographic.\n","authors":["Ashley Sheil","Jacob Camilleri","Michelle O'Keeffe","Melanie Gruben","Moya Cronin","Hazel Murray"],"pdf_url":"https://arxiv.org/pdf/2502.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11647v1","updated":"2025-02-17T10:39:21Z","published":"2025-02-17T10:39:21Z","title":"DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with\n  Model Editing","summary":"  Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection.\n","authors":["Yi Wang","Fenghua Weng","Sibei Yang","Zhan Qin","Minlie Huang","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11641v1","updated":"2025-02-17T10:35:18Z","published":"2025-02-17T10:35:18Z","title":"A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee\n  Metric","summary":"  The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to describe a zero-knowledge\nproof for this variant of the problem.\n","authors":["Mladen Kovaevi","Tatjana Grbi","Darko apko","Nemanja Nedi","Srdjan Vukmirovi"],"pdf_url":"https://arxiv.org/pdf/2502.11641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00463v2","updated":"2025-02-17T10:13:59Z","published":"2024-12-31T14:22:53Z","title":"SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion\n  Models with Self-Augmented Training","summary":"  The rapid proliferation of AI-generated images necessitates effective\nwatermarking techniques to protect intellectual property and detect fraudulent\ncontent. While existing training-based watermarking methods show promise, they\noften struggle with generalizing across diverse prompts and tend to introduce\nvisible artifacts. To this end, we propose a novel, provably generalizable\nimage watermarking approach for Latent Diffusion Models, termed Self-Augmented\nTraining (SAT-LDM). Our method aligns the training and testing phases through a\nfree generation distribution, thereby enhancing the watermarking module's\ngeneralization capabilities. We theoretically consolidate SAT-LDM by proving\nthat the free generation distribution contributes to its tight generalization\nbound, without the need for additional data collection. Extensive experiments\nshow that SAT-LDM not only achieves robust watermarking but also significantly\nimproves the quality of watermarked images across a wide range of prompts.\nMoreover, our experimental analyses confirm the strong generalization abilities\nof SAT-LDM. We hope that our method provides a practical and efficient solution\nfor securing high-fidelity AI-generated content.\n","authors":["Lu Zhang","Liang Zeng"],"pdf_url":"https://arxiv.org/pdf/2501.00463v2.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.11616v1","updated":"2025-02-17T09:59:25Z","published":"2025-02-17T09:59:25Z","title":"User-Centric Data Management in Decentralized Internet of Behaviors\n  System","summary":"  The Internet of Behaviors (IoB) is an emerging concept that utilizes devices\nto collect human behavior and provide intelligent services. Although some\nresearch has focused on human behavior analysis and data collection within IoB,\nthe associated security and privacy challenges remain insufficiently explored.\nThis paper analyzes the security and privacy risks at different stages of\nbehavioral data generating, uploading, and using, while also considering the\ndynamic characteristics of user activity areas. Then, we propose a\nblockchain-based distributed IoB data storage and sharing framework, which is\ncategorized into sensing, processing, and management layers based on these\nstages. To accommodate both identity authentication and behavioral privacy,\nzero-knowledge proofs are used in the sensing layer to separate the correlation\nbetween behavior and identity, which is further extended to a distributed\narchitecture for cross-domain authentication. In the processing layer, an\nimproved consensus protocol is proposed to enhance the decision-making\nefficiency of distributed IoB by analyzing the geographical and computational\ncapability of the servers. In the management layer, user permission differences\nand the privacy of access targets are considered. Different types of behavior\nare modeled as corresponding relationships between keys, and fine-grained\nsecure access is achieved through function secret sharing. Simulation results\ndemonstrate the effectiveness of the proposed framework in multi-scenario IoB,\nwith average consensus and authentication times reduced by 74% and 56%,\nrespectively.\n","authors":["Shiqi Zhang","Dapeng Wu","Honggang Wang","Ruyan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16205v4","updated":"2025-02-17T09:00:28Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Hongming Yang","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09827v2","updated":"2025-02-17T08:34:43Z","published":"2025-02-13T23:53:40Z","title":"Data and Decision Traceability for SDA TAP Lab's Prototype Battle\n  Management System","summary":"  Space Protocol is applying the principles derived from MITRE and NIST's\nSupply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a\ncomplex multi party system to achieve introspection, auditing, and replay of\ndata and decisions that ultimately lead to a end decision. The core goal of\ndecision traceability is to ensure transparency, accountability, and integrity\nwithin the WA system. This is accomplished by providing a clear, auditable path\nfrom the system's inputs all the way to the final decision. This traceability\nenables the system to track the various algorithms and data flows that have\ninfluenced a particular outcome.\n","authors":["Latha Pratti","Samya Bagchi","Yasir Latif"],"pdf_url":"https://arxiv.org/pdf/2502.09827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11550v1","updated":"2025-02-17T08:30:42Z","published":"2025-02-17T08:30:42Z","title":"Trinity: A Scalable and Forward-Secure DSSE for Spatio-Temporal Range\n  Query","summary":"  Cloud-based outsourced Location-based services have profound impacts on\nvarious aspects of people's lives but bring security concerns. Existing\nspatio-temporal data secure retrieval schemes have significant shortcomings\nregarding dynamic updates, either compromising privacy through leakage during\nupdates (forward insecurity) or incurring excessively high update costs that\nhinder practical application. Under these circumstances, we first propose a\nbasic filter-based spatio-temporal range query scheme \\TrinityI that supports\nlow-cost dynamic updates and automatic expansion. Furthermore, to improve\nsecurity, reduce storage cost, and false positives, we propose a forward secure\nand verifiable scheme \\TrinityII that simultaneously minimizes storage\noverhead. A formal security analysis proves that \\TrinityI and \\TrinityII are\nIndistinguishable under Selective Chosen-Plaintext Attack (IND-SCPA). Finally,\nextensive experiments demonstrate that our design \\TrinityII significantly\nreduces storage requirements by 80\\%, enables data retrieval at the 1\nmillion-record level in just 0.01 seconds, and achieves 10 $\\times$ update\nefficiency than state-of-art.\n","authors":["Zhijun Li","Kuizhi Liu","Minghui Xu","Xiangyu Wang","Yinbin Miao","Jianfeng Ma","Xiuzhen Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.11550v1.pdf","comment":"14pages,6 figures"},{"id":"http://arxiv.org/abs/2502.11521v1","updated":"2025-02-17T07:45:03Z","published":"2025-02-17T07:45:03Z","title":"DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning","summary":"  DeFi (Decentralized Finance) is one of the most important applications of\ntoday's cryptocurrencies and smart contracts. It manages hundreds of billions\nin Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi\nprice manipulation attacks. Despite state-of-the-art (SOTA) systems like\nDeFiRanger and DeFort, we found that they are less effective to non-standard\nprice models in custom DeFi protocols, which account for 44.2% of the 95 DeFi\nprice manipulation attacks reported over the past three years.\n  In this paper, we introduce the first LLM-based approach, DeFiScope, for\ndetecting DeFi price manipulation attacks in both standard and custom price\nmodels. Our insight is that large language models (LLMs) have certain\nintelligence to abstract price calculation from code and infer the trend of\ntoken price changes based on the extracted price models. To further strengthen\nLLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it\nto fine-tune a DeFi price-specific LLM. Together with the high-level DeFi\noperations recovered from low-level transaction data, DeFiScope detects various\nDeFi price manipulations according to systematically mined patterns.\nExperimental results show that DeFiScope achieves a high precision of 96% and a\nrecall rate of 80%, significantly outperforming SOTA approaches. Moreover, we\nevaluate DeFiScope's cost-effectiveness and demonstrate its practicality by\nhelping our industry partner confirm 147 real-world price manipulation attacks,\nincluding discovering 81 previously unknown historical incidents.\n","authors":["Juantao Zhong","Daoyuan Wu","Ye Liu","Maoyi Xie","Yang Liu","Yi Li","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11470v1","updated":"2025-02-17T06:01:06Z","published":"2025-02-17T06:01:06Z","title":"Optimized detection of cyber-attacks on IoT networks via hybrid deep\n  learning models","summary":"  The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.\n","authors":["Ahmed Bensaoud","Jugal Kalita"],"pdf_url":"https://arxiv.org/pdf/2502.11470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13682v5","updated":"2025-02-17T05:57:26Z","published":"2024-03-20T15:40:18Z","title":"Threats, Attacks, and Defenses in Machine Unlearning: A Survey","summary":"  Machine Unlearning (MU) has recently gained considerable attention due to its\npotential to achieve Safe AI by removing the influence of specific data from\ntrained Machine Learning (ML) models. This process, known as knowledge removal,\naddresses AI governance concerns of training data such as quality, sensitivity,\ncopyright restrictions, and obsolescence. This capability is also crucial for\nensuring compliance with privacy regulations such as the Right To Be Forgotten\n(RTBF). Furthermore, effective knowledge removal mitigates the risk of harmful\noutcomes, safeguarding against biases, misinformation, and unauthorized data\nexploitation, thereby enhancing the safe and responsible use of AI systems.\nEfforts have been made to design efficient unlearning approaches, with MU\nservices being examined for integration with existing machine learning as a\nservice (MLaaS), allowing users to submit requests to remove specific data from\nthe training corpus. However, recent research highlights vulnerabilities in\nmachine unlearning systems, such as information leakage and malicious\nunlearning, that can lead to significant security and privacy concerns.\nMoreover, extensive research indicates that unlearning methods and prevalent\nattacks fulfill diverse roles within MU systems. This underscores the intricate\nrelationship and complex interplay among these mechanisms in maintaining system\nfunctionality and safety. This survey aims to fill the gap between the\nextensive number of studies on threats, attacks, and defenses in machine\nunlearning and the absence of a comprehensive review that categorizes their\ntaxonomy, methods, and solutions, thus offering valuable insights for future\nresearch directions and practical implementations.\n","authors":["Ziyao Liu","Huanyi Ye","Chen Chen","Yongsen Zheng","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2403.13682v5.pdf","comment":"Accepted by IEEE Open Journal of the Computer Society"},{"id":"http://arxiv.org/abs/2502.11455v1","updated":"2025-02-17T05:28:47Z","published":"2025-02-17T05:28:47Z","title":"Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language\n  Models via Adversarial Training","summary":"  Safety alignment is critical in pre-training large language models (LLMs) to\ngenerate responses aligned with human values and refuse harmful queries. Unlike\nLLM, the current safety alignment of VLMs is often achieved with post-hoc\nsafety fine-tuning. However, these methods are less effective to white-box\nattacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a\nnovel training framework that explicitly considers adversarial.\n$\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO\nto enhance the safety alignment of VLMs under worst-case adversarial\nperturbations. $\\textit{ADPO}$ introduces two key components: (1) an\nadversarial-trained reference model that generates human-preferred responses\nunder worst-case perturbations, and (2) an adversarial-aware DPO loss that\ngenerates winner-loser pairs accounting for adversarial distortions. By\ncombining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust\nand reliable even in the presence of sophisticated jailbreak attacks. Extensive\nexperiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the\nsafety alignment and general utility of VLMs.\n","authors":["Fenghua Weng","Jian Lou","Jun Feng","Minlie Huang","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06662v4","updated":"2025-02-17T03:08:30Z","published":"2023-01-17T02:14:57Z","title":"Graph Learning Across Data Silos","summary":"  We consider the problem of inferring graph topology from smooth graph signals\nin a novel but practical scenario where data are located in distributed clients\nand prohibited from leaving local clients due to factors such as privacy\nconcerns. The main difficulty in this task is how to exploit the potentially\nheterogeneous data of all clients under data silos. To this end, we first\npropose an auto-weighted multiple graph learning model to jointly learn a\npersonalized graph for each local client and a single consensus graph for all\nclients. The personalized graphs match local data distributions, thereby\nmitigating data heterogeneity, while the consensus graph captures the global\ninformation. Moreover, the model can automatically assign appropriate\ncontribution weights to local graphs based on their similarity to the consensus\ngraph. We next devise a tailored algorithm to solve the induced problem, where\nall raw data are processed locally without leaving clients. Theoretically, we\nestablish a provable estimation error bound and convergence analysis for the\nproposed model and algorithm. Finally, extensive experiments on synthetic and\nreal data are carried out, and the results illustrate that our approach can\nlearn graphs effectively in the target scenario.\n","authors":["Xiang Zhang","Qiao Wang"],"pdf_url":"https://arxiv.org/pdf/2301.06662v4.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.11379v1","updated":"2025-02-17T02:49:26Z","published":"2025-02-17T02:49:26Z","title":"CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models","summary":"  Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.\n","authors":["Guanghao Zhou","Panjia Qiu","Mingyuan Fan","Cen Chen","Mingyuan Chu","Xin Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13115v2","updated":"2025-02-17T02:23:21Z","published":"2025-01-19T13:39:51Z","title":"Dagger Behind Smile: Fool LLMs with a Happy Ending Story","summary":"  The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.\n","authors":["Xurui Song","Zhixin Xie","Shuo Huai","Jiayi Kong","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2501.13115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11358v1","updated":"2025-02-17T02:15:46Z","published":"2025-02-17T02:15:46Z","title":"Mimicking the Familiar: Dynamic Command Generation for Information Theft\n  Attacks in LLM Tool-Learning System","summary":"  Information theft attacks pose a significant risk to Large Language Model\n(LLM) tool-learning systems. Adversaries can inject malicious commands through\ncompromised tools, manipulating LLMs to send sensitive information to these\ntools, which leads to potential privacy breaches. However, existing attack\napproaches are black-box oriented and rely on static commands that cannot adapt\nflexibly to the changes in user queries and the invocation chain of tools. It\nmakes malicious commands more likely to be detected by LLM and leads to attack\nfailure. In this paper, we propose AutoCMD, a dynamic attack comment generation\napproach for information theft attacks in LLM tool-learning systems. Inspired\nby the concept of mimicking the familiar, AutoCMD is capable of inferring the\ninformation utilized by upstream tools in the toolchain through learning on\nopen-source systems and reinforcement with target system examples, thereby\ngenerating more targeted commands for information theft. The evaluation results\nshow that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can\nbe generalized to new tool-learning systems to expose their information leakage\nrisks. We also design four defense methods to effectively protect tool-learning\nsystems from the attack.\n","authors":["Ziyou Jiang","Mingyang Li","Guowei Yang","Junjie Wang","Yuekai Huang","Zhiyuan Chang","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11358v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.11355v1","updated":"2025-02-17T02:11:17Z","published":"2025-02-17T02:11:17Z","title":"\"Nuclear Deployed!\": Analyzing Catastrophic Risks in Decision-making of\n  Autonomous LLM Agents","summary":"  Large language models (LLMs) are evolving into autonomous decision-makers,\nraising concerns about catastrophic risks in high-stakes scenarios,\nparticularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.\nBased on the insight that such risks can originate from trade-offs between the\nagent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel\nthree-stage evaluation framework, which is carefully constructed to effectively\nand naturally expose such risks. We conduct 14,400 agentic simulations across\n12 advanced LLMs, with extensive experiments and analysis. Results reveal that\nLLM agents can autonomously engage in catastrophic behaviors and deception,\nwithout being deliberately induced. Furthermore, stronger reasoning abilities\noften increase, rather than mitigate, these risks. We also show that these\nagents can violate instructions and superior commands. On the whole, we\nempirically prove the existence of catastrophic risks in autonomous LLM agents.\nWe will release our code upon request.\n","authors":["Rongwu Xu","Xiaojian Li","Shuo Chen","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11355v1.pdf","comment":"Our code will be available at\n  https://github.com/pillowsofwind/LLM-CBRN-Risks"},{"id":"http://arxiv.org/abs/2502.12382v1","updated":"2025-02-17T23:41:10Z","published":"2025-02-17T23:41:10Z","title":"Hybrid Machine Learning Models for Intrusion Detection in IoT:\n  Leveraging a Real-World IoT Dataset","summary":"  The rapid growth of the Internet of Things (IoT) has revolutionized\nindustries, enabling unprecedented connectivity and functionality. However,\nthis expansion also increases vulnerabilities, exposing IoT networks to\nincreasingly sophisticated cyberattacks. Intrusion Detection Systems (IDS) are\ncrucial for mitigating these threats, and recent advancements in Machine\nLearning (ML) offer promising avenues for improvement. This research explores a\nhybrid approach, combining several standalone ML models such as Random Forest\n(RF), XGBoost, K-Nearest Neighbors (KNN), and AdaBoost, in a voting-based\nhybrid classifier for effective IoT intrusion detection. This ensemble method\nleverages the strengths of individual algorithms to enhance accuracy and\naddress challenges related to data complexity and scalability. Using the\nwidely-cited IoT-23 dataset, a prominent benchmark in IoT cybersecurity\nresearch, we evaluate our hybrid classifiers for both binary and multi-class\nintrusion detection problems, ensuring a fair comparison with existing\nliterature. Results demonstrate that our proposed hybrid models, designed for\nrobustness and scalability, outperform standalone approaches in IoT\nenvironments. This work contributes to the development of advanced, intelligent\nIDS frameworks capable of addressing evolving cyber threats.\n","authors":["Md Ahnaf Akif","Ismail Butun","Andre Williams","Imadeldin Mahgoub"],"pdf_url":"https://arxiv.org/pdf/2502.12382v1.pdf","comment":"9 pages, 8 figures, 2 tables, journal submission"},{"id":"http://arxiv.org/abs/2502.09535v3","updated":"2025-02-17T22:41:20Z","published":"2025-02-13T17:50:58Z","title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based\n  Security","summary":"  Mobile sensor data has been proposed for security-critical applications such\nas device pairing, proximity detection, and continuous authentication. However,\nthe foundational assumption that these signals provide sufficient entropy\nremains under-explored. In this work, we systematically analyse the entropy of\nmobile sensor data across four diverse datasets spanning multiple application\ncontexts. Our findings reveal pervasive biases, with single-sensor mean\nmin-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite\nShannon entropy being several multiples higher. We further demonstrate that\ncorrelations between sensor modalities reduce the worst-case entropy of using\nmultiple sensors by up to approx. 75% compared to average-case Shannon entropy.\nThis brings joint min-entropy well below 10 bits in many cases and, in the best\ncase, yielding only approx. 24 bits of min-entropy when combining 20 sensor\nmodalities. These results call into question the widely held assumption that\nadding more sensors inherently yields higher security. We ultimately caution\nagainst relying on raw sensor data as a primary source of randomness.\n","authors":["Carlton Shepherd","Elliot Hurley"],"pdf_url":"https://arxiv.org/pdf/2502.09535v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13759v2","updated":"2025-02-17T22:16:30Z","published":"2024-10-17T16:54:41Z","title":"On the practicality of quantum sieving algorithms for the shortest\n  vector problem","summary":"  One of the main candidates of post-quantum cryptography is lattice-based\ncryptography. Its cryptographic security against quantum attackers is based on\nthe worst-case hardness of lattice problems like the shortest vector problem\n(SVP), which asks to find the shortest non-zero vector in an integer lattice.\nAsymptotic quantum speedups for solving SVP are known and rely on Grover's\nsearch. However, to assess the security of lattice-based cryptography against\nthese Grover-like quantum speedups, it is necessary to carry out a precise\nresource estimation beyond asymptotic scalings. In this work, we perform a\ncareful analysis on the resources required to implement several sieving\nalgorithms aided by Grover's search for dimensions of cryptographic interests.\nFor such, we take into account fixed-point quantum arithmetic operations,\nnon-asymptotic Grover's search, the cost of using quantum random access memory\n(QRAM), different physical architectures, and quantum error correction. We find\nthat even under very optimistic assumptions like circuit-level noise of\n$10^{-5}$, code cycles of 100 ns, reaction time of 1 $\\mu$s, and using\nstate-of-the-art arithmetic circuits and quantum error-correction protocols,\nthe best sieving algorithms require $\\approx 10^{13}$ physical qubits and\n$\\approx 10^{31}$ years to solve SVP on a lattice of dimension 400, which is\nroughly the dimension for minimally secure post-quantum cryptographic standards\ncurrently being proposed by NIST. We estimate that a 6-GHz-clock-rate\nsingle-core classical computer would take roughly the same amount of time to\nsolve the same problem. We conclude that there is currently little to no\nquantum speedup in the dimensions of cryptographic interest and the possibility\nof realising a considerable quantum speedup using quantum sieving algorithms\nwould require significant breakthroughs in theoretical protocols and hardware\ndevelopment.\n","authors":["Joao F. Doriguello","George Giapitzakis","Alessandro Luongo","Aditya Morolia"],"pdf_url":"https://arxiv.org/pdf/2410.13759v2.pdf","comment":"60+4 pages, 7+4 figures. v2: references added, extended the results\n  up to lattice dimension 1000"},{"id":"http://arxiv.org/abs/2502.12322v1","updated":"2025-02-17T20:54:56Z","published":"2025-02-17T20:54:56Z","title":"VIC: Evasive Video Game Cheating via Virtual Machine Introspection","summary":"  Video game cheats modify a video game behaviour to give unfair advantages to\nsome players while bypassing the methods game developers use to detect them.\nThis destroys the experience of online gaming and can result in financial\nlosses for game developers. In this work, we present a new type of game cheat,\nVirtual machine Introspection Cheat (VIC), that takes advantage of virtual\nmachines to stealthy execute game cheats. VIC employees a hypervisor with\nintrospection enabled to lower the bar of cheating against legacy and modern\nanti-cheat systems. We demonstrate the feasibility and stealthiness of VIC\nagainst three popular games (Fortnite, BlackSquad and Team Fortress 2) that\ninclude five different anti-cheats. In particular, we use VIC to implement a\ncheat radar, a wall-hack cheat and a trigger-bot. To support our claim that\nthis type of cheats can be effectively used, we present the performance impact\nVICs have on gameplay by monitoring the frames per second (fps) while the\ncheats are activated. Our experimentation also shows how these cheats are\ncurrently undetected by the most popular anti-cheat systems, enabling a new\nparadigm that can take advantage of cloud infrastructure to offer\ncheating-as-a-service.\n","authors":["Panicos Karkallis","Jorge Blasco Alis"],"pdf_url":"https://arxiv.org/pdf/2502.12322v1.pdf","comment":"17 pages, 8 figures,"},{"id":"http://arxiv.org/abs/2405.19272v5","updated":"2025-02-17T20:46:11Z","published":"2024-05-29T17:03:31Z","title":"Differentially Private Clustered Federated Learning","summary":"  Federated learning (FL), which is a decentralized machine learning (ML)\napproach, often incorporates differential privacy (DP) to provide rigorous data\nprivacy guarantees. Previous works attempted to address high structured data\nheterogeneity in vanilla FL settings through clustering clients (a.k.a\nclustered FL), but these methods remain sensitive and prone to errors, further\nexacerbated by the DP noise. This vulnerability makes the previous methods\ninappropriate for differentially private FL (DPFL) settings with structured\ndata heterogeneity. To address this gap, we propose an algorithm for\ndifferentially private clustered FL, which is robust to the DP noise in the\nsystem and identifies the underlying clients' clusters correctly. To this end,\nwe propose to cluster clients based on both their model updates and training\nloss values. Furthermore, for clustering clients' model updates at the end of\nthe first round, our proposed approach addresses the server's uncertainties by\nemploying large batch sizes as well as Gaussian Mixture Models (GMM) to reduce\nthe impact of DP and stochastic noise and avoid potential clustering errors.\nThis idea is efficient especially in privacy-sensitive scenarios with more DP\nnoise. We provide theoretical analysis to justify our approach and evaluate it\nacross diverse data distributions and privacy budgets. Our experimental results\nshow its effectiveness in addressing large structured data heterogeneity in\nDPFL.\n","authors":["Saber Malekmohammadi","Afaf Taik","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2405.19272v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04594v2","updated":"2025-02-17T19:37:58Z","published":"2024-11-07T10:25:20Z","title":"Verification of Neural Networks against Convolutional Perturbations via\n  Parameterised Kernels","summary":"  We develop a method for the efficient verification of neural networks against\nconvolutional perturbations such as blurring or sharpening. To define input\nperturbations we use well-known camera shake, box blur and sharpen kernels. We\ndemonstrate that these kernels can be linearly parameterised in a way that\nallows for a variation of the perturbation strength while preserving desired\nkernel properties. To facilitate their use in neural network verification, we\ndevelop an efficient way of convolving a given input with these parameterised\nkernels. The result of this convolution can be used to encode the perturbation\nin a verification setting by prepending a linear layer to a given network. This\nleads to tight bounds and a high effectiveness in the resulting verification\nstep. We add further precision by employing input splitting as a branch and\nbound strategy. We demonstrate that we are able to verify robustness on a\nnumber of standard benchmarks where the baseline is unable to provide any\nsafety certificates. To the best of our knowledge, this is the first solution\nfor verifying robustness against specific convolutional perturbations such as\ncamera shake.\n","authors":["Benedikt Brckner","Alessio Lomuscio"],"pdf_url":"https://arxiv.org/pdf/2411.04594v2.pdf","comment":"AAAI 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.12154v1","updated":"2025-02-17T18:59:50Z","published":"2025-02-17T18:59:50Z","title":"Diffusion Models without Classifier-free Guidance","summary":"  This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.\n","authors":["Zhicong Tang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2502.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12152v1","updated":"2025-02-17T18:59:06Z","published":"2025-02-17T18:59:06Z","title":"Learning Getting-Up Policies for Real-World Humanoid Robots","summary":"  Automatic fall recovery is a crucial prerequisite before humanoid robots can\nbe reliably deployed. Hand-designing controllers for getting up is difficult\nbecause of the varied configurations a humanoid can end up in after a fall and\nthe challenging terrains humanoid robots are expected to operate on. This paper\ndevelops a learning framework to produce controllers that enable humanoid\nrobots to get up from varying configurations on varying terrains. Unlike\nprevious successful applications of humanoid locomotion learning, the\ngetting-up task involves complex contact patterns, which necessitates\naccurately modeling the collision geometry and sparser rewards. We address\nthese challenges through a two-phase approach that follows a curriculum. The\nfirst stage focuses on discovering a good getting-up trajectory under minimal\nconstraints on smoothness or speed / torque limits. The second stage then\nrefines the discovered motions into deployable (i.e. smooth and slow) motions\nthat are robust to variations in initial configuration and terrains. We find\nthese innovations enable a real-world G1 humanoid robot to get up from two main\nsituations that we considered: a) lying face up and b) lying face down, both\ntested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass\nand snowfield). To the best of our knowledge, this is the first successful\ndemonstration of learned getting-up policies for human-sized humanoid robots in\nthe real world. Project page: https://humanoid-getup.github.io/\n","authors":["Xialin He","Runpei Dong","Zixuan Chen","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.12152v1.pdf","comment":"Project page: https://humanoid-getup.github.io/"},{"id":"http://arxiv.org/abs/2502.12147v1","updated":"2025-02-17T18:57:32Z","published":"2025-02-17T18:57:32Z","title":"Learning Smooth and Expressive Interatomic Potentials for Physical\n  Property Prediction","summary":"  Machine learning interatomic potentials (MLIPs) have become increasingly\neffective at approximating quantum mechanical calculations at a fraction of the\ncomputational cost. However, lower errors on held out test sets do not always\ntranslate to improved results on downstream physical property prediction tasks.\nIn this paper, we propose testing MLIPs on their practical ability to conserve\nenergy during molecular dynamic simulations. If passed, improved correlations\nare found between test errors and their performance on physical property\nprediction tasks. We identify choices which may lead to models failing this\ntest, and use these observations to improve upon highly-expressive models. The\nresulting model, eSEN, provides state-of-the-art results on a range of physical\nproperty prediction tasks, including materials stability prediction, thermal\nconductivity prediction, and phonon calculations.\n","authors":["Xiang Fu","Brandon M. Wood","Luis Barroso-Luque","Daniel S. Levine","Meng Gao","Misko Dzamba","C. Lawrence Zitnick"],"pdf_url":"https://arxiv.org/pdf/2502.12147v1.pdf","comment":"19 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.13697v2","updated":"2025-02-17T18:53:15Z","published":"2024-12-18T10:41:44Z","title":"Splitting criteria for ordinal decision trees: an experimental study","summary":"  Ordinal Classification (OC) is a machine learning field that addresses\nclassification tasks where the labels exhibit a natural order. Unlike nominal\nclassification, which treats all classes as equally distinct, OC takes the\nordinal relationship into account, producing more accurate and relevant\nresults. This is particularly critical in applications where the magnitude of\nclassification errors has implications. Despite this, OC problems are often\ntackled using nominal methods, leading to suboptimal solutions. Although\ndecision trees are one of the most popular classification approaches, ordinal\ntree-based approaches have received less attention when compared to other\nclassifiers. This work conducts an experimental study of tree-based\nmethodologies specifically designed to capture ordinal relationships. A\ncomprehensive survey of ordinal splitting criteria is provided, standardising\nthe notations used in the literature for clarity. Three ordinal splitting\ncriteria, Ordinal Gini (OGini), Weighted Information Gain (WIG), and Ranking\nImpurity (RI), are compared to the nominal counterparts of the first two (Gini\nand information gain), by incorporating them into a decision tree classifier.\nAn extensive repository considering 45 publicly available OC datasets is\npresented, supporting the first experimental comparison of ordinal and nominal\nsplitting criteria using well-known OC evaluation metrics. Statistical analysis\nof the results highlights OGini as the most effective ordinal splitting\ncriterion to date. Source code, datasets, and results are made available to the\nresearch community.\n","authors":["Rafael Aylln-Gaviln","Francisco Jos Martnez-Estudillo","David Guijo-Rubio","Csar Hervs-Martnez","Pedro Antonio Gutirrez"],"pdf_url":"https://arxiv.org/pdf/2412.13697v2.pdf","comment":"34 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.12128v1","updated":"2025-02-17T18:49:13Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. (Code is available at\nhttps://github.com/ml-jku/LaM-SLidE)\n","authors":["Florian Sestak","Artur Toshev","Andreas Frst","Gnter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v1.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2502.09606v2","updated":"2025-02-17T18:48:26Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12125v1","updated":"2025-02-17T18:47:01Z","published":"2025-02-17T18:47:01Z","title":"Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the\n  Lens of Class Hierarchy","summary":"  We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.\n","authors":["Roman Malashin","Valeria Yachnaya","Alexander Mullin"],"pdf_url":"https://arxiv.org/pdf/2502.12125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12123v1","updated":"2025-02-17T18:46:32Z","published":"2025-02-17T18:46:32Z","title":"On the Query Complexity of Verifier-Assisted Language Generation","summary":"  Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.\n","authors":["Edoardo Botta","Yuchen Li","Aashay Mehta","Jordan T. Ash","Cyril Zhang","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2502.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12122v1","updated":"2025-02-17T18:46:29Z","published":"2025-02-17T18:46:29Z","title":"Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA","summary":"  Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence.\n  In this work, we propose a novel parameter-efficient Bayesian LoRA,\ndemonstrating that effective uncertainty quantification can be achieved in very\nlow-dimensional parameter spaces. The proposed method achieves strong\nperformance with improved calibration and generalization while maintaining\ncomputational efficiency. Our empirical findings show that, with the\nappropriate projection of the weight space: (1) uncertainty can be effectively\nmodeled in a low-dimensional space, and (2) weight covariances exhibit low\nranks.\n","authors":["Patryk Marszaek","Klaudia Baazy","Jacek Tabor","Tomasz Kumierczyk"],"pdf_url":"https://arxiv.org/pdf/2502.12122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12120v1","updated":"2025-02-17T18:45:25Z","published":"2025-02-17T18:45:25Z","title":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws","summary":"  Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.\n","authors":["Prasanna Mayilvahanan","Thaddus Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2502.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12118v1","updated":"2025-02-17T18:43:24Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v2","updated":"2025-02-17T18:42:31Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v2.pdf","comment":"We will reflect comments from the reviewers and re-submit"},{"id":"http://arxiv.org/abs/2502.12115v1","updated":"2025-02-17T18:41:16Z","published":"2025-02-17T18:41:16Z","title":"SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\n  Software Engineering?","summary":"  We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\$1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https://github.com/openai/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development.\n","authors":["Samuel Miserendino","Michele Wang","Tejal Patwardhan","Johannes Heidecke"],"pdf_url":"https://arxiv.org/pdf/2502.12115v1.pdf","comment":"9 pages, 24 pages appendix"},{"id":"http://arxiv.org/abs/2406.11785v3","updated":"2025-02-17T18:37:13Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12108v1","updated":"2025-02-17T18:29:24Z","published":"2025-02-17T18:29:24Z","title":"Using the Path of Least Resistance to Explain Deep Networks","summary":"  Integrated Gradients (IG), a widely used axiomatic path-based attribution\nmethod, assigns importance scores to input features by integrating model\ngradients along a straight path from a baseline to the input. While effective\nin some cases, we show that straight paths can lead to flawed attributions. In\nthis paper, we identify the cause of these misattributions and propose an\nalternative approach that treats the input space as a Riemannian manifold,\ncomputing attributions by integrating gradients along geodesics. We call this\nmethod Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we\nintroduce two techniques: a k-Nearest Neighbours-based approach for smaller\nmodels and a Stochastic Variational Inference-based method for larger ones.\nAdditionally, we propose a new axiom, Strong Completeness, extending the axioms\nsatisfied by IG. We show that this property is desirable for attribution\nmethods and that GIG is the only method that satisfies it. Through experiments\non both synthetic and real-world data, we demonstrate that GIG outperforms\nexisting explainability methods, including IG.\n","authors":["Sina Salek","Joseph Enguehard"],"pdf_url":"https://arxiv.org/pdf/2502.12108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12089v1","updated":"2025-02-17T18:06:33Z","published":"2025-02-17T18:06:33Z","title":"How compositional generalization and creativity improve as diffusion\n  models are trained","summary":"  Natural data is often organized as a hierarchical composition of features.\nHow many samples do generative models need to learn the composition rules, so\nas to produce a combinatorial number of novel data? What signal in the data is\nexploited to learn? We investigate these questions both theoretically and\nempirically. Theoretically, we consider diffusion models trained on simple\nprobabilistic context-free grammars - tree-like graphical models used to\nrepresent the structure of data such as language and images. We demonstrate\nthat diffusion models learn compositional rules with the sample complexity\nrequired for clustering features with statistically similar context, a process\nsimilar to the word2vec algorithm. However, this clustering emerges\nhierarchically: higher-level, more abstract features associated with longer\ncontexts require more data to be identified. This mechanism leads to a sample\ncomplexity that scales polynomially with the said context size. As a result,\ndiffusion models trained on intermediate dataset size generate data coherent up\nto a certain scale, but that lacks global coherence. We test these predictions\nin different domains, and find remarkable agreement: both generated texts and\nimages achieve progressively larger coherence lengths as the training time or\ndataset size grows. We discuss connections between the hierarchical clustering\nmechanism we introduce here and the renormalization group in physics.\n","authors":["Alessandro Favero","Antonio Sclocchi","Francesco Cagnetta","Pascal Frossard","Matthieu Wyart"],"pdf_url":"https://arxiv.org/pdf/2502.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12088v1","updated":"2025-02-17T18:04:39Z","published":"2025-02-17T18:04:39Z","title":"Meta-Statistical Learning: Supervised Learning of Statistical Inference","summary":"  This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.\n","authors":["Maxime Peyrard","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.12088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12086v1","updated":"2025-02-17T18:01:07Z","published":"2025-02-17T18:01:07Z","title":"Unifying Explainable Anomaly Detection and Root Cause Analysis in\n  Dynamical Systems","summary":"  Dynamical systems, prevalent in various scientific and engineering domains,\nare susceptible to anomalies that can significantly impact their performance\nand reliability. This paper addresses the critical challenges of anomaly\ndetection, root cause localization, and anomaly type classification in\ndynamical systems governed by ordinary differential equations (ODEs). We define\ntwo categories of anomalies: cyber anomalies, which propagate through\ninterconnected variables, and measurement anomalies, which remain localized to\nindividual variables. To address these challenges, we propose the Interpretable\nCausality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic\nexplainable learning framework. ICODE leverages Neural ODEs for anomaly\ndetection while employing causality inference through an explanation channel to\nperform root cause analysis (RCA), elucidating why specific time periods are\nflagged as anomalous. ICODE is designed to simultaneously perform anomaly\ndetection, RCA, and anomaly type classification within a single, interpretable\nframework. Our approach is grounded in the hypothesis that anomalies alter the\nunderlying ODEs of the system, manifesting as changes in causal relationships\nbetween variables. We provide a theoretical analysis of how perturbations in\nlearned model parameters can be utilized to identify anomalies and their root\ncauses in time series data. Comprehensive experimental evaluations demonstrate\nthe efficacy of ICODE across various dynamical systems, showcasing its ability\nto accurately detect anomalies, classify their types, and pinpoint their\norigins.\n","authors":["Yue Sun","Rick S. Blum","Parv Venkitasubramaniam"],"pdf_url":"https://arxiv.org/pdf/2502.12086v1.pdf","comment":"Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS)"},{"id":"http://arxiv.org/abs/2502.12085v1","updated":"2025-02-17T17:59:56Z","published":"2025-02-17T17:59:56Z","title":"APB: Accelerating Distributed Long-Context Inference by Passing\n  Compressed Context Blocks across GPUs","summary":"  While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB.\n","authors":["Yuxiang Huang","Mingye Li","Xu Han","Chaojun Xiao","Weilin Zhao","Sun Ao","Hao Zhou","Jie Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12085v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12082v1","updated":"2025-02-17T17:56:23Z","published":"2025-02-17T17:56:23Z","title":"AdaSplash: Adaptive Sparse Flash Attention","summary":"  The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.\n","authors":["Nuno Gonalves","Marcos Treviso","Andr F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2502.12082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09115v2","updated":"2025-02-17T17:50:21Z","published":"2024-12-12T09:49:16Z","title":"Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations","summary":"  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n","authors":["Yudi Xie","Weichen Huang","Esther Alter","Jeremy Schwartz","Joshua B. Tenenbaum","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2412.09115v2.pdf","comment":"30 pages, 21 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12066v1","updated":"2025-02-17T17:35:42Z","published":"2025-02-17T17:35:42Z","title":"CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication\n  Facilities with Large Language Models","summary":"  Automating planning with LLMs presents transformative opportunities for\ntraditional industries, yet remains underexplored. In commercial construction,\nthe complexity of automated scheduling often requires manual intervention to\nensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to\noptimize construction schedules in complex projects like semiconductor\nfabrication. CONSTRUCTA addresses key challenges by: (1) integrating\nconstruction-specific knowledge through static RAG; (2) employing\ncontext-sampling techniques inspired by architectural expertise to provide\nrelevant input; and (3) deploying Construction DPO to align schedules with\nexpert preferences using RLHF. Experiments on proprietary data demonstrate\nperformance improvements of +42.3% in missing value prediction, +79.1% in\ndependency analysis, and +28.9% in automated planning compared to baseline\nmethods, showcasing its potential to revolutionize construction workflows and\ninspire domain-specific LLM advancements.\n","authors":["Yifan Zhang","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11427v2","updated":"2025-02-17T17:34:45Z","published":"2024-06-17T11:25:57Z","title":"DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors","summary":"  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n","authors":["Keon Lee","Dong Won Kim","Jaehyeon Kim","Seungjun Chung","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2406.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12063v1","updated":"2025-02-17T17:30:14Z","published":"2025-02-17T17:30:14Z","title":"Low-Rank Thinning","summary":"  The goal in thinning is to summarize a dataset using a small set of\nrepresentative points. Remarkably, sub-Gaussian thinning algorithms like Kernel\nHalving and Compress can match the quality of uniform subsampling while\nsubstantially reducing the number of summary points. However, existing\nguarantees cover only a restricted range of distributions and kernel-based\nquality measures and suffer from pessimistic dimension dependence. To address\nthese deficiencies, we introduce a new low-rank analysis of sub-Gaussian\nthinning that applies to any distribution and any kernel, guaranteeing\nhigh-quality compression whenever the kernel or data matrix is approximately\nlow-rank. To demonstrate the broad applicability of the techniques, we design\npractical sub-Gaussian thinning approaches that improve upon the best known\nguarantees for approximating attention in transformers, accelerating stochastic\ngradient training through reordering, and distinguishing distributions in\nnear-linear time.\n","authors":["Annabelle Michael Carrell","Albert Gong","Abhishek Shetty","Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2502.12063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12051v1","updated":"2025-02-17T17:20:41Z","published":"2025-02-17T17:20:41Z","title":"How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines","summary":"  Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.\n","authors":["Ayan Sengupta","Yash Goel","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.12051v1.pdf","comment":"20 pages, 8 tables, 4 figures"},{"id":"http://arxiv.org/abs/2502.12049v1","updated":"2025-02-17T17:16:42Z","published":"2025-02-17T17:16:42Z","title":"Classifying the Stoichiometry of Virus-like Particles with Interpretable\n  Machine Learning","summary":"  Virus-like particles (VLPs) are valuable for vaccine development due to their\nimmune-triggering properties. Understanding their stoichiometry, the number of\nprotein subunits to form a VLP, is critical for vaccine optimisation. However,\ncurrent experimental methods to determine stoichiometry are time-consuming and\nrequire highly purified proteins. To efficiently classify stoichiometry classes\nin proteins, we curate a new dataset and propose an interpretable, data-driven\npipeline leveraging linear machine learning models. We also explore the impact\nof feature encoding on model performance and interpretability, as well as\nmethods to identify key protein sequence features influencing classification.\nThe evaluation of our pipeline demonstrates that it can classify stoichiometry\nwhile revealing protein features that possibly influence VLP assembly. The data\nand code used in this work are publicly available at\nhttps://github.com/Shef-AIRE/StoicIML.\n","authors":["Jiayang Zhang","Xianyuan Liu","Wei Wu","Sina Tabakhi","Wenrui Fan","Shuo Zhou","Kang Lan Tee","Tuck Seng Wong","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2502.12049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12048v1","updated":"2025-02-17T17:16:41Z","published":"2025-02-17T17:16:41Z","title":"A Survey on Bridging EEG Signals and Generative AI: From Image and Text\n  to Beyond","summary":"  Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.\n","authors":["Shreya Shukla","Jose Torres","Abhijit Mishra","Jacek Gwizdka","Shounak Roychowdhury"],"pdf_url":"https://arxiv.org/pdf/2502.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05171v2","updated":"2025-02-17T17:14:04Z","published":"2025-02-07T18:55:02Z","title":"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach","summary":"  We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.\n","authors":["Jonas Geiping","Sean McLeish","Neel Jain","John Kirchenbauer","Siddharth Singh","Brian R. Bartoldson","Bhavya Kailkhura","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.05171v2.pdf","comment":"The model is available at\n  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can\n  be found at https://github.com/seal-rg/recurrent-pretraining"},{"id":"http://arxiv.org/abs/2410.15777v2","updated":"2025-02-17T17:11:46Z","published":"2024-10-21T08:42:10Z","title":"Revisiting the Equivalence of Bayesian Neural Networks and Gaussian\n  Processes: On the Importance of Learning Activations","summary":"  Gaussian Processes (GPs) provide a convenient framework for specifying\nfunction-space priors, making them a natural choice for modeling uncertainty.\nIn contrast, Bayesian Neural Networks (BNNs) offer greater scalability and\nextendability but lack the advantageous properties of GPs. This motivates the\ndevelopment of BNNs capable of replicating GP-like behavior. However, existing\nsolutions are either limited to specific GP kernels or rely on heuristics.\n  We demonstrate that trainable activations are crucial for effective mapping\nof GP priors to wide BNNs. Specifically, we leverage the closed-form\n2-Wasserstein distance for efficient gradient-based optimization of\nreparameterized priors and activations. Beyond learned activations, we also\nintroduce trainable periodic activations that ensure global stationarity by\ndesign, and functional priors conditioned on GP hyperparameters to allow\nefficient model selection.\n  Empirically, our method consistently outperforms existing approaches or\nmatches performance of the heuristic methods, while offering stronger\ntheoretical foundations.\n","authors":["Marcin Sendera","Amin Sorkhei","Tomasz Kumierczyk"],"pdf_url":"https://arxiv.org/pdf/2410.15777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12033v1","updated":"2025-02-17T17:03:12Z","published":"2025-02-17T17:03:12Z","title":"The geometry of BERT","summary":"  Transformer neural networks, particularly Bidirectional Encoder\nRepresentations from Transformers (BERT), have shown remarkable performance\nacross various tasks such as classification, text summarization, and question\nanswering. However, their internal mechanisms remain mathematically obscure,\nhighlighting the need for greater explainability and interpretability. In this\ndirection, this paper investigates the internal mechanisms of BERT proposing a\nnovel perspective on the attention mechanism of BERT from a theoretical\nperspective. The analysis encompasses both local and global network behavior.\nAt the local level, the concept of directionality of subspace selection as well\nas a comprehensive study of the patterns emerging from the self-attention\nmatrix are presented. Additionally, this work explores the semantic content of\nthe information stream through data distribution analysis and global\nstatistical measures including the novel concept of cone index. A case study on\nthe classification of SARS-CoV-2 variants using RNA which resulted in a very\nhigh accuracy has been selected in order to observe these concepts in an\napplication. The insights gained from this analysis contribute to a deeper\nunderstanding of BERT's classification process, offering potential avenues for\nfuture architectural improvements in Transformer models and further analysis in\nthe training process.\n","authors":["Matteo Bonino","Giorgia Ghione","Giansalvo Cirrincione"],"pdf_url":"https://arxiv.org/pdf/2502.12033v1.pdf","comment":"28 pages, 13 figures"},{"id":"http://arxiv.org/abs/2501.18592v3","updated":"2025-02-17T16:54:39Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v3.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2412.15218v2","updated":"2025-02-17T16:54:24Z","published":"2024-12-03T16:53:34Z","title":"Investigating the importance of social vulnerability in opioid-related\n  mortality across the United States","summary":"  The opioid crisis remains a critical public health challenge in the United\nStates. Despite national efforts to reduce opioid prescribing rates by nearly\n45\\% between 2011 and 2021, opioid overdose deaths more than tripled during\nthis same period. This alarming trend reflects a major shift in the crisis,\nwith illegal opioids now driving the majority of overdose deaths instead of\nprescription opioids. Although much attention has been given to supply-side\nfactors fueling this transition, the underlying socioeconomic conditions that\nperpetuate and exacerbate opioid misuse remain less understood. Moreover, the\nCOVID-19 pandemic intensified the opioid crisis through widespread social\nisolation and record-high unemployment; consequently, understanding the\nsocioeconomic drivers of this epidemic has become even more crucial in recent\nyears. To address this need, our study examines the correlation between\nopioid-related mortality and thirteen components of the Social Vulnerability\nIndex (SVI). Leveraging a nationwide county-level dataset spanning consecutive\nyears from 2010 to 2022, this study integrates empirical insights from\nexploratory data analysis with feature importance metrics derived from machine\nlearning models. Our findings highlight critical social factors strongly\ncorrelated with opioid-related mortality, emphasizing their potential roles in\nworsening the epidemic when their levels are high and mitigating it when their\nlevels are low.\n","authors":["Andrew Deas","Adam Spannaus","Dakotah D. Maguire","Jodie Trafton","Anuj J. Kapadia","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2412.15218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12018v1","updated":"2025-02-17T16:52:42Z","published":"2025-02-17T16:52:42Z","title":"Atom of Thoughts for Markov LLM Test-Time Scaling","summary":"  Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.\n","authors":["Fengwei Teng","Zhaoyang Yu","Quan Shi","Jiayi Zhang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12013v1","updated":"2025-02-17T16:48:16Z","published":"2025-02-17T16:48:16Z","title":"Unsupervised Structural-Counterfactual Generation under Domain Shift","summary":"  Motivated by the burgeoning interest in cross-domain learning, we present a\nnovel generative modeling challenge: generating counterfactual samples in a\ntarget domain based on factual observations from a source domain. Our approach\noperates within an unsupervised paradigm devoid of parallel or joint datasets,\nrelying exclusively on distinct observational samples and causal graphs for\neach domain. This setting presents challenges that surpass those of\nconventional counterfactual generation. Central to our methodology is the\ndisambiguation of exogenous causes into effect-intrinsic and domain-intrinsic\ncategories. This differentiation facilitates the integration of domain-specific\ncausal graphs into a unified joint causal graph via shared effect-intrinsic\nexogenous variables. We propose leveraging Neural Causal models within this\njoint framework to enable accurate counterfactual generation under standard\nidentifiability assumptions. Furthermore, we introduce a novel loss function\nthat effectively segregates effect-intrinsic from domain-intrinsic variables\nduring model training. Given a factual observation, our framework combines the\nposterior distribution of effect-intrinsic variables from the source domain\nwith the prior distribution of domain-intrinsic variables from the target\ndomain to synthesize the desired counterfactuals, adhering to Pearl's causal\nhierarchy. Intriguingly, when domain shifts are restricted to alterations in\ncausal mechanisms without accompanying covariate shifts, our training regimen\nparallels the resolution of a conditional optimal transport problem. Empirical\nevaluations on a synthetic dataset show that our framework generates\ncounterfactuals in the target domain that very closely resemble the ground\ntruth.\n","authors":["Krishn Vishwas Kher","Lokesh Venkata Siva Maruthi Badisa","Kusampudi Venkata Datta Sri Harsha","Chitneedi Geetha Sowmya","SakethaNath Jagarlapudi"],"pdf_url":"https://arxiv.org/pdf/2502.12013v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.12011v1","updated":"2025-02-17T16:46:15Z","published":"2025-02-17T16:46:15Z","title":"Reconfigurable Intelligent Surfaces-Assisted Integrated Access and\n  Backhaul","summary":"  In this paper, we study the impact of reconfigurable intelligent surfaces\n(RISs) on the coverage extension of integrated access and backhaul (IAB)\nnetworks. Particularly, using a finite stochastic geometry model, with random\ndistributions of user equipments (UEs) in a finite region, and planned\nhierachical architecture for IAB, we study the service coverage probability\ndefined as the probability of the event that the UEs' minimum rate requirements\nare satisfied. We present comparisons between different cases including\nIAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network\ncontrolled repeaters (NCRs). Our investigations focus on wide-area IAB assisted\nwith RIS through the lens of different design architectures and deployments,\nrevealing both conflicts and synergies for minimizing the effect of tree\nfoliage over seasonal changes. Our simulation results reveal both opportunities\nand challenges towards the implementation of RIS in IAB.\n","authors":["Charitha Madapatha","Behrooz Makki","Hao Guo","Tommy Svensson"],"pdf_url":"https://arxiv.org/pdf/2502.12011v1.pdf","comment":"Submitted to 2025 European Conference on Networks and Communications\n  (EuCNC) & 6G Summit, 2025, Poznan, Poland"},{"id":"http://arxiv.org/abs/2502.12001v1","updated":"2025-02-17T16:39:28Z","published":"2025-02-17T16:39:28Z","title":"Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition","summary":"  This paper investigates the integration of technical vocabulary in merged\nlanguage models. We explore the knowledge transfer mechanisms involved when\ncombining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.\n","authors":["Thibault Rousset","Taisei Kakibuchi","Yusuke Sasaki","Yoshihide Nomura"],"pdf_url":"https://arxiv.org/pdf/2502.12001v1.pdf","comment":"Presented at the 263rd IPSJ-NL Workshop"},{"id":"http://arxiv.org/abs/2407.02025v4","updated":"2025-02-17T16:36:37Z","published":"2024-07-02T07:48:22Z","title":"On the Expressive Power of Sparse Geometric MPNNs","summary":"  Motivated by applications in chemistry and other sciences, we study the\nexpressive power of message-passing neural networks for geometric graphs, whose\nnode features correspond to 3-dimensional positions. Recent work has shown that\nsuch models can separate generic pairs of non-isomorphic geometric graphs,\nthough they may fail to separate some rare and complicated instances. However,\nthese results assume a fully connected graph, where each node possesses\ncomplete knowledge of all other nodes. In contrast, often, in application,\nevery node only possesses knowledge of a small number of nearest neighbors.\n  This paper shows that generic pairs of non-isomorphic geometric graphs can be\nseparated by message-passing networks with rotation equivariant features as\nlong as the underlying graph is connected. When only invariant intermediate\nfeatures are allowed, generic separation is guaranteed for generically globally\nrigid graphs. We introduce a simple architecture, EGENNET, which achieves our\ntheoretical guarantees and compares favorably with alternative architecture on\nsynthetic and chemical benchmarks. Our code is available at\nhttps://github.com/yonatansverdlov/E-GenNet.\n","authors":["Yonatan Sverdlov","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2407.02025v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06665v2","updated":"2025-02-17T16:34:29Z","published":"2024-10-09T08:19:31Z","title":"Revisiting Multi-Permutation Equivariance through the Lens of\n  Irreducible Representations","summary":"  This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.\n","authors":["Yonatan Sverdlov","Ido Springer","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2410.06665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v2","updated":"2025-02-17T16:26:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tr"],"pdf_url":"https://arxiv.org/pdf/2502.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21745v4","updated":"2025-02-17T16:26:20Z","published":"2024-10-29T05:18:34Z","title":"RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in graph clustering, which has proven effective in many\napplications. Nonetheless, these methods often encounter difficulties when\ndealing with real-world graphs, particularly in the presence of noisy edges.\nAdditionally, many denoising graph clustering methods tend to suffer from lower\nperformance, training instability, and challenges in scaling to large datasets\ncompared to non-denoised models. To tackle these issues, we introduce a new\nframework called the Robust Deep Graph Clustering Framework via Dual Soft\nAssignment (RDSA). RDSA consists of three key components: (i) a node embedding\nmodule that effectively integrates the graph's topological features and node\nattributes; (ii) a structure-based soft assignment module that improves graph\nmodularity by utilizing an affinity matrix for node assignments; and (iii) a\nnode-based soft assignment module that identifies community landmarks and\nrefines node assignments to enhance the model's robustness. We assess RDSA on\nvarious real-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Xiaoying Pang","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v4.pdf","comment":"Accepted by DASFAA 2025; Complete version"},{"id":"http://arxiv.org/abs/2502.11986v1","updated":"2025-02-17T16:26:05Z","published":"2025-02-17T16:26:05Z","title":"Selective Task Group Updates for Multi-Task Optimization","summary":"  Multi-task learning enables the acquisition of task-generic knowledge by\ntraining multiple tasks within a unified architecture. However, training all\ntasks together in a single architecture can lead to performance degradation,\nknown as negative transfer, which is a main concern in multi-task learning.\nPrevious works have addressed this issue by optimizing the multi-task network\nthrough gradient manipulation or weighted loss adjustments. However, their\noptimization strategy focuses on addressing task imbalance in shared\nparameters, neglecting the learning of task-specific parameters. As a result,\nthey show limitations in mitigating negative transfer, since the learning of\nshared space and task-specific information influences each other during\noptimization. To address this, we propose a different approach to enhance\nmulti-task performance by selectively grouping tasks and updating them for each\nbatch during optimization. We introduce an algorithm that adaptively determines\nhow to effectively group tasks and update them during the learning process. To\ntrack inter-task relations and optimize multi-task networks simultaneously, we\npropose proximal inter-task affinity, which can be measured during the\noptimization process. We provide a theoretical analysis on how dividing tasks\ninto multiple groups and updating them sequentially significantly affects\nmulti-task performance by enhancing the learning of task-specific parameters.\nOur methods substantially outperform previous multi-task optimization\napproaches and are scalable to different architectures and various numbers of\ntasks.\n","authors":["Wooseong Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2502.11986v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11981v1","updated":"2025-02-17T16:22:46Z","published":"2025-02-17T16:22:46Z","title":"Machine Learning Should Maximize Welfare, Not (Only) Accuracy","summary":"  Decades of research in machine learning have given us powerful tools for\nmaking accurate predictions. But when used in social settings and on human\ninputs, better accuracy does not immediately translate to better social\noutcomes. This may not be surprising given that conventional learning\nframeworks are not designed to express societal preferences -- let alone\npromote them. This position paper argues that machine learning is currently\nmissing, and can gain much from incorporating, a proper notion of social\nwelfare. The field of welfare economics asks: how should we allocate limited\nresources to self-interested agents in a way that maximizes social benefit? We\nargue that this perspective applies to many modern applications of machine\nlearning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail\napplications and use-cases for which our framework can be effective, identify\ntechnical challenges and practical opportunities, and highlight future avenues\nworth pursuing.\n","authors":["Nir Rosenfeld","Haifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11969v1","updated":"2025-02-17T16:18:07Z","published":"2025-02-17T16:18:07Z","title":"Learning Generalizable Prompt for CLIP with Class Similarity Knowledge","summary":"  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n","authors":["Sehun Jung","Hyang-won Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11968v1","updated":"2025-02-17T16:18:00Z","published":"2025-02-17T16:18:00Z","title":"Theoretical Barriers in Bellman-Based Reinforcement Learning","summary":"  Reinforcement Learning algorithms designed for high-dimensional spaces often\nenforce the Bellman equation on a sampled subset of states, relying on\ngeneralization to propagate knowledge across the state space. In this paper, we\nidentify and formalize a fundamental limitation of this common approach.\nSpecifically, we construct counterexample problems with a simple structure that\nthis approach fails to exploit. Our findings reveal that such algorithms can\nneglect critical information about the problems, leading to inefficiencies.\nFurthermore, we extend this negative result to another approach from the\nliterature: Hindsight Experience Replay learning state-to-state reachability.\n","authors":["Brieuc Pinon","Raphal Jungers","Jean-Charles Delvenne"],"pdf_url":"https://arxiv.org/pdf/2502.11968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03540v3","updated":"2025-02-17T16:07:09Z","published":"2025-02-05T19:00:52Z","title":"Path Planning for Masked Diffusion Model Sampling","summary":"  In this paper, we explore how token unmasking order influences generative\nquality in masked diffusion models (MDMs). We derive an expanded evidence lower\nbound (ELBO) that introduces a planner to select which tokens to unmask at each\nstep. Our analysis reveals that alternative unmasking strategies can enhance\ngeneration performance. Building on this, we propose Path Planning (P2), a\nsampling framework that uses a pre-trained BERT model or the denoiser itself to\nguide unmasking decisions. P2 generalizes all known MDM sampling strategies and\nsignificantly improves performance across diverse domains, including language\ngeneration (in-context learning, code generation, story infilling, mathematical\nreasoning, reverse curse correction) and biological sequence generation\n(protein and RNA sequences).\n","authors":["Fred Zhangzhi Peng","Zachary Bezemek","Sawan Patel","Jarrid Rector-Brooks","Sherwood Yao","Alexander Tong","Pranam Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2502.03540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11953v1","updated":"2025-02-17T16:05:14Z","published":"2025-02-17T16:05:14Z","title":"Refined PAC-Bayes Bounds for Offline Bandits","summary":"  In this paper, we present refined probabilistic bounds on empirical reward\nestimates for off-policy learning in bandit problems. We build on the\nPAC-Bayesian bounds from Seldin et al. (2010) and improve on their results\nusing a new parameter optimization approach introduced by Rodr\\'iguez et al.\n(2024). This technique is based on a discretization of the space of possible\nevents to optimize the \"in probability\" parameter. We provide two\nparameter-free PAC-Bayes bounds, one based on Hoeffding-Azuma's inequality and\nthe other based on Bernstein's inequality. We prove that our bounds are almost\noptimal as they recover the same rate as would be obtained by setting the \"in\nprobability\" parameter after the realization of the data.\n","authors":["Amaury Gouverneur","Tobias J. Oechtering","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2502.11953v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.11951v1","updated":"2025-02-17T16:04:04Z","published":"2025-02-17T16:04:04Z","title":"Qubit-Based Framework for Quantum Machine Learning: Bridging Classical\n  Data and Quantum Algorithms","summary":"  This paper dives into the exciting and rapidly growing field of quantum\ncomputing, explaining its core ideas, current progress, and how it could\nrevolutionize the way we solve complex problems. It starts by breaking down the\nbasics, like qubits, quantum circuits, and how principles like superposition\nand entanglement make quantum computers fundamentally different-and far more\npowerful for certain tasks-than the classical computers we use today. We also\nexplore how quantum computing deals with complex problems and why it is\nuniquely suited for challenges classical systems struggle to handle. A big part\nof this paper focuses on Quantum Machine Learning (QML), where the strengths of\nquantum computing meet the world of artificial intelligence. By processing\nmassive datasets and optimizing intricate algorithms, quantum systems offer new\npossibilities for machine learning. We highlight different approaches to\ncombining quantum and classical computing, showing how they can work together\nto produce faster and more accurate results. Additionally, we explore the tools\nand platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are\nhelping researchers and developers bring these theories to life. Of course,\nquantum computing has its hurdles. Challenges like scaling up hardware,\ncorrecting errors, and keeping qubits stable are significant roadblocks. Yet,\nwith rapid advancements in cloud-based platforms and innovative technologies,\nthe potential of quantum computing feels closer than ever. This paper aims to\noffer readers a clear and comprehensive introduction to quantum computing, its\nrole in machine learning, and the immense possibilities it holds for the future\nof technology.\n","authors":["Bhavna Bose","Saurav Verma"],"pdf_url":"https://arxiv.org/pdf/2502.11951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11949v1","updated":"2025-02-17T16:02:54Z","published":"2025-02-17T16:02:54Z","title":"Massively Scaling Explicit Policy-conditioned Value Functions","summary":"  We introduce a scaling strategy for Explicit Policy-Conditioned Value\nFunctions (EPVFs) that significantly improves performance on challenging\ncontinuous-control tasks. EPVFs learn a value function V({\\theta}) that is\nexplicitly conditioned on the policy parameters, enabling direct gradient-based\nupdates to the parameters of any policy. However, EPVFs at scale struggle with\nunrestricted parameter growth and efficient exploration in the policy parameter\nspace. To address these issues, we utilize massive parallelization with\nGPU-based simulators, big batch sizes, weight clipping and scaled peturbations.\nOur results show that EPVFs can be scaled to solve complex tasks, such as a\ncustom Ant environment, and can compete with state-of-the-art Deep\nReinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)\nand Soft Actor-Critic (SAC). We further explore action-based policy parameter\nrepresentations from previous work and specialized neural network architectures\nto efficiently handle weight-space features, which have not been used in the\ncontext of DRL before.\n","authors":["Nico Bohlinger","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.11949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11942v1","updated":"2025-02-17T15:56:07Z","published":"2025-02-17T15:56:07Z","title":"Sharp-PINNs: staggered hard-constrained physics-informed neural networks\n  for phase field modelling of corrosion","summary":"  Physics-informed neural networks have shown significant potential in solving\npartial differential equations (PDEs) across diverse scientific fields.\nHowever, their performance often deteriorates when addressing PDEs with\nintricate and strongly coupled solutions. In this work, we present a novel\nSharp-PINN framework to tackle complex phase field corrosion problems. Instead\nof minimizing all governing PDE residuals simultaneously, the Sharp-PINNs\nintroduce a staggered training scheme that alternately minimizes the residuals\nof Allen-Cahn and Cahn-Hilliard equations, which govern the corrosion system.\nTo further enhance its efficiency and accuracy, we design an advanced neural\nnetwork architecture that integrates random Fourier features as coordinate\nembeddings, employs a modified multi-layer perceptron as the primary backbone,\nand enforces hard constraints in the output layer. This framework is\nbenchmarked through simulations of corrosion problems with multiple pits, where\nthe staggered training scheme and network architecture significantly improve\nboth the efficiency and accuracy of PINNs. Moreover, in three-dimensional\ncases, our approach is 5-10 times faster than traditional finite element\nmethods while maintaining competitive accuracy, demonstrating its potential for\nreal-world engineering applications in corrosion prediction.\n","authors":["Nanxi Chen","Chuanjie Cui","Rujin Ma","Airong Chen","Sifan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05816v4","updated":"2025-02-17T15:55:26Z","published":"2024-06-09T15:08:00Z","title":"Attention as a Hypernetwork","summary":"  Transformers can under some circumstances generalize to novel problem\ninstances whose constituent parts might have been encountered during training,\nbut whose compositions have not. What mechanisms underlie this ability for\ncompositional generalization? By reformulating multi-head attention as a\nhypernetwork, we reveal that a composable, low-dimensional latent code\nspecifies key-query specific operations. We find empirically that this latent\ncode is predictive of the subtasks the network performs on unseen task\ncompositions, revealing that latent codes acquired during training are reused\nto solve unseen problem instances. To further examine the hypothesis that the\nintrinsic hypernetwork of multi-head attention supports compositional\ngeneralization, we ablate whether making the hypernetwork-generated linear\nvalue network nonlinear strengthens compositionality. We find that this\nmodification improves compositional generalization on abstract reasoning tasks.\nIn particular, we introduce a symbolic version of the Raven's Progressive\nMatrices human intelligence test, which gives us precise control over the\nproblem compositions encountered during training and evaluation. We demonstrate\non this task how scaling model size and data enables compositional\ngeneralization in transformers and gives rise to a functionally structured\nlatent space.\n","authors":["Simon Schug","Seijin Kobayashi","Yassir Akram","Joo Sacramento","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2406.05816v4.pdf","comment":"ICLR 2025 (Oral); Code available at\n  https://github.com/smonsays/hypernetwork-attention"},{"id":"http://arxiv.org/abs/2412.18547v4","updated":"2025-02-17T15:55:08Z","published":"2024-12-24T16:55:45Z","title":"Token-Budget-Aware LLM Reasoning","summary":"  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n","authors":["Tingxu Han","Zhenting Wang","Chunrong Fang","Shiyu Zhao","Shiqing Ma","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18547v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11941v1","updated":"2025-02-17T15:52:22Z","published":"2025-02-17T15:52:22Z","title":"Deep Spatio-Temporal Neural Network for Air Quality Reanalysis","summary":"  Air quality prediction is key to mitigating health impacts and guiding\ndecisions, yet existing models tend to focus on temporal trends while\noverlooking spatial generalization. We propose AQ-Net, a spatiotemporal\nreanalysis model for both observed and unobserved stations in the near future.\nAQ-Net utilizes the LSTM and multi-head attention for the temporal regression.\nWe also propose a cyclic encoding technique to ensure continuous time\nrepresentation. To learn fine-grained spatial air quality estimation, we\nincorporate AQ-Net with the neural kNN to explore feature-based interpolation,\nsuch that we can fill the spatial gaps given coarse observation stations. To\ndemonstrate the efficiency of our model for spatiotemporal reanalysis, we use\ndata from 2013-2017 collected in northern China for PM2.5 analysis. Extensive\nexperiments show that AQ-Net excels in air quality reanalysis, highlighting the\npotential of hybrid spatio-temporal models to better capture environmental\ndynamics, especially in urban areas where both spatial and temporal variability\nare critical.\n","authors":["Ammar Kheder","Benjamin Foreback","Lili Wang","Zhi-Song Liu","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2502.11941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11937v1","updated":"2025-02-17T15:48:46Z","published":"2025-02-17T15:48:46Z","title":"FitLight: Federated Imitation Learning for Plug-and-Play Autonomous\n  Traffic Signal Control","summary":"  Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)\nmethods have been extensively studied, their practical applications still raise\nsome serious issues such as high learning cost and poor generalizability. This\nis because the ``trial-and-error'' training style makes RL agents extremely\ndependent on the specific traffic environment, which also requires a long\nconvergence time. To address these issues, we propose a novel Federated\nImitation Learning (FIL)-based framework for multi-intersection TSC, named\nFitLight, which allows RL agents to plug-and-play for any traffic environment\nwithout additional pre-training cost. Unlike existing imitation learning\napproaches that rely on pre-training RL agents with demonstrations, FitLight\nallows real-time imitation learning and seamless transition to reinforcement\nlearning. Due to our proposed knowledge-sharing mechanism and novel hybrid\npressure-based agent design, RL agents can quickly find a best control policy\nwith only a few episodes. Moreover, for resource-constrained TSC scenarios,\nFitLight supports model pruning and heterogeneous model aggregation, such that\nRL agents can work on a micro-controller with merely 16{\\it KB} RAM and 32{\\it\nKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art\nmethods, FitLight not only provides a superior starting point but also\nconverges to a better final solution on both real-world and synthetic datasets,\neven under extreme resource limitations.\n","authors":["Yutong Ye","Yingbo Zhou","Zhusen Liu","Xiao Du","Hao Zhou","Xiang Lian","Mingsong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06079v2","updated":"2025-02-17T15:44:24Z","published":"2025-02-10T00:27:54Z","title":"Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo","summary":"  Discrete diffusion models are a class of generative models that produce\nsamples from an approximated data distribution within a discrete state space.\nOften, there is a need to target specific regions of the data distribution.\nCurrent guidance methods aim to sample from a distribution with mass\nproportional to $p_0(x_0) p(\\zeta|x_0)^\\alpha$ but fail to achieve this in\npractice. We introduce a Sequential Monte Carlo algorithm that generates\nunbiasedly from this target distribution, utilising the learnt unconditional\nand guided process. We validate our approach on low-dimensional distributions,\ncontrolled images and text generations. For text generation, our method\nprovides strong control while maintaining low perplexity compared to\nguidance-based approaches.\n","authors":["Cheuk Kit Lee","Paul Jeha","Jes Frellsen","Pietro Lio","Michael Samuel Albergo","Francisco Vargas"],"pdf_url":"https://arxiv.org/pdf/2502.06079v2.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.11927v1","updated":"2025-02-17T15:40:13Z","published":"2025-02-17T15:40:13Z","title":"Continual Learning Should Move Beyond Incremental Classification","summary":"  Continual learning (CL) is the sub-field of machine learning concerned with\naccumulating knowledge in dynamic environments. So far, CL research has mainly\nfocused on incremental classification tasks, where models learn to classify new\ncategories while retaining knowledge of previously learned ones. Here, we argue\nthat maintaining such a focus limits both theoretical development and practical\napplicability of CL methods. Through a detailed analysis of concrete examples -\nincluding multi-target classification, robotics with constrained output spaces,\nlearning in continuous task domains, and higher-level concept memorization - we\ndemonstrate how current CL approaches often fail when applied beyond standard\nclassification. We identify three fundamental challenges: (C1) the nature of\ncontinuity in learning problems, (C2) the choice of appropriate spaces and\nmetrics for measuring similarity, and (C3) the role of learning objectives\nbeyond classification. For each challenge, we provide specific recommendations\nto help move the field forward, including formalizing temporal dynamics through\ndistribution processes, developing principled approaches for continuous task\nspaces, and incorporating density estimation and generative objectives. In so\ndoing, this position paper aims to broaden the scope of CL research while\nstrengthening its theoretical foundations, making it more applicable to\nreal-world problems.\n","authors":["Rupert Mitchell","Antonio Alliegro","Raffaello Camoriano","Dustin Carrin-Ojeda","Antonio Carta","Georgia Chalvatzaki","Nikhil Churamani","Carlo D'Eramo","Samin Hamidi","Robin Hesse","Fabian Hinder","Roshni Ramanna Kamath","Vincenzo Lomonaco","Subarnaduti Paul","Francesca Pistilli","Tinne Tuytelaars","Gido M van de Ven","Kristian Kersting","Simone Schaub-Meyer","Martin Mundt"],"pdf_url":"https://arxiv.org/pdf/2502.11927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17884v4","updated":"2025-02-17T15:37:58Z","published":"2024-04-27T12:43:02Z","title":"Generalization capabilities and robustness of hybrid models grounded in\n  physics compared to purely deep learning models","summary":"  This study investigates the generalization capabilities and robustness of\npurely deep learning (DL) models and hybrid models based on physical principles\nin fluid dynamics applications, specifically focusing on iteratively\nforecasting the temporal evolution of flow dynamics. Three autoregressive\nmodels were compared: a hybrid model (POD-DL) that combines proper orthogonal\ndecomposition (POD) with a long-short term memory (LSTM) layer, a convolutional\nautoencoder combined with a convolutional LSTM (ConvLSTM) layer and a\nvariational autoencoder (VAE) combined with a ConvLSTM layer. These models were\ntested on two high-dimensional, nonlinear datasets representing the velocity\nfield of flow past a circular cylinder in both laminar and turbulent regimes.\nThe study used latent dimension methods, enabling a bijective reduction of\nhigh-dimensional dynamics into a lower-order space to facilitate future\npredictions. While the VAE and ConvLSTM models accurately predicted laminar\nflow, the hybrid POD-DL model outperformed the others across both laminar and\nturbulent flow regimes. This success is attributed to the model's ability to\nincorporate modal decomposition, reducing the dimensionality of the data, by a\nnon-parametric method, and simplifying the forecasting component. By leveraging\nPOD, the model not only gained insight into the underlying physics, improving\nprediction accuracy with less training data, but also reduce the number of\ntrainable parameters as POD is non-parametric. The findings emphasize the\npotential of hybrid models, particularly those integrating modal decomposition\nand deep learning, in predicting complex flow dynamics.\n","authors":["Rodrigo Abada-Heredia","Adrin Corrochano","Manuel Lopez-Martin","Soledad Le Clainche"],"pdf_url":"https://arxiv.org/pdf/2404.17884v4.pdf","comment":"24 pages, two column, 26 figures and 11 tables"},{"id":"http://arxiv.org/abs/2502.11925v1","updated":"2025-02-17T15:35:36Z","published":"2025-02-17T15:35:36Z","title":"GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.\n","authors":["Yi Fang","Bowen Jin","Jiacheng Shen","Sirui Ding","Qiaoyu Tan","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2502.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11918v1","updated":"2025-02-17T15:32:14Z","published":"2025-02-17T15:32:14Z","title":"VLP: Vision-Language Preference Learning for Embodied Manipulation","summary":"  Reward engineering is one of the key challenges in Reinforcement Learning\n(RL). Preference-based RL effectively addresses this issue by learning from\nhuman feedback. However, it is both time-consuming and expensive to collect\nhuman preference labels. In this paper, we propose a novel\n\\textbf{V}ision-\\textbf{L}anguage \\textbf{P}reference learning framework, named\n\\textbf{VLP}, which learns a vision-language preference model to provide\npreference feedback for embodied manipulation tasks. To achieve this, we define\nthree types of language-conditioned preferences and construct a vision-language\npreference dataset, which contains versatile implicit preference orders without\nhuman annotations. The preference model learns to extract language-related\nfeatures, and then serves as a preference annotator in various downstream\ntasks. The policy can be learned according to the annotated preferences via\nreward learning or direct policy optimization. Extensive empirical results on\nsimulated embodied manipulation tasks demonstrate that our method provides\naccurate preferences and generalizes to unseen tasks and unseen language\ninstructions, outperforming the baselines by a large margin.\n","authors":["Runze Liu","Chenjia Bai","Jiafei Lyu","Shengjie Sun","Yali Du","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2502.11918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11913v1","updated":"2025-02-17T15:30:17Z","published":"2025-02-17T15:30:17Z","title":"PreAdaptFWI: Pretrained-Based Adaptive Residual Learning for\n  Full-Waveform Inversion Without Dataset Dependency","summary":"  Full-waveform inversion (FWI) is a method that utilizes seismic data to\ninvert the physical parameters of subsurface media by minimizing the difference\nbetween simulated and observed waveforms. Due to its ill-posed nature, FWI is\nsusceptible to getting trapped in local minima. Consequently, various research\nefforts have attempted to combine neural networks with FWI to stabilize the\ninversion process. This study presents a simple yet effective training\nframework that is independent of dataset reliance and requires only moderate\npre-training on a simple initial model to stabilize network outputs. During the\ntransfer learning phase, the conventional FWI gradients will simultaneously\nupdate both the neural network and the proposed adaptive residual learning\nmodule, which learns the residual mapping of large-scale distribution features\nin the network's output, rather than directly fitting the target mapping.\nThrough this synergistic training paradigm, the proposed algorithm effectively\ninfers the physically-informed prior knowledge into a global representation of\nstratigraphic distribution, as well as capturing subtle variations in\ninter-layer velocities within local details, thereby escaping local optima.\nEvaluating the method on two benchmark models under various conditions,\nincluding absent low-frequency data, noise interference, and differing initial\nmodels, along with corresponding ablation experiments, consistently\ndemonstrates the superiority of the proposed approach.\n","authors":["Xintong Dong","Zhengyi Yuan","Jun Lin","Shiqi Dong","Xunqian Tong","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2502.11913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11910v1","updated":"2025-02-17T15:28:40Z","published":"2025-02-17T15:28:40Z","title":"Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives","summary":"  Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes.\n","authors":["Leo Schwinn","Yan Scholten","Tom Wollschlger","Sophie Xhonneux","Stephen Casper","Stephan Gnnemann","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.11910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11909v1","updated":"2025-02-17T15:28:19Z","published":"2025-02-17T15:28:19Z","title":"Neural Guided Diffusion Bridges","summary":"  We propose a novel method for simulating conditioned diffusion processes\n(diffusion bridges) in Euclidean spaces. By training a neural network to\napproximate bridge dynamics, our approach eliminates the need for\ncomputationally intensive Markov Chain Monte Carlo (MCMC) methods or\nreverse-process modeling. Compared to existing methods, it offers greater\nrobustness across various diffusion specifications and conditioning scenarios.\nThis applies in particular to rare events and multimodal distributions, which\npose challenges for score-learning- and MCMC-based approaches. We propose a\nflexible variational family for approximating the diffusion bridge path measure\nwhich is partially specified by a neural network. Once trained, it enables\nefficient independent sampling at a cost comparable to sampling the\nunconditioned (forward) process.\n","authors":["Gefan Yang","Frank van der Meulen","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2502.11909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11900v1","updated":"2025-02-17T15:23:59Z","published":"2025-02-17T15:23:59Z","title":"Ansatz-free Hamiltonian learning with Heisenberg-limited scaling","summary":"  Learning the unknown interactions that govern a quantum system is crucial for\nquantum information processing, device benchmarking, and quantum sensing. The\nproblem, known as Hamiltonian learning, is well understood under the assumption\nthat interactions are local, but this assumption may not hold for arbitrary\nHamiltonians. Previous methods all require high-order inverse polynomial\ndependency with precision, unable to surpass the standard quantum limit and\nreach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited\nHamiltonian learning is possible without prior assumptions about the\ninteraction structures, a challenge we term \\emph{ansatz-free Hamiltonian\nlearning}, remains an open question. In this work, we present a quantum\nalgorithm to learn arbitrary sparse Hamiltonians without any structure\nconstraints using only black-box queries of the system's real-time evolution\nand minimal digital controls to attain Heisenberg-limited scaling in estimation\nerror. Our method is also resilient to state-preparation-and-measurement\nerrors, enhancing its practical feasibility. Moreover, we establish a\nfundamental trade-off between total evolution time and quantum control on\nlearning arbitrary interactions, revealing the intrinsic interplay between\ncontrollability and total evolution time complexity for any learning algorithm.\nThese results pave the way for further exploration into Heisenberg-limited\nHamiltonian learning in complex quantum systems under minimal assumptions,\npotentially enabling new benchmarking and verification protocols.\n","authors":["Hong-Ye Hu","Muzhou Ma","Weiyuan Gong","Qi Ye","Yu Tong","Steven T. Flammia","Susanne F. Yelin"],"pdf_url":"https://arxiv.org/pdf/2502.11900v1.pdf","comment":"5 pages, 1 figure with Supplementary Materials (17 pages, 1 figure).\n  HYH and MM contributed equally"},{"id":"http://arxiv.org/abs/2502.11896v1","updated":"2025-02-17T15:22:19Z","published":"2025-02-17T15:22:19Z","title":"CAMEL: Continuous Action Masking Enabled by Large Language Models for\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.\n","authors":["Yanxiao Zhao","Yangge Qian","Jingyang Shan","Xiaolin Qin"],"pdf_url":"https://arxiv.org/pdf/2502.11896v1.pdf","comment":"Accepted at RLDM 2025"},{"id":"http://arxiv.org/abs/2502.11895v1","updated":"2025-02-17T15:21:11Z","published":"2025-02-17T15:21:11Z","title":"Continual Quantization-Aware Pre-Training: When to transition from\n  16-bit to 1.58-bit pre-training for BitNet language models?","summary":"  Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.\n","authors":["Jacob Nielsen","Peter Schneider-Kamp","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2502.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11893v1","updated":"2025-02-17T15:20:04Z","published":"2025-02-17T15:20:04Z","title":"Rethinking Benign Overfitting in Two-Layer Neural Networks","summary":"  Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed\na sharp phase transition from benign to harmful overfitting when the\nnoise-to-feature ratio exceeds a threshold-a situation common in long-tailed\ndata distributions where atypical data is prevalent. However, harmful\noverfitting rarely happens in overparameterized neural networks. Further\nexperimental results suggested that memorization is necessary for achieving\nnear-optimal generalization error in long-tailed data distributions (Feldman &\nZhang, 2020). We argue that this discrepancy between theoretical predictions\nand empirical observations arises because previous feature-noise data models\noverlook the heterogeneous nature of noise across different data classes. In\nthis paper, we refine the feature-noise data model by incorporating\nclass-dependent heterogeneous noise and re-examine the overfitting phenomenon\nin neural networks. Through a comprehensive analysis of the training dynamics,\nwe establish test loss bounds for the refined model. Our findings reveal that\nneural networks can leverage \"data noise\", previously deemed harmful, to learn\nimplicit features that improve the classification accuracy for long-tailed\ndata. Experimental validation on both synthetic and real-world datasets\nsupports our theoretical results.\n","authors":["Ruichen Xu","Kexin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11886v1","updated":"2025-02-17T15:13:29Z","published":"2025-02-17T15:13:29Z","title":"LIMR: Less is More for RL Scaling","summary":"  In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.\n","authors":["Xuefeng Li","Haoyang Zou","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11886v1.pdf","comment":"6pages"},{"id":"http://arxiv.org/abs/2502.11882v1","updated":"2025-02-17T15:09:45Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2501.15369v2","updated":"2025-02-17T15:09:31Z","published":"2025-01-26T02:34:58Z","title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","summary":"  We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.\n","authors":["Chuanyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.15369v2.pdf","comment":"Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer"},{"id":"http://arxiv.org/abs/2502.11880v1","updated":"2025-02-17T15:06:28Z","published":"2025-02-17T15:06:28Z","title":"Bitnet.cpp: Efficient Edge Inference for Ternary LLMs","summary":"  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shijie Cao","Yan Xia","Ting Cao","Jianyu Wei","Shuming Ma","Hongyu Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11880v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.11877v1","updated":"2025-02-17T15:03:54Z","published":"2025-02-17T15:03:54Z","title":"JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs","summary":"  We introduce a simple method for probabilistic predictions on tabular data\nbased on Large Language Models (LLMs) called JoLT (Joint LLM Process for\nTabular data). JoLT uses the in-context learning capabilities of LLMs to define\njoint distributions over tabular data conditioned on user-specified side\ninformation about the problem, exploiting the vast repository of latent\nproblem-relevant knowledge encoded in LLMs. JoLT defines joint distributions\nfor multiple target variables with potentially heterogeneous data types without\nany data conversion, data preprocessing, special handling of missing data, or\nmodel training, making it accessible and efficient for practitioners. Our\nexperiments show that JoLT outperforms competitive methods on low-shot\nsingle-target and multi-target tabular classification and regression tasks.\nFurthermore, we show that JoLT can automatically handle missing data and\nperform data imputation by leveraging textual side information. We argue that\ndue to its simplicity and generality, JoLT is an effective approach for a wide\nvariety of real prediction problems.\n","authors":["Aliaksandra Shysheya","John Bronskill","James Requeima","Shoaib Ahmed Siddiqui","Javier Gonzalez","David Duvenaud","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.11877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11863v1","updated":"2025-02-17T14:55:46Z","published":"2025-02-17T14:55:46Z","title":"FedEAT: A Robustness Optimization Framework for Federated LLMs","summary":"  Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.\n","authors":["Yahao Pang","Xingyuan Wu","Xiaojin Zhang","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2502.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11854v1","updated":"2025-02-17T14:46:58Z","published":"2025-02-17T14:46:58Z","title":"Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on\n  the CICIoMT2024 Dataset","summary":"  The rapid proliferation of Internet of Medical Things (IoMT) devices in\nhealthcare has introduced unique cybersecurity challenges, primarily due to the\ndiverse communication protocols and critical nature of these devices This\nresearch aims to develop an advanced, real-time anomaly detection framework\ntailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024\ndataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS,\nDDoS), time-series (active/idle states), and device-specific (Bluetooth) data,\nour study captures a comprehensive range of IoMT interactions As part of our\ndata analysis, various machine learning techniques are employed which include\nan ensemble model using XGBoost for improved performance against specific\nattack types, sequential models comprised of LSTM and CNN-LSTM that leverage\ntime dependencies, and unsupervised models such as Autoencoders and Isolation\nForest that are good in general anomaly detection The results of the experiment\nprove with an ensemble model lowers false positive rates and reduced\ndetections.\n","authors":["Prathamesh Chandekar","Mansi Mehta","Swet Chandan"],"pdf_url":"https://arxiv.org/pdf/2502.11854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11853v1","updated":"2025-02-17T14:46:38Z","published":"2025-02-17T14:46:38Z","title":"StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models","summary":"  In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g. SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing\n\\textit{content transformations}, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware, and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.\n","authors":["Shehel Yoosuf","Temoor Ali","Ahmed Lekssays","Mashael AlSabah","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2502.11853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11850v1","updated":"2025-02-17T14:44:12Z","published":"2025-02-17T14:44:12Z","title":"Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif\n  Discovery","summary":"  Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.\n","authors":["Aras Yurtman","Daan Van Wesenbeeck","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2502.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11844v1","updated":"2025-02-17T14:37:47Z","published":"2025-02-17T14:37:47Z","title":"BaxBench: Can LLMs Generate Correct and Secure Backends?","summary":"  The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.\n","authors":["Mark Vero","Niels Mndler","Victor Chibotaru","Veselin Raychev","Maximilian Baader","Nikola Jovanovi","Jingxuan He","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2502.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11836v1","updated":"2025-02-17T14:31:00Z","published":"2025-02-17T14:31:00Z","title":"Model Generalization on Text Attribute Graphs: Principles with Large\n  Language Models","summary":"  Large language models (LLMs) have recently been introduced to graph learning,\naiming to extend their zero-shot generalization success to tasks where labeled\ngraph data is scarce. Among these applications, inference over text-attributed\ngraphs (TAGs) presents unique challenges: existing methods struggle with LLMs'\nlimited context length for processing large node neighborhoods and the\nmisalignment between node embeddings and the LLM token space. To address these\nissues, we establish two key principles for ensuring generalization and derive\nthe framework LLM-BP accordingly: (1) Unifying the attribute space with\ntask-adaptive embeddings, where we leverage LLM-based encoders and task-aware\nprompting to enhance generalization of the text attribute embeddings; (2)\nDeveloping a generalizable graph information aggregation mechanism, for which\nwe adopt belief propagation with LLM-estimated parameters that adapt across\ngraphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP\nsignificantly outperforms existing approaches, achieving 8.10% improvement with\ntask-conditional embeddings and an additional 1.71% gain from adaptive\naggregation.\n","authors":["Haoyu Wang","Shikun Liu","Rongzhe Wei","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2502.11836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11828v1","updated":"2025-02-17T14:25:33Z","published":"2025-02-17T14:25:33Z","title":"Intersectional Fairness in Reinforcement Learning with Large State and\n  Constraint Spaces","summary":"  In traditional reinforcement learning (RL), the learner aims to solve a\nsingle objective optimization problem: find the policy that maximizes expected\nreward. However, in many real-world settings, it is important to optimize over\nmultiple objectives simultaneously. For example, when we are interested in\nfairness, states might have feature annotations corresponding to multiple\n(intersecting) demographic groups to whom reward accrues, and our goal might be\nto maximize the reward of the group receiving the minimal reward. In this work,\nwe consider a multi-objective optimization problem in which each objective is\ndefined by a state-based reweighting of a single scalar reward function. This\ngeneralizes the problem of maximizing the reward of the minimum reward group.\nWe provide oracle-efficient algorithms to solve these multi-objective RL\nproblems even when the number of objectives is exponentially large-for tabular\nMDPs, as well as for large MDPs when the group functions have additional\nstructure. Finally, we experimentally validate our theoretical results and\ndemonstrate applications on a preferential attachment graph MDP.\n","authors":["Eric Eaton","Marcel Hussing","Michael Kearns","Aaron Roth","Sikata Bela Sengupta","Jessica Sorrell"],"pdf_url":"https://arxiv.org/pdf/2502.11828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14467v2","updated":"2025-02-17T14:21:20Z","published":"2024-11-18T15:46:39Z","title":"Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device\n  Triggers for Insect Camera Traps","summary":"  Camera traps, combined with AI, have emerged as a way to achieve automated,\nscalable biodiversity monitoring. However, the passive infrared (PIR) sensors\nthat trigger camera traps are poorly suited for detecting small, fast-moving\nectotherms such as insects. Insects comprise over half of all animal species\nand are key components of ecosystems and agriculture. The need for an\nappropriate and scalable insect camera trap is critical in the wake of\nconcerning reports of declines in insect populations. This study proposes an\nalternative to the PIR trigger: ultra-lightweight convolutional neural networks\nrunning on low-powered hardware to detect insects in a continuous stream of\ncaptured images. We train a suite of models to distinguish insect images from\nbackgrounds. Our design achieves zero latency between trigger and image\ncapture. Our models are rigorously tested and achieve high accuracy ranging\nfrom 91.8% to 96.4% AUC on validation data and >87% AUC on data from\ndistributions unseen during training. The high specificity of our models\nensures minimal saving of false positive images, maximising deployment storage\nefficiency. High recall scores indicate a minimal false negative rate,\nmaximising insect detection. Further analysis with saliency maps shows the\nlearned representation of our models to be robust, with low reliance on\nspurious background features. Our system is also shown to operate deployed on\noff-the-shelf, low-powered microcontroller units, consuming a maximum power\ndraw of less than 300mW. This enables longer deployment times using cheap and\nreadily available battery components. Overall we offer a step change in the\ncost, efficiency and scope of insect monitoring. Solving the challenging\ntrigger problem, we demonstrate a system which can be deployed for far longer\nthan existing designs and budgets power and bandwidth effectively, moving\ntowards a generic insect camera trap.\n","authors":["Ross Gardiner","Sareh Rowands","Benno I. Simmons"],"pdf_url":"https://arxiv.org/pdf/2411.14467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11817v1","updated":"2025-02-17T14:09:51Z","published":"2025-02-17T14:09:51Z","title":"AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling","summary":"  Knowledge Tracing (KT) aims to predict students' future performances based on\ntheir former exercises and additional information in educational settings. KT\nhas received significant attention since it facilitates personalized\nexperiences in educational situations. Simultaneously, the autoregressive\nmodeling on the sequence of former exercises has been proven effective for this\ntask. One of the primary challenges in autoregressive modeling for Knowledge\nTracing is effectively representing the anterior (pre-response) and posterior\n(post-response) states of learners across exercises. Existing methods often\nemploy complex model architectures to update learner states using question and\nresponse records. In this study, we propose a novel perspective on knowledge\ntracing task by treating it as a generative process, consistent with the\nprinciples of autoregressive models. We demonstrate that knowledge states can\nbe directly represented through autoregressive encodings on a question-response\nalternate sequence, where model generate the most probable representation in\nhidden state space by analyzing history interactions. This approach underpins\nour framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).\nAdditionally, we incorporate supplementary educational information, such as\nquestion-related skills, into our framework through an auxiliary task, and\ninclude extra exercise details, like response time, as additional inputs. Our\nproposed framework is implemented using advanced autoregressive technologies\nfrom Natural Language Generation (NLG) for both training and prediction.\nEmpirical evaluations on four real-world KT datasets indicate that AAKT\nconsistently outperforms all baseline models in terms of AUC, ACC, and RMSE.\nFurthermore, extensive ablation studies and visualized analysis validate the\neffectiveness of key components in AAKT.\n","authors":["Hao Zhou","Wenge Rong","Jianfei Zhang","Qing Sun","Yuanxin Ouyang","Zhang Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11816v1","updated":"2025-02-17T14:06:36Z","published":"2025-02-17T14:06:36Z","title":"IMTS-Mixer: Mixer-Networks for Irregular Multivariate Time Series\n  Forecasting","summary":"  Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as\na distinct research field, necessitating specialized models to address its\nunique challenges. While most forecasting literature assumes regularly spaced\nobservations without missing values, many real-world datasets - particularly in\nhealthcare, climate research, and biomechanics - violate these assumptions.\nTime Series (TS)-mixer models have achieved remarkable success in regular\nmultivariate time series forecasting. However, they remain unexplored for IMTS\ndue to their requirement for complete and evenly spaced observations. To bridge\nthis gap, we introduce IMTS-Mixer, a novel forecasting architecture designed\nspecifically for IMTS. Our approach retains the core principles of TS mixer\nmodels while introducing innovative methods to transform IMTS into fixed-size\nmatrix representations, enabling their seamless integration with mixer modules.\nWe evaluate IMTS-Mixer on a benchmark of four real-world datasets from various\ndomains. Our results demonstrate that IMTS-Mixer establishes a new\nstate-of-the-art in forecasting accuracy while also improving computational\nefficiency.\n","authors":["Christian Kltergens","Tim Dernedde","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2502.11816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v1","updated":"2025-02-17T13:59:41Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2411.08133v3","updated":"2025-02-17T13:50:54Z","published":"2024-11-12T19:24:42Z","title":"Impactful Bit-Flip Search on Full-precision Models","summary":"  Neural networks have shown remarkable performance in various tasks, yet they\nremain susceptible to subtle changes in their input or model parameters. One\nparticularly impactful vulnerability arises through the Bit-Flip Attack (BFA),\nwhere flipping a small number of critical bits in a model's parameters can\nseverely degrade its performance. A common technique for inducing bit flips in\nDRAM is the Row-Hammer attack, which exploits frequent uncached memory accesses\nto alter data. Identifying susceptible bits can be achieved through exhaustive\nsearch or progressive layer-by-layer analysis, especially in quantized\nnetworks. In this work, we introduce Impactful Bit-Flip Search (IBS), a novel\nmethod for efficiently pinpointing and flipping critical bits in full-precision\nnetworks. Additionally, we propose a Weight-Stealth technique that\nstrategically modifies the model's parameters in a way that maintains the float\nvalues within the original distribution, thereby bypassing simple range checks\noften used in tamper detection.\n","authors":["Nadav Benedek","Matan Levy","Mahmood Sharif"],"pdf_url":"https://arxiv.org/pdf/2411.08133v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10020v2","updated":"2025-02-17T13:48:42Z","published":"2025-02-14T09:01:12Z","title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","summary":"  In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $ for sufficiently large\n$T$, where $\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$\nis the dimension of the contexts, and $T$ is the total number of rounds.\nFurthermore, we introduce a Maximum Likelihood Estimation (MLE)-based\nalgorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O\n\\Big( d \\log (BT) \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2502.10020v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.11801v1","updated":"2025-02-17T13:46:47Z","published":"2025-02-17T13:46:47Z","title":"3D Gaussian Inpainting with Depth-Guided Cross-View Consistency","summary":"  When performing 3D inpainting using novel-view rendering methods like Neural\nRadiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture\nand geometry consistency across camera views has been a challenge. In this\npaper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided\nCross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided\nby the rendered depth information from each training view, our 3DGIC exploits\nbackground pixels visible across different views for updating the inpainting\nmask, allowing us to refine the 3DGS for inpainting purposes.Through extensive\nexperiments on benchmark datasets, we confirm that our 3DGIC outperforms\ncurrent state-of-the-art 3D inpainting methods quantitatively and\nqualitatively.\n","authors":["Sheng-Yu Huang","Zi-Ting Chou","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07930v2","updated":"2025-02-17T13:45:03Z","published":"2024-10-10T13:57:27Z","title":"Cost-aware simulation-based inference","summary":"  Simulation-based inference (SBI) is the preferred framework for estimating\nparameters of intractable models in science and engineering. A significant\nchallenge in this context is the large computational cost of simulating data\nfrom complex models, and the fact that this cost often depends on parameter\nvalues. We therefore propose \\textit{cost-aware SBI methods} which can\nsignificantly reduce the cost of existing sampling-based SBI methods, such as\nneural SBI and approximate Bayesian computation. This is achieved through a\ncombination of rejection and self-normalised importance sampling, which\nsignificantly reduces the number of expensive simulations needed. Our approach\nis studied extensively on models from epidemiology to telecommunications\nengineering, where we obtain significant reductions in the overall cost of\ninference.\n","authors":["Ayush Bharti","Daolang Huang","Samuel Kaski","Franois-Xavier Briol"],"pdf_url":"https://arxiv.org/pdf/2410.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07758v2","updated":"2025-02-17T13:44:46Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago Due","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rlle","Christina C. Westhoff","Bndicte Lenoir","Niels Halama","Inka Zrnig","Dirk Jger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v2.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.07739v2","updated":"2025-02-17T13:39:51Z","published":"2025-02-11T17:59:35Z","title":"HRP: High-Rank Preheating for Superior LoRA Initialization","summary":"  This paper studies the crucial impact of initialization on the convergence\nproperties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that\nrandom initialization, a widely used schema, will likely lead LoRA to random\nlow-rank results, rather than the best low-rank result. While this issue can be\nmitigated by adjusting initialization towards a well-informed direction, it\nrelies on prior knowledge of the target, which is typically unknown in\nreal-world scenarios. To approximate this well-informed initial direction, we\npropose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few\nsteps and uses the singular value decomposition of the preheated result as a\nsuperior initialization. HRP initialization is theory-supported to combine the\nconvergence strengths of high-rank LoRA and the generalization strengths of\nlow-rank LoRA. Extensive experiments demonstrate that HRP significantly\nenhances LoRA's effectiveness across various models and tasks, achieving\nperformance comparable to full-parameter fine-tuning and outperforming other\ninitialization strategies.\n","authors":["Yuzhu Chen","Yingjie Wang","Shi Fu","Li Shen","Yongcheng Jing","Xinmei Tian","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.07739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v2","updated":"2025-02-17T13:30:15Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions through\nsubnetworks that can be composed to perform more complex tasks. Recent advances\nin mechanistic interpretability have made progress in identifying\n$\\textit{circuits}$, which represent the minimal computational subgraphs\nresponsible for a model's behavior on specific tasks. However, most studies\nfocus on identifying circuits for individual tasks without investigating how\nfunctionally similar circuits $\\textit{relate}$ to each other. To address this\ngap, we study the modularity of neural networks by analyzing circuits for\nhighly compositional subtasks within a transformer-based language model.\nSpecifically, given a probabilistic context-free grammar, we identify and\ncompare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through set operations to represent more\ncomplex functional model capabilities.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v2.pdf","comment":"22 pages, 21 figures"},{"id":"http://arxiv.org/abs/2501.07824v2","updated":"2025-02-17T13:26:52Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08474v2","updated":"2025-02-17T13:22:54Z","published":"2024-09-13T02:00:16Z","title":"Rethinking Meta-Learning from a Learning Lens","summary":"  Meta-learning has emerged as a powerful approach for leveraging knowledge\nfrom previous tasks to solve new tasks. The mainstream methods focus on\ntraining a well-generalized model initialization, which is then adapted to\ndifferent tasks with limited data and updates. However, it pushes the model\noverfitting on the training tasks. Previous methods mainly attributed this to\nthe lack of data and used augmentations to address this issue, but they were\nlimited by sufficient training and effective augmentation strategies. In this\nwork, we focus on the more fundamental learning to learn strategy of\nmeta-learning to explore what causes errors and how to eliminate these errors\nwithout changing the environment. Specifically, we first rethink the\nalgorithmic procedure of meta-learning from a learning lens. Through\ntheoretical and empirical analyses, we find that (i) this paradigm faces the\nrisk of both overfitting and underfitting and (ii) the model adapted to\ndifferent tasks promote each other where the effect is stronger if the tasks\nare more similar. Based on this insight, we propose using task relations to\ncalibrate the optimization process of meta-learning and propose a plug-and-play\nmethod called Task Relation Learner (TRLearner) to achieve this goal.\nSpecifically, it first obtains task relation matrices from the extracted\ntask-specific meta-data. Then, it uses the obtained matrices with\nrelation-aware consistency regularization to guide optimization. Extensive\ntheoretical and empirical analyses demonstrate the effectiveness of TRLearner.\n","authors":["Jingyao Wang","Wenwen Qiang","Chuxiong Sun","Changwen Zheng","Jiangmeng Li"],"pdf_url":"https://arxiv.org/pdf/2409.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01220v2","updated":"2025-02-17T13:20:37Z","published":"2025-02-03T10:24:55Z","title":"Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts","summary":"  Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.\n","authors":["Hichem Ammar Khodja","Frdric Bchet","Quentin Brabant","Alexis Nasr","Gwnol Lecorv"],"pdf_url":"https://arxiv.org/pdf/2502.01220v2.pdf","comment":"preprint v2"},{"id":"http://arxiv.org/abs/2405.15506v3","updated":"2025-02-17T13:19:33Z","published":"2024-05-24T12:51:23Z","title":"Learning to Discretize Denoising Diffusion ODEs","summary":"  Diffusion Probabilistic Models (DPMs) are generative models showing\ncompetitive performance in various domains, including image synthesis and 3D\npoint cloud generation. Sampling from pre-trained DPMs involves multiple neural\nfunction evaluations (NFEs) to transform Gaussian noise samples into images,\nresulting in higher computational costs compared to single-step generative\nmodels such as GANs or VAEs. Therefore, reducing the number of NFEs while\npreserving generation quality is crucial. To address this, we propose LD3, a\nlightweight framework designed to learn the optimal time discretization for\nsampling. LD3 can be combined with various samplers and consistently improves\ngeneration quality without having to retrain resource-intensive neural\nnetworks. We demonstrate analytically and empirically that LD3 improves\nsampling efficiency with much less computational overhead. We evaluate our\nmethod with extensive experiments on 7 pre-trained models, covering\nunconditional and conditional sampling in both pixel-space and latent-space\nDPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional\nCIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient\napproach to sampling from pre-trained diffusion models. Code is available at\nhttps://github.com/vinhsuhi/LD3.\n","authors":["Vinh Tong","Trung-Dung Hoang","Anji Liu","Guy Van den Broeck","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2405.15506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11778v1","updated":"2025-02-17T13:13:16Z","published":"2025-02-17T13:13:16Z","title":"Private Synthetic Graph Generation and Fused Gromov-Wasserstein Distance","summary":"  Networks are popular for representing complex data. In particular,\ndifferentially private synthetic networks are much in demand for method and\nalgorithm development. The network generator should be easy to implement and\nshould come with theoretical guarantees. Here we start with complex data as\ninput and jointly provide a network representation as well as a synthetic\nnetwork generator. Using a random connection model, we devise an effective\nalgorithmic approach for generating attributed synthetic graphs which is\n$\\epsilon$-differentially private at the vertex level, while preserving utility\nunder an appropriate notion of distance which we develop. We provide\ntheoretical guarantees for the accuracy of the private synthetic graphs using\nthe fused Gromov-Wasserstein distance, which extends the Wasserstein metric to\nstructured data. Our method draws inspiration from the PSMM method of\n\\citet{he2023}.\n","authors":["Leoni Carla Wirth","Gholamali Aminian","Gesine Reinert"],"pdf_url":"https://arxiv.org/pdf/2502.11778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06650v3","updated":"2025-02-17T13:11:43Z","published":"2024-11-11T01:34:10Z","title":"Quantum Policy Gradient in Reproducing Kernel Hilbert Space","summary":"  Parametrised quantum circuits offer expressive and data-efficient\nrepresentations for machine learning. Due to quantum states residing in a\nhigh-dimensional Hilbert space, parametrised quantum circuits have a natural\ninterpretation in terms of kernel methods. The representation of quantum\ncircuits in terms of quantum kernels has been studied widely in quantum\nsupervised learning, but has been overlooked in the context of quantum RL. This\npaper proposes parametric and non-parametric policy gradient and actor-critic\nalgorithms with quantum kernel policies in quantum environments. This approach,\nimplemented with both numerical and analytical quantum policy gradient\ntechniques, allows exploiting the many advantages of kernel methods, including\navailable analytic forms for the gradient of the policy and tunable\nexpressiveness. The proposed approach is suitable for vector-valued action\nspaces and each of the formulations demonstrates a quadratic reduction in query\ncomplexity compared to their classical counterparts. Two actor-critic\nalgorithms, one based on stochastic policy gradient and one based on\ndeterministic policy gradient (comparable to the popular DDPG algorithm),\ndemonstrate additional query complexity reductions compared to quantum policy\ngradient algorithms under favourable conditions.\n","authors":["David M. Bossens","Kishor Bharti","Jayne Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.06650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11774v1","updated":"2025-02-17T13:07:37Z","published":"2025-02-17T13:07:37Z","title":"Interpretable Machine Learning for Kronecker Coefficients","summary":"  We analyze the saliency of neural networks and employ interpretable machine\nlearning models to predict whether the Kronecker coefficients of the symmetric\ngroup are zero or not. Our models use triples of partitions as input features,\nas well as b-loadings derived from the principal component of an embedding that\ncaptures the differences between partitions. Across all approaches, we achieve\nan accuracy of approximately 83% and derive explicit formulas for a decision\nfunction in terms of b-loadings. Additionally, we develop transformer-based\nmodels for prediction, achieving the highest reported accuracy of over 99%.\n","authors":["Giorgi Butbaia","Kyu-Hwan Lee","Fabian Ruehle"],"pdf_url":"https://arxiv.org/pdf/2502.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04064v2","updated":"2025-02-17T13:04:24Z","published":"2024-10-05T07:25:56Z","title":"Text2Chart31: Instruction Tuning for Chart Generation with Automatic\n  Feedback","summary":"  Large language models (LLMs) have demonstrated strong capabilities across\nvarious language tasks, notably through instruction-tuning methods. However,\nLLMs face challenges in visualizing complex, real-world data through charts and\nplots. Firstly, existing datasets rarely cover a full range of chart types,\nsuch as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning\nmethods do not fully leverage the intricate relationships within rich datasets,\nincluding text, code, and figures. To address these challenges, we propose a\nhierarchical pipeline and a new dataset for chart generation. Our dataset,\nText2Chart31, includes 31 unique plot types referring to the Matplotlib\nlibrary, with 11.1K tuples of descriptions, code, data tables, and plots.\nMoreover, we introduce a reinforcement learning-based instruction tuning\ntechnique for chart generation tasks without requiring human feedback. Our\nexperiments show that this approach significantly enhances the model\nperformance, enabling smaller models to outperform larger open-source models\nand be comparable to state-of-the-art proprietary models in data visualization\ntasks. We make the code and dataset available at\nhttps://github.com/fatemehpesaran310/Text2Chart31.\n","authors":["Fatemeh Pesaran Zadeh","Juyeon Kim","Jin-Hwa Kim","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04064v2.pdf","comment":"EMNLP 2024 Main Oral. Code and dataset are released at\n  https://github.com/fatemehpesaran310/Text2Chart31"},{"id":"http://arxiv.org/abs/2502.11767v1","updated":"2025-02-17T12:58:17Z","published":"2025-02-17T12:58:17Z","title":"From Selection to Generation: A Survey of LLM-based Active Learning","summary":"  Active Learning (AL) has been a powerful paradigm for improving model\nefficiency and performance by selecting the most informative data points for\nlabeling and training. In recent active learning frameworks, Large Language\nModels (LLMs) have been employed not only for selection but also for generating\nentirely new data instances and providing more cost-effective annotations.\nMotivated by the increasing importance of high-quality data and efficient model\ntraining in the era of LLMs, we present a comprehensive survey on LLM-based\nActive Learning. We introduce an intuitive taxonomy that categorizes these\ntechniques and discuss the transformative roles LLMs can play in the active\nlearning loop. We further examine the impact of AL on LLM learning paradigms\nand its applications across various domains. Finally, we identify open\nchallenges and propose future research directions. This survey aims to serve as\nan up-to-date resource for researchers and practitioners seeking to gain an\nintuitive understanding of LLM-based AL techniques and deploy them to new\napplications.\n","authors":["Yu Xia","Subhojyoti Mukherjee","Zhouhang Xie","Junda Wu","Xintong Li","Ryan Aponte","Hanjia Lyu","Joe Barrow","Hongjie Chen","Franck Dernoncourt","Branislav Kveton","Tong Yu","Ruiyi Zhang","Jiuxiang Gu","Nesreen K. Ahmed","Yu Wang","Xiang Chen","Hanieh Deilamsalehy","Sungchul Kim","Zhengmian Hu","Yue Zhao","Nedim Lipka","Seunghyun Yoon","Ting-Hao Kenneth Huang","Zichao Wang","Puneet Mathur","Soumyabrata Pal","Koyel Mukherjee","Zhehao Zhang","Namyong Park","Thien Huu Nguyen","Jiebo Luo","Ryan A. Rossi","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11767v1.pdf","comment":null}]},"2025-02-16T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.11308v1","updated":"2025-02-16T23:11:13Z","published":"2025-02-16T23:11:13Z","title":"ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment\n  and Generation","summary":"  With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP.\n","authors":["Yiyi Chen","Qiongkai Xu","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2502.11308v1.pdf","comment":"18 pages, 13 tables, 6 figures"},{"id":"http://arxiv.org/abs/2403.17983v3","updated":"2025-02-16T22:31:00Z","published":"2024-03-24T21:41:29Z","title":"Is The Watermarking Of LLM-Generated Code Robust?","summary":"  We present the first in depth study on the robustness of existing\nwatermarking techniques applied to code generated by large language models\n(LLMs). As LLMs increasingly contribute to software development, watermarking\nhas emerged as a potential solution for detecting AI generated code and\nmitigating misuse, such as plagiarism or the automated generation of malicious\nprograms. While previous research has demonstrated the resilience of\nwatermarking in the text setting, our work reveals that watermarking techniques\nare significantly more fragile in code-based contexts. Specifically, we show\nthat simple semantic-preserving transformations, such as variable renaming and\ndead code insertion, can effectively erase watermarks without altering the\nprogram's functionality. To systematically evaluate watermark robustness, we\ndevelop an algorithm that traverses the Abstract Syntax Tree (AST) of a\nwatermarked program and applies a sequence of randomized, semantics-preserving\ntransformations. Our experimental results, conducted on Python code generated\nby different LLMs, indicate that even minor modifications can drastically\nreduce watermark detectability, with true positive rates (TPR) dropping below\n50% in many cases. Our code is publicly available at\nhttps://github.com/uiuc-arc/llm-code-watermark.\n","authors":["Tarun Suresh","Shubham Ugare","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2403.17983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19785v3","updated":"2025-02-16T21:54:38Z","published":"2024-10-14T22:25:06Z","title":"How to Backdoor Consistency Models?","summary":"  Consistency models are a new class of models that generate images by directly\nmapping noise to data, allowing for one-step generation and significantly\naccelerating the sampling process. However, their robustness against\nadversarial attacks has not yet been thoroughly investigated. In this work, we\nconduct the first study on the vulnerability of consistency models to backdoor\nattacks. While previous research has explored backdoor attacks on diffusion\nmodels, those studies have primarily focused on conventional diffusion models,\nemploying a customized backdoor training process and objective, whereas\nconsistency models have distinct training processes and objectives. Our\nproposed framework demonstrates the vulnerability of consistency models to\nbackdoor attacks. During image generation, poisoned consistency models produce\nimages with a Fr\\'echet Inception Distance (FID) comparable to that of a clean\nmodel when sampling from Gaussian noise. However, once the trigger is\nactivated, they generate backdoor target images. We explore various trigger and\ntarget configurations to evaluate the vulnerability of consistency models,\nincluding the use of random noise as a trigger. This novel trigger is visually\ninconspicuous, more challenging to detect, and aligns well with the sampling\nprocess of consistency models. Across all configurations, our framework\nsuccessfully compromises the consistency models while maintaining high utility\nand specificity. We also examine the stealthiness of our proposed attack, which\nis attributed to the unique properties of consistency models and the elusive\nnature of the Gaussian noise trigger. Our code is available at\n\\href{https://github.com/chengenw/backdoorCM}{https://github.com/chengenw/backdoorCM}.\n","authors":["Chengen Wang","Murat Kantarcioglu"],"pdf_url":"https://arxiv.org/pdf/2410.19785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11191v1","updated":"2025-02-16T16:34:49Z","published":"2025-02-16T16:34:49Z","title":"Primus: A Pioneering Collection of Open-Source Datasets for\n  Cybersecurity LLM Training","summary":"  Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.88% improvement in the aggregate score, while reasoning\ndistillation leads to a 10% gain in security certification (CISSP). We will\nrelease all datasets and trained cybersecurity LLMs under the ODC-BY and MIT\nlicenses to encourage further research in the community. For access to all\ndatasets and model weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.\n","authors":["Yao-Ching Yu","Tsun-Han Chiang","Cheng-Wei Tsai","Chien-Ming Huang","Wen-Kwang Tsao"],"pdf_url":"https://arxiv.org/pdf/2502.11191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11173v1","updated":"2025-02-16T15:49:25Z","published":"2025-02-16T15:49:25Z","title":"Evaluating the Potential of Quantum Machine Learning in Cybersecurity: A\n  Case-Study on PCA-based Intrusion Detection Systems","summary":"  Quantum computing promises to revolutionize our understanding of the limits\nof computation, and its implications in cryptography have long been evident.\nToday, cryptographers are actively devising post-quantum solutions to counter\nthe threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists\nare innovating quantum protocols to empower defenders. However, the broader\nimpact of quantum computing and quantum machine learning (QML) on other\ncybersecurity domains still needs to be explored. In this work, we investigate\nthe potential impact of QML on cybersecurity applications of traditional ML.\nFirst, we explore the potential advantages of quantum computing in machine\nlearning problems specifically related to cybersecurity. Then, we describe a\nmethodology to quantify the future impact of fault-tolerant QML algorithms on\nreal-world problems. As a case study, we apply our approach to standard methods\nand datasets in network intrusion detection, one of the most studied\napplications of machine learning in cybersecurity. Our results provide insight\ninto the conditions for obtaining a quantum advantage and the need for future\nquantum hardware and software advancements.\n","authors":["Armando Bellante","Tommaso Fioravanti","Michele Carminati","Stefano Zanero","Alessandro Luongo"],"pdf_url":"https://arxiv.org/pdf/2502.11173v1.pdf","comment":"Computers & Security (2025): 104341"},{"id":"http://arxiv.org/abs/2502.11143v1","updated":"2025-02-16T14:21:52Z","published":"2025-02-16T14:21:52Z","title":"VulRG: Multi-Level Explainable Vulnerability Patch Ranking for Complex\n  Systems Using Graphs","summary":"  As interconnected systems proliferate, safeguarding complex infrastructures\nagainst an escalating array of cyber threats has become an urgent challenge.\nThe increasing number of vulnerabilities, combined with resource constraints,\nmakes addressing every vulnerability impractical, making effective\nprioritization essential. However, existing risk prioritization methods often\nrely on expert judgment or focus solely on exploit likelihood and consequences,\nlacking the granularity and adaptability needed for complex systems. This work\nintroduces a graph-based framework for vulnerability patch prioritization that\noptimizes security by integrating diverse data sources and metrics into a\nuniversally applicable model. Refined risk metrics enable detailed assessments\nat the component, asset, and system levels. The framework employs two key\ngraphs: a network communication graph to model potential attack paths and\nidentify the shortest routes to critical assets, and a system dependency graph\nto capture risk propagation from exploited vulnerabilities across\ninterconnected components. Asset criticality and component dependency rules\nsystematically assess and mitigate risks. Benchmarking against state-of-the-art\nmethods demonstrates superior accuracy in vulnerability patch ranking, with\nenhanced explainability. This framework advances vulnerability management and\nsets the stage for future research in adaptive cybersecurity strategies.\n","authors":["Yuning Jiang","Nay Oo","Qiaoran Meng","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2502.11143v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2502.11127v1","updated":"2025-02-16T13:48:41Z","published":"2025-02-16T13:48:41Z","title":"G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based\n  Multi-agent Systems","summary":"  Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated\nremarkable capabilities in various complex tasks, ranging from collaborative\nproblem-solving to autonomous decision-making. However, as these systems become\nincreasingly integrated into critical applications, their vulnerability to\nadversarial attacks, misinformation propagation, and unintended behaviors have\nraised significant concerns. To address this challenge, we introduce\nG-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS,\nwhich leverages graph neural networks to detect anomalies on the multi-agent\nutterance graph and employ topological intervention for attack remediation.\nExtensive experiments demonstrate that G-Safeguard: (I) exhibits significant\neffectiveness under various attack strategies, recovering over 40% of the\nperformance for prompt injection; (II) is highly adaptable to diverse LLM\nbackbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS\nwith security guarantees. The code is available at\nhttps://github.com/wslong20/G-safeguard.\n","authors":["Shilong Wang","Guibin Zhang","Miao Yu","Guancheng Wan","Fanci Meng","Chongye Guo","Kun Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11121v1","updated":"2025-02-16T13:35:12Z","published":"2025-02-16T13:35:12Z","title":"Reversible Data Hiding over Encrypted Images via Intrinsic Correlation\n  in Block-Based Secret Sharing","summary":"  With the rapid advancements in information technology, reversible data hiding\nover encrypted images (RDH-EI) has become essential for secure image management\nin cloud services. However, existing RDH-EI schemes often suffer from high\ncomputational complexity, low embedding rates, and excessive data expansion.\nThis paper addresses these challenges by first analyzing the block-based secret\nsharing in existing schemes, revealing significant data redundancy within image\nblocks. Based on this observation, we propose two space-preserving methods: the\ndirect space-vacating method and the image-shrinking-based space-vacating\nmethod. Using these techniques, we design two novel RDH-EI schemes: a\nhigh-capacity RDH-EI scheme and a size-reduced RDH-EI scheme. The high-capacity\nRDH-EI scheme directly creates embedding space in encrypted images, eliminating\nthe need for complex space-vacating operations and achieving higher and more\nstable embedding rates. In contrast, the size-reduced RDH-EI scheme minimizes\ndata expansion by discarding unnecessary shares, resulting in smaller encrypted\nimages. Experimental results show that the high-capacity RDH-EI scheme\noutperforms existing methods in terms of embedding capacity, while the\nsize-reduced RDH-EI scheme excels in minimizing data expansion. Both schemes\nprovide effective solutions to the challenges in RDH-EI, offering promising\napplications in fields such as medical imaging and cloud storage.\n","authors":["Jianhui Zou","Weijia Cao","Shuang Yi","Yifeng Zheng","Zhongyun Hua"],"pdf_url":"https://arxiv.org/pdf/2502.11121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11110v1","updated":"2025-02-16T12:53:23Z","published":"2025-02-16T12:53:23Z","title":"Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and\n  LLM-based Code Generation","summary":"  Homomorphic encryption (HE) is a core building block in privacy-preserving\nmachine learning (PPML), but HE is also widely known as its efficiency\nbottleneck. Therefore, many GPU-accelerated cryptographic schemes have been\nproposed to improve the performance of HE. However, these methods often require\ncomplex modifications tailored to specific algorithms and are tightly coupled\nwith specific GPU and operating systems. It is interesting to ask how to\ngenerally offer more practical GPU-accelerated cryptographic algorithm\nimplementations. Given the powerful code generation capabilities of large\nlanguage models (LLMs), we aim to explore their potential to automatically\ngenerate practical GPU-friendly algorithm code using CPU-friendly code. In this\npaper, we focus on number theoretic transform (NTT) -- the core mechanism of\nHE. We first develop and optimize a GPU-friendly NTT (GNTT) family that\nexploits PyTorch's fast matrix computation and precomputation, achieving an\napproximately 62x speedup -- a significant boost over existing ones. Then we\nexplore GPU-friendly code generation using various LLMs, including DeepSeek-R1,\nOpenAI o1 and o3-mini. We discover many interesting findings throughout the\nprocess. For instance, somewhat surprisingly, our experiments demonstrate that\nDeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot\nbeat our optimized protocol. The findings provide valuable insights for\nturbocharging PPML and enhancing code generation capabilities of LLMs. Codes\nare available at: https://github.com/LMPC-Lab/GenGPUCrypto.\n","authors":["Yu Cui","Hang Fu","Licheng Wang","Haibin Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11070v1","updated":"2025-02-16T10:33:37Z","published":"2025-02-16T10:33:37Z","title":"A Survey on Vulnerability Prioritization: Taxonomy, Metrics, and\n  Research Challenges","summary":"  In the highly interconnected digital landscape of today, safeguarding complex\ninfrastructures against cyber threats has become increasingly challenging due\nto the exponential growth in the number and complexity of vulnerabilities.\nResource constraints necessitate effective vulnerability prioritization\nstrategies, focusing efforts on the most critical risks. This paper presents a\nsystematic literature review of 82 studies, introducing a novel taxonomy that\ncategorizes metrics into severity, exploitability, contextual factors,\npredictive indicators, and aggregation methods. Our analysis reveals\nsignificant gaps in existing approaches and challenges with multi-domain\napplicability. By emphasizing the need for dynamic, context-aware metrics and\nscalable solutions, we provide actionable insights to bridge the gap between\nresearch and real-world applications. This work contributes to the field by\noffering a comprehensive framework for evaluating vulnerability prioritization\nmethodologies and setting a research agenda to advance the state of practice.\n","authors":["Yuning Jiang","Nay Oo","Qiaoran Meng","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2502.11070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11054v1","updated":"2025-02-16T09:27:44Z","published":"2025-02-16T09:27:44Z","title":"Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models","summary":"  Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.\n","authors":["Zonghao Ying","Deyue Zhang","Zonglei Jing","Yisong Xiao","Quanchen Zou","Aishan Liu","Siyuan Liang","Xiangzheng Zhang","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.11054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11087v4","updated":"2025-02-16T07:57:37Z","published":"2024-06-16T22:11:41Z","title":"DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient\n  Language Models","summary":"  Large language models have repeatedly shown outstanding performance across\ndiverse applications. However, deploying these models can inadvertently risk\nuser privacy. The significant memory demands during training pose a major\nchallenge in terms of resource consumption. This substantial size places a\nheavy load on memory resources, raising considerable practical concerns. In\nthis paper, we introduce DP-MemArc, a novel training framework aimed at\nreducing the memory costs of large language models while emphasizing the\nprotection of user data privacy. DP-MemArc incorporates side network or\nreversible network designs to support a variety of differential privacy\nmemory-efficient fine-tuning schemes. Our approach not only achieves in memory\noptimization but also ensures robust privacy protection, keeping user data\nsecure and confidential. Extensive experiments have demonstrated that DP-MemArc\neffectively provides differential privacy-efficient fine-tuning across\ndifferent task scenarios.\n","authors":["Yanming Liu","Xinyue Peng","Yuwei Zhang","Xiaolan Ke","Songhang Deng","Jiannan Cao","Chen Ma","Mengchen Fu","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du","Xuhong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.11087v4.pdf","comment":"Camera Ready version of AAAI 2025"},{"id":"http://arxiv.org/abs/2401.06030v2","updated":"2025-02-16T07:54:35Z","published":"2024-01-11T16:42:10Z","title":"Protecting Model Adaptation from Trojans in the Unlabeled Data","summary":"  Model adaptation tackles the distribution shift problem with a pre-trained\nmodel instead of raw data, which has become a popular paradigm due to its great\nprivacy protection. Existing methods always assume adapting to a clean target\ndomain, overlooking the security risks of unlabeled samples. This paper for the\nfirst time explores the potential trojan attacks on model adaptation launched\nby well-designed poisoning target data. Concretely, we provide two trigger\npatterns with two poisoning strategies for different prior knowledge owned by\nattackers. These attacks achieve a high success rate while maintaining the\nnormal performance on clean samples in the test stage. To defend against such\nbackdoor injection, we propose a plug-and-play method named DiffAdapt, which\ncan be seamlessly integrated with existing adaptation algorithms. Experiments\nacross commonly used benchmarks and adaptation methods demonstrate the\neffectiveness of DiffAdapt. We hope this work will shed light on the safety of\ntransfer learning with unlabeled data.\n","authors":["Lijun Sheng","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.06030v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2502.11029v1","updated":"2025-02-16T07:49:27Z","published":"2025-02-16T07:49:27Z","title":"HawkEye: Statically and Accurately Profiling the Communication Cost of\n  Models in Multi-party Learning","summary":"  Multi-party computation (MPC) based machine learning, referred to as\nmulti-party learning (MPL), has become an important technology for utilizing\ndata from multiple parties with privacy preservation. In recent years, in order\nto apply MPL in more practical scenarios, various MPC-friendly models have been\nproposedto reduce the extraordinary communication overhead of MPL. Within the\noptimization of MPC-friendly models, a critical element to tackle the challenge\nis profiling the communication cost of models. However, the current solutions\nmainly depend on manually establishing the profiles to identify communication\nbottlenecks of models, often involving burdensome human efforts in a monotonous\nprocedure.\n  In this paper, we propose HawkEye, a static model communication cost\nprofiling framework, which enables model designers to get the accurate\ncommunication cost of models in MPL frameworks without dynamically running the\nsecure model training or inference processes on a specific MPL framework.\nFirstly, to profile the communication cost of models with complex structures,\nwe propose a static communication cost profiling method based on a prefix\nstructure that records the function calling chain during the static analysis.\nSecondly, HawkEye employs an automatic differentiation library to assist model\ndesigners in profiling the communication cost of models in PyTorch. Finally, we\ncompare the static profiling results of HawkEye against the profiling results\nobtained through dynamically running secure model training and inference\nprocesses on five popular MPL frameworks, CryptFlow2, CrypTen, Delphi, Cheetah,\nand SecretFlow-SEMI2K. The experimental results show that HawkEye can\naccurately profile the model communication cost without dynamic profiling.\n","authors":["Wenqiang Ruan","Xin Lin","Ruisheng Zhou","Guopeng Lin","Shui Yu","Weili Han"],"pdf_url":"https://arxiv.org/pdf/2502.11029v1.pdf","comment":"This paper has been accepted for publication at USENIX Security 2025.\n  Please cite this paper as 'Wenqiang Ruan, Xin Lin, Ruisheng Zhou, Guopeng\n  Lin, Shui Yu, Weili Han, HawkEye: Statically and Accurately Profiling the\n  Communication Cost of Models in Multi-party Learning. In Proceedings of the\n  34th USENIX Security, August 13-15, 2025, Seattle, WA, USA.'"},{"id":"http://arxiv.org/abs/2408.14853v2","updated":"2025-02-16T07:47:15Z","published":"2024-08-27T08:12:08Z","title":"Atoxia: Red-teaming Large Language Models with Target Toxic Answers","summary":"  Despite the substantial advancements in artificial intelligence, large\nlanguage models (LLMs) remain being challenged by generation safety. With\nadversarial jailbreaking prompts, one can effortlessly induce LLMs to output\nharmful content, causing unexpected negative social impacts. This vulnerability\nhighlights the necessity for robust LLM red-teaming strategies to identify and\nmitigate such risks before large-scale application. To detect specific types of\nrisks, we propose a novel red-teaming method that $\\textbf{A}$ttacks LLMs with\n$\\textbf{T}$arget $\\textbf{Toxi}$c $\\textbf{A}$nswers ($\\textbf{Atoxia}$).\nGiven a particular harmful answer, Atoxia generates a corresponding user query\nand a misleading answer opening to examine the internal defects of a given LLM.\nThe proposed attacker is trained within a reinforcement learning scheme with\nthe LLM outputting probability of the target answer as the reward. We verify\nthe effectiveness of our method on various red-teaming benchmarks, such as\nAdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can\nsuccessfully detect safety risks in not only open-source models but also\nstate-of-the-art black-box models such as GPT-4o.\n","authors":["Yuhao Du","Zhuo Li","Pengyu Cheng","Xiang Wan","Anningzhe Gao"],"pdf_url":"https://arxiv.org/pdf/2408.14853v2.pdf","comment":"Accepted to Findings of NAACL-2025"},{"id":"http://arxiv.org/abs/2502.11014v1","updated":"2025-02-16T06:39:36Z","published":"2025-02-16T06:39:36Z","title":"Leveraging Large Language Models for Cybersecurity: Enhancing SMS Spam\n  Detection with Robust and Context-Aware Text Classification","summary":"  This study evaluates the effectiveness of different feature extraction\ntechniques and classification algorithms in detecting spam messages within SMS\ndata. We analyzed six classifiers Naive Bayes, K-Nearest Neighbors, Support\nVector Machines, Linear Discriminant Analysis, Decision Trees, and Deep Neural\nNetworks using two feature extraction methods: bag-of-words and TF-IDF. The\nprimary objective was to determine the most effective classifier-feature\ncombination for SMS spam detection. Our research offers two main contributions:\nfirst, by systematically examining various classifier and feature extraction\npairings, and second, by empirically evaluating their ability to distinguish\nspam messages. Our results demonstrate that the TF-IDF method consistently\noutperforms the bag-of-words approach across all six classifiers. Specifically,\nNaive Bayes with TF-IDF achieved the highest accuracy of 96.2%, with a\nprecision of 0.976 for non-spam and 0.754 for spam messages. Similarly, Support\nVector Machines with TF-IDF exhibited an accuracy of 94.5%, with a precision of\n0.926 for non-spam and 0.891 for spam. Deep Neural Networks using TF-IDF\nyielded an accuracy of 91.0%, with a recall of 0.991 for non-spam and 0.415 for\nspam messages. In contrast, classifiers such as K-Nearest Neighbors, Linear\nDiscriminant Analysis, and Decision Trees showed weaker performance, regardless\nof the feature extraction method employed. Furthermore, we observed substantial\nvariability in classifier effectiveness depending on the chosen feature\nextraction technique. Our findings emphasize the significance of feature\nselection in SMS spam detection and suggest that TF-IDF, when paired with Naive\nBayes, Support Vector Machines, or Deep Neural Networks, provides the most\nreliable performance. These insights provide a foundation for improving SMS\nspam detection through optimized feature extraction and classification methods.\n","authors":["Mohsen Ahmadi","Matin Khajavi","Abbas Varmaghani","Ali Ala","Kasra Danesh","Danial Javaheri"],"pdf_url":"https://arxiv.org/pdf/2502.11014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07261v2","updated":"2025-02-16T06:20:55Z","published":"2024-12-10T07:42:46Z","title":"MemHunter: Automated and Verifiable Memorization Detection at\n  Dataset-scale in LLMs","summary":"  Large language models (LLMs) have been shown to memorize and reproduce\ncontent from their training data, raising significant privacy concerns,\nespecially with web-scale datasets. Existing methods for detecting memorization\nare primarily sample-specific, relying on manually crafted or discretely\noptimized memory-inducing prompts generated on a per-sample basis, which become\nimpractical for dataset-level detection due to the prohibitive computational\ncost of iterating through all samples. In real-world scenarios, data owners may\nneed to verify whether a susceptible LLM has memorized their dataset,\nparticularly if the LLM may have collected the data from the web without\nauthorization. To address this, we introduce MemHunter, which trains a\nmemory-inducing LLM and employs hypothesis testing to efficiently detect\nmemorization at the dataset level, without requiring sample-specific memory\ninducing. Experiments on models like Pythia and Llama demonstrate that\nMemHunter can extract up to 40% more training data than existing methods under\nconstrained time resources and reduce search time by up to 80% when integrated\nas a plug-in. Crucially, MemHunter is the first method capable of dataset-level\nmemorization detection, providing a critical tool for assessing privacy risks\nin LLMs powered by large-scale datasets.\n","authors":["Zhenpeng Wu","Jian Lou","Zibin Zheng","Chuan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.07261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11006v1","updated":"2025-02-16T06:16:00Z","published":"2025-02-16T06:16:00Z","title":"Prompt Inject Detection with Generative Explanation as an Investigative\n  Tool","summary":"  Large Language Models (LLMs) are vulnerable to adversarial prompt based\ninjects. These injects could jailbreak or exploit vulnerabilities within these\nmodels with explicit prompt requests leading to undesired responses. In the\ncontext of investigating prompt injects, the challenge is the sheer volume of\ninput prompts involved that are likely to be largely benign. This investigative\nchallenge is further complicated by the semantics and subjectivity of the input\nprompts involved in the LLM conversation with its user and the context of the\nenvironment to which the conversation is being carried out. Hence, the\nchallenge for AI security investigators would be two-fold. The first is to\nidentify adversarial prompt injects and then to assess whether the input prompt\nis contextually benign or adversarial. For the first step, this could be done\nusing existing AI security solutions like guardrails to detect and protect the\nLLMs. Guardrails have been developed using a variety of approaches. A popular\napproach is to use signature based. Another popular approach to develop AI\nmodels to classify such prompts include the use of NLP based models like a\nlanguage model. However, in the context of conducting an AI security\ninvestigation of prompt injects, these guardrails lack the ability to aid\ninvestigators in triaging or assessing the identified input prompts. In this\napplied research exploration, we explore the use of a text generation\ncapabilities of LLM to detect prompt injects and generate explanation for its\ndetections to aid AI security investigators in assessing and triaging of such\nprompt inject detections. The practical benefit of such a tool is to ease the\ntask of conducting investigation into prompt injects.\n","authors":["Jonathan Pan","Swee Liang Wong","Yidi Yuan","Xin Wei Chia"],"pdf_url":"https://arxiv.org/pdf/2502.11006v1.pdf","comment":"5 pages, 4 tables, 3 diagrams"},{"id":"http://arxiv.org/abs/2502.10997v1","updated":"2025-02-16T05:13:51Z","published":"2025-02-16T05:13:51Z","title":"New Rates in Stochastic Decision-Theoretic Online Learning under\n  Differential Privacy","summary":"  Hu and Mehta (2024) posed an open problem: what is the optimal\ninstance-dependent rate for the stochastic decision-theoretic online learning\n(with $K$ actions and $T$ rounds) under $\\varepsilon$-differential privacy?\nBefore, the best known upper bound and lower bound are $O\\left(\\frac{\\log\nK}{\\Delta_{\\min}} + \\frac{\\log K\\log T}{\\varepsilon}\\right)$ and\n$\\Omega\\left(\\frac{\\log K}{\\Delta_{\\min}} + \\frac{\\log K}{\\varepsilon}\\right)$\n(where $\\Delta_{\\min}$ is the gap between the optimal and the second actions).\nIn this paper, we partially address this open problem by having two new\nresults. First, we provide an improved upper bound for this problem\n$O\\left(\\frac{\\log K}{\\Delta_{\\min}} + \\frac{\\log^2K}{\\varepsilon}\\right)$,\nwhere the $T$-dependency has been removed. Second, we introduce the\ndeterministic setting, a weaker setting of this open problem, where the\nreceived loss vector is deterministic and we can focus on the analysis for\n$\\varepsilon$ regardless of the sampling error. At the deterministic setting,\nwe prove upper and lower bounds that match at $\\Theta\\left(\\frac{\\log\nK}{\\varepsilon}\\right)$, while a direct application of the analysis and\nalgorithms from the original setting still leads to an extra log factor.\nTechnically, we introduce the Bernoulli resampling trick, which enforces a\nmonotonic property for the output from report-noisy-max mechanism that enables\na tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise,\nwe derived explicit integral form that gives a tight characterization of the\nregret in the deterministic case.\n","authors":["Ruihan Wu","Yu-Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.10997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10984v1","updated":"2025-02-16T04:02:56Z","published":"2025-02-16T04:02:56Z","title":"Sound Conveyors for Stealthy Data Transmission","summary":"  Hiding messages for countless security purposes has become a highly\nfascinating subject nowadays. Encryption facilitates the data hiding. With the\nexpress development of technology, people tend to figure out a method capable\nof hiding a message and the survival of the message. The present-day study is\nconducted to hide information in an audio file. Generally, steganography\nadvantages are not used among industry and learners even though it is an\nextensively discussed area in the present information world. This\nimplementation aims to hide a document such as txt, doc, and pdf file formats\nin an audio file and retrieve the hidden document when necessary. This system\nis called DeepAudio v1.0. The system supports AES encryption and tolerates both\nwave and MP3 files. The sub-aims of this work were the creation of a free,\nopenly available, bug-free software tool with additional features that are new\nto the area.\n","authors":["Sachith Dassanayaka"],"pdf_url":"https://arxiv.org/pdf/2502.10984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12701v3","updated":"2025-02-16T03:19:01Z","published":"2024-11-19T18:11:36Z","title":"When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations","summary":"  Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability.\n","authors":["Huaizhi Ge","Yiming Li","Qifan Wang","Yongfeng Zhang","Ruixiang Tang"],"pdf_url":"https://arxiv.org/pdf/2411.12701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04234v2","updated":"2025-02-16T02:16:15Z","published":"2024-10-05T17:22:39Z","title":"Functional Homotopy: Smoothing Discrete Optimization via Continuous\n  Parameters for LLM Jailbreak Attacks","summary":"  Optimization methods are widely employed in deep learning to identify and\nmitigate undesired model responses. While gradient-based techniques have proven\neffective for image models, their application to language models is hindered by\nthe discrete nature of the input space. This study introduces a novel\noptimization approach, termed the \\emph{functional homotopy} method, which\nleverages the functional duality between model training and input generation.\nBy constructing a series of easy-to-hard optimization problems, we iteratively\nsolve these problems using principles derived from established homotopy\nmethods. We apply this approach to jailbreak attack synthesis for large\nlanguage models (LLMs), achieving a $20\\%-30\\%$ improvement in success rate\nover existing methods in circumventing established safe open-source models such\nas Llama-2 and Llama-3.\n","authors":["Zi Wang","Divyam Anshumaan","Ashish Hooda","Yudong Chen","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2410.04234v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2302.01177v5","updated":"2025-02-16T02:03:20Z","published":"2023-02-02T15:54:27Z","title":"The Case of FBA as a DEX Processing Model","summary":"  We investigate the welfare loss of continuous and discrete order matching\nmodels in blockchain-based decentralized exchanges (DEX) that utilize order\nbooks to record outstanding orders. Continuous processing matches each incoming\ntransaction against the current order book. The discrete processing model,\ni.e., frequent batch auction (FBA), executes transactions discretely in batches\nwith a uniform price double auction: Orders are first matched according to\nprice, then the exact transaction order if competing orders specify the same\nprice.\n  We find that FBA imposes less welfare loss and provides better liquidity than\ncontinuous processing in typical scenarios, e.g., when few parties are\nprivately informed about asset valuations. Even otherwise, it achieves better\nsocial welfare and liquidity provision in the following settings: when price\ntakers and public information reflecting asset value changes arrive\nsufficiently frequently compared to private information, when the priority fees\n(for faster transaction inclusion into blockchains) are small, or when the\nmarket is more balanced on both buy and sell sides. Our empirical analysis on\nthe BTC-USD and ETH-USD transactions on a DEX named dYdX indicates that FBA can\nreduce transaction costs by $21\\%-37\\%$.\n","authors":["Tiantian Gong","Zeyu Liu","Aniket Kate"],"pdf_url":"https://arxiv.org/pdf/2302.01177v5.pdf","comment":"13 pages, 28 figures"}]},"2025-02-15T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.10931v1","updated":"2025-02-15T23:43:18Z","published":"2025-02-15T23:43:18Z","title":"D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and\n  Heterogeneous Execution for Enhanced Reasoning in Offensive Security","summary":"  Large Language Models (LLMs) have been used in cybersecurity in many ways,\nincluding their recent use as intelligent agent systems for autonomous security\nanalysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing\nthe automated task-planning abilities of LLM agents across various\ncybersecurity skill sets. Early attempts to apply LLMs for solving CTF\nchallenges relied on single-agent systems, where feedback was restricted to a\nsingle reasoning-action loop. This approach proved inadequate for handling\ncomplex CTF tasks. Drawing inspiration from real-world CTF competitions, where\nteams of experts collaborate, we introduce the D-CIPHER multi-agent LLM\nframework for collaborative CTF challenge solving. D-CIPHER integrates agents\nwith distinct roles, enabling dynamic feedback loops to enhance reasoning on\nCTF challenges. It introduces the Planner-Executor agent system, consisting of\na Planner agent for overall problem-solving along with multiple heterogeneous\nExecutor agents for individual tasks, facilitating efficient allocation of\nresponsibilities among the LLMs. Additionally, D-CIPHER incorporates an\nAuto-prompter agent, which improves problem-solving by exploring the challenge\nenvironment and generating a highly relevant initial prompt. We evaluate\nD-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive\nstudies to highlight the impact of our enhancements. Our results demonstrate\nthat the multi-agent D-CIPHER system achieves a significant improvement in\nchallenges solved, setting a state-of-the-art performance on three benchmarks:\n22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is\navailable at https://github.com/NYU-LLM-CTF/nyuctf_agents as the\nnyuctf_multiagent package.\n","authors":["Meet Udeshi","Minghao Shao","Haoran Xi","Nanda Rani","Kimberly Milner","Venkata Sai Charan Putrevu","Brendan Dolan-Gavitt","Sandeep Kumar Shukla","Prashanth Krishnamurthy","Farshad Khorrami","Ramesh Karri","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2502.10931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15417v3","updated":"2025-02-15T19:45:53Z","published":"2022-11-13T13:51:38Z","title":"Proof-of-randomness protocol for blockchain consensus: a case of Macau\n  algorithms","summary":"  A proof-of-randomness (PoR) protocol is presented as a fair and low\nenergy-cost consensus mechanism for blockchains. Each network node of a\nblockchain may use a true random number generator (TRNG) and hash algorism to\nfulfil the PoR protocol. In this paper, we give the consensus mechanism of the\nPoR protocol, and classify it into a new kind of randomized algorithms called\nMacau. The PoR protocol could generate a blockchain without any competition of\ncomputing power or stake of cryptocurrency. Besides, we give some advantages of\nintegrating quantum random number generator (QRNG) chips into hardware wallets,\nand also discuss the way to cooperate with quantum key distribution (QKD)\ntechnology.\n","authors":["Wen-Zhuo Zhang","Victor Kai"],"pdf_url":"https://arxiv.org/pdf/2211.15417v3.pdf","comment":"8 pages, 2 figure"},{"id":"http://arxiv.org/abs/2502.07492v2","updated":"2025-02-15T15:36:48Z","published":"2025-02-11T11:51:12Z","title":"RoMA: Robust Malware Attribution via Byte-level Adversarial Training\n  with Global Perturbations and Adversarial Consistency Regularization","summary":"  Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios.\n","authors":["Yuxia Sun","Huihong Chen","Jingcai Guo","Aoxiang Sun","Zhetao Li","Haolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07492v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.10825v1","updated":"2025-02-15T15:01:04Z","published":"2025-02-15T15:01:04Z","title":"MITRE ATT&CK Applications in Cybersecurity and The Way Forward","summary":"  The MITRE ATT&CK framework is a widely adopted tool for enhancing\ncybersecurity, supporting threat intelligence, incident response, attack\nmodeling, and vulnerability prioritization. This paper synthesizes research on\nits application across these domains by analyzing 417 peer-reviewed\npublications. We identify commonly used adversarial tactics, techniques, and\nprocedures (TTPs) and examine the integration of natural language processing\n(NLP) and machine learning (ML) with ATT&CK to improve threat detection and\nresponse. Additionally, we explore the interoperability of ATT&CK with other\nframeworks, such as the Cyber Kill Chain, NIST guidelines, and STRIDE,\nhighlighting its versatility. The paper further evaluates the framework from\nmultiple perspectives, including its effectiveness, validation methods, and\nsector-specific challenges, particularly in industrial control systems (ICS)\nand healthcare. We conclude by discussing current limitations and proposing\nfuture research directions to enhance the applicability of ATT&CK in dynamic\ncybersecurity environments.\n","authors":["Yuning Jiang","Qiaoran Meng","Feiyang Shang","Nay Oo","Le Thi Hong Minh","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2502.10825v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2411.09359v2","updated":"2025-02-15T14:46:44Z","published":"2024-11-14T11:06:34Z","title":"Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation\n  Attack against EaaS Watermark","summary":"  Embedding-as-a-Service (EaaS) has emerged as a successful business pattern\nbut faces significant challenges related to various forms of copyright\ninfringement, particularly, the API misuse and model extraction attacks.\nVarious studies have proposed backdoor-based watermarking schemes to protect\nthe copyright of EaaS services. In this paper, we reveal that previous\nwatermarking schemes possess semantic-independent characteristics and propose\nthe Semantic Perturbation Attack (SPA). Our theoretical and experimental\nanalysis demonstrate that this semantic-independent nature makes current\nwatermarking schemes vulnerable to adaptive attacks that exploit semantic\nperturbations tests to bypass watermark verification. Extensive experimental\nresults across multiple datasets demonstrate that the True Positive Rate (TPR)\nfor identifying watermarked samples under SPA can reach up to more than 95\\%,\nrendering watermarks ineffective while maintaining the high utility of\nembeddings. Furthermore, we discuss potential defense strategies to mitigate\nSPA. Our code is available at\nhttps://github.com/Zk4-ps/EaaS-Embedding-Watermark.\n","authors":["Zekun Fei","Biao Yi","Jianing Geng","Ruiqi He","Lihai Nie","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2411.09359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16028v2","updated":"2025-02-15T14:28:31Z","published":"2024-08-28T03:28:17Z","title":"ANVIL: Anomaly-based Vulnerability Identification without Labelled\n  Training Data","summary":"  Supervised learning-based software vulnerability detectors often fall short\ndue to the inadequate availability of labelled training data. In contrast,\nLarge Language Models (LLMs) such as GPT-4, are not trained on labelled data,\nbut when prompted to detect vulnerabilities, LLM prediction accuracy is only\nmarginally better than random guessing. In this paper, we explore a different\napproach by reframing vulnerability detection as one of anomaly detection.\nSince the vast majority of code does not contain vulnerabilities and LLMs are\ntrained on massive amounts of such code, vulnerable code can be viewed as an\nanomaly from the LLM's predicted code distribution, freeing the model from the\nneed for labelled data to provide a learnable representation of vulnerable\ncode. Leveraging this perspective, we demonstrate that LLMs trained for code\ngeneration exhibit a significant gap in prediction accuracy when prompted to\nreconstruct vulnerable versus non-vulnerable code.\n  Using this insight, we implement ANVIL, a detector that identifies software\nvulnerabilities at line-level granularity. Our experiments explore the\ndiscriminating power of different anomaly scoring methods, as well as the\nsensitivity of ANVIL to context size. We also study the effectiveness of ANVIL\non various LLM families, and conduct leakage experiments on vulnerabilities\nthat were discovered after the knowledge cutoff of our evaluated LLMs. On a\ncollection of vulnerabilities from the Magma benchmark, ANVIL outperforms\nstate-of-the-art line-level vulnerability detectors, LineVul and LineVD, which\nhave been trained with labelled data, despite ANVIL having never been trained\nwith labelled vulnerabilities. Specifically, our approach achieves $1.62\\times$\nto $2.18\\times$ better Top-5 accuracies and $1.02\\times$ to $1.29\\times$ times\nbetter ROC scores on line-level vulnerability detection tasks.\n","authors":["Weizhou Wang","Eric Liu","Xiangyu Guo","Xiao Hu","Ilya Grishchenko","David Lie"],"pdf_url":"https://arxiv.org/pdf/2408.16028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10803v1","updated":"2025-02-15T13:55:34Z","published":"2025-02-15T13:55:34Z","title":"PDA: Generalizable Detection of AI-Generated Images via Post-hoc\n  Distribution Alignment","summary":"  The rapid advancement of generative models has led to the proliferation of\nhighly realistic AI-generated images, posing significant challenges for\ndetection methods to generalize across diverse and evolving generative\ntechniques. Existing approaches often fail to adapt to unknown models without\ncostly retraining, limiting their practicability. To fill this gap, we propose\nPost-hoc Distribution Alignment (PDA), a novel approach for the generalizable\ndetection for AI-generated images. The key idea is to use the known generative\nmodel to regenerate undifferentiated test images. This process aligns the\ndistributions of the re-generated real images with the known fake images,\nenabling effective distinction from unknown fake images. PDA employs a two-step\ndetection framework: 1) evaluating whether a test image aligns with the known\nfake distribution based on deep k-nearest neighbor (KNN) distance, and 2)\nre-generating test images using known generative models to create pseudo-fake\nimages for further classification. This alignment strategy allows PDA to\neffectively detect fake images without relying on unseen data or requiring\nretraining. Extensive experiments demonstrate the superiority of PDA, achieving\n96.73\\% average accuracy across six state-of-the-art generative models,\nincluding GANs, diffusion models, and text-to-image models, and improving by\n16.07\\% over the best baseline. Through t-SNE visualizations and KNN distance\nanalysis, we provide insights into PDA's effectiveness in separating real and\nfake images. Our work provides a flexible and effective solution for real-world\nfake image detection, advancing the generalization ability of detection\nsystems.\n","authors":["Li Wang","Wenyu Chen","Zheng Li","Shanqing Guo"],"pdf_url":"https://arxiv.org/pdf/2502.10803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10801v1","updated":"2025-02-15T13:45:19Z","published":"2025-02-15T13:45:19Z","title":"FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through\n  Identity Obfuscation","summary":"  DeepFakes pose a significant threat to our society. One representative\nDeepFake application is face-swapping, which replaces the identity in a facial\nimage with that of a victim. Although existing methods partially mitigate these\nrisks by degrading the quality of swapped images, they often fail to disrupt\nthe identity transformation effectively. To fill this gap, we propose\nFaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake\nface-swapping threats. Specifically, FSG introduces imperceptible perturbations\nto a user's facial image, disrupting the features extracted by identity\nencoders. When shared online, these perturbed images mislead face-swapping\ntechniques, causing them to generate facial images with identities\nsignificantly different from the original user. Extensive experiments\ndemonstrate the effectiveness of FSG against multiple face-swapping techniques,\nreducing the face match rate from 90\\% (without defense) to below 10\\%. Both\nqualitative and quantitative studies further confirm its ability to confuse\nhuman perception, highlighting its practical utility. Additionally, we\ninvestigate key factors that may influence FSG and evaluate its robustness\nagainst various adaptive adversaries.\n","authors":["Li Wang","Zheng Li","Xuhong Zhang","Shouling Ji","Shanqing Guo"],"pdf_url":"https://arxiv.org/pdf/2502.10801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10771v1","updated":"2025-02-15T11:26:30Z","published":"2025-02-15T11:26:30Z","title":"Assessing the Trustworthiness of Electronic Identity Management Systems:\n  Framework and Insights from Inception to Deployment","summary":"  The growing dependence on Electronic Identity Management Systems (EIDS) and\nrecent advancements, such as non-human ID management, require a thorough\nevaluation of their trustworthiness. Assessing EIDS's trustworthiness ensures\nsecurity, privacy, and reliability in managing sensitive user information. It\nsafeguards against fraud, unauthorised access, and data breaches, fostering\nuser confidence. Existing frameworks primarily focus on specific dimensions\nsuch as security and privacy, often neglecting critical dimensions such as\nethics, resilience, robustness, and reliability. This paper introduces an\nintegrated Digital Identity Systems Trustworthiness Assessment Framework\n(DISTAF) encapsulating these six pillars. It is supported by over 65 mechanisms\nand over 400 metrics derived from international standards and technical\nguidelines. By addressing the lifecycle of DIMS from design to deployment, our\nDISTAF evaluates trustworthiness at granular levels while remaining accessible\nto diverse stakeholders. We demonstrate the application of DISTAF through a\nreal-world implementation using a Modular Open Source Identity Platform (MOSIP)\ninstance, refining its metrics to simplify trustworthiness assessment. Our\napproach introduces clustering mechanisms for metrics, hierarchical scoring,\nand mandatory criteria to ensure robust and consistent evaluations across an\nEIDS in both the design and operation stages. Furthermore, DISTAF is adaptable\nto emerging technologies like Self-Sovereign Identity (SSI), integrating\nprivacy-enhancing techniques and ethical considerations to meet modern\nchallenges. The assessment tool developed alongside DISTAF provides a\nuser-centric methodology and a simplified yet effective self-assessment\nprocess, enabling system designers and assessors to identify system gaps,\nimprove configurations, and enhance public trust.\n","authors":["Mirko Bottarelli","Gregory Epiphaniou","Shah Mahmood","Mark Hooper","Carsten Maple"],"pdf_url":"https://arxiv.org/pdf/2502.10771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01882v2","updated":"2025-02-15T10:06:51Z","published":"2024-06-04T01:31:20Z","title":"HoneyGPT: Breaking the Trilemma in Terminal Honeypots with Large\n  Language Model","summary":"  Honeypots, as a strategic cyber-deception mechanism designed to emulate\nauthentic interactions and bait unauthorized entities, often struggle with\nbalancing flexibility, interaction depth, and deception. They typically fail to\nadapt to evolving attacker tactics, with limited engagement and information\ngathering. Fortunately, the emergent capabilities of large language models and\ninnovative prompt-based engineering offer a transformative shift in honeypot\ntechnologies. This paper introduces HoneyGPT, a pioneering shell honeypot\narchitecture based on ChatGPT, characterized by its cost-effectiveness and\nproactive engagement. In particular, we propose a structured prompt engineering\nframework that incorporates chain-of-thought tactics to improve long-term\nmemory and robust security analytics, enhancing deception and engagement. Our\nevaluation of HoneyGPT comprises a baseline comparison based on a collected\ndataset and a three-month field evaluation. The baseline comparison\ndemonstrates HoneyGPT's remarkable ability to strike a balance among\nflexibility, interaction depth, and deceptive capability. The field evaluation\nfurther validates HoneyGPT's superior performance in engaging attackers more\ndeeply and capturing a wider array of novel attack vectors.\n","authors":["Ziyang Wang","Jianzhou You","Haining Wang","Tianwei Yuan","Shichao Lv","Yang Wang","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2406.01882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10722v1","updated":"2025-02-15T08:26:27Z","published":"2025-02-15T08:26:27Z","title":"PMU-Data: Data Traces Could be Distinguished","summary":"  Modern processors widely equip the Performance Monitoring Unit (PMU) to\ncollect various architecture and microarchitecture events. Software developers\noften utilize the PMU to enhance program's performance, but the potential side\neffects that arise from its activation are often disregarded. In this paper, we\nfind that the PMU can be employed to retrieve instruction operands. Based on\nthis discovery, we introduce PMU-Data, a novel category of side-channel attacks\naimed at leaking secret by identifying instruction operands with PMU.\n  To achieve the PMU-Data attack, we develop five gadgets to encode the\nconfidential data into distinct data-related traces while maintaining the\ncontrol-flow unchanged. We then measure all documented PMU events on three\nphysical machines with different processors while those gadgets are performing.\nWe successfully identify two types of vulnerable gadgets caused by DIV and MOV\ninstructions. Additionally, we discover 40 vulnerable PMU events that can be\nused to carry out the PMU-Data attack. We through real experiments to\ndemonstrate the perniciousness of the PMU-Data attack by implementing three\nattack goals: (1) leaking the kernel data illegally combined with the transient\nexecution vulnerabilities including Meltdown, Spectre, and Zombieload; (2)\nbuilding a covert-channel to secretly transfer data; (3) extracting the secret\ndata protected by the Trusted Execution Environment (TEE) combined with the\nZombieload vulnerability.\n","authors":["Zhouyang Li","Pengfei Qiu","Yu Qing","Chunlu Wang","Dongsheng Wang","Xiao Zhang","Gang Qu"],"pdf_url":"https://arxiv.org/pdf/2502.10722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10719v1","updated":"2025-02-15T08:22:07Z","published":"2025-02-15T08:22:07Z","title":"Reverse Engineering the Apple M1 Conditional Branch Predictor for\n  Out-of-Place Spectre Mistraining","summary":"  Spectre v1 information disclosure attacks, which exploit CPU conditional\nbranch misprediction, remain unsolved in deployed software. Certain Spectre v1\ngadgets can be exploited only by out-of-place mistraining, in which the\nattacker controls a victim branch's prediction, possibly from another address\nspace, by training a branch that aliases with the victim in the branch\npredictor unit (BPU) structure. However, constructing a BPU-alias for a victim\nbranch is hard. Consequently, practical out-of-place mistraining attacks use\nbrute-force searches to randomly achieve aliasing. To date, such attacks have\nbeen demonstrated only on Intel x86 CPUs.\n  This paper explores the vulnerability of Apple M-Series CPUs to practical\nout-of-place Spectre v1 mistraining. We show that brute-force out-of-place\nmistraining fails on the M1. We analytically explain the failure is due to the\nsearch space size, assuming (based on Apple patents) that the M1 CPU uses a\nvariant of the TAGE conditional branch predictor. Based on our analysis, we\ndesign a new BPU-alias search technique with reduced search space. Our\ntechnique requires knowledge of certain M1 BPU parameters and mechanisms, which\nwe reverse engineer. We also use our newfound ability to perform out-of-place\nSpectre v1 mistraining to test if the M1 CPU implements hardware mitigations\nagainst cross-address space out-of-place mistraining -- and find evidence for\npartial mitigations.\n","authors":["Adam Tuby","Adam Morrison"],"pdf_url":"https://arxiv.org/pdf/2502.10719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10711v1","updated":"2025-02-15T07:50:55Z","published":"2025-02-15T07:50:55Z","title":"A Computational Model for Ransomware Detection Using Cross-Domain\n  Entropy Signatures","summary":"  Detecting encryption-driven cyber threats remains a large challenge due to\nthe evolving techniques employed to evade traditional detection mechanisms. An\nentropy-based computational framework was introduced to analyze multi-domain\nsystem variations, enabling the identification of malicious encryption\nbehaviors through entropy deviations. By integrating entropy patterns across\nfile operations, memory allocations, and network transmissions, a detection\nmethodology was developed to differentiate between benign and\nransomware-induced entropy shifts. A mathematical model was formulated to\nquantify entropy dynamics, incorporating time-dependent variations and weighted\ndomain contributions to enhance anomaly detection. Experimental evaluations\ndemonstrated that the proposed approach achieved high accuracy across diverse\nransomware families while maintaining low false positive rates. Computational\nefficiency analysis indicated minimal processing overhead, suggesting\nfeasibility for real-time implementation in security-sensitive environments.\nThe study highlighted entropy fluctuations as a useful indicator for\nidentifying malicious encryption processes, reinforcing entropy-driven\nmethodologies as a viable component of cybersecurity strategies.\n","authors":["Michael Mannon","Evan Statham","Quentin Featherstone","Sebastian Arkwright","Clive Fenwick","Gareth Willoughby"],"pdf_url":"https://arxiv.org/pdf/2502.10711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10673v1","updated":"2025-02-15T04:56:45Z","published":"2025-02-15T04:56:45Z","title":"Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs","summary":"  Retrieval-Augmented Generation (RAG) has become an effective method for\nenhancing large language models (LLMs) with up-to-date knowledge. However, it\nposes a significant risk of IP infringement, as IP datasets may be incorporated\ninto the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs)\nwithout authorization. To protect the rights of the dataset owner, an effective\ndataset membership inference algorithm for RA-LLMs is needed. In this work, we\nintroduce a novel approach to safeguard the ownership of text datasets and\neffectively detect unauthorized use by the RA-LLMs. Our approach preserves the\noriginal data completely unchanged while protecting it by inserting\nspecifically designed canary documents into the IP dataset. These canary\ndocuments are created with synthetic content and embedded watermarks to ensure\nuniqueness, stealthiness, and statistical provability. During the detection\nprocess, unauthorized usage is identified by querying the canary documents and\nanalyzing the responses of RA-LLMs for statistical evidence of the embedded\nwatermark. Our experimental results demonstrate high query efficiency,\ndetectability, and stealthiness, along with minimal perturbation to the\noriginal dataset, all without compromising the performance of the RAG system.\n","authors":["Yepeng Liu","Xuandong Zhao","Dawn Song","Yuheng Bu"],"pdf_url":"https://arxiv.org/pdf/2502.10673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10646v1","updated":"2025-02-15T02:47:14Z","published":"2025-02-15T02:47:14Z","title":"Dark Deceptions in DHCP: Dismantling Network Defenses","summary":"  This paper explores vulnerabilities in the Dynamic Host Configuration\nProtocol (DHCP) and their implications on the Confidentiality, Integrity, and\nAvailability (CIA) triad. Through an analysis of various attacks, including\nDHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits,\nthe paper provides a taxonomic classification of threats, assesses risks, and\nproposes appropriate controls. The discussion also highlights the dangers of\nVPN decloaking through DHCP exploits and underscores the importance of\nsafeguarding network infrastructures. By bringing awareness to the TunnelVision\nexploit, this paper aims to mitigate risks associated with these prevalent\nvulnerabilities.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2502.10646v1.pdf","comment":"8 pages, 4 tables"},{"id":"http://arxiv.org/abs/2502.10637v1","updated":"2025-02-15T02:25:57Z","published":"2025-02-15T02:25:57Z","title":"Proof of Response","summary":"  We present a mechanism that for a network of participants allows one\nparticipant of the network (Alice) to request some data from another\nparticipant (Bob) and either receive a response from Bob within a\nknown-in-advance, bounded time b, or receive a proof that at least one edge on\nthe way to Bob was broken within b, or receive a streaming payment proportional\nto time passed beyond b during which neither was received. This mechanism\nallows for building downstream applications that require provable responses\nfrom other participants, such as decentralized storage solutions, decentralized\nAI agents, and more.\n","authors":["Illia Polosukhin","Alex Skidanov"],"pdf_url":"https://arxiv.org/pdf/2502.10637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10635v1","updated":"2025-02-15T02:25:27Z","published":"2025-02-15T02:25:27Z","title":"Privacy Preservation through Practical Machine Unlearning","summary":"  Machine Learning models thrive on vast datasets, continuously adapting to\nprovide accurate predictions and recommendations. However, in an era dominated\nby privacy concerns, Machine Unlearning emerges as a transformative approach,\nenabling the selective removal of data from trained models. This paper examines\nmethods such as Naive Retraining and Exact Unlearning via the SISA framework,\nevaluating their Computational Costs, Consistency, and feasibility using the\n\\texttt{HSpam14} dataset. We explore the potential of integrating unlearning\nprinciples into Positive Unlabeled (PU) Learning to address challenges posed by\npartially labeled datasets. Our findings highlight the promise of unlearning\nframeworks like \\textit{DaRE} for ensuring privacy compliance while maintaining\nmodel performance, albeit with significant computational trade-offs. This study\nunderscores the importance of Machine Unlearning in achieving ethical AI and\nfostering trust in data-driven systems.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2502.10635v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.10624v1","updated":"2025-02-15T01:25:13Z","published":"2025-02-15T01:25:13Z","title":"Network evasion detection with Bi-LSTM model","summary":"  Network evasion detection aims to distinguish whether the network flow comes\nfrom link layer exists network evasion threat, which is a means to disguise the\ndata traffic on detection system by confusing the signature. Since the previous\nresearch works has all sorts of frauds, we propose a architecture with deep\nlearning network to handle this problem. In this paper, we extract the critical\ninformation as key features from data frame and also specifically propose to\nuse bidirectional long short-term memory (Bi-LSTM) neural network which shows\nan outstanding performance to trace the serial information, to encode both the\npast and future trait on the network flows. Furthermore we introduce a\nclassifier named Softmax at the bottom of Bi-LSTM, holding a character to\nselect the correct class. All experiments results shows that we can achieve a\nsignificant performance with a deep Bi-LSTM in network evasion detection and\nit's average accuracy reaches 96.1%.\n","authors":["Kehua Chen","Jingping Jia"],"pdf_url":"https://arxiv.org/pdf/2502.10624v1.pdf","comment":"4 pages,5 figures"},{"id":"http://arxiv.org/abs/2501.09895v3","updated":"2025-02-15T01:20:12Z","published":"2025-01-17T00:51:37Z","title":"Optimizing Secure Quantum Information Transmission in\n  Entanglement-Assisted Quantum Networks","summary":"  Quantum security improves cryptographic protocols by applying quantum\nmechanics principles, assuring resistance to both quantum and conventional\ncomputer attacks. This work addresses these issues by integrating Quantum Key\nDistribution (QKD) utilizing the E91 method with Multi-Layer Chaotic\nEncryption, which employs a variety of patterns to detect eavesdropping,\nresulting in a highly secure image-transmission architecture. The method\nleverages entropy calculations to determine the unpredictability and integrity\nof encrypted and decrypted pictures, guaranteeing strong security. Extensive\nstatistical scenarios illustrate the framework's effectiveness in image\nencryption while preserving high entropy and sensitivity to the original\nvisuals. The findings indicate significant improvement in encryption and\ndecryption performance, demonstrating the framework's potential as a robust\nresponse to weaknesses introduced by advances in quantum computing. Several\nmetrics, such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index\n(SSIM), Normalized Cross-Correlation (NCC), Bit Error Rate (BER), entropy\nvalues for original, encrypted, and decrypted images, and the correlation\nbetween original and decrypted images, validate the framework's effectiveness.\nThe combination of QKD with Multi-Layer Chaotic Encryption provides a scalable\nand resilient technique to secure image communication. As quantum computing\nadvances, this framework offers a future-proof approach for defining secure\ncommunication protocols in crucial sectors such as medical treatment, forensic\ncomputing, and national security, where information confidentiality is\nvaluable.\n","authors":["Tasmin Karim","Md. Shazzad Hossain Shaon","Md. Fahim Sultan","Mst Shapna Akter"],"pdf_url":"https://arxiv.org/pdf/2501.09895v3.pdf","comment":"no"}]},"2025-02-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.13141v1","updated":"2025-02-18T18:59:00Z","published":"2025-02-18T18:59:00Z","title":"UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models","summary":"  Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.\n","authors":["Huawei Lin","Yingjie Lao","Tong Geng","Tan Yu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.13141v1.pdf","comment":"18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks"},{"id":"http://arxiv.org/abs/2502.13135v1","updated":"2025-02-18T18:56:44Z","published":"2025-02-18T18:56:44Z","title":"Sleepless Nights, Sugary Days: Creating Synthetic Users with Health\n  Conditions for Realistic Coaching Agent Interactions","summary":"  We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.\n","authors":["Taedong Yun","Eric Yang","Mustafa Safdari","Jong Ha Lee","Vaishnavi Vinod Kumar","S. Sara Mahdavi","Jonathan Amar","Derek Peyton","Reut Aharony","Andreas Michaelides","Logan Schneider","Isaac Galatzer-Levy","Yugang Jia","John Canny","Arthur Gretton","Maja Matari"],"pdf_url":"https://arxiv.org/pdf/2502.13135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13131v1","updated":"2025-02-18T18:55:26Z","published":"2025-02-18T18:55:26Z","title":"Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis","summary":"  Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.\n","authors":["Feng Luo","Rui Yang","Hao Sun","Chunyuan Deng","Jiarui Yao","Jingyan Shen","Huan Zhang","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.13131v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.12118v2","updated":"2025-02-18T18:54:12Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13127v1","updated":"2025-02-18T18:50:06Z","published":"2025-02-18T18:50:06Z","title":"Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning","summary":"  Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset.\n","authors":["Jingyang Lin","Andy Wong","Tian Xia","Shenghua He","Hui Wei","Mei Han","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2502.13127v1.pdf","comment":"15 Pages, 6 Tables, 8 Figures"},{"id":"http://arxiv.org/abs/2502.13125v1","updated":"2025-02-18T18:47:11Z","published":"2025-02-18T18:47:11Z","title":"RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises","summary":"  Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%.\n","authors":["Zenan Zhai","Hao Li","Xudong Han","Zhenxuan Zhang","Yixuan Zhang","Timothy Baldwin","Haonan Li"],"pdf_url":"https://arxiv.org/pdf/2502.13125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13124v1","updated":"2025-02-18T18:46:57Z","published":"2025-02-18T18:46:57Z","title":"NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions","summary":"  Scaling reasoning capabilities beyond traditional domains such as math and\ncoding is hindered by the lack of diverse and high-quality questions. To\novercome this limitation, we introduce a scalable approach for generating\ndiverse and challenging reasoning questions, accompanied by reference answers.\nWe present NaturalReasoning, a comprehensive dataset comprising 2.8 million\nquestions that span multiple domains, including STEM fields (e.g., Physics,\nComputer Science), Economics, Social Sciences, and more. We demonstrate the\nutility of the questions in NaturalReasoning through knowledge distillation\nexperiments which show that NaturalReasoning can effectively elicit and\ntransfer reasoning capabilities from a strong teacher model. Furthermore, we\ndemonstrate that NaturalReasoning is also effective for unsupervised\nself-training using external reward models or self-rewarding.\n","authors":["Weizhe Yuan","Jane Yu","Song Jiang","Karthik Padthe","Yang Li","Dong Wang","Ilia Kulikov","Kyunghyun Cho","Yuandong Tian","Jason E Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.13124v1.pdf","comment":"Dataset at https://huggingface.co/datasets/facebook/natural_reasoning"},{"id":"http://arxiv.org/abs/2502.13120v1","updated":"2025-02-18T18:42:11Z","published":"2025-02-18T18:42:11Z","title":"Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context","summary":"  Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.\n","authors":["Marion Bartl","Thomas Brendan Murphy","Susan Leavy"],"pdf_url":"https://arxiv.org/pdf/2502.13120v1.pdf","comment":"9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)"},{"id":"http://arxiv.org/abs/2502.13119v1","updated":"2025-02-18T18:42:09Z","published":"2025-02-18T18:42:09Z","title":"STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models","summary":"  How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics.\n","authors":["Narun Raman","Taylor Lundy","Thiago Amin","Jesse Perla","Kevin-Leyton Brown"],"pdf_url":"https://arxiv.org/pdf/2502.13119v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.13114v1","updated":"2025-02-18T18:33:50Z","published":"2025-02-18T18:33:50Z","title":"The influence of motion features in temporal perception","summary":"  This paper examines the role of manner-of-motion verbs in shaping subjective\ntemporal perception and emotional resonance. Through four complementary\nstudies, we explore how these verbs influence the conceptualization of time,\nexamining their use in literal and metaphorical (temporal) contexts. Our\nfindings reveal that faster verbs (e.g., fly, zoom) evoke dynamic and engaging\ntemporal experiences, often linked to positive emotions and greater agency. In\ncontrast, slower verbs (e.g., crawl, drag) convey passivity, monotony, and\nnegative emotions, reflecting tedious or constrained experiences of time. These\neffects are amplified in metaphorical contexts, where manner verbs encode\nemotional and experiential nuances that transcend their literal meanings. We\nalso find that participants prefer manner verbs over path verbs (e.g., go,\npass) in emotionally charged temporal contexts, as manner verbs capture the\nexperiential and emotional qualities of time more effectively. These findings\nhighlight the interplay between language, motion, and emotion in shaping\ntemporal perception, offering insights into how linguistic framing influences\nsubjective experiences of time.\n","authors":["Rosa Illan Castillo","Javier Valenzuela"],"pdf_url":"https://arxiv.org/pdf/2502.13114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01171v2","updated":"2025-02-18T18:32:25Z","published":"2024-10-02T01:59:07Z","title":"Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness","summary":"  The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines.\n","authors":["Bryan Li","Fiona Luo","Samar Haider","Adwait Agashe","Tammy Li","Runqi Liu","Muqing Miao","Shriya Ramakrishnan","Yuan Yuan","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.01171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13108v1","updated":"2025-02-18T18:20:37Z","published":"2025-02-18T18:20:37Z","title":"Improving Clinical Question Answering with Multi-Task Learning: A Joint\n  Approach for Answer Extraction and Medical Categorization","summary":"  Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.\n","authors":["Priyaranjan Pattnayak","Hitesh Laxmichand Patel","Amit Agarwal","Bhargava Kumar","Srikant Panda","Tejaswini Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.13108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08820v2","updated":"2025-02-18T18:08:56Z","published":"2025-02-12T22:18:34Z","title":"Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model","summary":"  Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents.\n","authors":["Emre Can Acikgoz","Jeremiah Greer","Akul Datta","Ze Yang","William Zeng","Oussama Elachqar","Emmanouil Koukoumidis","Dilek Hakkani-Tr","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2502.08820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v3","updated":"2025-02-18T18:07:34Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SciCap Challenge 2023","summary":"  Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v3.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.13095v1","updated":"2025-02-18T18:06:48Z","published":"2025-02-18T18:06:48Z","title":"Understanding and Rectifying Safety Perception Distortion in VLMs","summary":"  Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.\n","authors":["Xiaohan Zou","Jian Kang","George Kesidis","Lu Lin"],"pdf_url":"https://arxiv.org/pdf/2502.13095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13092v1","updated":"2025-02-18T17:59:48Z","published":"2025-02-18T17:59:48Z","title":"Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation","summary":"  Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.\n","authors":["Mengkang Hu","Tianxing Chen","Yude Zou","Yuheng Lei","Qiguang Chen","Ming Li","Hongyuan Zhang","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2502.13092v1.pdf","comment":"Project page: https://text-to-world.github.io/"},{"id":"http://arxiv.org/abs/2411.01077v2","updated":"2025-02-18T17:57:26Z","published":"2024-11-01T23:18:32Z","title":"Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection","summary":"  Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards.\n","authors":["Zhipeng Wei","Yuqi Liu","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2411.01077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.07269v4","updated":"2025-02-18T17:42:20Z","published":"2022-02-15T09:25:02Z","title":"Media Slant is Contagious","summary":"  This paper examines the diffusion of media slant. We document the influence\nof Fox News Channel (FNC) on the partisan slant of local newspapers in the U.S.\nover the years 1995-2008. We measure the political slant of local newspapers by\nscaling the news article texts to Republicans' and Democrats' speeches in\nCongress. Using channel positioning as an instrument for viewership, we find\nthat higher FNC viewership causes local newspapers to adopt more right-wing\nslant. The effect emerges gradually, only several years after FNC's\nintroduction, mirroring the channel's growing influence on voting behavior. A\nmain driver of the shift in newspaper slant appears to be a change in local\npolitical preferences.\n","authors":["Philine Widmer","Clmentine Abed Meraim","Sergio Galletta","Elliott Ash"],"pdf_url":"https://arxiv.org/pdf/2202.07269v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03371v3","updated":"2025-02-18T17:31:48Z","published":"2024-05-06T11:24:13Z","title":"Explainable Fake News Detection With Large Language Model via Defense\n  Among Competing Wisdom","summary":"  Most fake news detection methods learn latent feature representations based\non neural networks, which makes them black boxes to classify a piece of news\nwithout giving any justification. Existing explainable systems generate\nveracity justifications from investigative journalism, which suffer from\ndebunking delayed and low efficiency. Recent studies simply assume that the\njustification is equivalent to the majority opinions expressed in the wisdom of\ncrowds. However, the opinions typically contain some inaccurate or biased\ninformation since the wisdom of crowds is uncensored. To detect fake news from\na sea of diverse, crowded and even competing narratives, in this paper, we\npropose a novel defense-based explainable fake news detection framework.\nSpecifically, we first propose an evidence extraction module to split the\nwisdom of crowds into two competing parties and respectively detect salient\nevidences. To gain concise insights from evidences, we then design a\nprompt-based module that utilizes a large language model to generate\njustifications by inferring reasons towards two possible veracities. Finally,\nwe propose a defense-based inference module to determine veracity via modeling\nthe defense among these justifications. Extensive experiments conducted on two\nreal-world benchmarks demonstrate that our proposed method outperforms\nstate-of-the-art baselines in terms of fake news detection and provides\nhigh-quality justifications.\n","authors":["Bo Wang","Jing Ma","Hongzhan Lin","Zhiwei Yang","Ruichao Yang","Yuan Tian","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2405.03371v3.pdf","comment":"12 pages, WWW'2024"},{"id":"http://arxiv.org/abs/2412.19437v2","updated":"2025-02-18T17:26:38Z","published":"2024-12-27T04:03:16Z","title":"DeepSeek-V3 Technical Report","summary":"  We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.\n","authors":[" DeepSeek-AI","Aixin Liu","Bei Feng","Bing Xue","Bingxuan Wang","Bochao Wu","Chengda Lu","Chenggang Zhao","Chengqi Deng","Chenyu Zhang","Chong Ruan","Damai Dai","Daya Guo","Dejian Yang","Deli Chen","Dongjie Ji","Erhang Li","Fangyun Lin","Fucong Dai","Fuli Luo","Guangbo Hao","Guanting Chen","Guowei Li","H. Zhang","Han Bao","Hanwei Xu","Haocheng Wang","Haowei Zhang","Honghui Ding","Huajian Xin","Huazuo Gao","Hui Li","Hui Qu","J. L. Cai","Jian Liang","Jianzhong Guo","Jiaqi Ni","Jiashi Li","Jiawei Wang","Jin Chen","Jingchang Chen","Jingyang Yuan","Junjie Qiu","Junlong Li","Junxiao Song","Kai Dong","Kai Hu","Kaige Gao","Kang Guan","Kexin Huang","Kuai Yu","Lean Wang","Lecong Zhang","Lei Xu","Leyi Xia","Liang Zhao","Litong Wang","Liyue Zhang","Meng Li","Miaojun Wang","Mingchuan Zhang","Minghua Zhang","Minghui Tang","Mingming Li","Ning Tian","Panpan Huang","Peiyi Wang","Peng Zhang","Qiancheng Wang","Qihao Zhu","Qinyu Chen","Qiushi Du","R. J. Chen","R. L. Jin","Ruiqi Ge","Ruisong Zhang","Ruizhe Pan","Runji Wang","Runxin Xu","Ruoyu Zhang","Ruyi Chen","S. S. Li","Shanghao Lu","Shangyan Zhou","Shanhuang Chen","Shaoqing Wu","Shengfeng Ye","Shengfeng Ye","Shirong Ma","Shiyu Wang","Shuang Zhou","Shuiping Yu","Shunfeng Zhou","Shuting Pan","T. Wang","Tao Yun","Tian Pei","Tianyu Sun","W. L. Xiao","Wangding Zeng","Wanjia Zhao","Wei An","Wen Liu","Wenfeng Liang","Wenjun Gao","Wenqin Yu","Wentao Zhang","X. Q. Li","Xiangyue Jin","Xianzu Wang","Xiao Bi","Xiaodong Liu","Xiaohan Wang","Xiaojin Shen","Xiaokang Chen","Xiaokang Zhang","Xiaosha Chen","Xiaotao Nie","Xiaowen Sun","Xiaoxiang Wang","Xin Cheng","Xin Liu","Xin Xie","Xingchao Liu","Xingkai Yu","Xinnan Song","Xinxia Shan","Xinyi Zhou","Xinyu Yang","Xinyuan Li","Xuecheng Su","Xuheng Lin","Y. K. Li","Y. Q. Wang","Y. X. Wei","Y. X. Zhu","Yang Zhang","Yanhong Xu","Yanhong Xu","Yanping Huang","Yao Li","Yao Zhao","Yaofeng Sun","Yaohui Li","Yaohui Wang","Yi Yu","Yi Zheng","Yichao Zhang","Yifan Shi","Yiliang Xiong","Ying He","Ying Tang","Yishi Piao","Yisong Wang","Yixuan Tan","Yiyang Ma","Yiyuan Liu","Yongqiang Guo","Yu Wu","Yuan Ou","Yuchen Zhu","Yuduan Wang","Yue Gong","Yuheng Zou","Yujia He","Yukun Zha","Yunfan Xiong","Yunxian Ma","Yuting Yan","Yuxiang Luo","Yuxiang You","Yuxuan Liu","Yuyang Zhou","Z. F. Wu","Z. Z. Ren","Zehui Ren","Zhangli Sha","Zhe Fu","Zhean Xu","Zhen Huang","Zhen Zhang","Zhenda Xie","Zhengyan Zhang","Zhewen Hao","Zhibin Gou","Zhicheng Ma","Zhigang Yan","Zhihong Shao","Zhipeng Xu","Zhiyu Wu","Zhongyu Zhang","Zhuoshu Li","Zihui Gu","Zijia Zhu","Zijun Liu","Zilin Li","Ziwei Xie","Ziyang Song","Ziyi Gao","Zizheng Pan"],"pdf_url":"https://arxiv.org/pdf/2412.19437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13076v1","updated":"2025-02-18T17:24:00Z","published":"2025-02-18T17:24:00Z","title":"KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based\n  Portraits","summary":"  Patent analysis highly relies on concise and interpretable document\nrepresentations, referred to as patent portraits. Keyphrases, both present and\nabsent, are ideal candidates for patent portraits due to their brevity,\nrepresentativeness, and clarity. In this paper, we introduce KAPPA, an\nintegrated framework designed to construct keyphrase-based patent portraits and\nenhance patent analysis. KAPPA operates in two phases: patent portrait\nconstruction and portrait-based analysis. To ensure effective portrait\nconstruction, we propose a semantic-calibrated keyphrase generation paradigm\nthat integrates pre-trained language models with a prompt-based hierarchical\ndecoding strategy to leverage the multi-level structural characteristics of\npatents. For portrait-based analysis, we develop a comprehensive framework that\nemploys keyphrase-based patent portraits to enable efficient and accurate\npatent analysis. Extensive experiments on benchmark datasets of keyphrase\ngeneration, the proposed model achieves significant improvements compared to\nstate-of-the-art baselines. Further experiments conducted on real-world patent\napplications demonstrate that our keyphrase-based portraits effectively capture\ndomain-specific knowledge and enrich semantic representation for patent\nanalysis tasks.\n","authors":["Xin Xia","Yujin Wang","Jun Zhou","Guisheng Zhong","Linning Cai","Chen Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15939v2","updated":"2025-02-18T17:19:24Z","published":"2024-10-21T12:12:21Z","title":"CausalGraph2LLM: Evaluating LLMs for Causal Queries","summary":"  Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.\n","authors":["Ivaxi Sheth","Bahare Fatemi","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2410.15939v2.pdf","comment":"NAACL'25 Findings, Code - https://github.com/ivaxi0s/CausalGraph2LLM"},{"id":"http://arxiv.org/abs/2407.13399v3","updated":"2025-02-18T17:16:55Z","published":"2024-07-18T11:08:40Z","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overoptimization via Chi-Squared Preference Optimization","summary":"  Language model alignment methods such as reinforcement learning from human\nfeedback (RLHF) have led to impressive advances in language model capabilities,\nbut are limited by a widely observed phenomenon known as overoptimization,\nwhere the quality of the language model degrades over the course of the\nalignment process. As the model optimizes performance with respect to an\noffline reward model, it overfits to inaccuracies and drifts away from\npreferred responses covered by the data. To discourage such distribution shift,\nKL-regularization is widely employed in existing offline alignment methods, but\noveroptimization continues to harm performance. Lending theoretical insight\ninto the source of these empirical observations, we first show that the\nKL-regularization is too weak to prevent overfitting, then raise the following\nquestion: is it possible to design an efficient algorithm that is provably\nrobust to overoptimization?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","authors":["Audrey Huang","Wenhao Zhan","Tengyang Xie","Jason D. Lee","Wen Sun","Akshay Krishnamurthy","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2407.13399v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06254v2","updated":"2025-02-18T17:10:39Z","published":"2025-01-09T02:54:19Z","title":"Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words","summary":"  Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.\n","authors":["Gouki Minegishi","Hiroki Furuta","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2501.06254v2.pdf","comment":"Published at ICLR2025"},{"id":"http://arxiv.org/abs/2502.13063v1","updated":"2025-02-18T17:08:45Z","published":"2025-02-18T17:08:45Z","title":"Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity","summary":"  A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.\n","authors":["Yuri Kuratov","Mikhail Arkhipov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2502.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13061v1","updated":"2025-02-18T17:07:29Z","published":"2025-02-18T17:07:29Z","title":"Improved Fine-Tuning of Large Multimodal Models for Hateful Meme\n  Detection","summary":"  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While large multimodal models\nhave shown strong generalization across various tasks, they exhibit poor\ngeneralization to hateful meme detection due to the dynamic nature of memes\ntied to emerging social trends and breaking news. Recent work further\nhighlights the limitations of conventional supervised fine-tuning for large\nmultimodal models in this context. To address these challenges, we propose\nLarge Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a\nnovel two-stage fine-tuning framework designed to improve both in-domain\naccuracy and cross-domain generalization. Experimental results on six widely\nused meme classification datasets demonstrate that LMM-RGCL achieves\nstate-of-the-art performance, outperforming agent-based systems such as\nVPD-PALI-X-55B. Furthermore, our method effectively generalizes to\nout-of-domain memes under low-resource settings, surpassing models like GPT-4o.\n","authors":["Jingbiao Mei","Jinghong Chen","Guangyu Yang","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2502.13061v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.13059v1","updated":"2025-02-18T17:04:26Z","published":"2025-02-18T17:04:26Z","title":"SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models","summary":"  The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.\n","authors":["Xianfu Cheng","Wei Zhang","Shiwei Zhang","Jian Yang","Xiangyuan Guan","Xianjie Wu","Xiang Li","Ge Zhang","Jiaheng Liu","Yuying Mai","Yutao Zeng","Zhoufutu Wen","Ke Jin","Baorui Wang","Weixiao Zhou","Yunhong Lu","Tongliang Li","Wenhao Huang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2502.13059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13053v1","updated":"2025-02-18T17:01:28Z","published":"2025-02-18T17:01:28Z","title":"AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks","summary":"  As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.\n","authors":["Yurun Chen","Xueyu Hu","Keting Yin","Juncheng Li","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13044v1","updated":"2025-02-18T16:56:15Z","published":"2025-02-18T16:56:15Z","title":"Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction","summary":"  Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.\n","authors":["Nils Constantin Hellwig","Jakob Fehle","Udo Kruschwitz","Christian Wolff"],"pdf_url":"https://arxiv.org/pdf/2502.13044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18585v2","updated":"2025-02-18T16:51:53Z","published":"2025-01-30T18:58:18Z","title":"Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs","summary":"  Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.\n","authors":["Yue Wang","Qiuzhi Liu","Jiahao Xu","Tian Liang","Xingyu Chen","Zhiwei He","Linfeng Song","Dian Yu","Juntao Li","Zhuosheng Zhang","Rui Wang","Zhaopeng Tu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2501.18585v2.pdf","comment":"1. We have updated the results for DeepSeek-R1, and all of our\n  original conclusions remain valid. 2. Our proposed Tip approach remains\n  effective in Best-of-N scenarios (e.g., self-consistency and Laconic\n  Decoding) when built on DeepSeek-R1"},{"id":"http://arxiv.org/abs/2502.13034v1","updated":"2025-02-18T16:48:18Z","published":"2025-02-18T16:48:18Z","title":"Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions","summary":"  The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.\n","authors":["Aditya K Surikuchi","Raquel Fernndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2502.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13031v1","updated":"2025-02-18T16:46:47Z","published":"2025-02-18T16:46:47Z","title":"HPSS: Heuristic Prompting Strategy Search for LLM Evaluators","summary":"  Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods.\n","authors":["Bosi Wen","Pei Ke","Yufei Sun","Cunxiang Wang","Xiaotao Gu","Jinfeng Zhou","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.13031v1.pdf","comment":"32 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.13028v1","updated":"2025-02-18T16:45:41Z","published":"2025-02-18T16:45:41Z","title":"Whose story is it? Personalizing story generation by inferring author\n  styles","summary":"  Personalization has become essential for improving user experience in\ninteractive writing and educational applications, yet its potential in story\ngeneration remains largely unexplored. In this work, we propose a novel\ntwo-stage pipeline for personalized story generation. Our approach first infers\nan author's implicit story-writing characteristics from their past work and\norganizes them into an Author Writing Sheet, inspired by narrative theory. The\nsecond stage uses this sheet to simulate the author's persona through tailored\npersona descriptions and personalized story writing rules. To enable and\nvalidate our approach, we construct Mythos, a dataset of 590 stories from 64\nauthors across five distinct sources that reflect diverse story-writing\nsettings. A head-to-head comparison with a non-personalized baseline\ndemonstrates our pipeline's effectiveness in generating high-quality\npersonalized stories. Our personalized stories achieve a 75 percent win rate\n(versus 14 percent for the baseline and 11 percent ties) in capturing authors'\nwriting style based on their past works. Human evaluation highlights the high\nquality of our Author Writing Sheet and provides valuable insights into the\npersonalized story generation task. Notable takeaways are that writings from\ncertain sources, such as Reddit, are easier to personalize than others, like\nAO3, while narrative aspects, like Creativity and Language Use, are easier to\npersonalize than others, like Plot.\n","authors":["Nischal Ashok Kumar","Chau Minh Pham","Mohit Iyyer","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2502.13028v1.pdf","comment":"preprint 52 pages"},{"id":"http://arxiv.org/abs/2502.13025v1","updated":"2025-02-18T16:44:42Z","published":"2025-02-18T16:44:42Z","title":"Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks","summary":"  We present an agentic, autonomous graph expansion framework that iteratively\nstructures and refines knowledge in situ. Unlike conventional knowledge graph\nconstruction methods relying on static extraction or single-pass learning, our\napproach couples a reasoning-native large language model with a continually\nupdated graph representation. At each step, the system actively generates new\nconcepts and relationships, merges them into a global graph, and formulates\nsubsequent prompts based on its evolving structure. Through this\nfeedback-driven loop, the model organizes information into a scale-free network\ncharacterized by hub formation, stable modularity, and bridging nodes that link\ndisparate knowledge clusters. Over hundreds of iterations, new nodes and edges\ncontinue to appear without saturating, while centrality measures and shortest\npath distributions evolve to yield increasingly distributed connectivity. Our\nanalysis reveals emergent patterns, such as the rise of highly connected 'hub'\nconcepts and the shifting influence of 'bridge' nodes, indicating that agentic,\nself-reinforcing graph construction can yield open-ended, coherent knowledge\nstructures. Applied to materials design problems, we present compositional\nreasoning experiments by extracting node-specific and synergy-level principles\nto foster genuinely novel knowledge synthesis, yielding cross-domain ideas that\ntranscend rote summarization and strengthen the framework's potential for\nopen-ended scientific discovery. We discuss other applications in scientific\ndiscovery and outline future directions for enhancing scalability and\ninterpretability.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2502.13025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02302v2","updated":"2025-02-18T16:39:48Z","published":"2024-07-02T14:35:10Z","title":"Towards Human Understanding of Paraphrase Types in Large Language Models","summary":"  Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 800 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT and a\nDPO-trained LLama 7B model can generate simple APTs, such as additions and\ndeletions, but struggle with complex structures (e.g., subordination changes).\nThis study contributes to understanding which aspects of paraphrasing language\nmodels have already succeeded at understanding and what remains elusive. In\naddition, we show how our curated datasets can be used to develop language\nmodels with specific linguistic capabilities.\n","authors":["Dominik Meier","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2407.02302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13019v1","updated":"2025-02-18T16:38:39Z","published":"2025-02-18T16:38:39Z","title":"Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation","summary":"  Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.\n","authors":["Sha Li","Naren Ramarkrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.13019v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.13012v1","updated":"2025-02-18T16:33:33Z","published":"2025-02-18T16:33:33Z","title":"Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents","summary":"  Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods.\n","authors":["Chaoran Chen","Bingsheng Yao","Ruishi Zou","Wenyue Hua","Weimin Lyu","Toby Jia-Jun Li","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13010v1","updated":"2025-02-18T16:29:45Z","published":"2025-02-18T16:29:45Z","title":"Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge","summary":"  Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.\n","authors":["Mohammad Reza Rezaei","Reza Saadati Fard","Jayson Parker","Rahul G. Krishnan","Milad Lankarany"],"pdf_url":"https://arxiv.org/pdf/2502.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13931v3","updated":"2025-02-18T16:27:26Z","published":"2024-09-20T22:34:37Z","title":"On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists","summary":"  On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs.\n","authors":["Dongyang Fan","Bettina Messmer","Nikita Doikov","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2409.13931v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07623v2","updated":"2025-02-18T16:26:58Z","published":"2025-02-11T15:10:23Z","title":"Lexical categories of stem-forming roots in Mapudngun verb forms","summary":"  After developing a computational system for morphological analysis of the\nMapuche language, and evaluating it with texts from various authors and styles,\nit became necessary to verify the linguistic assumptions of the source used as\nthe basis for implementing this tool.\n  In the present work, the primary focus is on the lexical category\nclassification of Mapud\\\"ungun roots recognised as verbal in the source\nutilised for the development of the morphological analysis system.\n  The results of this lexical category revision directly benefit the\ncomputational analyser, as they are implemented as soon as they are verified.\nAdditionally, it is hoped that these results will help clarify some\nuncertainties about lexical categories in the Mapuche language.\n  This work addresses a preliminary task to identify the valency of true verbal\nroots, the results of which will be presented in a subsequent work that\ncomplements this article.\n","authors":["Andrs Chanda"],"pdf_url":"https://arxiv.org/pdf/2502.07623v2.pdf","comment":"22 pages, 2 large tables, 2 sample tables"},{"id":"http://arxiv.org/abs/2502.13004v1","updated":"2025-02-18T16:22:43Z","published":"2025-02-18T16:22:43Z","title":"Language Barriers: Evaluating Cross-Lingual Performance of CNN and\n  Transformer Architectures for Speech Quality Estimation","summary":"  Objective speech quality models aim to predict human-perceived speech quality\nusing automated methods. However, cross-lingual generalization remains a major\nchallenge, as Mean Opinion Scores (MOS) vary across languages due to\nlinguistic, perceptual, and dataset-specific differences. A model trained\nprimarily on English data may struggle to generalize to languages with\ndifferent phonetic, tonal, and prosodic characteristics, leading to\ninconsistencies in objective assessments. This study investigates the\ncross-lingual performance of two speech quality models: NISQA, a CNN-based\nmodel, and a Transformer-based Audio Spectrogram Transformer (AST) model. Both\nmodels were trained exclusively on English datasets containing over 49,000\nspeech samples and subsequently evaluated on speech in German, French,\nMandarin, Swedish, and Dutch. We analyze model performance using Pearson\nCorrelation Coefficient (PCC) and Root Mean Square Error (RMSE) across five\nspeech quality dimensions: coloration, discontinuity, loudness, noise, and MOS.\nOur findings show that while AST achieves a more stable cross-lingual\nperformance, both models exhibit noticeable biases. Notably, Mandarin speech\nquality predictions correlate highly with human MOS scores, whereas Swedish and\nDutch present greater prediction challenges. Discontinuities remain difficult\nto model across all languages. These results highlight the need for more\nbalanced multilingual datasets and architecture-specific adaptations to improve\ncross-lingual generalization.\n","authors":["Wafaa Wardah","Tue Melike Koak Bykta","Kirill Shchegelskiy","Sebastian Mller","Robert P. Spang"],"pdf_url":"https://arxiv.org/pdf/2502.13004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13001v1","updated":"2025-02-18T16:21:22Z","published":"2025-02-18T16:21:22Z","title":"You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations","summary":"  Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.\n","authors":["Frederic Kirstein","Muneeb Khan","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2502.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12996v1","updated":"2025-02-18T16:16:14Z","published":"2025-02-18T16:16:14Z","title":"Eager Updates For Overlapped Communication and Computation in DiLoCo","summary":"  Distributed optimization methods such as DiLoCo have been shown to be\neffective in training very large models across multiple distributed workers,\nsuch as datacenters. These methods split updates into two parts: an inner\noptimization phase, where the workers independently execute multiple\noptimization steps on their own local data, and an outer optimization step,\nwhere the inner updates are synchronized. While such approaches require orders\nof magnitude less communication than standard data-parallel training, in\nsettings where the workers are datacenters, even the limited communication\nrequirements of these approaches can still cause significant slow downs due to\nthe blocking necessary at each outer optimization step. In this paper, we\ninvestigate techniques to mitigate this issue by overlapping communication with\ncomputation in a manner that allows the outer optimization step to fully\noverlap with the inner optimization phase. We show that a particular variant,\ndubbed eager updates, provides competitive performance with standard DiLoCo in\nsettings with low bandwidth between workers.\n","authors":["Satyen Kale","Arthur Douillard","Yanislav Donchev"],"pdf_url":"https://arxiv.org/pdf/2502.12996v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2501.18512"},{"id":"http://arxiv.org/abs/2408.07832v9","updated":"2025-02-18T16:14:02Z","published":"2024-07-31T14:49:35Z","title":"LADDER: Language Driven Slice Discovery and Error Rectification","summary":"  Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.\n","authors":["Shantanu Ghosh","Rayan Syed","Chenyu Wang","Clare B. Poynton","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2408.07832v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12992v1","updated":"2025-02-18T16:13:08Z","published":"2025-02-18T16:13:08Z","title":"B-cos LM: Efficiently Transforming Pre-trained Language Models for\n  Improved Explainability","summary":"  Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\nimprove model explainability through architectural and computational\nadaptations, but their application has so far been limited to computer vision\nmodels and their associated training pipelines. In this work, we introduce\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\ntransforms pre-trained language models into B-cos LMs by combining B-cos\nconversion and task fine-tuning, improving efficiency compared to previous\nB-cos methods. Our automatic and human evaluation results demonstrate that\nB-cos LMs produce more faithful and human interpretable explanations than post\nhoc methods, while maintaining task performance comparable to conventional\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\nconventionally fine-tuned models in their learning processes and explanation\npatterns. Finally, we provide practical guidelines for effectively building\nB-cos LMs based on our findings. Our code is available at\nhttps://anonymous.4open.science/r/bcos_lm.\n","authors":["Yifan Wang","Sukrut Rao","Ji-Ung Lee","Mayank Jobanputra","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.12992v1.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.12988v1","updated":"2025-02-18T16:11:54Z","published":"2025-02-18T16:11:54Z","title":"Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs","summary":"  Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.\n","authors":["Zixiao Wang","Duzhen Zhang","Ishita Agrawal","Shen Gao","Le Song","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12988v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09992v2","updated":"2025-02-18T16:08:59Z","published":"2025-02-14T08:23:51Z","title":"Large Language Diffusion Models","summary":"  Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.\n","authors":["Shen Nie","Fengqi Zhu","Zebin You","Xiaolu Zhang","Jingyang Ou","Jun Hu","Jun Zhou","Yankai Lin","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12982v1","updated":"2025-02-18T16:04:57Z","published":"2025-02-18T16:04:57Z","title":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","summary":"  Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.\n","authors":["Longxu Dou","Qian Liu","Fan Zhou","Changyu Chen","Zili Wang","Ziqi Jin","Zichen Liu","Tongyao Zhu","Cunxiao Du","Penghui Yang","Haonan Wang","Jiaheng Liu","Yongchi Zhao","Xiachong Feng","Xin Mao","Man Tsung Yeung","Kunat Pipatanakul","Fajri Koto","Min Si Thu","Hynek Kydlek","Zeyi Liu","Qunshu Lin","Sittipong Sripaisarnmongkol","Kridtaphad Sae-Khow","Nirattisai Thongchim","Taechawat Konkaew","Narong Borijindargoon","Anh Dao","Matichon Maneegard","Phakphum Artkaew","Zheng-Xin Yong","Quan Nguyen","Wannaphong Phatthiyaphaibun","Hoang H. Tran","Mike Zhang","Shiqi Chen","Tianyu Pang","Chao Du","Xinyi Wan","Wei Lu","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2502.12982v1.pdf","comment":"49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/"},{"id":"http://arxiv.org/abs/2409.13203v4","updated":"2025-02-18T15:58:08Z","published":"2024-09-20T04:17:13Z","title":"Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks","summary":"  In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.\n","authors":["Huanxuan Liao","Shizhu He","Yao Xu","Yuanzhe Zhang","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.13203v4.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.12382v5","updated":"2025-02-18T15:54:28Z","published":"2024-06-18T08:14:28Z","title":"From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions","summary":"  Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.\n","authors":["Huanxuan Liao","Shizhu He","Yao Xu","Yuanzhe Zhang","Yanchao Hao","Shengping Liu","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.12382v5.pdf","comment":"accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22071v2","updated":"2025-02-18T15:52:52Z","published":"2024-10-29T14:31:33Z","title":"Distinguishing Ignorance from Error in LLM Hallucinations","summary":"  Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .\n","authors":["Adi Simhi","Jonathan Herzig","Idan Szpektor","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.22071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17513v2","updated":"2025-02-18T15:49:04Z","published":"2024-09-26T03:48:47Z","title":"Comparing Unidirectional, Bidirectional, and Word2vec Models for\n  Discovering Vulnerabilities in Compiled Lifted Code","summary":"  Ransomware and other forms of malware cause significant financial and\noperational damage to organizations by exploiting long-standing and often\ndifficult-to-detect software vulnerabilities. To detect vulnerabilities such as\nbuffer overflows in compiled code, this research investigates the application\nof unidirectional transformer-based embeddings, specifically GPT-2. Using a\ndataset of LLVM functions, we trained a GPT-2 model to generate embeddings,\nwhich were subsequently used to build LSTM neural networks to differentiate\nbetween vulnerable and non-vulnerable code. Our study reveals that embeddings\nfrom the GPT-2 model significantly outperform those from bidirectional models\nof BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.\nLSTM neural networks were developed with both frozen and unfrozen embedding\nmodel layers. The model with the highest performance was achieved when the\nembedding layers were unfrozen. Further, the research finds that, in exploring\nthe impact of different optimizers within this domain, the SGD optimizer\ndemonstrates superior performance over Adam. Overall, these findings reveal\nimportant insights into the potential of unidirectional transformer-based\napproaches in enhancing cybersecurity defenses.\n","authors":["Gary A. McCully","John D. Hastings","Shengjie Xu","Adam Fortier"],"pdf_url":"https://arxiv.org/pdf/2409.17513v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.12970v1","updated":"2025-02-18T15:48:46Z","published":"2025-02-18T15:48:46Z","title":"Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking","summary":"  The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks.\n","authors":["Junda Zhu","Lingyong Yan","Shuaiqiang Wang","Dawei Yin","Lei Sha"],"pdf_url":"https://arxiv.org/pdf/2502.12970v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2502.12965v1","updated":"2025-02-18T15:46:54Z","published":"2025-02-18T15:46:54Z","title":"A Survey of Text Classification Under Class Distribution Shift","summary":"  The basic underlying assumption of machine learning (ML) models is that the\ntraining and test data are sampled from the same distribution. However, in\ndaily practice, this assumption is often broken, i.e.~the distribution of the\ntest data changes over time, which hinders the application of conventional ML\nmodels. One domain where the distribution shift naturally occurs is text\nclassification, since people always find new topics to discuss. To this end, we\nsurvey research articles studying open-set text classification and related\ntasks. We divide the methods in this area based on the constraints that define\nthe kind of distribution shift and the corresponding problem formulation,\ni.e.~learning with the Universum, zero-shot learning, and open-set learning. We\nnext discuss the predominant mitigation approaches for each problem setup.\nFinally, we identify several future work directions, aiming to push the\nboundaries beyond the state of the art. Interestingly, we find that continual\nlearning can solve many of the issues caused by the shifting class\ndistribution. We maintain a list of relevant papers at\nhttps://github.com/Eduard6421/Open-Set-Survey.\n","authors":["Adriana Valentina Costache","Silviu Florin Gheorghe","Eduard Gabriel Poesina","Paul Irofti","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12964v1","updated":"2025-02-18T15:46:31Z","published":"2025-02-18T15:46:31Z","title":"Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs","summary":"  Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .\n","authors":["Adi Simhi","Itay Itzhak","Fazl Barez","Gabriel Stanovsky","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2502.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12962v1","updated":"2025-02-18T15:45:36Z","published":"2025-02-18T15:45:36Z","title":"Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing","summary":"  Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link.\n","authors":["Xiaoju Ye","Zhichun Wang","Jingyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12962v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.12961v1","updated":"2025-02-18T15:45:01Z","published":"2025-02-18T15:45:01Z","title":"Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger","summary":"  Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.\n","authors":["Wenjun Li","Dexun Li","Kuicai Dong","Cong Zhang","Hao Zhang","Weiwen Liu","Yasheng Wang","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12959v1","updated":"2025-02-18T15:43:27Z","published":"2025-02-18T15:43:27Z","title":"AlignFreeze: Navigating the Impact of Realignment on the Layers of\n  Multilingual Models Across Diverse Languages","summary":"  Realignment techniques are often employed to enhance cross-lingual transfer\nin multilingual language models, still, they can sometimes degrade performance\nin languages that differ significantly from the fine-tuned source language.\nThis paper introduces AlignFreeze, a method that freezes either the layers'\nlower half or upper half during realignment. Through controlled experiments on\n4 tasks, 3 models, and in 35 languages, we find that realignment affects all\nthe layers but can be the most detrimental to the lower ones. Freezing the\nlower layers can prevent performance degradation. Particularly, AlignFreeze\nimproves Part-of-Speech (PoS) tagging performances in languages where full\nrealignment fails: with XLM-R, it provides improvements of more than one\nstandard deviation in accuracy in seven more languages than full realignment.\n","authors":["Steve Bakos","Flix Gaschi","David Guzmn","Riddhi More","Kelly Chutong Li","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2502.12959v1.pdf","comment":"24 pages, 2 figures, to be published in Proceedings of NAACL 2025"},{"id":"http://arxiv.org/abs/2410.05193v2","updated":"2025-02-18T15:38:18Z","published":"2024-10-07T16:50:47Z","title":"RevisEval: Improving LLM-as-a-Judge via Response-Adapted References","summary":"  With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.\n","authors":["Qiyuan Zhang","Yufei Wang","Tiezheng YU","Yuxin Jiang","Chuhan Wu","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Fuyuan Lyu","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12953v1","updated":"2025-02-18T15:36:16Z","published":"2025-02-18T15:36:16Z","title":"Task-Informed Anti-Curriculum by Masking Improves Downstream Performance\n  on Text","summary":"  Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.\n","authors":["Andrei Jarca","Florinel Alin Croitoru","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12947v1","updated":"2025-02-18T15:30:34Z","published":"2025-02-18T15:30:34Z","title":"Every Expert Matters: Towards Effective Knowledge Distillation for\n  Mixture-of-Experts Language Models","summary":"  With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.\n","authors":["Gyeongman Kim","Gyouk Chu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12945v1","updated":"2025-02-18T15:29:05Z","published":"2025-02-18T15:29:05Z","title":"LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation","summary":"  Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.\n","authors":["Junchen Fu","Xuri Ge","Kaiwen Zheng","Ioannis Arapakis","Xin Xin","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2502.12945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11874v2","updated":"2025-02-18T15:27:28Z","published":"2025-02-17T15:02:09Z","title":"VAQUUM: Are Vague Quantifiers Grounded in Visual Data?","summary":"  Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.\n","authors":["Hugh Mee Wong","Rick Nouwen","Albert Gatt"],"pdf_url":"https://arxiv.org/pdf/2502.11874v2.pdf","comment":"Under review, 12 pages for main paper (5 figures), 15 pages including\n  appendix (2 figures)"},{"id":"http://arxiv.org/abs/2412.12509v2","updated":"2025-02-18T15:24:25Z","published":"2024-12-17T03:37:31Z","title":"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge","summary":"  Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications.\n","authors":["Kayla Schroeder","Zach Wood-Doughty"],"pdf_url":"https://arxiv.org/pdf/2412.12509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15175v3","updated":"2025-02-18T15:22:49Z","published":"2024-11-18T00:21:14Z","title":"ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?","summary":"  Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools.\n","authors":["Zheng Hui","Zhaoxiao Guo","Hang Zhao","Juanyong Duan","Lin Ai","Yinheng Li","Julia Hirschberg","Congrui Huang"],"pdf_url":"https://arxiv.org/pdf/2411.15175v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.12932v1","updated":"2025-02-18T15:14:58Z","published":"2025-02-18T15:14:58Z","title":"Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages","summary":"  Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation.\n","authors":["Salsabila Zahirah Pranida","Rifo Ahmad Genadi","Fajri Koto"],"pdf_url":"https://arxiv.org/pdf/2502.12932v1.pdf","comment":"18 pages total: 8 pages of main body, 6 pages of appendix. 4 figures\n  in main body, 6 figures in appendix. Submitted to ARR on February 2025"},{"id":"http://arxiv.org/abs/2502.12929v1","updated":"2025-02-18T15:11:46Z","published":"2025-02-18T15:11:46Z","title":"Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options","summary":"  We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.\n","authors":["Lakshmi Nair","Ian Trase","Mark Kim"],"pdf_url":"https://arxiv.org/pdf/2502.12929v1.pdf","comment":"Github code: https://github.com/flagshippioneering/Flow-of-Options"},{"id":"http://arxiv.org/abs/2405.07764v3","updated":"2025-02-18T15:11:44Z","published":"2024-05-13T14:07:15Z","title":"LGDE: Local Graph-based Dictionary Expansion","summary":"  We present Local Graph-based Dictionary Expansion (LGDE), a method for\ndata-driven discovery of the semantic neighbourhood of words using tools from\nmanifold learning and network science. At the heart of LGDE lies the creation\nof a word similarity graph from the geometry of word embeddings followed by\nlocal community detection based on graph diffusion. The diffusion in the local\ngraph manifold allows the exploration of the complex nonlinear geometry of word\nembeddings to capture word similarities based on paths of semantic association,\nover and above direct pairwise similarities. Exploiting such semantic\nneighbourhoods enables the expansion of dictionaries of pre-selected keywords,\nan important step for tasks in information retrieval, such as database queries\nand online data collection. We validate LGDE on two user-generated\nEnglish-language corpora and show that LGDE enriches the list of keywords with\nimproved performance relative to methods based on direct word similarities or\nco-occurrences. We further demonstrate our method through a real-world use case\nfrom communication science, where LGDE is evaluated quantitatively on the\nexpansion of a conspiracy-related dictionary from online data collected and\nanalysed by domain experts. Our empirical results and expert user assessment\nindicate that LGDE expands the seed dictionary with more useful keywords due to\nthe manifold-learning-based similarity network.\n","authors":["Dominik J. Schindler","Sneha Jha","Xixuan Zhang","Kilian Buehling","Annett Heft","Mauricio Barahona"],"pdf_url":"https://arxiv.org/pdf/2405.07764v3.pdf","comment":"Python code available at:\n  https://github.com/barahona-research-group/LGDE"},{"id":"http://arxiv.org/abs/2502.12928v1","updated":"2025-02-18T15:09:58Z","published":"2025-02-18T15:09:58Z","title":"Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts","summary":"  Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels.\n","authors":["Leiyu Pan","Zhenpeng Su","Minxuan Lv","Yizhe Xiong","Xiangwen Zhang","Zijia Lin","Hui Chen","Jungong Han","Guiguang Ding","Cheng Luo","Di Zhang","Kun Gai","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.12928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12927v1","updated":"2025-02-18T15:09:29Z","published":"2025-02-18T15:09:29Z","title":"SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems","summary":"  Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles.\n","authors":["Mike Zhang","Amalie Pernille Dilling","Lon Gondelman","Niels Erik Ruan Lyngdorf","Euan D. Lindsay","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2502.12927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05759v2","updated":"2025-02-18T15:07:53Z","published":"2025-02-09T03:37:06Z","title":"Reinforced Lifelong Editing for Language Models","summary":"  Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.\n","authors":["Zherui Li","Houcheng Jiang","Hao Chen","Baolong Bi","Zhenhong Zhou","Fei Sun","Junfeng Fang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12924v1","updated":"2025-02-18T15:04:13Z","published":"2025-02-18T15:04:13Z","title":"Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data","summary":"  Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.\n","authors":["Maite Heredia","Gorka Labaka","Jeremy Barnes","Aitor Soroa"],"pdf_url":"https://arxiv.org/pdf/2502.12924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12923v1","updated":"2025-02-18T15:03:17Z","published":"2025-02-18T15:03:17Z","title":"On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation","summary":"  This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware.\n","authors":["Rune Birkmose","Nathan Mrkeberg Reece","Esben Hofstedt Norvin","Johannes Bjerva","Mike Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12921v1","updated":"2025-02-18T15:01:30Z","published":"2025-02-18T15:01:30Z","title":"Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison","summary":"  Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS.\n","authors":["George-Kirollos Saad","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2502.12921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12913v1","updated":"2025-02-18T14:54:55Z","published":"2025-02-18T14:54:55Z","title":"GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning","summary":"  Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.\n","authors":["Sifan Zhou","Shuo Wang","Zhihang Yuan","Mingjia Shi","Yuzhang Shang","Dawei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12911v1","updated":"2025-02-18T14:53:45Z","published":"2025-02-18T14:53:45Z","title":"Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation","summary":"  Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes.\n","authors":["Zheng Yuan","Hao Chen","Zijin Hong","Qinggang Zhang","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2502.12911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15334v3","updated":"2025-02-18T14:49:52Z","published":"2023-08-29T14:29:57Z","title":"The Responsible Development of Automated Student Feedback with\n  Generative AI","summary":"  Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]\n","authors":["Euan D Lindsay","Mike Zhang","Aditya Johri","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2308.15334v3.pdf","comment":"Pre-print of version accepted to EDUCON 2025"},{"id":"http://arxiv.org/abs/2502.12904v1","updated":"2025-02-18T14:47:02Z","published":"2025-02-18T14:47:02Z","title":"Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements","summary":"  We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities.\n","authors":["Shu Yang","Shenzhe Zhu","Zeyu Wu","Keyu Wang","Junchi Yao","Junchao Wu","Lijie Hu","Mengdi Li","Derek F. Wong","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12900v1","updated":"2025-02-18T14:36:39Z","published":"2025-02-18T14:36:39Z","title":"Soundwave: Less is More for Speech-Text Alignment in LLMs","summary":"  Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.\n","authors":["Yuhao Zhang","Zhiheng Liu","Fan Bu","Ruiyu Zhang","Benyou Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2502.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12896v1","updated":"2025-02-18T14:32:44Z","published":"2025-02-18T14:32:44Z","title":"None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks","summary":"  In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.\n","authors":["Eva Snchez Salido","Julio Gonzalo","Guillermo Marco"],"pdf_url":"https://arxiv.org/pdf/2502.12896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12895v1","updated":"2025-02-18T14:32:17Z","published":"2025-02-18T14:32:17Z","title":"Multilingual European Language Models: Benchmarking Approaches and\n  Challenges","summary":"  The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.\n","authors":["Fabio Barth","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2502.12895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12893v1","updated":"2025-02-18T14:29:12Z","published":"2025-02-18T14:29:12Z","title":"H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to\n  Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and\n  Gemini 2.0 Flash Thinking","summary":"  Large Reasoning Models (LRMs) have recently extended their powerful reasoning\ncapabilities to safety checks-using chain-of-thought reasoning to decide\nwhether a request should be answered. While this new approach offers a\npromising route for balancing model utility and safety, its robustness remains\nunderexplored. To address this gap, we introduce Malicious-Educator, a\nbenchmark that disguises extremely dangerous or malicious requests beneath\nseemingly legitimate educational prompts. Our experiments reveal severe\nsecurity flaws in popular commercial-grade LRMs, including OpenAI o1/o3,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1\nmodel initially maintains a high refusal rate of about 98%, subsequent model\nupdates significantly compromise its safety; and attackers can easily extract\ncriminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any\nadditional tricks. To further highlight these vulnerabilities, we propose\nHijacking Chain-of-Thought (H-CoT), a universal and transferable attack method\nthat leverages the model's own displayed intermediate reasoning to jailbreak\nits safety reasoning mechanism. Under H-CoT, refusal rates sharply\ndecline-dropping from 98% to below 2%-and, in some instances, even transform\ninitially cautious tones into ones that are willing to provide harmful content.\nWe hope these findings underscore the urgent need for more robust safety\nmechanisms to preserve the benefits of advanced reasoning capabilities without\ncompromising ethical standards.\n","authors":["Martin Kuo","Jianyi Zhang","Aolin Ding","Qinsi Wang","Louis DiValentin","Yujia Bao","Wei Wei","Da-Cheng Juan","Hai Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12886v1","updated":"2025-02-18T14:20:27Z","published":"2025-02-18T14:20:27Z","title":"Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?","summary":"  Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.\n","authors":["Georg Rehm","Annika Grtzner-Zahn","Fabio Barth"],"pdf_url":"https://arxiv.org/pdf/2502.12886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12884v1","updated":"2025-02-18T14:16:03Z","published":"2025-02-18T14:16:03Z","title":"How desirable is alignment between LLMs and linguistically diverse human\n  users?","summary":"  We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development.\n","authors":["Pia Knoeferle","Sebastian Mller","Dorothea Kolossa","Veronika Solopova","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2502.12884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18069v2","updated":"2025-02-18T14:15:34Z","published":"2024-12-24T00:55:59Z","title":"Improving Factuality with Explicit Working Memory","summary":"  Large language models can generate factually inaccurate content, a problem\nknown as hallucination. Recent works have built upon retrieved-augmented\ngeneration to improve factuality through iterative prompting but these methods\nare limited by the traditional RAG design. To address these challenges, we\nintroduce EWE (Explicit Working Memory), a novel approach that enhances\nfactuality in long-form text generation by integrating a working memory that\nreceives real-time feedback from external resources. The memory is refreshed\nbased on online fact-checking and retrieval feedback, allowing EWE to rectify\nfalse claims during the generation process and ensure more accurate and\nreliable outputs. Our experiments demonstrate that Ewe outperforms strong\nbaselines on four fact-seeking long-form generation datasets, increasing the\nfactuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the\nhelpfulness of the responses. Further analysis reveals that the design of rules\nfor memory updates, configurations of memory units, and the quality of the\nretrieval datastore are crucial factors for influencing model performance.\n","authors":["Mingda Chen","Yang Li","Karthik Padthe","Rulin Shao","Alicia Sun","Luke Zettlemoyer","Gargi Ghosh","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2412.18069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15091v2","updated":"2025-02-18T14:12:31Z","published":"2024-08-27T14:22:02Z","title":"Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models","summary":"  The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch.\n","authors":["Xiyu Liu","Zhengxiao Liu","Naibin Gu","Zheng Lin","Wanli Ma","Ji Xiang","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15091v2.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2501.00152v2","updated":"2025-02-18T14:02:12Z","published":"2024-12-30T21:54:33Z","title":"Temporal reasoning for timeline summarisation in social media","summary":"  This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.\n","authors":["Jiayu Song","Mahmud Akhter","Dana Atzil Slonim","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2501.00152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10774v3","updated":"2025-02-18T13:53:51Z","published":"2024-08-20T12:13:04Z","title":"Flexora: Flexible Low Rank Adaptation for Large Language Models","summary":"  Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.\n","authors":["Chenxing Wei","Yao Shu","Ying Tiffany He","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2408.10774v3.pdf","comment":"39 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.12859v1","updated":"2025-02-18T13:46:47Z","published":"2025-02-18T13:46:47Z","title":"PAFT: Prompt-Agnostic Fine-Tuning","summary":"  While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.\n","authors":["Chenxing Wei","Yao Shu","Mingwen Ou","Ying Tiffany He","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2502.12859v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.12858v1","updated":"2025-02-18T13:45:42Z","published":"2025-02-18T13:45:42Z","title":"Rejected Dialects: Biases Against African American Language in Reward\n  Models","summary":"  Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.\n","authors":["Joel Mire","Zubin Trivadi Aysola","Daniel Chechelnitsky","Nicholas Deas","Chrysoula Zerva","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2502.12858v1.pdf","comment":"Accepted to NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.12855v1","updated":"2025-02-18T13:43:06Z","published":"2025-02-18T13:43:06Z","title":"Integrating Arithmetic Learning Improves Mathematical Reasoning in\n  Smaller Models","summary":"  While large models pre-trained on high-quality data exhibit excellent\nperformance across various reasoning tasks, including mathematical reasoning\n(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical\nreasoning remains a challenging problem. Common approaches to address this\nchallenge include knowledge distillation, where smaller student models learn\nfrom large pre-trained teacher models, and data augmentation, such as\nrephrasing questions. Despite these efforts, smaller models struggle with\narithmetic computations, leading to errors in mathematical reasoning. In this\nwork, we focus on leveraging a programmatically generated arithmetic dataset to\nenhance the reasoning capabilities of smaller models. We investigate two key\napproaches to incorporate this dataset -- (1) intermediate fine-tuning, where a\nmodel is fine-tuned on the arithmetic dataset before being trained on a\nreasoning dataset, and (2) integrating the arithmetic dataset into the\ninstruction-tuning mixture, allowing the model to learn arithmetic skills\nalongside general instruction-following abilities. Our experiments on multiple\nreasoning benchmarks demonstrate that incorporating an arithmetic dataset,\nwhether through targeted fine-tuning or within the instruction-tuning mixture,\nenhances the models' arithmetic capabilities, which in turn improves their\nmathematical reasoning performance.\n","authors":["Neeraj Gangwar","Suma P Bhat","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2502.12855v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12853v1","updated":"2025-02-18T13:40:22Z","published":"2025-02-18T13:40:22Z","title":"S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning","summary":"  Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.\n","authors":["Ruotian Ma","Peisong Wang","Cheng Liu","Xingyan Liu","Jiaqi Chen","Bang Zhang","Xin Zhou","Nan Du","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2502.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12852v1","updated":"2025-02-18T13:40:05Z","published":"2025-02-18T13:40:05Z","title":"MVL-SIB: A Massively Multilingual Vision-Language Benchmark for\n  Cross-Modal Topical Matching","summary":"  Existing multilingual vision-language (VL) benchmarks often only cover a\nhandful of languages. Consequently, evaluations of large vision-language models\n(LVLMs) predominantly target high-resource languages, underscoring the need for\nevaluation data for low-resource languages. To address this limitation, we\nintroduce MVL-SIB, a massively multilingual vision-language benchmark that\nevaluates both cross-modal and text-only topical matching across 205 languages\n-- over 100 more than the most multilingual existing VL benchmarks encompass.\nWe then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini)\non MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic\nmatching in lower-resource languages, performing no better than chance on\nlanguages like N'Koo. Our analysis further reveals that VL support in LVLMs\ndeclines disproportionately relative to textual support for lower-resource\nlanguages, as evidenced by comparison of cross-modal and text-only topical\nmatching performance. We further observe that open-weight LVLMs do not benefit\nfrom representing a topic with more than one image, suggesting that these\nmodels are not yet fully effective at handling multi-image tasks. By\ncorrelating performance on MVL-SIB with other multilingual VL benchmarks, we\nhighlight that MVL-SIB serves as a comprehensive probe of multilingual VL\nunderstanding in LVLMs.\n","authors":["Fabian David Schmidt","Florian Schneider","Chris Biemann","Goran Glava"],"pdf_url":"https://arxiv.org/pdf/2502.12852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12851v1","updated":"2025-02-18T13:39:22Z","published":"2025-02-18T13:39:22Z","title":"MeMo: Towards Language Models with Associative Memory Mechanisms","summary":"  Memorization is a fundamental ability of Transformer-based Large Language\nModels, achieved through learning. In this paper, we propose a paradigm shift\nby designing an architecture to memorize text directly, bearing in mind the\nprinciple that memorization precedes learning. We introduce MeMo, a novel\narchitecture for language modeling that explicitly memorizes sequences of\ntokens in layered associative memories. By design, MeMo offers transparency and\nthe possibility of model editing, including forgetting texts. We experimented\nwith the MeMo architecture, showing the memorization power of the one-layer and\nthe multi-layer configurations.\n","authors":["Fabio Massimo Zanzotto","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Leonardo Ranaldi","Davide Venditti","Federico Ranaldi","Cristina Giannone","Andrea Favalli","Raniero Romagnoli"],"pdf_url":"https://arxiv.org/pdf/2502.12851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12838v1","updated":"2025-02-18T13:11:16Z","published":"2025-02-18T13:11:16Z","title":"Towards Equitable AI: Detecting Bias in Using Large Language Models for\n  Marketing","summary":"  The recent advances in large language models (LLMs) have revolutionized\nindustries such as finance, marketing, and customer service by enabling\nsophisticated natural language processing tasks. However, the broad adoption of\nLLMs brings significant challenges, particularly in the form of social biases\nthat can be embedded within their outputs. Biases related to gender, age, and\nother sensitive attributes can lead to unfair treatment, raising ethical\nconcerns and risking both company reputation and customer trust. This study\nexamined bias in finance-related marketing slogans generated by LLMs (i.e.,\nChatGPT) by prompting tailored ads targeting five demographic categories:\ngender, marital status, age, income level, and education level. A total of\n1,700 slogans were generated for 17 unique demographic groups, and key terms\nwere categorized into four thematic groups: empowerment, financial, benefits\nand features, and personalization. Bias was systematically assessed using\nrelative bias calculations and statistically tested with the Kolmogorov-Smirnov\n(KS) test against general slogans generated for any individual. Results\nrevealed that marketing slogans are not neutral; rather, they emphasize\ndifferent themes based on demographic factors. Women, younger individuals,\nlow-income earners, and those with lower education levels receive more distinct\nmessaging compared to older, higher-income, and highly educated individuals.\nThis underscores the need to consider demographic-based biases in AI-generated\nmarketing strategies and their broader societal implications. The findings of\nthis study provide a roadmap for developing more equitable AI systems,\nhighlighting the need for ongoing bias detection and mitigation efforts in\nLLMs.\n","authors":["Berk Yilmaz","Huthaifa I. Ashqar"],"pdf_url":"https://arxiv.org/pdf/2502.12838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12836v1","updated":"2025-02-18T13:09:59Z","published":"2025-02-18T13:09:59Z","title":"An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation","summary":"  Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent features an orchestrator that integrates user\ninteraction, data sources, and analytical tools to generate accurate health\ninsights. To evaluate its effectiveness, we implement a case study on heart\nrate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of\nPPG and Electrocardiogram (ECG) recordings in a remote health monitoring study.\nThe agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o,\nwith ECG serving as the gold standard for HR estimation. Results demonstrate\nthat our agent significantly outperforms benchmark models by achieving lower\nerror rates and more reliable HR estimations. The agent implementation is\npublicly available on GitHub.\n","authors":["Mohammad Feli","Iman Azimi","Pasi Liljeberg","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2502.12836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12835v1","updated":"2025-02-18T13:09:16Z","published":"2025-02-18T13:09:16Z","title":"Subword models struggle with word learning, but surprisal hides it","summary":"  We study word learning in subword and character language models with the\npsycholinguistic lexical decision task. While subword LMs struggle to discern\nwords and non-words with high accuracy, character LMs solve this task easily\nand consistently. Furthermore, when comparing word learning and syntactic\nlearning, both processes are separable in character LM where word learning\npredates syntactic learning, whereas these processes are simultaneous in\nsubword LM. This raises questions about the adequacy of subword LMs for\nmodeling language acquisition and positions character LMs as a viable\nalternative.\n","authors":["Bastian Bunzeck","Sina Zarrie"],"pdf_url":"https://arxiv.org/pdf/2502.12835v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.08115v2","updated":"2025-02-18T12:50:00Z","published":"2024-10-10T17:00:06Z","title":"Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System","summary":"  Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).\n","authors":["Weize Chen","Jiarui Yuan","Chen Qian","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.08115v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.12829v1","updated":"2025-02-18T12:48:37Z","published":"2025-02-18T12:48:37Z","title":"KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan","summary":"  Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.\n","authors":["Mukhammed Togmanov","Nurdaulet Mukhituly","Diana Turmakhan","Jonibek Mansurov","Maiya Goloburda","Akhmed Sakip","Zhuohan Xie","Yuxia Wang","Bekassyl Syzdykov","Nurkhan Laiyk","Alham Fikri Aji","Ekaterina Kochmar","Preslav Nakov","Fajri Koto"],"pdf_url":"https://arxiv.org/pdf/2502.12829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12825v1","updated":"2025-02-18T12:46:18Z","published":"2025-02-18T12:46:18Z","title":"Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models","summary":"  When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.\n","authors":["Rubing Lu","Joo Sedoc","Arun Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2502.12825v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.13143v1","updated":"2025-02-18T18:59:02Z","published":"2025-02-18T18:59:02Z","title":"SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and\n  Object Manipulation","summary":"  Spatial intelligence is a critical component of embodied AI, promoting robots\nto understand and interact with their environments. While recent advances have\nenhanced the ability of VLMs to perceive object locations and positional\nrelationships, they still lack the capability to precisely understand object\norientations-a key requirement for tasks involving fine-grained manipulations.\nAddressing this limitation not only requires geometric reasoning but also an\nexpressive and intuitive way to represent orientation. In this context, we\npropose that natural language offers a more flexible representation space than\ncanonical frames, making it particularly suitable for instruction-following\nrobotic systems. In this paper, we introduce the concept of semantic\norientation, which defines object orientations using natural language in a\nreference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the\n''handle'' direction of a knife). To support this, we construct OrienText300K,\na large-scale dataset of 3D models annotated with semantic orientations that\nlink geometric understanding to functional semantics. By integrating semantic\norientation into a VLM system, we enable robots to generate manipulation\nactions with both positional and orientational constraints. Extensive\nexperiments in simulation and real world demonstrate that our approach\nsignificantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy\non Open6DOR and 74.9% accuracy on SIMPLER.\n","authors":["Zekun Qi","Wenyao Zhang","Yufei Ding","Runpei Dong","Xinqiang Yu","Jingwen Li","Lingyun Xu","Baoyu Li","Xialin He","Guofan Fan","Jiazhao Zhang","Jiawei He","Jiayuan Gu","Xin Jin","Kaisheng Ma","Zhizheng Zhang","He Wang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.13143v1.pdf","comment":"Project page: https://qizekun.github.io/sofar/"},{"id":"http://arxiv.org/abs/2502.13142v1","updated":"2025-02-18T18:59:01Z","published":"2025-02-18T18:59:01Z","title":"Pre-training Auto-regressive Robotic Models with 4D Representations","summary":"  Foundation models pre-trained on massive unlabeled datasets have\nrevolutionized natural language and computer vision, exhibiting remarkable\ngeneralization capabilities, thus highlighting the importance of pre-training.\nYet, efforts in robotics have struggled to achieve similar success, limited by\neither the need for costly robotic annotations or the lack of representations\nthat effectively model the physical world. In this paper, we introduce ARM4R,\nan Auto-regressive Robotic Model that leverages low-level 4D Representations\nlearned from human video data to yield a better pre-trained robotic model.\nSpecifically, we focus on utilizing 3D point tracking representations from\nvideos derived by lifting 2D representations into 3D space via monocular depth\nestimation across time. These 4D representations maintain a shared geometric\nstructure between the points and robot state representations up to a linear\ntransformation, enabling efficient transfer learning from human video data to\nlow-level robotic control. Our experiments show that ARM4R can transfer\nefficiently from human video data to robotics and consistently improves\nperformance on tasks across various robot environments and configurations.\n","authors":["Dantong Niu","Yuvan Sharma","Haoru Xue","Giscard Biamby","Junyi Zhang","Ziteng Ji","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2502.13142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13141v1","updated":"2025-02-18T18:59:00Z","published":"2025-02-18T18:59:00Z","title":"UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models","summary":"  Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.\n","authors":["Huawei Lin","Yingjie Lao","Tong Geng","Tan Yu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.13141v1.pdf","comment":"18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks"},{"id":"http://arxiv.org/abs/2502.13138v1","updated":"2025-02-18T18:57:21Z","published":"2025-02-18T18:57:21Z","title":"AIDE: AI-Driven Exploration in the Space of Code","summary":"  Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.\n","authors":["Zhengyao Jiang","Dominik Schmidt","Dhruv Srikanth","Dixing Xu","Ian Kaplan","Deniss Jacenko","Yuxiang Wu"],"pdf_url":"https://arxiv.org/pdf/2502.13138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13137v1","updated":"2025-02-18T18:57:09Z","published":"2025-02-18T18:57:09Z","title":"Theorem Prover as a Judge for Synthetic Data Generation","summary":"  The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.\n","authors":["Joshua Ong Jun Leang","Giwon Hong","Wenda Li","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13135v1","updated":"2025-02-18T18:56:44Z","published":"2025-02-18T18:56:44Z","title":"Sleepless Nights, Sugary Days: Creating Synthetic Users with Health\n  Conditions for Realistic Coaching Agent Interactions","summary":"  We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.\n","authors":["Taedong Yun","Eric Yang","Mustafa Safdari","Jong Ha Lee","Vaishnavi Vinod Kumar","S. Sara Mahdavi","Jonathan Amar","Derek Peyton","Reut Aharony","Andreas Michaelides","Logan Schneider","Isaac Galatzer-Levy","Yugang Jia","John Canny","Arthur Gretton","Maja Matari"],"pdf_url":"https://arxiv.org/pdf/2502.13135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13132v1","updated":"2025-02-18T18:55:53Z","published":"2025-02-18T18:55:53Z","title":"Learning to Defer for Causal Discovery with Imperfect Experts","summary":"  Integrating expert knowledge, e.g. from large language models, into causal\ndiscovery algorithms can be challenging when the knowledge is not guaranteed to\nbe correct. Expert recommendations may contradict data-driven results, and\ntheir reliability can vary significantly depending on the domain or specific\nquery. Existing methods based on soft constraints or inconsistencies in\npredicted causal relationships fail to account for these variations in\nexpertise. To remedy this, we propose L2D-CD, a method for gauging the\ncorrectness of expert recommendations and optimally combining them with\ndata-driven causal discovery results. By adapting learning-to-defer (L2D)\nalgorithms for pairwise causal discovery (CD), we learn a deferral function\nthat selects whether to rely on classical causal discovery methods using\nnumerical data or expert recommendations based on textual meta-data. We\nevaluate L2D-CD on the canonical T\\\"ubingen pairs dataset and demonstrate its\nsuperior performance compared to both the causal discovery method and the\nexpert used in isolation. Moreover, our approach identifies domains where the\nexpert's performance is strong or weak. Finally, we outline a strategy for\ngeneralizing this approach to causal discovery on graphs with more than two\nvariables, paving the way for further research in this area.\n","authors":["Oscar Clivio","Divyat Mahajan","Perouz Taslakian","Sara Magliacane","Ioannis Mitliagkas","Valentina Zantedeschi","Alexandre Drouin"],"pdf_url":"https://arxiv.org/pdf/2502.13132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11687v2","updated":"2025-02-18T18:55:39Z","published":"2024-10-15T15:22:38Z","title":"State-space models can learn in-context by gradient descent","summary":"  Deep state-space models (Deep SSMs) are becoming popular as effective\napproaches to model sequence data. They have also been shown to be capable of\nin-context learning, much like transformers. However, a complete picture of how\nSSMs might be able to do in-context learning has been missing. In this study,\nwe provide a direct and explicit construction to show that state-space models\ncan perform gradient-based learning and use it for in-context learning in much\nthe same way as transformers. Specifically, we prove that a single structured\nstate-space model layer, augmented with multiplicative input and output gating,\ncan reproduce the outputs of an implicit linear model with least squares loss\nafter one step of gradient descent. We then show a straightforward extension to\nmulti-step linear and non-linear regression tasks. We validate our construction\nby training randomly initialized augmented SSMs on linear and non-linear\nregression tasks. The empirically obtained parameters through optimization\nmatch the ones predicted analytically by the theoretical construction. Overall,\nwe elucidate the role of input- and output-gating in recurrent architectures as\nthe key inductive biases for enabling the expressive power typical of\nfoundation models. We also provide novel insights into the relationship between\nstate-space models and linear self-attention, and their ability to learn\nin-context.\n","authors":["Neeraj Mohan Sushma","Yudou Tian","Harshvardhan Mestha","Nicolo Colombo","David Kappel","Anand Subramoney"],"pdf_url":"https://arxiv.org/pdf/2410.11687v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.13131v1","updated":"2025-02-18T18:55:26Z","published":"2025-02-18T18:55:26Z","title":"Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis","summary":"  Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.\n","authors":["Feng Luo","Rui Yang","Hao Sun","Chunyuan Deng","Jiarui Yao","Jingyan Shen","Huan Zhang","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.13131v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.13130v1","updated":"2025-02-18T18:55:21Z","published":"2025-02-18T18:55:21Z","title":"Magma: A Foundation Model for Multimodal AI Agents","summary":"  We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.\n","authors":["Jianwei Yang","Reuben Tan","Qianhui Wu","Ruijie Zheng","Baolin Peng","Yongyuan Liang","Yu Gu","Mu Cai","Seonghyeon Ye","Joel Jang","Yuquan Deng","Lars Liden","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.13130v1.pdf","comment":"29 pages, 16 figures, technical report from MSR"},{"id":"http://arxiv.org/abs/2502.13128v1","updated":"2025-02-18T18:52:21Z","published":"2025-02-18T18:52:21Z","title":"SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation","summary":"  Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .\n","authors":["Zihan Liu","Shuangrui Ding","Zhixiong Zhang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.13128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13120v1","updated":"2025-02-18T18:42:11Z","published":"2025-02-18T18:42:11Z","title":"Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context","summary":"  Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.\n","authors":["Marion Bartl","Thomas Brendan Murphy","Susan Leavy"],"pdf_url":"https://arxiv.org/pdf/2502.13120v1.pdf","comment":"9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)"},{"id":"http://arxiv.org/abs/2502.13117v1","updated":"2025-02-18T18:37:15Z","published":"2025-02-18T18:37:15Z","title":"Performance Evaluation of Large Language Models in Statistical\n  Programming","summary":"  The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.\n","authors":["Xinyi Song","Kexin Xie","Lina Lee","Ruizhe Chen","Jared M. Clark","Hao He","Haoran He","Jie Min","Xinlei Zhang","Simin Zheng","Zhiyang Zhang","Xinwei Deng","Yili Hong"],"pdf_url":"https://arxiv.org/pdf/2502.13117v1.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.13115v1","updated":"2025-02-18T18:35:24Z","published":"2025-02-18T18:35:24Z","title":"Near-Optimal Private Learning in Linear Contextual Bandits","summary":"  We analyze the problem of private learning in generalized linear contextual\nbandits. Our approach is based on a novel method of re-weighted regression,\nyielding an efficient algorithm with regret of order\n$\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ in the joint and local model\nof $\\alpha$-privacy, respectively. Further, we provide near-optimal private\nprocedures that achieve dimension-independent rates in private linear models\nand linear contextual bandits. In particular, our results imply that joint\nprivacy is almost \"for free\" in all the settings we consider, partially\naddressing the open problem posed by Azize and Basu (2024).\n","authors":["Fan Chen","Jiachun Li","Alexander Rakhlin","David Simchi-Levi"],"pdf_url":"https://arxiv.org/pdf/2502.13115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13108v1","updated":"2025-02-18T18:20:37Z","published":"2025-02-18T18:20:37Z","title":"Improving Clinical Question Answering with Multi-Task Learning: A Joint\n  Approach for Answer Extraction and Medical Categorization","summary":"  Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.\n","authors":["Priyaranjan Pattnayak","Hitesh Laxmichand Patel","Amit Agarwal","Bhargava Kumar","Srikant Panda","Tejaswini Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.13108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07186v2","updated":"2025-02-18T18:20:08Z","published":"2025-01-13T10:31:36Z","title":"Generalizable Graph Neural Networks for Robust Power Grid Topology\n  Control","summary":"  The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs.\n","authors":["Matthijs de Jong","Jan Viebahn","Yuliya Shapovalova"],"pdf_url":"https://arxiv.org/pdf/2501.07186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13107v1","updated":"2025-02-18T18:19:36Z","published":"2025-02-18T18:19:36Z","title":"MatterChat: A Multi-Modal LLM for Material Science","summary":"  Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.\n","authors":["Yingheng Tang","Wenbin Xu","Jie Cao","Jianzhu Ma","Weilu Gao","Steve Farrell","Benjamin Erichson","Michael W. Mahoney","Andy Nonaka","Zhi Yao"],"pdf_url":"https://arxiv.org/pdf/2502.13107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14700v3","updated":"2025-02-18T18:19:07Z","published":"2025-01-24T18:22:37Z","title":"An Attentive Graph Agent for Topology-Adaptive Cyber Defence","summary":"  As cyber threats grow increasingly sophisticated, reinforcement learning (RL)\nis emerging as a promising technique to create intelligent and adaptive cyber\ndefense systems. However, most existing autonomous defensive agents have\noverlooked the inherent graph structure of computer networks subject to cyber\nattacks, potentially missing critical information and constraining their\nadaptability. To overcome these limitations, we developed a custom version of\nthe Cyber Operations Research Gym (CybORG) environment, encoding network state\nas a directed graph with realistic low-level features. We employ a Graph\nAttention Network (GAT) architecture to process node, edge, and global\nfeatures, and adapt its output to be compatible with policy gradient methods in\nRL. Our GAT-based approach offers key advantages over flattened alternatives:\npolicies that demonstrate resilience to certain types of unexpected dynamic\nnetwork topology changes, reasonable generalisation to networks of varying\nsizes within the same structural distribution, and interpretable defensive\nactions grounded in tangible network properties. We demonstrate that GAT\ndefensive policies can be trained using our low-level directed graph\nobservations, even when unexpected connections arise during simulation.\nEvaluations across networks of different sizes, but consistent subnetwork\nstructure, show our policies achieve comparable performance to policies trained\nspecifically for each network configuration. Our study contributes to the\ndevelopment of robust cyber defence systems that can better adapt to real-world\nnetwork security challenges.\n","authors":["Ilya Orson Sandoval","Isaac Symes Thompson","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2501.14700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08820v2","updated":"2025-02-18T18:08:56Z","published":"2025-02-12T22:18:34Z","title":"Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model","summary":"  Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents.\n","authors":["Emre Can Acikgoz","Jeremiah Greer","Akul Datta","Ze Yang","William Zeng","Oussama Elachqar","Emmanouil Koukoumidis","Dilek Hakkani-Tr","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2502.08820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v3","updated":"2025-02-18T18:07:34Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SciCap Challenge 2023","summary":"  Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v3.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.13092v1","updated":"2025-02-18T17:59:48Z","published":"2025-02-18T17:59:48Z","title":"Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation","summary":"  Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.\n","authors":["Mengkang Hu","Tianxing Chen","Yude Zou","Yuheng Lei","Qiguang Chen","Ming Li","Hongyuan Zhang","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2502.13092v1.pdf","comment":"Project page: https://text-to-world.github.io/"},{"id":"http://arxiv.org/abs/2412.02251v2","updated":"2025-02-18T17:42:49Z","published":"2024-12-03T08:28:47Z","title":"Selective Reviews of Bandit Problems in AI via a Statistical View","summary":"  Reinforcement Learning (RL) is a widely researched area in artificial\nintelligence that focuses on teaching agents decision-making through\ninteractions with their environment. A key subset includes stochastic\nmulti-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which\nmodel sequential decision-making under uncertainty. This review outlines the\nfoundational models and assumptions of bandit problems, explores non-asymptotic\ntheoretical tools like concentration inequalities and minimax regret bounds,\nand compares frequentist and Bayesian algorithms for managing\nexploration-exploitation trade-offs. Additionally, we explore K-armed\ncontextual bandits and SCAB, focusing on their methodologies and regret\nanalyses. We also examine the connections between SCAB problems and functional\ndata analysis. Finally, we highlight recent advances and ongoing challenges in\nthe field.\n","authors":["Pengjie Zhou","Haoyu Wei","Huiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.02251v2.pdf","comment":"52 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.13080v1","updated":"2025-02-18T17:33:41Z","published":"2025-02-18T17:33:41Z","title":"BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression\n  Classification","summary":"  Gene expression classification is a pivotal yet challenging task in\nbioinformatics, primarily due to the high dimensionality of genomic data and\nthe risk of overfitting. To bridge this gap, we propose BOLIMES, a novel\nfeature selection algorithm designed to enhance gene expression classification\nby systematically refining the feature subset. Unlike conventional methods that\nrely solely on statistical ranking or classifier-specific selection, we\nintegrate the robustness of Boruta with the interpretability of LIME, ensuring\nthat only the most relevant and influential genes are retained. BOLIMES first\nemploys Boruta to filter out non-informative genes by comparing each feature\nagainst its randomized counterpart, thus preserving valuable information. It\nthen uses LIME to rank the remaining genes based on their local importance to\nthe classifier. Finally, an iterative classification evaluation determines the\noptimal feature subset by selecting the number of genes that maximizes\npredictive accuracy. By combining exhaustive feature selection with\ninterpretability-driven refinement, our solution effectively balances\ndimensionality reduction with high classification performance, offering a\npowerful solution for high-dimensional gene expression analysis.\n","authors":["Bich-Chung Phan","Thanh Ma","Huu-Hoa Nguyen","and Thanh-Nghi Do"],"pdf_url":"https://arxiv.org/pdf/2502.13080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19437v2","updated":"2025-02-18T17:26:38Z","published":"2024-12-27T04:03:16Z","title":"DeepSeek-V3 Technical Report","summary":"  We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.\n","authors":[" DeepSeek-AI","Aixin Liu","Bei Feng","Bing Xue","Bingxuan Wang","Bochao Wu","Chengda Lu","Chenggang Zhao","Chengqi Deng","Chenyu Zhang","Chong Ruan","Damai Dai","Daya Guo","Dejian Yang","Deli Chen","Dongjie Ji","Erhang Li","Fangyun Lin","Fucong Dai","Fuli Luo","Guangbo Hao","Guanting Chen","Guowei Li","H. Zhang","Han Bao","Hanwei Xu","Haocheng Wang","Haowei Zhang","Honghui Ding","Huajian Xin","Huazuo Gao","Hui Li","Hui Qu","J. L. Cai","Jian Liang","Jianzhong Guo","Jiaqi Ni","Jiashi Li","Jiawei Wang","Jin Chen","Jingchang Chen","Jingyang Yuan","Junjie Qiu","Junlong Li","Junxiao Song","Kai Dong","Kai Hu","Kaige Gao","Kang Guan","Kexin Huang","Kuai Yu","Lean Wang","Lecong Zhang","Lei Xu","Leyi Xia","Liang Zhao","Litong Wang","Liyue Zhang","Meng Li","Miaojun Wang","Mingchuan Zhang","Minghua Zhang","Minghui Tang","Mingming Li","Ning Tian","Panpan Huang","Peiyi Wang","Peng Zhang","Qiancheng Wang","Qihao Zhu","Qinyu Chen","Qiushi Du","R. J. Chen","R. L. Jin","Ruiqi Ge","Ruisong Zhang","Ruizhe Pan","Runji Wang","Runxin Xu","Ruoyu Zhang","Ruyi Chen","S. S. Li","Shanghao Lu","Shangyan Zhou","Shanhuang Chen","Shaoqing Wu","Shengfeng Ye","Shengfeng Ye","Shirong Ma","Shiyu Wang","Shuang Zhou","Shuiping Yu","Shunfeng Zhou","Shuting Pan","T. Wang","Tao Yun","Tian Pei","Tianyu Sun","W. L. Xiao","Wangding Zeng","Wanjia Zhao","Wei An","Wen Liu","Wenfeng Liang","Wenjun Gao","Wenqin Yu","Wentao Zhang","X. Q. Li","Xiangyue Jin","Xianzu Wang","Xiao Bi","Xiaodong Liu","Xiaohan Wang","Xiaojin Shen","Xiaokang Chen","Xiaokang Zhang","Xiaosha Chen","Xiaotao Nie","Xiaowen Sun","Xiaoxiang Wang","Xin Cheng","Xin Liu","Xin Xie","Xingchao Liu","Xingkai Yu","Xinnan Song","Xinxia Shan","Xinyi Zhou","Xinyu Yang","Xinyuan Li","Xuecheng Su","Xuheng Lin","Y. K. Li","Y. Q. Wang","Y. X. Wei","Y. X. Zhu","Yang Zhang","Yanhong Xu","Yanhong Xu","Yanping Huang","Yao Li","Yao Zhao","Yaofeng Sun","Yaohui Li","Yaohui Wang","Yi Yu","Yi Zheng","Yichao Zhang","Yifan Shi","Yiliang Xiong","Ying He","Ying Tang","Yishi Piao","Yisong Wang","Yixuan Tan","Yiyang Ma","Yiyuan Liu","Yongqiang Guo","Yu Wu","Yuan Ou","Yuchen Zhu","Yuduan Wang","Yue Gong","Yuheng Zou","Yujia He","Yukun Zha","Yunfan Xiong","Yunxian Ma","Yuting Yan","Yuxiang Luo","Yuxiang You","Yuxuan Liu","Yuyang Zhou","Z. F. Wu","Z. Z. Ren","Zehui Ren","Zhangli Sha","Zhe Fu","Zhean Xu","Zhen Huang","Zhen Zhang","Zhenda Xie","Zhengyan Zhang","Zhewen Hao","Zhibin Gou","Zhicheng Ma","Zhigang Yan","Zhihong Shao","Zhipeng Xu","Zhiyu Wu","Zhongyu Zhang","Zhuoshu Li","Zihui Gu","Zijia Zhu","Zijun Liu","Zilin Li","Ziwei Xie","Ziyang Song","Ziyi Gao","Zizheng Pan"],"pdf_url":"https://arxiv.org/pdf/2412.19437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13399v3","updated":"2025-02-18T17:16:55Z","published":"2024-07-18T11:08:40Z","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overoptimization via Chi-Squared Preference Optimization","summary":"  Language model alignment methods such as reinforcement learning from human\nfeedback (RLHF) have led to impressive advances in language model capabilities,\nbut are limited by a widely observed phenomenon known as overoptimization,\nwhere the quality of the language model degrades over the course of the\nalignment process. As the model optimizes performance with respect to an\noffline reward model, it overfits to inaccuracies and drifts away from\npreferred responses covered by the data. To discourage such distribution shift,\nKL-regularization is widely employed in existing offline alignment methods, but\noveroptimization continues to harm performance. Lending theoretical insight\ninto the source of these empirical observations, we first show that the\nKL-regularization is too weak to prevent overfitting, then raise the following\nquestion: is it possible to design an efficient algorithm that is provably\nrobust to overoptimization?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","authors":["Audrey Huang","Wenhao Zhan","Tengyang Xie","Jason D. Lee","Wen Sun","Akshay Krishnamurthy","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2407.13399v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13069v1","updated":"2025-02-18T17:12:26Z","published":"2025-02-18T17:12:26Z","title":"Interactive Agents to Overcome Ambiguity in Software Engineering","summary":"  AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements.\n","authors":["Sanidhya Vijayvargiya","Xuhui Zhou","Akhila Yerukola","Maarten Sap","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2502.13069v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.06254v2","updated":"2025-02-18T17:10:39Z","published":"2025-01-09T02:54:19Z","title":"Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words","summary":"  Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.\n","authors":["Gouki Minegishi","Hiroki Furuta","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2501.06254v2.pdf","comment":"Published at ICLR2025"},{"id":"http://arxiv.org/abs/2502.13062v1","updated":"2025-02-18T17:08:21Z","published":"2025-02-18T17:08:21Z","title":"AI-Assisted Decision Making with Human Learning","summary":"  AI systems increasingly support human decision-making. In many cases, despite\nthe algorithm's superior performance, the final decision remains in human\nhands. For example, an AI may assist doctors in determining which diagnostic\ntests to run, but the doctor ultimately makes the diagnosis. This paper studies\nsuch AI-assisted decision-making settings, where the human learns through\nrepeated interactions with the algorithm. In our framework, the algorithm --\ndesigned to maximize decision accuracy according to its own model -- determines\nwhich features the human can consider. The human then makes a prediction based\non their own less accurate model. We observe that the discrepancy between the\nalgorithm's model and the human's model creates a fundamental tradeoff. Should\nthe algorithm prioritize recommending more informative features, encouraging\nthe human to recognize their importance, even if it results in less accurate\npredictions in the short term until learning occurs? Or is it preferable to\nforgo educating the human and instead select features that align more closely\nwith their existing understanding, minimizing the immediate cost of learning?\nThis tradeoff is shaped by the algorithm's time-discounted objective and the\nhuman's learning ability. Our results show that optimal feature selection has a\nsurprisingly clean combinatorial characterization, reducible to a stationary\nsequence of feature subsets that is tractable to compute. As the algorithm\nbecomes more \"patient\" or the human's learning improves, the algorithm\nincreasingly selects more informative features, enhancing both prediction\naccuracy and the human's understanding. Notably, early investment in learning\nleads to the selection of more informative features than a later investment. We\ncomplement our analysis by showing that the impact of errors in the algorithm's\nknowledge is limited as it does not make the prediction directly.\n","authors":["Gali Noti","Kate Donahue","Jon Kleinberg","Sigal Oren"],"pdf_url":"https://arxiv.org/pdf/2502.13062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13061v1","updated":"2025-02-18T17:07:29Z","published":"2025-02-18T17:07:29Z","title":"Improved Fine-Tuning of Large Multimodal Models for Hateful Meme\n  Detection","summary":"  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While large multimodal models\nhave shown strong generalization across various tasks, they exhibit poor\ngeneralization to hateful meme detection due to the dynamic nature of memes\ntied to emerging social trends and breaking news. Recent work further\nhighlights the limitations of conventional supervised fine-tuning for large\nmultimodal models in this context. To address these challenges, we propose\nLarge Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a\nnovel two-stage fine-tuning framework designed to improve both in-domain\naccuracy and cross-domain generalization. Experimental results on six widely\nused meme classification datasets demonstrate that LMM-RGCL achieves\nstate-of-the-art performance, outperforming agent-based systems such as\nVPD-PALI-X-55B. Furthermore, our method effectively generalizes to\nout-of-domain memes under low-resource settings, surpassing models like GPT-4o.\n","authors":["Jingbiao Mei","Jinghong Chen","Guangyu Yang","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2502.13061v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.05001v2","updated":"2025-02-18T17:06:41Z","published":"2025-02-07T15:22:15Z","title":"A New Paradigm in Tuning Learned Indexes: A Reinforcement Learning\n  Enhanced Approach","summary":"  Learned Index Structures (LIS) have significantly advanced data management by\nleveraging machine learning models to optimize data indexing. However,\ndesigning these structures often involves critical trade-offs, making it\nchallenging for both designers and end-users to find an optimal balance\ntailored to specific workloads and scenarios. While some indexes offer\nadjustable parameters that demand intensive manual tuning, others rely on fixed\nconfigurations based on heuristic auto-tuners or expert knowledge, which may\nnot consistently deliver optimal performance. This paper introduces LITune, a\nnovel framework for end-to-end automatic tuning of Learned Index Structures.\nLITune employs an adaptive training pipeline equipped with a tailor-made Deep\nReinforcement Learning (DRL) approach to ensure stable and efficient tuning. To\naccommodate long-term dynamics arising from online tuning, we further enhance\nLITune with an on-the-fly updating mechanism termed the O2 system. These\ninnovations allow LITune to effectively capture state transitions in online\ntuning scenarios and dynamically adjust to changing data distributions and\nworkloads, marking a significant improvement over other tuning methods. Our\nexperimental results demonstrate that LITune achieves up to a 98% reduction in\nruntime and a 17-fold increase in throughput compared to default parameter\nsettings given a selected Learned Index instance. These findings highlight\nLITune's effectiveness and its potential to facilitate broader adoption of LIS\nin real-world applications.\n","authors":["Taiyi Wang","Liang Liang","Guang Yang","Thomas Heinis","Eiko Yoneki"],"pdf_url":"https://arxiv.org/pdf/2502.05001v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.06656v2","updated":"2025-02-18T17:05:33Z","published":"2025-02-10T16:47:00Z","title":"A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management","summary":"  The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.\n","authors":["Simeon Campos","Henry Papadatos","Fabien Roger","Chlo Touzet","Malcolm Murray","Otter Quarks"],"pdf_url":"https://arxiv.org/pdf/2502.06656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13055v1","updated":"2025-02-18T17:01:37Z","published":"2025-02-18T17:01:37Z","title":"LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs","summary":"  The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.\n","authors":["Xingzhi Qian","Xinran Zheng","Yiling He","Shuo Yang","Lorenzo Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2502.13055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13034v1","updated":"2025-02-18T16:48:18Z","published":"2025-02-18T16:48:18Z","title":"Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions","summary":"  The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.\n","authors":["Aditya K Surikuchi","Raquel Fernndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2502.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13030v1","updated":"2025-02-18T16:46:44Z","published":"2025-02-18T16:46:44Z","title":"Likelihood-Ratio Regularized Quantile Regression: Adapting Conformal\n  Prediction to High-Dimensional Covariate Shifts","summary":"  We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, and an image classification task\nfrom the WILDS repository.\n","authors":["Sunay Joshi","Shayan Kiyani","George Pappas","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2502.13030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08643v2","updated":"2025-02-18T16:45:59Z","published":"2025-02-12T18:57:22Z","title":"A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards","summary":"  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n","authors":["Shivansh Patel","Xinchen Yin","Wenlong Huang","Shubham Garg","Hooshang Nayyeri","Li Fei-Fei","Svetlana Lazebnik","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2502.08643v2.pdf","comment":"ICRA 2025, Project Page: https://iker-robot.github.io/"},{"id":"http://arxiv.org/abs/2502.13025v1","updated":"2025-02-18T16:44:42Z","published":"2025-02-18T16:44:42Z","title":"Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks","summary":"  We present an agentic, autonomous graph expansion framework that iteratively\nstructures and refines knowledge in situ. Unlike conventional knowledge graph\nconstruction methods relying on static extraction or single-pass learning, our\napproach couples a reasoning-native large language model with a continually\nupdated graph representation. At each step, the system actively generates new\nconcepts and relationships, merges them into a global graph, and formulates\nsubsequent prompts based on its evolving structure. Through this\nfeedback-driven loop, the model organizes information into a scale-free network\ncharacterized by hub formation, stable modularity, and bridging nodes that link\ndisparate knowledge clusters. Over hundreds of iterations, new nodes and edges\ncontinue to appear without saturating, while centrality measures and shortest\npath distributions evolve to yield increasingly distributed connectivity. Our\nanalysis reveals emergent patterns, such as the rise of highly connected 'hub'\nconcepts and the shifting influence of 'bridge' nodes, indicating that agentic,\nself-reinforcing graph construction can yield open-ended, coherent knowledge\nstructures. Applied to materials design problems, we present compositional\nreasoning experiments by extracting node-specific and synergy-level principles\nto foster genuinely novel knowledge synthesis, yielding cross-domain ideas that\ntranscend rote summarization and strengthen the framework's potential for\nopen-ended scientific discovery. We discuss other applications in scientific\ndiscovery and outline future directions for enhancing scalability and\ninterpretability.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2502.13025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16813v2","updated":"2025-02-18T16:36:25Z","published":"2024-09-25T11:09:39Z","title":"PeerArg: Argumentative Peer Review with LLMs","summary":"  Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM.\n","authors":["Purin Sukpanichnant","Anna Rapberger","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2409.16813v2.pdf","comment":"Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)"},{"id":"http://arxiv.org/abs/2502.13016v1","updated":"2025-02-18T16:34:45Z","published":"2025-02-18T16:34:45Z","title":"LLM-Powered Proactive Data Systems","summary":"  With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda.\n","authors":["Sepanta Zeighami","Yiming Lin","Shreya Shankar","Aditya Parameswaran"],"pdf_url":"https://arxiv.org/pdf/2502.13016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13013v1","updated":"2025-02-18T16:33:38Z","published":"2025-02-18T16:33:38Z","title":"HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit","summary":"  Current humanoid teleoperation systems either lack reliable low-level control\npolicies, or struggle to acquire accurate whole-body control commands, making\nit difficult to teleoperate humanoids for loco-manipulation tasks. To solve\nthese issues, we propose HOMIE, a novel humanoid teleoperation cockpit\nintegrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based\nhardware system. The policy enables humanoid robots to walk and squat to\nspecific heights while accommodating arbitrary upper-body poses. This is\nachieved through our novel reinforcement learning-based training framework that\nincorporates upper-body pose curriculum, height-tracking reward, and symmetry\nutilization, without relying on any motion priors. Complementing the policy,\nthe hardware system integrates isomorphic exoskeleton arms, a pair of\nmotion-sensing gloves, and a pedal, allowing a single operator to achieve full\ncontrol of the humanoid robot. Our experiments show our cockpit facilitates\nmore stable, rapid, and precise humanoid loco-manipulation teleoperation,\naccelerating task completion and eliminating retargeting errors compared to\ninverse kinematics-based methods. We also validate the effectiveness of the\ndata collected by our cockpit for imitation learning. Our project is fully\nopen-sourced, demos and code can be found in https://homietele.github.io/.\n","authors":["Qingwei Ben","Feiyu Jia","Jia Zeng","Junting Dong","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.13013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13006v1","updated":"2025-02-18T16:26:21Z","published":"2025-02-18T16:26:21Z","title":"Integrating Reinforcement Learning, Action Model Learning, and Numeric\n  Planning for Tackling Complex Tasks","summary":"  Automated Planning algorithms require a model of the domain that specifies\nthe preconditions and effects of each action. Obtaining such a domain model is\nnotoriously hard. Algorithms for learning domain models exist, yet it remains\nunclear whether learning a domain model and planning is an effective approach\nfor numeric planning environments, i.e., where states include discrete and\nnumeric state variables. In this work, we explore the benefits of learning a\nnumeric domain model and compare it with alternative model-free solutions. As a\ncase study, we use two tasks in Minecraft, a popular sandbox game that has been\nused as an AI challenge. First, we consider an offline learning setting, where\na set of expert trajectories are available to learn from. This is the standard\nsetting for learning domain models. We used the Numeric Safe Action Model\nLearning (NSAM) algorithm to learn a numeric domain model and solve new\nproblems with the learned domain model and a numeric planner. We call this\nmodel-based solution NSAM_(+p), and compare it to several model-free Imitation\nLearning (IL) and Offline Reinforcement Learning (RL) algorithms. Empirical\nresults show that some IL algorithms can learn faster to solve simple tasks,\nwhile NSAM_(+p) allows solving tasks that require long-term planning and\nenables generalizing to solve problems in larger environments. Then, we\nconsider an online learning setting, where learning is done by moving an agent\nin the environment. For this setting, we introduce RAMP. In RAMP, observations\ncollected during the agent's execution are used to simultaneously train an RL\npolicy and learn a planning domain action model. This forms a positive feedback\nloop between the RL policy and the learned domain model. We demonstrate\nexperimentally the benefits of using RAMP, showing that it finds more efficient\nplans and solves more problems than several RL baselines.\n","authors":["Yarin Benyamin","Argaman Mordoch","Shahaf S. Shperberg","Roni Stern"],"pdf_url":"https://arxiv.org/pdf/2502.13006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13001v1","updated":"2025-02-18T16:21:22Z","published":"2025-02-18T16:21:22Z","title":"You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations","summary":"  Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.\n","authors":["Frederic Kirstein","Muneeb Khan","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2502.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12998v1","updated":"2025-02-18T16:19:08Z","published":"2025-02-18T16:19:08Z","title":"Personalized Top-k Set Queries Over Predicted Scores","summary":"  This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications.\n","authors":["Sohrab Namazi Nia","Subhodeep Ghosh","Senjuti Basu Roy","Sihem Amer-Yahia"],"pdf_url":"https://arxiv.org/pdf/2502.12998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12995v1","updated":"2025-02-18T16:15:36Z","published":"2025-02-18T16:15:36Z","title":"Free Argumentative Exchanges for Explaining Image Classifiers","summary":"  Deep learning models are powerful image classifiers but their opacity hinders\ntheir trustworthiness. Explanation methods for capturing the reasoning process\nwithin these classifiers faithfully and in a clear manner are scarce, due to\ntheir sheer complexity and size. We provide a solution for this problem by\ndefining a novel method for explaining the outputs of image classifiers with\ndebates between two agents, each arguing for a particular class. We obtain\nthese debates as concrete instances of Free Argumentative eXchanges (FAXs), a\nnovel argumentation-based multi-agent framework allowing agents to internalise\nopinions by other agents differently than originally stated. We define two\nmetrics (consensus and persuasion rate) to assess the usefulness of FAXs as\nargumentative explanations for image classifiers. We then conduct a number of\nempirical experiments showing that FAXs perform well along these metrics as\nwell as being more faithful to the image classifiers than conventional,\nnon-argumentative explanation methods. All our implementations can be found at\nhttps://github.com/koriavinash1/FAX.\n","authors":["Avinash Kori","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2502.12995v1.pdf","comment":"10 pages, 3 figures. To be published at AAMAS 2025"},{"id":"http://arxiv.org/abs/2502.12992v1","updated":"2025-02-18T16:13:08Z","published":"2025-02-18T16:13:08Z","title":"B-cos LM: Efficiently Transforming Pre-trained Language Models for\n  Improved Explainability","summary":"  Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\nimprove model explainability through architectural and computational\nadaptations, but their application has so far been limited to computer vision\nmodels and their associated training pipelines. In this work, we introduce\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\ntransforms pre-trained language models into B-cos LMs by combining B-cos\nconversion and task fine-tuning, improving efficiency compared to previous\nB-cos methods. Our automatic and human evaluation results demonstrate that\nB-cos LMs produce more faithful and human interpretable explanations than post\nhoc methods, while maintaining task performance comparable to conventional\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\nconventionally fine-tuned models in their learning processes and explanation\npatterns. Finally, we provide practical guidelines for effectively building\nB-cos LMs based on our findings. Our code is available at\nhttps://anonymous.4open.science/r/bcos_lm.\n","authors":["Yifan Wang","Sukrut Rao","Ji-Ung Lee","Mayank Jobanputra","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.12992v1.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.01534v4","updated":"2025-02-18T16:08:53Z","published":"2023-11-02T18:33:32Z","title":"Approximate Multiagent Reinforcement Learning for On-Demand Urban\n  Mobility Problem on a Large Map (extended version)","summary":"  In this paper, we focus on the autonomous multiagent taxi routing problem for\na large urban environment where the location and number of future ride requests\nare unknown a-priori, but can be estimated by an empirical distribution. Recent\ntheory has shown that a rollout algorithm with a stable base policy produces a\nnear-optimal stable policy. In the routing setting, a policy is stable if its\nexecution keeps the number of outstanding requests uniformly bounded over time.\nAlthough, rollout-based approaches are well-suited for learning cooperative\nmultiagent policies with considerations for future demand, applying such\nmethods to a large urban environment can be computationally expensive due to\nthe large number of taxis required for stability. In this paper, we aim to\naddress the computational bottleneck of multiagent rollout by proposing an\napproximate multiagent rollout-based two phase algorithm that reduces\ncomputational costs, while still achieving a stable near-optimal policy. Our\napproach partitions the graph into sectors based on the predicted demand and\nthe maximum number of taxis that can run sequentially given the user's\ncomputational resources. The algorithm then applies instantaneous assignment\n(IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout\nalgorithm that is executed in parallel for each sector. We provide two main\ntheoretical results: 1) characterize the number of taxis $m$ that is sufficient\nfor IA to be stable; 2) derive a necessary condition on $m$ to maintain\nstability for IA as time goes to infinity. Our numerical results show that our\napproach achieves stability for an $m$ that satisfies the theoretical\nconditions. We also empirically demonstrate that our proposed two phase\nalgorithm has equivalent performance to the one-at-a-time rollout over the\nentire map, but with significantly lower runtimes.\n","authors":["Daniel Garces","Sushmita Bhattacharya","Dimitri Bertsekas","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2311.01534v4.pdf","comment":"12 pages, 5 figures, 1 lemma, and 2 theorems"},{"id":"http://arxiv.org/abs/2502.12985v1","updated":"2025-02-18T16:08:47Z","published":"2025-02-18T16:08:47Z","title":"PartSDF: Part-Based Implicit Neural Representation for Composite 3D\n  Shape Parametrization and Optimization","summary":"  Accurate 3D shape representation is essential in engineering applications\nsuch as design, optimization, and simulation. In practice, engineering\nworkflows require structured, part-aware representations, as objects are\ninherently designed as assemblies of distinct components. However, most\nexisting methods either model shapes holistically or decompose them without\npredefined part structures, limiting their applicability in real-world design\ntasks. We propose PartSDF, a supervised implicit representation framework that\nexplicitly models composite shapes with independent, controllable parts while\nmaintaining shape consistency. Despite its simple single-decoder architecture,\nPartSDF outperforms both supervised and unsupervised baselines in\nreconstruction and generation tasks. We further demonstrate its effectiveness\nas a structured shape prior for engineering applications, enabling precise\ncontrol over individual components while preserving overall coherence. Code\navailable at https://github.com/cvlab-epfl/PartSDF.\n","authors":["Nicolas Talabot","Olivier Clerc","Arda Cinar Demirtas","Doruk Oner","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2502.12985v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.12982v1","updated":"2025-02-18T16:04:57Z","published":"2025-02-18T16:04:57Z","title":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","summary":"  Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.\n","authors":["Longxu Dou","Qian Liu","Fan Zhou","Changyu Chen","Zili Wang","Ziqi Jin","Zichen Liu","Tongyao Zhu","Cunxiao Du","Penghui Yang","Haonan Wang","Jiaheng Liu","Yongchi Zhao","Xiachong Feng","Xin Mao","Man Tsung Yeung","Kunat Pipatanakul","Fajri Koto","Min Si Thu","Hynek Kydlek","Zeyi Liu","Qunshu Lin","Sittipong Sripaisarnmongkol","Kridtaphad Sae-Khow","Nirattisai Thongchim","Taechawat Konkaew","Narong Borijindargoon","Anh Dao","Matichon Maneegard","Phakphum Artkaew","Zheng-Xin Yong","Quan Nguyen","Wannaphong Phatthiyaphaibun","Hoang H. Tran","Mike Zhang","Shiqi Chen","Tianyu Pang","Chao Du","Xinyi Wan","Wei Lu","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2502.12982v1.pdf","comment":"49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/"},{"id":"http://arxiv.org/abs/2502.12965v1","updated":"2025-02-18T15:46:54Z","published":"2025-02-18T15:46:54Z","title":"A Survey of Text Classification Under Class Distribution Shift","summary":"  The basic underlying assumption of machine learning (ML) models is that the\ntraining and test data are sampled from the same distribution. However, in\ndaily practice, this assumption is often broken, i.e.~the distribution of the\ntest data changes over time, which hinders the application of conventional ML\nmodels. One domain where the distribution shift naturally occurs is text\nclassification, since people always find new topics to discuss. To this end, we\nsurvey research articles studying open-set text classification and related\ntasks. We divide the methods in this area based on the constraints that define\nthe kind of distribution shift and the corresponding problem formulation,\ni.e.~learning with the Universum, zero-shot learning, and open-set learning. We\nnext discuss the predominant mitigation approaches for each problem setup.\nFinally, we identify several future work directions, aiming to push the\nboundaries beyond the state of the art. Interestingly, we find that continual\nlearning can solve many of the issues caused by the shifting class\ndistribution. We maintain a list of relevant papers at\nhttps://github.com/Eduard6421/Open-Set-Survey.\n","authors":["Adriana Valentina Costache","Silviu Florin Gheorghe","Eduard Gabriel Poesina","Paul Irofti","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12961v1","updated":"2025-02-18T15:45:01Z","published":"2025-02-18T15:45:01Z","title":"Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger","summary":"  Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.\n","authors":["Wenjun Li","Dexun Li","Kuicai Dong","Cong Zhang","Hao Zhang","Weiwen Liu","Yasheng Wang","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12959v1","updated":"2025-02-18T15:43:27Z","published":"2025-02-18T15:43:27Z","title":"AlignFreeze: Navigating the Impact of Realignment on the Layers of\n  Multilingual Models Across Diverse Languages","summary":"  Realignment techniques are often employed to enhance cross-lingual transfer\nin multilingual language models, still, they can sometimes degrade performance\nin languages that differ significantly from the fine-tuned source language.\nThis paper introduces AlignFreeze, a method that freezes either the layers'\nlower half or upper half during realignment. Through controlled experiments on\n4 tasks, 3 models, and in 35 languages, we find that realignment affects all\nthe layers but can be the most detrimental to the lower ones. Freezing the\nlower layers can prevent performance degradation. Particularly, AlignFreeze\nimproves Part-of-Speech (PoS) tagging performances in languages where full\nrealignment fails: with XLM-R, it provides improvements of more than one\nstandard deviation in accuracy in seven more languages than full realignment.\n","authors":["Steve Bakos","Flix Gaschi","David Guzmn","Riddhi More","Kelly Chutong Li","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2502.12959v1.pdf","comment":"24 pages, 2 figures, to be published in Proceedings of NAACL 2025"},{"id":"http://arxiv.org/abs/2410.07708v2","updated":"2025-02-18T15:41:56Z","published":"2024-10-10T08:20:57Z","title":"Learning Tree Pattern Transformations","summary":"  Explaining why and how a tree $t$ structurally differs from another tree\n$t^\\star$ is a question that is encountered throughout computer science,\nincluding in understanding tree-structured data such as XML or JSON data. In\nthis article, we explore how to learn explanations for structural differences\nbetween pairs of trees from sample data: suppose we are given a set $\\{(t_1,\nt_1^\\star),\\dots, (t_n, t_n^\\star)\\}$ of pairs of labelled, ordered trees; is\nthere a small set of rules that explains the structural differences between all\npairs $(t_i, t_i^\\star)$? This raises two research questions: (i) what is a\ngood notion of \"rule\" in this context?; and (ii) how can sets of rules\nexplaining a data set be learned algorithmically?\n  We explore these questions from the perspective of database theory by (1)\nintroducing a pattern-based specification language for tree transformations;\n(2) exploring the computational complexity of variants of the above algorithmic\nproblem, e.g. showing NP-hardness for very restricted variants; and (3)\ndiscussing how to solve the problem for data from CS education research using\nSAT solvers.\n","authors":["Daniel Neider","Leif Sabellek","Johannes Schmidt","Fabian Vehlken","Thomas Zeume"],"pdf_url":"https://arxiv.org/pdf/2410.07708v2.pdf","comment":"Full version of the ICDT 2025 paper"},{"id":"http://arxiv.org/abs/2502.12953v1","updated":"2025-02-18T15:36:16Z","published":"2025-02-18T15:36:16Z","title":"Task-Informed Anti-Curriculum by Masking Improves Downstream Performance\n  on Text","summary":"  Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.\n","authors":["Andrei Jarca","Florinel Alin Croitoru","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12948v1","updated":"2025-02-18T15:30:48Z","published":"2025-02-18T15:30:48Z","title":"Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for\n  Improved Text-Based Learning for LGE Detection","summary":"  Detection of hyperenhancement from cardiac LGE MRI images is a complex task\nrequiring significant clinical expertise. Although deep learning-based models\nhave shown promising results for the task, they require large amounts of data\nwith fine-grained annotations. Clinical reports generated for cardiac MR\nstudies contain rich, clinically relevant information, including the location,\nextent and etiology of any scars present. Although recently developed\nCLIP-based training enables pretraining models with image-text pairs, it\nrequires large amounts of data and further finetuning strategies on downstream\ntasks. In this study, we use various strategies rooted in domain knowledge to\ntrain a model for LGE detection solely using text from clinical reports, on a\nrelatively small clinical cohort of 965 patients. We improve performance\nthrough the use of synthetic data augmentation, by systematically creating scar\nimages and associated text. In addition, we standardize the orientation of the\nimages in an anatomy-informed way to enable better alignment of spatial and\ntext features. We also use a captioning loss to enable fine-grained supervision\nand explore the effect of pretraining of the vision encoder on performance.\nFinally, ablation studies are carried out to elucidate the contributions of\neach design component to the overall performance of the model.\n","authors":["Athira J Jacob","Puneet Sharma","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2502.12948v1.pdf","comment":"Poster at Workshop on Large Language Models and Generative AI for\n  Health at AAAI 2025"},{"id":"http://arxiv.org/abs/2405.03524v5","updated":"2025-02-18T15:30:43Z","published":"2024-05-06T14:40:50Z","title":"A short Survey: Exploring knowledge graph-based neural-symbolic system\n  from application perspective","summary":"  Advancements in Artificial Intelligence (AI) and deep neural networks have\ndriven significant progress in vision and text processing. However, achieving\nhuman-like reasoning and interpretability in AI systems remains a substantial\nchallenge. The Neural-Symbolic paradigm, which integrates neural networks with\nsymbolic systems, presents a promising pathway toward more interpretable AI.\nWithin this paradigm, Knowledge Graphs (KG) are crucial, offering a structured\nand dynamic method for representing knowledge through interconnected entities\nand relationships, typically as triples (subject, predicate, object). This\npaper explores recent advancements in neural-symbolic integration based on KG,\nexamining how it supports integration in three categories: enhancing the\nreasoning and interpretability of neural networks with symbolic knowledge\n(Symbol for Neural), refining the completeness and accuracy of symbolic systems\nvia neural network methodologies (Neural for Symbol), and facilitating their\ncombined application in Hybrid Neural-Symbolic Integration. It highlights\ncurrent trends and proposes future research directions in Neural-Symbolic AI.\n","authors":["Shenzhe Zhu","Shengxiang Sun"],"pdf_url":"https://arxiv.org/pdf/2405.03524v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12947v1","updated":"2025-02-18T15:30:34Z","published":"2025-02-18T15:30:34Z","title":"Every Expert Matters: Towards Effective Knowledge Distillation for\n  Mixture-of-Experts Language Models","summary":"  With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.\n","authors":["Gyeongman Kim","Gyouk Chu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15175v3","updated":"2025-02-18T15:22:49Z","published":"2024-11-18T00:21:14Z","title":"ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?","summary":"  Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools.\n","authors":["Zheng Hui","Zhaoxiao Guo","Hang Zhao","Juanyong Duan","Lin Ai","Yinheng Li","Julia Hirschberg","Congrui Huang"],"pdf_url":"https://arxiv.org/pdf/2411.15175v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2404.12917v3","updated":"2025-02-18T15:17:38Z","published":"2024-04-19T14:42:42Z","title":"R3L: Relative Representations for Reinforcement Learning","summary":"  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.\n","authors":["Antonio Pio Ricciardi","Valentino Maiorca","Luca Moschella","Riccardo Marin","Emanuele Rodol"],"pdf_url":"https://arxiv.org/pdf/2404.12917v3.pdf","comment":"12 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.01706v3","updated":"2025-02-18T15:15:20Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to\nachieve computationally efficient processing of multi-agent observations with\nlong context memory for temporal reasoning. Through extensive evaluations\nacross six diverse environments, we demonstrate how Sable is able to\nsignificantly outperform existing state-of-the-art methods in a large number of\ndiverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance\nas we scale the number of agents, handling environments with more than a\nthousand agents while exhibiting a linear increase in memory usage. Finally, we\nconduct ablation studies to isolate the source of Sable's performance gains and\nconfirm its efficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12929v1","updated":"2025-02-18T15:11:46Z","published":"2025-02-18T15:11:46Z","title":"Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options","summary":"  We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.\n","authors":["Lakshmi Nair","Ian Trase","Mark Kim"],"pdf_url":"https://arxiv.org/pdf/2502.12929v1.pdf","comment":"Github code: https://github.com/flagshippioneering/Flow-of-Options"},{"id":"http://arxiv.org/abs/2502.12926v1","updated":"2025-02-18T15:07:06Z","published":"2025-02-18T15:07:06Z","title":"Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework","summary":"  Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.\n","authors":["Mourad Aouini","Jinan Loubani"],"pdf_url":"https://arxiv.org/pdf/2502.12926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12925v1","updated":"2025-02-18T15:04:33Z","published":"2025-02-18T15:04:33Z","title":"Keep what you need : extracting efficient subnetworks from large audio\n  representation models","summary":"  Recently, research on audio foundation models has witnessed notable advances,\nas illustrated by the ever improving results on complex downstream tasks.\nSubsequently, those pretrained networks have quickly been used for various\naudio applications. These improvements have however resulted in a considerable\nincrease both in size and complexity of these models. Along the environmental\nconcerns this issue raises, this prevents the deployment of such networks on\nconsumer-level devices, and precludes their use for real-time applications.\nMoreover, this appears contradictory with the specificity of the tasks for\nwhich these models are used, which are often simpler compared to extracting a\nrich, multi-purpose representation from any type of audio data. In this paper,\nwe address this issue with a simple, yet effective method to extract\nlightweight specialist subnetworks from large foundation models. Specifically,\nwe introduce learnable binary masks in-between the layers of a pretrained\nrepresentation model. When training the end-to-end model on a downstream task,\nwe add a sparsity-inducing loss to the overall objective, hence learning a\ncompact subnetwork specialized on a single task. Importantly, the weights of\nthe foundation model are kept frozen, resulting into low additional training\ncosts. Once trained, the masked computational units can then be removed from\nthe network, implying significant performance gains. We assess our method on\nthree widespread audio foundation models, each based on a different backbone\narchitecture, and illustrate its effectiveness on common audio representation\nevaluation tasks, as well as its versatility on both speech, music, and general\naudio. Code for reproducing the results and supporting webpage are available at\nhttps://github.com/gnvIRCAM/Audio-representation-trimming\n","authors":["David Genova","Philippe Esling","Tom Hurlin"],"pdf_url":"https://arxiv.org/pdf/2502.12925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12924v1","updated":"2025-02-18T15:04:13Z","published":"2025-02-18T15:04:13Z","title":"Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data","summary":"  Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.\n","authors":["Maite Heredia","Gorka Labaka","Jeremy Barnes","Aitor Soroa"],"pdf_url":"https://arxiv.org/pdf/2502.12924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12913v1","updated":"2025-02-18T14:54:55Z","published":"2025-02-18T14:54:55Z","title":"GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning","summary":"  Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.\n","authors":["Sifan Zhou","Shuo Wang","Zhihang Yuan","Mingjia Shi","Yuzhang Shang","Dawei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12908v1","updated":"2025-02-18T14:51:50Z","published":"2025-02-18T14:51:50Z","title":"Graph Neural Networks for Databases: A Survey","summary":"  Graph neural networks (GNNs) are powerful deep learning models for\ngraph-structured data, demonstrating remarkable success across diverse domains.\nRecently, the database (DB) community has increasingly recognized the\npotentiality of GNNs, prompting a surge of researches focusing on improving\ndatabase systems through GNN-based approaches. However, despite notable\nadvances, There is a lack of a comprehensive review and understanding of how\nGNNs could improve DB systems. Therefore, this survey aims to bridge this gap\nby providing a structured and in-depth overview of GNNs for DB systems.\nSpecifically, we propose a new taxonomy that classifies existing methods into\ntwo key categories: (1) Relational Databases, which includes tasks like\nperformance prediction, query optimization, and text-to-SQL, and (2) Graph\nDatabases, addressing challenges like efficient graph query processing and\ngraph similarity computation. We systematically review key methods in each\ncategory, highlighting their contributions and practical implications. Finally,\nwe suggest promising avenues for integrating GNNs into Database systems.\n","authors":["Ziming Li","Youhuan Li","Yuyu Luo","Guoliang Li","Chuxu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12908v1.pdf","comment":"A survey focus on GNNs and databases. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.15334v3","updated":"2025-02-18T14:49:52Z","published":"2023-08-29T14:29:57Z","title":"The Responsible Development of Automated Student Feedback with\n  Generative AI","summary":"  Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]\n","authors":["Euan D Lindsay","Mike Zhang","Aditya Johri","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2308.15334v3.pdf","comment":"Pre-print of version accepted to EDUCON 2025"},{"id":"http://arxiv.org/abs/2403.11734v2","updated":"2025-02-18T14:42:02Z","published":"2024-03-18T12:42:53Z","title":"Learning More Expressive General Policies for Classical Planning Domains","summary":"  GNN-based approaches for learning general policies across planning domains\nare limited by the expressive power of $C_2$, namely; first-order logic with\ntwo variables and counting. This limitation can be overcame by transitioning to\n$k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet\nembeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-\nand $2$-GNNs that are confined to $C_2$, they require quartic time for message\nexchange and cubic space to store embeddings, rendering them infeasible in\npractice. In this work, we introduce a parameterized version R-GNN[$t$] (with\nparameter $t$) of Relational GNNs. Unlike GNNs, that are designed to perform\ncomputation on graphs, Relational GNNs are designed to do computation on\nrelational structures. When $t=\\infty$, R-GNN[$t$] approximates $3$-GNNs over\ngraphs, but using only quadratic space for embeddings. For lower values of $t$,\nsuch as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by\nexchanging fewer messages, yet interestingly, often yield the expressivity\nrequired in several planning domains. Furthermore, the new R-GNN[$t$]\narchitecture is the original R-GNN architecture with a suitable transformation\napplied to the inputs only. Experimental results illustrate the clear\nperformance gains of R-GNN[$1$] over the plain R-GNNs, and also over Edge\nTransformers that also approximate $3$-GNNs.\n","authors":["Simon Sthlberg","Blai Bonet","Hector Geffner"],"pdf_url":"https://arxiv.org/pdf/2403.11734v2.pdf","comment":"Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2410.19546v2","updated":"2025-02-18T14:38:12Z","published":"2024-10-25T13:19:26Z","title":"Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?","summary":"  Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's o1,\nhave emerged, seemingly demonstrating advanced reasoning capabilities across\ntext and image modalities. However, the depth of these advances in\nlanguage-guided perception and abstract reasoning remains underexplored, and it\nis unclear whether these models can truly live up to their ambitious promises.\nTo assess the progress and identify shortcomings, we enter the wonderland of\nBongard problems, a set of classic visual reasoning puzzles that require\nhuman-like abilities of pattern recognition and abstract reasoning. With our\nextensive evaluation setup, we show that while VLMs occasionally succeed in\nidentifying discriminative concepts and solving some of the problems, they\nfrequently falter. Surprisingly, even elementary concepts that may seem trivial\nto humans, such as simple spirals, pose significant challenges. Moreover, when\nexplicitly asked to recognize ground truth concepts, they continue to falter,\nsuggesting not only a lack of understanding of these elementary visual concepts\nbut also an inability to generalize to unseen concepts. We compare the results\nof VLMs to human performance and observe that a significant gap remains between\nhuman visual reasoning capabilities and machine cognition.\n","authors":["Antonia Wst","Tim Tobiasch","Lukas Helff","Inga Ibs","Wolfgang Stammer","Devendra S. Dhami","Constantin A. Rothkopf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12900v1","updated":"2025-02-18T14:36:39Z","published":"2025-02-18T14:36:39Z","title":"Soundwave: Less is More for Speech-Text Alignment in LLMs","summary":"  Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.\n","authors":["Yuhao Zhang","Zhiheng Liu","Fan Bu","Ruiyu Zhang","Benyou Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2502.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09410v2","updated":"2025-02-18T14:20:02Z","published":"2024-01-17T18:47:30Z","title":"Through the Looking-Glass: Transparency Implications and Challenges in\n  Enterprise AI Knowledge Systems","summary":"  Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\nthese systems get embedded in organizational settings, the information that is\nbrought to the foreground and the information that's pushed to the periphery\ncan influence how individuals see each other and how they see themselves at\nwork. In this paper, we present the looking-glass metaphor and use it to\nconceptualize AI knowledge systems as systems that reflect and distort,\nexpanding our view on transparency requirements, implications and challenges.\nWe formulate transparency as a key mediator in shaping different ways of\nseeing, including seeing into the system, which unveils its capabilities,\nlimitations and behavior, and seeing through the system, which shapes workers'\nperceptions of their own contributions and others within the organization.\nRecognizing the sociotechnical nature of these systems, we identify three\ntransparency dimensions necessary to realize the value of AI knowledge systems,\nnamely system transparency, procedural transparency and transparency of\noutcomes. We discuss key challenges hindering the implementation of these forms\nof transparency, bringing to light the wider sociotechnical gap and\nhighlighting directions for future Computer-supported Cooperative Work (CSCW)\nresearch.\n","authors":["Karina Cortias-Lorenzo","Sin Lindley","Ida Larsen-Ledet","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2401.09410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01584v3","updated":"2025-02-18T14:09:38Z","published":"2024-08-02T21:37:46Z","title":"GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS","summary":"  Multi-agent learning algorithms have been successful at generating superhuman\nplanning in various games but have had limited impact on the design of deployed\nmulti-agent planners. A key bottleneck in applying these techniques to\nmulti-agent planning is that they require billions of steps of experience. To\nenable the study of multi-agent planning at scale, we present GPUDrive.\nGPUDrive is a GPU-accelerated, multi-agent simulator built on top of the\nMadrona Game Engine capable of generating over a million simulation steps per\nsecond. Observation, reward, and dynamics functions are written directly in\nC++, allowing users to define complex, heterogeneous agent behaviors that are\nlowered to high-performance CUDA. Despite these low-level optimizations,\nGPUDrive is fully accessible through Python, offering a seamless and efficient\nworkflow for multi-agent, closed-loop simulation. Using GPUDrive, we train\nreinforcement learning agents on the Waymo Open Motion Dataset, achieving\nefficient goal-reaching in minutes and scaling to thousands of scenarios in\nhours. We open-source the code and pre-trained agents at\nhttps://github.com/Emerge-Lab/gpudrive.\n","authors":["Saman Kazemkhani","Aarav Pandya","Daphne Cornelisse","Brennan Shacklett","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2408.01584v3.pdf","comment":"ICLR 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2502.12876v1","updated":"2025-02-18T14:05:59Z","published":"2025-02-18T14:05:59Z","title":"Continuous Learning Conversational AI: A Personalized Agent Framework\n  via A2C Reinforcement Learning","summary":"  Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques.\n","authors":["Nandakishor M","Anjali M"],"pdf_url":"https://arxiv.org/pdf/2502.12876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00152v2","updated":"2025-02-18T14:02:12Z","published":"2024-12-30T21:54:33Z","title":"Temporal reasoning for timeline summarisation in social media","summary":"  This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.\n","authors":["Jiayu Song","Mahmud Akhter","Dana Atzil Slonim","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2501.00152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10774v3","updated":"2025-02-18T13:53:51Z","published":"2024-08-20T12:13:04Z","title":"Flexora: Flexible Low Rank Adaptation for Large Language Models","summary":"  Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.\n","authors":["Chenxing Wei","Yao Shu","Ying Tiffany He","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2408.10774v3.pdf","comment":"39 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.12859v1","updated":"2025-02-18T13:46:47Z","published":"2025-02-18T13:46:47Z","title":"PAFT: Prompt-Agnostic Fine-Tuning","summary":"  While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.\n","authors":["Chenxing Wei","Yao Shu","Mingwen Ou","Ying Tiffany He","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2502.12859v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.12858v1","updated":"2025-02-18T13:45:42Z","published":"2025-02-18T13:45:42Z","title":"Rejected Dialects: Biases Against African American Language in Reward\n  Models","summary":"  Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.\n","authors":["Joel Mire","Zubin Trivadi Aysola","Daniel Chechelnitsky","Nicholas Deas","Chrysoula Zerva","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2502.12858v1.pdf","comment":"Accepted to NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.12855v1","updated":"2025-02-18T13:43:06Z","published":"2025-02-18T13:43:06Z","title":"Integrating Arithmetic Learning Improves Mathematical Reasoning in\n  Smaller Models","summary":"  While large models pre-trained on high-quality data exhibit excellent\nperformance across various reasoning tasks, including mathematical reasoning\n(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical\nreasoning remains a challenging problem. Common approaches to address this\nchallenge include knowledge distillation, where smaller student models learn\nfrom large pre-trained teacher models, and data augmentation, such as\nrephrasing questions. Despite these efforts, smaller models struggle with\narithmetic computations, leading to errors in mathematical reasoning. In this\nwork, we focus on leveraging a programmatically generated arithmetic dataset to\nenhance the reasoning capabilities of smaller models. We investigate two key\napproaches to incorporate this dataset -- (1) intermediate fine-tuning, where a\nmodel is fine-tuned on the arithmetic dataset before being trained on a\nreasoning dataset, and (2) integrating the arithmetic dataset into the\ninstruction-tuning mixture, allowing the model to learn arithmetic skills\nalongside general instruction-following abilities. Our experiments on multiple\nreasoning benchmarks demonstrate that incorporating an arithmetic dataset,\nwhether through targeted fine-tuning or within the instruction-tuning mixture,\nenhances the models' arithmetic capabilities, which in turn improves their\nmathematical reasoning performance.\n","authors":["Neeraj Gangwar","Suma P Bhat","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2502.12855v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12851v1","updated":"2025-02-18T13:39:22Z","published":"2025-02-18T13:39:22Z","title":"MeMo: Towards Language Models with Associative Memory Mechanisms","summary":"  Memorization is a fundamental ability of Transformer-based Large Language\nModels, achieved through learning. In this paper, we propose a paradigm shift\nby designing an architecture to memorize text directly, bearing in mind the\nprinciple that memorization precedes learning. We introduce MeMo, a novel\narchitecture for language modeling that explicitly memorizes sequences of\ntokens in layered associative memories. By design, MeMo offers transparency and\nthe possibility of model editing, including forgetting texts. We experimented\nwith the MeMo architecture, showing the memorization power of the one-layer and\nthe multi-layer configurations.\n","authors":["Fabio Massimo Zanzotto","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Leonardo Ranaldi","Davide Venditti","Federico Ranaldi","Cristina Giannone","Andrea Favalli","Raniero Romagnoli"],"pdf_url":"https://arxiv.org/pdf/2502.12851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09207v2","updated":"2025-02-18T13:36:43Z","published":"2024-06-13T15:08:44Z","title":"Investigating potential causes of Sepsis with Bayesian network structure\n  learning","summary":"  Sepsis is a life-threatening and serious global health issue. This study\ncombines knowledge with available hospital data to investigate the potential\ncauses of Sepsis that can be affected by policy decisions. We investigate the\nunderlying causal structure of this problem by combining clinical expertise\nwith score-based, constraint-based, and hybrid structure learning algorithms. A\nnovel approach to model averaging and knowledge-based constraints was\nimplemented to arrive at a consensus structure for causal inference. The\nstructure learning process highlighted the importance of exploring data-driven\napproaches alongside clinical expertise. This includes discovering unexpected,\nalthough reasonable, relationships from a clinical perspective. Hypothetical\ninterventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and\nDiabetes suggest that the presence of any of these risk factors in patients\nincreases the likelihood of Sepsis. This finding, alongside measuring the\neffect of these risk factors on Sepsis, has potential policy implications.\nRecognising the importance of prediction in improving health outcomes related\nto Sepsis, the model is also assessed in its ability to predict Sepsis by\nevaluating accuracy, sensitivity, and specificity. These three indicators all\nhad results around 70%, and the AUC was 80%, which means the causal structure\nof the model is reasonably accurate given that the models were trained on data\navailable for commissioning purposes only.\n","authors":["Bruno Petrungaro","Neville K. Kitson","Anthony C. Constantinou"],"pdf_url":"https://arxiv.org/pdf/2406.09207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12842v1","updated":"2025-02-18T13:22:14Z","published":"2025-02-18T13:22:14Z","title":"Towards Adaptive Feedback with AI: Comparing the Feedback Quality of\n  LLMs and Teachers on Experimentation Protocols","summary":"  Effective feedback is essential for fostering students' success in scientific\ninquiry. With advancements in artificial intelligence, large language models\n(LLMs) offer new possibilities for delivering instant and adaptive feedback.\nHowever, this feedback often lacks the pedagogical validation provided by\nreal-world practitioners. To address this limitation, our study evaluates and\ncompares the feedback quality of LLM agents with that of human teachers and\nscience education experts on student-written experimentation protocols. Four\nblinded raters, all professionals in scientific inquiry and science education,\nevaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and\n3) the science education experts using a five-point Likert scale based on six\ncriteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive\nTone, Linguistic Clarity, and Technical Terminology. Our results indicate that\nLLM-generated feedback shows no significant difference to that of teachers and\nexperts in overall quality. However, the LLM agent's performance lags in the\nFeed Back dimension, which involves identifying and explaining errors within\nthe student's work context. Qualitative analysis highlighted the LLM agent's\nlimitations in contextual understanding and in the clear communication of\nspecific errors. Our findings suggest that combining LLM-generated feedback\nwith human expertise can enhance educational practices by leveraging the\nefficiency of LLMs and the nuanced understanding of educators.\n","authors":["Kathrin Seler","Arne Bewersdorff","Claudia Nerdel","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2502.12842v1.pdf","comment":"This work has been submitted to the IJAIED for possible publication"},{"id":"http://arxiv.org/abs/2501.13622v3","updated":"2025-02-18T13:05:36Z","published":"2025-01-23T12:44:45Z","title":"Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning","summary":"  The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.\n","authors":["Yulan Hu","Ge Chen","Jinman Zhao","Sheng Ouyang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.13622v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11409v5","updated":"2025-02-18T12:53:47Z","published":"2023-10-17T17:15:41Z","title":"LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks","summary":"  Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT.\n","authors":["Andreas Happe","Aaron Kaplan","Juergen Cito"],"pdf_url":"https://arxiv.org/pdf/2310.11409v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08115v2","updated":"2025-02-18T12:50:00Z","published":"2024-10-10T17:00:06Z","title":"Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System","summary":"  Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).\n","authors":["Weize Chen","Jiarui Yuan","Chen Qian","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.08115v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2408.07982v2","updated":"2025-02-18T12:48:27Z","published":"2024-08-15T07:03:00Z","title":"Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera","summary":"  The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.\n","authors":["Hiroki Tanioka","Tetsushi Ueta","Masahiko Sano"],"pdf_url":"https://arxiv.org/pdf/2408.07982v2.pdf","comment":"4 pages, 5 figures, 1 table, The 1st InterAI: Interactive AI for\n  Human-Centered Robotics workshop in conjunction with IEEE Ro-MAN 2024,\n  Pasadona, LA, USA, Aug. 2024"},{"id":"http://arxiv.org/abs/2502.12825v1","updated":"2025-02-18T12:46:18Z","published":"2025-02-18T12:46:18Z","title":"Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models","summary":"  When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.\n","authors":["Rubing Lu","Joo Sedoc","Arun Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2502.12825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20891v5","updated":"2025-02-18T12:44:35Z","published":"2024-07-30T15:07:13Z","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks","summary":"  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n","authors":["Bao Gia Doan","Afshar Shamsi","Xiao-Yu Guo","Arash Mohammadi","Hamid Alinejad-Rokny","Dino Sejdinovic","Damien Teney","Damith C. Ranasinghe","Ehsan Abbasnejad"],"pdf_url":"https://arxiv.org/pdf/2407.20891v5.pdf","comment":"This paper is accepted in AAAI'25\", and the code is available at\n  https://bnn-bella.github.io/BNN-Bella/"},{"id":"http://arxiv.org/abs/2410.12537v2","updated":"2025-02-18T12:40:13Z","published":"2024-10-16T13:19:03Z","title":"Is Complex Query Answering Really Complex?","summary":"  Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum\nas a challenging reasoning task. In this paper, we show that the current\nbenchmarks for CQA might not be as complex as we think, as the way they are\nbuilt distorts our perception of progress in this field. For example, we find\nthat in these benchmarks, most queries (up to 98% for some query types) can be\nreduced to simpler problems, e.g., link prediction, where only one link needs\nto be predicted. The performance of state-of-the-art CQA models decreases\nsignificantly when such models are evaluated on queries that cannot be reduced\nto easier types. Thus, we propose a set of more challenging benchmarks composed\nof queries that require models to reason over multiple hops and better reflect\nthe construction of real-world KGs. In a systematic empirical investigation,\nthe new benchmarks show that current methods leave much to be desired from\ncurrent CQA methods.\n","authors":["Cosimo Gregucci","Bo Xiong","Daniel Hernandez","Lorenzo Loconte","Pasquale Minervini","Steffen Staab","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2410.12537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12866v2","updated":"2025-02-18T12:28:03Z","published":"2024-10-13T18:09:12Z","title":"Towards Homogeneous Lexical Tone Decoding from Heterogeneous\n  Intracranial Recordings","summary":"  Recent advancements in brain-computer interfaces (BCIs) have enabled the\ndecoding of lexical tones from intracranial recordings, offering the potential\nto restore the communication abilities of speech-impaired tonal language\nspeakers. However, data heterogeneity induced by both physiological and\ninstrumental factors poses a significant challenge for unified invasive brain\ntone decoding. Traditional subject-specific models, which operate under a\nheterogeneous decoding paradigm, fail to capture generalized neural\nrepresentations and cannot effectively leverage data across subjects. To\naddress these limitations, we introduce Homogeneity-Heterogeneity Disentangled\nLearning for neural Representations (H2DiLR), a novel framework that\ndisentangles and learns both the homogeneity and heterogeneity from\nintracranial recordings across multiple subjects. To evaluate H2DiLR, we\ncollected stereoelectroencephalography (sEEG) data from multiple participants\nreading Mandarin materials comprising 407 syllables, representing nearly all\nMandarin characters. Extensive experiments demonstrate that H2DiLR, as a\nunified decoding paradigm, significantly outperforms the conventional\nheterogeneous decoding approach. Furthermore, we empirically confirm that\nH2DiLR effectively captures both homogeneity and heterogeneity during neural\nrepresentation learning.\n","authors":["Di Wu","Siyuan Li","Chen Feng","Lu Cao","Yue Zhang","Jie Yang","Mohamad Sawan"],"pdf_url":"https://arxiv.org/pdf/2410.12866v2.pdf","comment":"ICLR2025 Poster (Preprint V2)"},{"id":"http://arxiv.org/abs/2406.05590v3","updated":"2025-02-18T12:26:33Z","published":"2024-06-08T22:21:42Z","title":"NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security","summary":"  Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized benchmark, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\nbenchmark dataset open source to public\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground\nautomated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.\n","authors":["Minghao Shao","Sofija Jancheska","Meet Udeshi","Brendan Dolan-Gavitt","Haoran Xi","Kimberly Milner","Boyuan Chen","Max Yin","Siddharth Garg","Prashanth Krishnamurthy","Farshad Khorrami","Ramesh Karri","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2406.05590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10658v3","updated":"2025-02-18T12:23:13Z","published":"2024-12-14T03:04:05Z","title":"Combining Priors with Experience: Confidence Calibration Based on\n  Binomial Process Modeling","summary":"  Confidence calibration of classification models is a technique to estimate\nthe true posterior probability of the predicted class, which is critical for\nensuring reliable decision-making in practical applications. Existing\nconfidence calibration methods mostly use statistical techniques to estimate\nthe calibration curve from data or fit a user-defined calibration function, but\noften overlook fully mining and utilizing the prior distribution behind the\ncalibration curve. However, a well-informed prior distribution can provide\nvaluable insights beyond the empirical data under the limited data or\nlow-density regions of confidence scores. To fill this gap, this paper proposes\na new method that integrates the prior distribution behind the calibration\ncurve with empirical data to estimate a continuous calibration curve, which is\nrealized by modeling the sampling process of calibration data as a binomial\nprocess and maximizing the likelihood function of the binomial process. We\nprove that the calibration curve estimating method is Lipschitz continuous with\nrespect to data distribution and requires a sample size of $3/B$ of that\nrequired for histogram binning, where $B$ represents the number of bins. Also,\na new calibration metric ($TCE_{bpm}$), which leverages the estimated\ncalibration curve to estimate the true calibration error (TCE), is designed.\n$TCE_{bpm}$ is proven to be a consistent calibration measure. Furthermore,\nrealistic calibration datasets can be generated by the binomial process\nmodeling from a preset true calibration curve and confidence score\ndistribution, which can serve as a benchmark to measure and compare the\ndiscrepancy between existing calibration metrics and the true calibration\nerror. The effectiveness of our calibration method and metric are verified in\nreal-world and simulated data.\n","authors":["Jinzong Dong","Zhaohui Jiang","Dong Pan","Haoyang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10658v3.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2404.13425v2","updated":"2025-02-18T12:16:57Z","published":"2024-04-20T17:19:54Z","title":"AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models","summary":"  Vision-Language Models (VLMs) play a crucial role in the advancement of\nArtificial General Intelligence (AGI). As AGI rapidly evolves, addressing\nsecurity concerns has emerged as one of the most significant challenges for\nVLMs. In this paper, we present extensive experiments that expose the\nvulnerabilities of conventional adaptation methods for VLMs, highlighting\nsignificant security risks. Moreover, as VLMs grow in size, the application of\ntraditional adversarial adaptation techniques incurs substantial computational\ncosts. To address these issues, we propose a parameter-efficient adversarial\nadaptation method called \\textbf{\\textit{AdvLoRA}} based on Low-Rank\nAdaptation. We investigate and reveal the inherent low-rank properties involved\nin adversarial adaptation for VLMs. Different from LoRA, we enhance the\nefficiency and robustness of adversarial adaptation by introducing a novel\nreparameterization method that leverages parameter clustering and alignment.\nAdditionally, we propose an adaptive parameter update strategy to further\nbolster robustness. These innovations enable our AdvLoRA to mitigate issues\nrelated to model security and resource wastage. Extensive experiments confirm\nthe effectiveness and efficiency of AdvLoRA.\n","authors":["Yuheng Ji","Yue Liu","Zhicheng Zhang","Zhao Zhang","Yuting Zhao","Xiaoshuai Hao","Gang Zhou","Xingwei Zhang","Xiaolong Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04563v2","updated":"2025-02-18T12:16:48Z","published":"2025-02-06T23:32:19Z","title":"WaferLLM: A Wafer-Scale LLM Inference System","summary":"  Emerging AI accelerators increasingly adopt wafer-scale manufacturing\ntechnologies, integrating hundreds of thousands of AI cores in a mesh-based\narchitecture with large distributed on-chip memory (tens of GB in total) and\nultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM\ninference systems, optimized for shared memory architectures like GPUs, fail to\nfully exploit these accelerators.\n  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM\nis guided by a novel PLMR model (pronounced as \"Plummer\") that captures the\nunique hardware characteristics of wafer-scale architectures. Leveraging this\nmodel, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the\nutilization of hundreds of thousands of on-chip cores. It also introduces\nMeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to\nscale effectively on wafer-scale accelerators.\n  Evaluations show that WaferLLM achieves 200$\\times$ better wafer-scale\naccelerator utilization than state-of-the-art systems. On a commodity\nwafer-scale accelerator, WaferLLM delivers 606$\\times$ faster and 22$\\times$\nmore energy-efficient GEMV compared to an advanced GPU. For LLMs, based on\n16-bit data type, WaferLLM achieves 2700 toks/sec/req decode speed on Llama3-8B\nmodel and 840 toks/sec/req decode speed on Qwen2-72B model, which enables\n39$\\times$ faster decoding with 1.7$\\times$ better energy efficiency. We\nanticipate these numbers will grow significantly as wafer-scale AI models,\nsoftware, and hardware continue to mature.\n","authors":["Congjie He","Yeqi Huang","Pei Mu","Ziming Miao","Jilong Xue","Lingxiao Ma","Fan Yang","Luo Mai"],"pdf_url":"https://arxiv.org/pdf/2502.04563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13348v2","updated":"2025-02-18T12:16:27Z","published":"2024-06-19T08:51:54Z","title":"Textual Unlearning Gives a False Sense of Unlearning","summary":"  Language Models (LMs) are prone to ''memorizing'' training data, including\nsubstantial sensitive user information. To mitigate privacy risks and safeguard\nthe right to be forgotten, machine unlearning has emerged as a promising\napproach for enabling LMs to efficiently ''forget'' specific texts. However,\ndespite the good intentions, is textual unlearning really as effective and\nreliable as expected? To address the concern, we first propose Unlearning\nLikelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing\nmethod, and find that unlearned texts can still be detected with very high\nconfidence after unlearning. Further, we conduct an in-depth investigation on\nthe privacy risks of textual unlearning mechanisms in deployment and present\nthe Textual Unlearning Leakage Attack (TULA), along with its variants in both\nblack- and white-box scenarios. We show that textual unlearning mechanisms\ncould instead reveal more about the unlearned texts, exposing them to\nsignificant membership inference and data reconstruction risks. Our findings\nhighlight that existing textual unlearning actually gives a false sense of\nunlearning, underscoring the need for more robust and secure unlearning\nmechanisms.\n","authors":["Jiacheng Du","Zhibo Wang","Jie Zhang","Xiaoyi Pang","Jiahui Hu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2406.13348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08805v2","updated":"2025-02-18T12:06:39Z","published":"2024-12-11T22:37:45Z","title":"GAMA: Generative Agents for Multi-Agent Autoformalization","summary":"  Multi-agent simulations facilitate the exploration of interactions among both\nnatural and artificial agents. However, modelling real-world scenarios and\ndeveloping simulations often requires substantial expertise and effort. To\nstreamline this process, we present a framework that enables the\nautoformalization of interaction scenarios using agents augmented by large\nlanguage models (LLMs) utilising game-theoretic formalisms. The agents\ntranslate natural language descriptions of interactions into executable logic\nprograms that define the rules of each game, ensuring syntactic correctness\nthrough validation by a solver. A tournament simulation then tests the\nfunctionality of the generated game rules and strategies. After the tournament,\nif a ground truth payoff matrix is available, an exact semantic validation is\nperformed. We evaluate our approach on a diverse set of 110 natural language\ndescriptions exemplifying five $2\\times2$ simultaneous-move games, achieving\n100% syntactic and 76.5% semantic correctness in the generated game rules for\nClaude 3.5 Sonnet, and 99.82% syntactic and 77% semantic correctness for\nGPT-4o. Additionally, we demonstrate high semantic correctness in\nautoformalizing gameplay strategies. Overall, the results highlight the\npotential of autoformalization to leverage LLMs in generating formal reasoning\nmodules for decision-making agents.\n","authors":["Agnieszka Mensfelt","Kostas Stathis","Vince Trencsenyi"],"pdf_url":"https://arxiv.org/pdf/2412.08805v2.pdf","comment":"This is a revised version of the paper that incorporates feedback\n  from the reviewers of the first version. It includes an improved\n  presentation, enhanced experiments, and additional validation. This version\n  also uses the latest version of the framework, now available at:\n  https://github.com/dicelab-rhul/GAMA"},{"id":"http://arxiv.org/abs/2502.12798v1","updated":"2025-02-18T12:00:35Z","published":"2025-02-18T12:00:35Z","title":"Envious Explore and Exploit","summary":"  Explore-and-exploit tradeoffs play a key role in recommendation systems\n(RSs), aiming at serving users better by learning from previous interactions.\nDespite their commercial success, the societal effects of explore-and-exploit\nmechanisms are not well understood, especially regarding the utility\ndiscrepancy they generate between different users. In this work, we measure\nsuch discrepancy using the economic notion of envy. We present a multi-armed\nbandit-like model in which every round consists of several sessions, and\nrewards are realized once per round. We call the latter property reward\nconsistency, and show that the RS can leverage this property for better\nsocietal outcomes. On the downside, doing so also generates envy, as\nlate-to-arrive users enjoy the information gathered by early-to-arrive users.\nWe examine the generated envy under several arrival order mechanisms and\nvirtually any anonymous algorithm, i.e., any algorithm that treats all similar\nusers similarly without leveraging their identities. We provide tight envy\nbounds on uniform arrival and upper bound the envy for nudged arrival, in which\nthe RS can affect the order of arrival by nudging its users. Furthermore, we\nstudy the efficiency-fairness trade-off by devising an algorithm that allows\nconstant envy and approximates the optimal welfare in restricted settings.\nFinally, we validate our theoretical results empirically using simulations.\n","authors":["Omer Ben-Porat","Yotam Gafni","Or Markovetzki"],"pdf_url":"https://arxiv.org/pdf/2502.12798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12793v1","updated":"2025-02-18T11:54:12Z","published":"2025-02-18T11:54:12Z","title":"Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport","summary":"  Detecting anomalies in datasets is a longstanding problem in machine\nlearning. In this context, anomalies are defined as a sample that significantly\ndeviates from the remaining data. Meanwhile, optimal transport (OT) is a field\nof mathematics concerned with the transportation, between two probability\nmeasures, at least effort. In classical OT, the optimal transportation strategy\nof a measure to itself is the identity. In this paper, we tackle anomaly\ndetection by forcing samples to displace its mass, while keeping the least\neffort objective. We call this new transportation problem Mass Repulsing\nOptimal Transport (MROT). Naturally, samples lying in low density regions of\nspace will be forced to displace mass very far, incurring a higher\ntransportation cost. We use these concepts to design a new anomaly score.\nThrough a series of experiments in existing benchmarks, and fault detection\nproblems, we show that our algorithm improves over existing methods.\n","authors":["Eduardo Fernandes Montesuma","Adel El Habazi","Fred Ngole Mboula"],"pdf_url":"https://arxiv.org/pdf/2502.12793v1.pdf","comment":"15 pages, 9 figures, 1 table, under review"},{"id":"http://arxiv.org/abs/2502.12782v1","updated":"2025-02-18T11:42:17Z","published":"2025-02-18T11:42:17Z","title":"VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation","summary":"  The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps://github.com/VidCapBench/VidCapBench.\n","authors":["Xinlong Chen","Yuanxing Zhang","Chongling Rao","Yushuo Guan","Jiaheng Liu","Fuzheng Zhang","Chengru Song","Qiang Liu","Di Zhang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2502.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02089v2","updated":"2025-02-18T11:39:46Z","published":"2024-10-02T23:25:17Z","title":"RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement\n  Learning","summary":"  Large language models (LLMs) deployed as agents solve user-specified tasks\nover multiple steps while keeping the required manual engagement to a minimum.\nCrucially, such LLMs need to ground their generations in any feedback obtained\nto reliably achieve the desired outcomes. We propose an end-to-end\nreinforcement learning method for teaching models to leverage execution\nfeedback in the realm of code synthesis, where state-of-the-art LLMs struggle\nto improve code iteratively compared to independent sampling. We benchmark on\ncompetitive programming tasks, where we achieve new state-of-the art results\nwith both small (8B parameters) and large (70B) models while reducing the\namount of samples required by an order of magnitude. Our analysis of\ninference-time behavior demonstrates that our method produces LLMs that\neffectively leverage automatic feedback over multiple steps.\n","authors":["Jonas Gehring","Kunhao Zheng","Jade Copet","Vegard Mella","Quentin Carbonneaux","Taco Cohen","Gabriel Synnaeve"],"pdf_url":"https://arxiv.org/pdf/2410.02089v2.pdf","comment":"Add repair model ablation, update related work"},{"id":"http://arxiv.org/abs/2502.12777v1","updated":"2025-02-18T11:36:59Z","published":"2025-02-18T11:36:59Z","title":"Evaluating link prediction: New perspectives and recommendations","summary":"  Link prediction (LP) is an important problem in network science and machine\nlearning research. The state-of-the-art LP methods are usually evaluated in a\nuniform setup, ignoring several factors associated with the data and\napplication specific needs. We identify a number of such factors, such as,\nnetwork-type, problem-type, geodesic distance between the end nodes and its\ndistribution over the classes, nature and applicability of LP methods, class\nimbalance and its impact on early retrieval, evaluation metric, etc., and\npresent an experimental setup which allows us to evaluate LP methods in a\nrigorous and controlled manner. We perform extensive experiments with a variety\nof LP methods over real network datasets in this controlled setup, and gather\nvaluable insights on the interactions of these factors with the performance of\nLP through an array of carefully designed hypotheses. Following the insights,\nwe provide recommendations to be followed as best practice for evaluating LP\nmethods.\n","authors":["Bhargavi Kalyani I","A Rama Prasad Mathi","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2502.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12776v1","updated":"2025-02-18T11:36:33Z","published":"2025-02-18T11:36:33Z","title":"Portable Reward Tuning: Towards Reusable Fine-Tuning across Different\n  Pretrained Models","summary":"  While foundation models have been exploited for various expert tasks through\nfine-tuning, any foundation model will become outdated due to its old knowledge\nor limited capability. Thus the underlying foundation model should be\neventually replaced by new ones, which leads to repeated cost of fine-tuning\nthese new models. Existing work addresses this problem by inference-time\ntuning, i.e., modifying the output probabilities from the new foundation model\nwith the outputs from the old foundation model and its fine-tuned model, which\ninvolves an additional overhead in inference by the latter two models. In this\npaper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT),\nthat reduces the inference overhead by its nature, based on the reformulation\nof fine-tuning as the reward maximization. Specifically, instead of fine-tuning\nparameters of the foundation models, PRT trains the reward model explicitly\nthrough the same loss function as in fine-tuning. During inference, the reward\nmodel can be used with any foundation model (with the same set of vocabularies\nor labels) through the formulation of reward maximization. Experimental\nresults, covering both vision and language models, demonstrate that the\nPRT-trained model can achieve comparable accuracy to the existing work of\ninference-time tuning, with less inference cost.\n","authors":["Daiki Chijiwa","Taku Hasegawa","Kyosuke Nishida","Kuniko Saito","Susumu Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2502.12776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12769v1","updated":"2025-02-18T11:32:43Z","published":"2025-02-18T11:32:43Z","title":"How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild","summary":"  In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.\n","authors":["Saad Obaid ul Islam","Anne Lauscher","Goran Glava"],"pdf_url":"https://arxiv.org/pdf/2502.12769v1.pdf","comment":"Under Review"}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2502.13115v1","updated":"2025-02-18T18:35:24Z","published":"2025-02-18T18:35:24Z","title":"Near-Optimal Private Learning in Linear Contextual Bandits","summary":"  We analyze the problem of private learning in generalized linear contextual\nbandits. Our approach is based on a novel method of re-weighted regression,\nyielding an efficient algorithm with regret of order\n$\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ in the joint and local model\nof $\\alpha$-privacy, respectively. Further, we provide near-optimal private\nprocedures that achieve dimension-independent rates in private linear models\nand linear contextual bandits. In particular, our results imply that joint\nprivacy is almost \"for free\" in all the settings we consider, partially\naddressing the open problem posed by Azize and Basu (2024).\n","authors":["Fan Chen","Jiachun Li","Alexander Rakhlin","David Simchi-Levi"],"pdf_url":"https://arxiv.org/pdf/2502.13115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14700v3","updated":"2025-02-18T18:19:07Z","published":"2025-01-24T18:22:37Z","title":"An Attentive Graph Agent for Topology-Adaptive Cyber Defence","summary":"  As cyber threats grow increasingly sophisticated, reinforcement learning (RL)\nis emerging as a promising technique to create intelligent and adaptive cyber\ndefense systems. However, most existing autonomous defensive agents have\noverlooked the inherent graph structure of computer networks subject to cyber\nattacks, potentially missing critical information and constraining their\nadaptability. To overcome these limitations, we developed a custom version of\nthe Cyber Operations Research Gym (CybORG) environment, encoding network state\nas a directed graph with realistic low-level features. We employ a Graph\nAttention Network (GAT) architecture to process node, edge, and global\nfeatures, and adapt its output to be compatible with policy gradient methods in\nRL. Our GAT-based approach offers key advantages over flattened alternatives:\npolicies that demonstrate resilience to certain types of unexpected dynamic\nnetwork topology changes, reasonable generalisation to networks of varying\nsizes within the same structural distribution, and interpretable defensive\nactions grounded in tangible network properties. We demonstrate that GAT\ndefensive policies can be trained using our low-level directed graph\nobservations, even when unexpected connections arise during simulation.\nEvaluations across networks of different sizes, but consistent subnetwork\nstructure, show our policies achieve comparable performance to policies trained\nspecifically for each network configuration. Our study contributes to the\ndevelopment of robust cyber defence systems that can better adapt to real-world\nnetwork security challenges.\n","authors":["Ilya Orson Sandoval","Isaac Symes Thompson","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2501.14700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13075v1","updated":"2025-02-18T17:22:42Z","published":"2025-02-18T17:22:42Z","title":"Variable Read Disturbance: An Experimental Analysis of Temporal\n  Variation in DRAM Read Disturbance","summary":"  Modern DRAM chips are subject to read disturbance errors. State-of-the-art\nread disturbance mitigations rely on accurate and exhaustive characterization\nof the read disturbance threshold (RDT) (e.g., the number of aggressor row\nactivations needed to induce the first RowHammer or RowPress bitflip) of every\nDRAM row (of which there are millions or billions in a modern system) to\nprevent read disturbance bitflips securely and with low overhead. We\nexperimentally demonstrate for the first time that the RDT of a DRAM row\nsignificantly and unpredictably changes over time. We call this new phenomenon\nvariable read disturbance (VRD). Our experiments using 160 DDR4 chips and 4\nHBM2 chips from three major manufacturers yield two key observations. First, it\nis very unlikely that relatively few RDT measurements can accurately identify\nthe RDT of a DRAM row. The minimum RDT of a DRAM row appears after tens of\nthousands of measurements (e.g., up to 94,467), and the minimum RDT of a DRAM\nrow is 3.5X smaller than the maximum RDT observed for that row. Second, the\nprobability of accurately identifying a row's RDT with a relatively small\nnumber of measurements reduces with increasing chip density or smaller\ntechnology node size. Our empirical results have implications for the security\nguarantees of read disturbance mitigation techniques: if the RDT of a DRAM row\nis not identified accurately, these techniques can easily become insecure. We\ndiscuss and evaluate using a guardband for RDT and error-correcting codes for\nmitigating read disturbance bitflips in the presence of RDTs that change\nunpredictably over time. We conclude that a >10% guardband for the minimum\nobserved RDT combined with SECDED or Chipkill-like SSC error-correcting codes\ncould prevent read disturbance bitflips at the cost of large read disturbance\nmitigation performance overheads (e.g., 45% performance loss for an RDT\nguardband of 50%).\n","authors":["Ataberk Olgun","F. Nisa Bostanci","Ismail Emir Yuksel","Oguzhan Canpolat","Haocong Luo","Geraldo F. Oliveira","A. Giray Yaglikci","Minesh Patel","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2502.13075v1.pdf","comment":"Extended version of our publication at the 31st IEEE International\n  Symposium on High-Performance Computer Architecture (HPCA-31), 2025"},{"id":"http://arxiv.org/abs/2502.13065v1","updated":"2025-02-18T17:08:59Z","published":"2025-02-18T17:08:59Z","title":"Improving Algorithmic Efficiency using Cryptography","summary":"  Cryptographic primitives have been used for various non-cryptographic\nobjectives, such as eliminating or reducing randomness and interaction. We show\nhow to use cryptography to improve the time complexity of solving computational\nproblems. Specifically, we show that under standard cryptographic assumptions,\nwe can design algorithms that are asymptotically faster than existing ones\nwhile maintaining correctness.\n  As a concrete demonstration, we construct a distribution of trapdoored\nmatrices with the following properties: (a) computationally bounded adversaries\ncannot distinguish a random matrix from one drawn from this distribution, and\n(b) given a secret key, we can multiply such a n-by-n matrix with any vector in\nnear-linear (in n) time. We provide constructions both over finite fields and\nthe reals. This enables a broad speedup technique: any algorithm relying on a\nrandom matrix - such as those using various notions of dimensionality reduction\n- can replace it with a matrix from our distribution, achieving computational\nspeedups while preserving correctness.\n","authors":["Vinod Vaikuntanathan","Or Zamir"],"pdf_url":"https://arxiv.org/pdf/2502.13065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13060v1","updated":"2025-02-18T17:05:17Z","published":"2025-02-18T17:05:17Z","title":"Sublinear-Overhead Secure Linear Algebra on a Dishonest Server","summary":"  Most heavy computation occurs on servers owned by a second party. This\nreduces data privacy, resulting in interest in data-oblivious computation,\nwhich typically severely degrades performance. Secure and fast remote\ncomputation is particularly important for linear algebra, which comprises a\nlarge fraction of total computation and is best run on highly specialized\nhardware often only accessible through the cloud. We state the natural\nefficiency and security desiderata for fast, remote, and data-oblivious linear\nalgebra, conjecture the existence of matrix and vector families implying\nsatisfactory algorithms, and provide such an algorithm contingent on common\ncryptographic assumptions. We achieve sublinear overhead for the server,\ndramatically reduced computation cost for the client, and various other\npractical advantages over previous algorithms.\n  Keywords: Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic\nEncryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN,\nMatrix Multiplication.\n","authors":["Mark Braverman","Stephen Newman"],"pdf_url":"https://arxiv.org/pdf/2502.13060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13055v1","updated":"2025-02-18T17:01:37Z","published":"2025-02-18T17:01:37Z","title":"LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs","summary":"  The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.\n","authors":["Xingzhi Qian","Xinran Zheng","Yiling He","Shuo Yang","Lorenzo Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2502.13055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01068v2","updated":"2025-02-18T16:17:43Z","published":"2024-10-01T20:52:08Z","title":"Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness","summary":"  We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD\nalgorithms over a bounded domain. Standard privacy analysis for Noisy-SGD\nassumes all internal states are revealed, which leads to a divergent R'enyi DP\nbound with respect to the number of iterations. Ye & Shokri (2022) and\nAltschuler & Talwar (2022) proved convergent bounds for smooth (strongly)\nconvex losses, and raise open questions about whether these assumptions can be\nrelaxed. We provide positive answers by proving convergent R'enyi DP bound for\nnon-convex non-smooth losses, where we show that requiring losses to have\nH\\\"older continuous gradient is sufficient. We also provide a strictly better\nprivacy bound compared to state-of-the-art results for smooth strongly convex\nlosses. Our analysis relies on the improvement of shifted divergence analysis\nin multiple aspects, including forward Wasserstein distance tracking,\nidentifying the optimal shifts allocation, and the H\"older reduction lemma. Our\nresults further elucidate the benefit of hidden-state analysis for DP and its\napplicability.\n","authors":["Eli Chien","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2410.01068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12976v1","updated":"2025-02-18T15:56:52Z","published":"2025-02-18T15:56:52Z","title":"Does Training with Synthetic Data Truly Protect Privacy?","summary":"  As synthetic data becomes increasingly popular in machine learning tasks,\nnumerous methods--without formal differential privacy guarantees--use synthetic\ndata for training. These methods often claim, either explicitly or implicitly,\nto protect the privacy of the original training data. In this work, we explore\nfour different training paradigms: coreset selection, dataset distillation,\ndata-free knowledge distillation, and synthetic data generated from diffusion\nmodels. While all these methods utilize synthetic data for training, they lead\nto vastly different conclusions regarding privacy preservation. We caution that\nempirical approaches to preserving data privacy require careful and rigorous\nevaluation; otherwise, they risk providing a false sense of privacy.\n","authors":["Yunpeng Zhao","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12976v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2409.17513v2","updated":"2025-02-18T15:49:04Z","published":"2024-09-26T03:48:47Z","title":"Comparing Unidirectional, Bidirectional, and Word2vec Models for\n  Discovering Vulnerabilities in Compiled Lifted Code","summary":"  Ransomware and other forms of malware cause significant financial and\noperational damage to organizations by exploiting long-standing and often\ndifficult-to-detect software vulnerabilities. To detect vulnerabilities such as\nbuffer overflows in compiled code, this research investigates the application\nof unidirectional transformer-based embeddings, specifically GPT-2. Using a\ndataset of LLVM functions, we trained a GPT-2 model to generate embeddings,\nwhich were subsequently used to build LSTM neural networks to differentiate\nbetween vulnerable and non-vulnerable code. Our study reveals that embeddings\nfrom the GPT-2 model significantly outperform those from bidirectional models\nof BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.\nLSTM neural networks were developed with both frozen and unfrozen embedding\nmodel layers. The model with the highest performance was achieved when the\nembedding layers were unfrozen. Further, the research finds that, in exploring\nthe impact of different optimizers within this domain, the SGD optimizer\ndemonstrates superior performance over Adam. Overall, these findings reveal\nimportant insights into the potential of unidirectional transformer-based\napproaches in enhancing cybersecurity defenses.\n","authors":["Gary A. McCully","John D. Hastings","Shengjie Xu","Adam Fortier"],"pdf_url":"https://arxiv.org/pdf/2409.17513v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.12966v1","updated":"2025-02-18T15:47:49Z","published":"2025-02-18T15:47:49Z","title":"The Early Days of the Ethereum Blob Fee Market and Lessons Learnt","summary":"  Ethereum has adopted a rollup-centric roadmap to scale by making rollups\n(layer 2 scaling solutions) the primary method for handling transactions. The\nfirst significant step towards this goal was EIP-4844, which introduced blob\ntransactions that are designed to meet the data availability needs of layer 2\nprotocols. This work constitutes the first rigorous and comprehensive empirical\nanalysis of transaction- and mempool-level data since the institution of blobs\non Ethereum on March 13, 2024. We perform a longitudinal study of the early\ndays of the blob fee market analyzing the landscape and the behaviors of its\nparticipants. We identify and measure the inefficiencies arising out of\nsuboptimal block packing, showing that at times it has resulted in up to 70%\nrelative fee loss. We hone in and give further insight into two (congested)\npeak demand periods for blobs. Finally, we document a market design issue\nrelating to subset bidding due to the inflexibility of the transaction\nstructure on packing data as blobs and suggest possible ways to fix it. The\nlatter market structure issue also applies more generally for any discrete\nobjects included within transactions.\n","authors":["Lioba Heimbach","Jason Milionis"],"pdf_url":"https://arxiv.org/pdf/2502.12966v1.pdf","comment":"23 pages, 13 figures, published at FC'25"},{"id":"http://arxiv.org/abs/2502.12958v1","updated":"2025-02-18T15:43:14Z","published":"2025-02-18T15:43:14Z","title":"Preventing the Popular Item Embedding Based Attack in Federated\n  Recommendations","summary":"  Privacy concerns have led to the rise of federated recommender systems (FRS),\nwhich can create personalized models across distributed clients. However, FRS\nis vulnerable to poisoning attacks, where malicious users manipulate gradients\nto promote their target items intentionally. Existing attacks against FRS have\nlimitations, as they depend on specific models and prior knowledge, restricting\ntheir real-world applicability. In our exploration of practical FRS\nvulnerabilities, we devise a model-agnostic and prior-knowledge-free attack,\nnamed PIECK (Popular Item Embedding based Attack). The core module of PIECK is\npopular item mining, which leverages embedding changes during FRS training to\neffectively identify the popular items. Built upon the core module, PIECK\nbranches into two diverse solutions: The PIECKIPE solution employs an item\npopularity enhancement module, which aligns the embeddings of targeted items\nwith the mined popular items to increase item exposure. The PIECKUEA further\nenhances the robustness of the attack by using a user embedding approximation\nmodule, which approximates private user embeddings using mined popular items.\nUpon identifying PIECK, we evaluate existing federated defense methods and find\nthem ineffective against PIECK, as poisonous gradients inevitably overwhelm the\ncold target items. We then propose a novel defense method by introducing two\nregularization terms during user training, which constrain item popularity\nenhancement and user embedding approximation while preserving FRS performance.\nWe evaluate PIECK and its defense across two base models, three real datasets,\nfour top-tier attacks, and six general defense methods, affirming the efficacy\nof both PIECK and its defense.\n","authors":["Jun Zhang","Huan Li","Dazhong Rong","Yan Zhao","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2502.12958v1.pdf","comment":"Accepted at ICDE 2024, Extension"},{"id":"http://arxiv.org/abs/2410.00792v2","updated":"2025-02-18T14:50:40Z","published":"2024-10-01T15:32:02Z","title":"Fast Multiplication and the PLWE-RLWE Equivalence for an Infinite Family\n  of Maximal Real Subfields of Cyclotomic Fields","summary":"  We prove the equivalence between the Ring Learning With Errors (RLWE) and the\nPolynomial Learning With Errors (PLWE) problems for the maximal totally real\nsubfield of the $2^r 3^s$-th cyclotomic field for $r \\geq 3$ and $s \\geq 1$.\nMoreover, we describe a fast algorithm for computing the product of two\nelements in the ring of integers of these subfields. This multiplication\nalgorithm has quasilinear complexity in the dimension of the field, as it makes\nuse of the fast Discrete Cosine Transform (DCT). Our approach assumes that the\ntwo input polynomials are given in a basis of Chebyshev-like polynomials, in\ncontrast to the customary power basis. To validate this assumption, we prove\nthat the change of basis from the power basis to the Chebyshev-like basis can\nbe computed with $\\mathcal{O}(n \\log n)$ arithmetic operations, where $n$ is\nthe problem dimension. Finally, we provide a heuristic and theoretical\ncomparison of the vulnerability to some attacks for the $p$-th cyclotomic field\nversus the maximal totally real subextension of the $4p$-th cyclotomic field\nfor a reasonable set of parameters of cryptographic size.\n","authors":["Joonas Ahola","Ivn Blanco-Chacn","Wilmar Bolaos","Antti Haavikko","Camilla Hollanti","Rodrigo Martn Snchez-Ledesma"],"pdf_url":"https://arxiv.org/pdf/2410.00792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10635v2","updated":"2025-02-18T14:16:06Z","published":"2025-02-15T02:25:27Z","title":"Privacy Preservation through Practical Machine Unlearning","summary":"  Machine Learning models thrive on vast datasets, continuously adapting to\nprovide accurate predictions and recommendations. However, in an era dominated\nby privacy concerns, Machine Unlearning emerges as a transformative approach,\nenabling the selective removal of data from trained models. This paper examines\nmethods such as Naive Retraining and Exact Unlearning via the SISA framework,\nevaluating their Computational Costs, Consistency, and feasibility using the\n$\\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning\nprinciples into Positive Unlabeled (PU) Learning to address challenges posed by\npartially labeled datasets. Our findings highlight the promise of unlearning\nframeworks like $\\textit{DaRE}$ for ensuring privacy compliance while\nmaintaining model performance, albeit with significant computational\ntrade-offs. This study underscores the importance of Machine Unlearning in\nachieving ethical AI and fostering trust in data-driven systems.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2502.10635v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.12863v1","updated":"2025-02-18T13:51:56Z","published":"2025-02-18T13:51:56Z","title":"Malware Detection based on API calls","summary":"  Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.\n","authors":["Christofer Fellicious","Manuel Bischof","Kevin Mayer","Dorian Eikenberg","Stefan Hausotte","Hans P. Reiser","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2502.12863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12848v1","updated":"2025-02-18T13:34:58Z","published":"2025-02-18T13:34:58Z","title":"Strands Rocq: Why is a Security Protocol Correct, Mechanically?","summary":"  Strand spaces are a formal framework for symbolic protocol verification that\nallows for pen-and-paper proofs of security. While extremely insightful,\npen-and-paper proofs are error-prone, and it is hard to gain confidence on\ntheir correctness. To overcome this problem, we developed StrandsRocq, a full\nmechanization of the strand spaces in Coq (soon to be renamed Rocq). The\nmechanization was designed to be faithful to the original pen-and-paper\ndevelopment, and it was engineered to be modular and extensible. StrandsRocq\nincorporates new original proof techniques, a novel notion of maximal\npenetrator that enables protocol compositionality, and a set of Coq tactics\ntailored to the domain, facilitating proof automation and reuse, and\nsimplifying the work of protocol analysts. To demonstrate the versatility of\nour approach, we modelled and analyzed a family of authentication protocols,\ndrawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical\nNeedham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis\nfor a key management API. The analyses in StrandsRocq confirmed the high degree\nof proof reuse, and enabled us to distill the minimal requirements for protocol\nsecurity. Through mechanization, we identified and addressed several issues in\nthe original proofs and we were able to significantly improve the precision of\nthe static analysis for the key management API. Moreover, we were able to\nleverage the novel notion of maximal penetrator to provide a compositional\nproof of security for two simple authentication protocols.\n","authors":["Matteo Busi","Riccardo Focardi","Flaminia L. Luccio"],"pdf_url":"https://arxiv.org/pdf/2502.12848v1.pdf","comment":"To appear at IEEE CSF'25, June 16-20, 2025, Santa Cruz, CA, USA"},{"id":"http://arxiv.org/abs/2502.12837v1","updated":"2025-02-18T13:10:47Z","published":"2025-02-18T13:10:47Z","title":"Toward Cybersecurity Testing and Monitoring of IoT Ecosystems","summary":"  We describe a framework and tool specification that represents a step towards\ncybersecurity testing and monitoring of IoT ecosystems. We begin with\nchallenges from a previous paper and discuss an integrated approach and tools\nto enable testing and monitoring to address these challenges. We also describe\nexemplary use cases of IoT ecosystems and propose approaches to address the\nchallenges using the framework and tools. The current status of this work is\nthat the specification and conceptualisation is complete, use cases are\nunderstood with clear challenges and implementation / extension of the tools\nand framework is underway with tools at different stages of development.\nSeveral key observations have been made throughout this work, as follows. 1)\nTools may be used in multiple different combinations, and ad-hoc use is also\nencouraged, where one tool may provide clues and other tools executed to\nundertake further investigations based on initial results. 2) Automated\nexecution of tool chains is supported by workflows. 3) support for immutable\nstorage of audit records of tests and results is an important requirement. 4)\nIndicators (observations or measurements representing information of relevance\nfor assessment of cyber security) are a key mechanism for intercommunication\nbetween one tool and another, or with the operator. 5) Mapping this work to\nestablished security development lifecycles is a useful means of determining\napplicability and utility of the tools and framework. 6) There is a key\ninterplay between devices and systems. 7) Anomaly detection in multiple forms\nis a key means of runtime monitoring. 8) Considerable investigation is needed\nrelated to the specifics of each device / system as an item of further work.\n","authors":["Steve Taylor","Panos Melas","Martin Gile Jaatun","Aida Omerovic","Robert Seidl","Norbert Goetze","Jens Kuhr","Dmytro Prosvirin","Manuel Leone","Paolo De Lutiis","Andrey Kuznetsov","Anatoliy Gritskevich","George N. Triantafyllou","Antonis Mpantis","Oscar Garcia Perales","Bernd-Ludwig Wenning","Sayon Duttagupta"],"pdf_url":"https://arxiv.org/pdf/2502.12837v1.pdf","comment":"Preprint of Paper submitted to Springer Nature Computer Science"},{"id":"http://arxiv.org/abs/2310.11409v5","updated":"2025-02-18T12:53:47Z","published":"2023-10-17T17:15:41Z","title":"LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks","summary":"  Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT.\n","authors":["Andreas Happe","Aaron Kaplan","Juergen Cito"],"pdf_url":"https://arxiv.org/pdf/2310.11409v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05590v3","updated":"2025-02-18T12:26:33Z","published":"2024-06-08T22:21:42Z","title":"NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating\n  LLMs in Offensive Security","summary":"  Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized benchmark, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\nbenchmark dataset open source to public\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground\nautomated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.\n","authors":["Minghao Shao","Sofija Jancheska","Meet Udeshi","Brendan Dolan-Gavitt","Haoran Xi","Kimberly Milner","Boyuan Chen","Max Yin","Siddharth Garg","Prashanth Krishnamurthy","Farshad Khorrami","Ramesh Karri","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2406.05590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07494v2","updated":"2025-02-18T12:26:02Z","published":"2024-09-09T07:13:44Z","title":"Ethereum Fraud Detection via Joint Transaction Language Model and Graph\n  Representation Learning","summary":"  Ethereum faces growing fraud threats. Current fraud detection methods,\nwhether employing graph neural networks or sequence models, fail to consider\nthe semantic information and similarity patterns within transactions. Moreover,\nthese approaches do not leverage the potential synergistic benefits of\ncombining both types of models. To address these challenges, we propose\nTLMG4Eth that combines a transaction language model with graph-based methods to\ncapture semantic, similarity, and structural features of transaction data in\nEthereum. We first propose a transaction language model that converts numerical\ntransaction data into meaningful transaction sentences, enabling the model to\nlearn explicit transaction semantics. Then, we propose a transaction attribute\nsimilarity graph to learn transaction similarity information, enabling us to\ncapture intuitive insights into transaction anomalies. Additionally, we\nconstruct an account interaction graph to capture the structural information of\nthe account transaction network. We employ a deep multi-head attention network\nto fuse transaction semantic and similarity embeddings, and ultimately propose\na joint training approach for the multi-head attention network and the account\ninteraction graph to obtain the synergistic benefits of both.\n","authors":["Jianguo Sun","Yifan Jia","Yanbin Wang","Yiwei Liu","Zhang Sheng","Ye Tian"],"pdf_url":"https://arxiv.org/pdf/2409.07494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13348v2","updated":"2025-02-18T12:16:27Z","published":"2024-06-19T08:51:54Z","title":"Textual Unlearning Gives a False Sense of Unlearning","summary":"  Language Models (LMs) are prone to ''memorizing'' training data, including\nsubstantial sensitive user information. To mitigate privacy risks and safeguard\nthe right to be forgotten, machine unlearning has emerged as a promising\napproach for enabling LMs to efficiently ''forget'' specific texts. However,\ndespite the good intentions, is textual unlearning really as effective and\nreliable as expected? To address the concern, we first propose Unlearning\nLikelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing\nmethod, and find that unlearned texts can still be detected with very high\nconfidence after unlearning. Further, we conduct an in-depth investigation on\nthe privacy risks of textual unlearning mechanisms in deployment and present\nthe Textual Unlearning Leakage Attack (TULA), along with its variants in both\nblack- and white-box scenarios. We show that textual unlearning mechanisms\ncould instead reveal more about the unlearned texts, exposing them to\nsignificant membership inference and data reconstruction risks. Our findings\nhighlight that existing textual unlearning actually gives a false sense of\nunlearning, underscoring the need for more robust and secure unlearning\nmechanisms.\n","authors":["Jiacheng Du","Zhibo Wang","Jie Zhang","Xiaoyi Pang","Jiahui Hu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2406.13348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12794v1","updated":"2025-02-18T11:56:51Z","published":"2025-02-18T11:56:51Z","title":"RAPID: Retrieval Augmented Training of Differentially Private Diffusion\n  Models","summary":"  Differentially private diffusion models (DPDMs) harness the remarkable\ngenerative capabilities of diffusion models while enforcing differential\nprivacy (DP) for sensitive data. However, existing DPDM training approaches\noften suffer from significant utility loss, large memory footprint, and\nexpensive inference cost, impeding their practical uses. To overcome such\nlimitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a\nnovel approach that integrates retrieval augmented generation (RAG) into DPDM\ntraining. Specifically, RAPID leverages available public data to build a\nknowledge base of sample trajectories; when training the diffusion model on\nprivate data, RAPID computes the early sampling steps as queries, retrieves\nsimilar trajectories from the knowledge base as surrogates, and focuses on\ntraining the later sampling steps in a differentially private manner. Extensive\nevaluation using benchmark datasets and models demonstrates that, with the same\nprivacy guarantee, RAPID significantly outperforms state-of-the-art approaches\nby large margins in generative quality, memory footprint, and inference cost,\nsuggesting that retrieval-augmented DP training represents a promising\ndirection for developing future privacy-preserving generative models. The code\nis available at: https://github.com/TanqiuJiang/RAPID\n","authors":["Tanqiu Jiang","Changjiang Li","Fenglong Ma","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12794v1.pdf","comment":"Published in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12734v1","updated":"2025-02-18T10:48:53Z","published":"2025-02-18T10:48:53Z","title":"Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text\n  Detection with Adversarial Training","summary":"  Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 9 text perturbation strategies and 5\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches.\n","authors":["Yuanfan Li","Zhaohan Zhang","Chengzhengxu Li","Chao Shen","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12734v1.pdf","comment":"Submitted to ACL 2025, Preprint, Under review"},{"id":"http://arxiv.org/abs/2502.11641v2","updated":"2025-02-18T10:38:21Z","published":"2025-02-17T10:35:18Z","title":"A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee\n  Metric","summary":"  The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to describe a zero-knowledge\nproof for this variant of the problem.\n","authors":["Mladen Kovaevi","Tatjana Grbi","Darko apko","Nemanja Nedi","Srdjan Vukmirovi"],"pdf_url":"https://arxiv.org/pdf/2502.11641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12721v1","updated":"2025-02-18T10:38:06Z","published":"2025-02-18T10:38:06Z","title":"Computation of the Hilbert Series for the Support-Minors Modeling of the\n  MinRank Problem","summary":"  The MinRank problem is a simple linear algebra problem: given matrices with\ncoefficients in a field, find a non trivial linear combination of the matrices\nthat has a small rank. There are several algebraic modeling of the problem. The\nmain ones are: the Kipnis-Shamir modeling, the Minors modeling and the\nSupport-Minors modeling. The Minors modeling has been studied by Faug\\`ere et\nal. in 2010, where the authors provide an analysis of the complexity of\ncomputing a Gr\\\"obner basis of the modeling, through the computation of the\nexact Hilbert Series for a generic instance. For the Support-Minors modeling,\nthe first terms of the Hilbert Series are given by Bardet et al. in 2020 based\non an heuristic and experimental work. In this work, we provide a formula and a\nproof for the complete Hilbert Series of the Support Minors modeling for\ngeneric instances. This is done by adapting well known results on determinantal\nideals to an ideal generated by a particular subset of the set of all minors of\na matrix of variables. We then show that this ideal is generated by a\nparticular subset of the set of all minors of a matrix of variables. We then\nshow that this ideal is generated by standard monomials having a particular\nshape, and derive the Hilbert Series by counting the number of such standard\nmonomials. Following the work done for the Minors Modeling, we then transfer\nthe properties of this particular determinantal ideal to ideals generated by\nthe Support Minors system, by adding generic forms. This work allows to make a\nprecise comparison between the Minors and Support Minors modeling, and a\nprecise estimate of the complexity of solving MinRank instances for the\nparameters of the Mirath signature scheme that is currently at the second round\nof the NIST standardization process for Additional Digital Signature Schemes.\n","authors":["Magali Bardet","Alban Gilard"],"pdf_url":"https://arxiv.org/pdf/2502.12721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12710v1","updated":"2025-02-18T10:21:27Z","published":"2025-02-18T10:21:27Z","title":"TREND: A Whitespace Replacement Information Hiding Method","summary":"  Large Language Models (LLMs) have gained significant popularity in recent\nyears. Differentiating between a text written by a human and a text generated\nby an LLM has become almost impossible. Information hiding techniques such as\ndigital watermarking or steganography can help by embedding information inside\ntext without being noticed. However, existing techniques, such as\nlinguistic-based or format-based methods, change the semantics or do not work\non pure, unformatted text. In this paper, we introduce a novel method for\ninformation hiding termed TREND, which is able to conceal any byte-encoded\nsequence within a cover text. The proposed method is implemented as a\nmulti-platform library using the Kotlin programming language, accompanied by a\ncommand-line tool and a web interface provided as examples of usage. By\nsubstituting conventional whitespace characters with visually similar Unicode\nwhitespace characters, our proposed scheme preserves the semantics of the cover\ntext without increasing the number of characters. Furthermore, we propose a\nspecified structure for secret messages that enables configurable compression,\nencryption, hashing, and error correction. Our experimental benchmark\ncomparison on a dataset of one million Wikipedia articles compares ten\nalgorithms from literature and practice. It proves the robustness of our\nproposed method in various applications while remaining imperceptible to\nhumans. We discuss the limitations of limited embedding capacity and further\nrobustness, which guide implications for future work.\n","authors":["Malte Hellmeier","Hendrik Norkowski","Ernst-Christoph Schrewe","Haydar Qarawlus","Falk Howar"],"pdf_url":"https://arxiv.org/pdf/2502.12710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11308v2","updated":"2025-02-18T10:07:55Z","published":"2025-02-16T23:11:13Z","title":"ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment\n  and Generation","summary":"  With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP.\n","authors":["Yiyi Chen","Qiongkai Xu","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2502.11308v2.pdf","comment":"18 pages, 13 tables, 6 figures"},{"id":"http://arxiv.org/abs/2502.11658v2","updated":"2025-02-18T09:24:14Z","published":"2025-02-17T10:49:23Z","title":"\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data","summary":"  Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.\n","authors":["Antoine Boutet","Victor Morel"],"pdf_url":"https://arxiv.org/pdf/2502.11658v2.pdf","comment":"Submitted to ICWSM on January 15, 2025"},{"id":"http://arxiv.org/abs/2502.12650v1","updated":"2025-02-18T08:54:49Z","published":"2025-02-18T08:54:49Z","title":"Chronus: Understanding and Securing the Cutting-Edge Industry Solutions\n  to DRAM Read Disturbance","summary":"  We 1) present the first rigorous security, performance, energy, and cost\nanalyses of the state-of-the-art on-DRAM-die read disturbance mitigation\nmethod, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new\nmechanism that addresses PRAC's two major weaknesses. Our analysis shows that\nPRAC's system performance overhead on benign applications is non-negligible for\nmodern DRAM chips and prohibitively large for future DRAM chips that are more\nvulnerable to read disturbance. We identify two weaknesses of PRAC that cause\nthese overheads. First, PRAC increases critical DRAM access latency parameters\ndue to the additional time required to increment activation counters. Second,\nPRAC performs a constant number of preventive refreshes at a time, making it\nvulnerable to an adversarial access pattern, known as the wave attack, and\nconsequently requiring it to be configured for significantly smaller activation\nthresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die\nRowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation\ncounters concurrently while serving accesses by separating counters from the\ndata and 2) prevents the wave attack by dynamically controlling the number of\npreventive refreshes performed. Our performance analysis shows that Chronus's\nsystem performance overhead is near-zero for modern DRAM chips and very low for\nfuture DRAM chips. Chronus outperforms three variants of PRAC and three other\nstate-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's\nimplications for future systems and foreshadow future research directions. To\naid future research, we open-source our Chronus implementation at\nhttps://github.com/CMU-SAFARI/Chronus.\n","authors":["Ouzhan Canpolat","A. Giray Yalk","Geraldo F. Oliveira","Ataberk Olgun","Nisa Bostanc","smail Emir Yksel","Haocong Luo","Ouz Ergin","Onur Mutlu"],"pdf_url":"https://arxiv.org/pdf/2502.12650v1.pdf","comment":"To appear in HPCA'25. arXiv admin note: text overlap with\n  arXiv:2406.19094"},{"id":"http://arxiv.org/abs/2502.12630v1","updated":"2025-02-18T08:17:32Z","published":"2025-02-18T08:17:32Z","title":"Automating Prompt Leakage Attacks on Large Language Models Using Agentic\n  Approach","summary":"  This paper presents a novel approach to evaluating the security of large\nlanguage models (LLMs) against prompt leakage-the exposure of system-level\nprompts or proprietary configurations. We define prompt leakage as a critical\nthreat to secure LLM deployment and introduce a framework for testing the\nrobustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we\nimplement a multi-agent system where cooperative agents are tasked with probing\nand exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further\ndefine a prompt leakage-safe system as one in which an attacker cannot\ndistinguish between two agents: one initialized with an original prompt and the\nother with a prompt stripped of all sensitive information. In a safe system,\nthe agents' outputs will be indistinguishable to the attacker, ensuring that\nsensitive information remains secure. This cryptographically inspired framework\nprovides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of\nprompt leakage, bridging the gap between automated threat modeling and\npractical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub.\n","authors":["Tvrtko Sternak","Davor Runje","Dorian Granoa","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12628v1","updated":"2025-02-18T08:13:10Z","published":"2025-02-18T08:13:10Z","title":"Cryptanalysis on Lightweight Verifiable Homomorphic Encryption","summary":"  Verifiable Homomorphic Encryption (VHE) is a cryptographic technique that\nintegrates Homomorphic Encryption (HE) with Verifiable Computation (VC). It\nserves as a crucial technology for ensuring both privacy and integrity in\noutsourced computation, where a client sends input ciphertexts $\\mathsf{ct}$\nand a function $f$ to a server and verifies the correctness of the evaluation\nupon receiving the evaluation result $f(\\mathsf{ct})$ from the server. In CCS\n2024, Chatel et al. [CKP+24] introduced two lightweight VHE schemes:\nReplication Encoding (REP) and Polynomial Encoding (PE). A similar approach to\nREP was used by Albrecht et al. [ADDG24] in Eurocrypt 2024 to develop a\nVerifiable Oblivious PRF scheme (vADDG). A key approach in these schemes is to\nembed specific secret information within HE ciphertexts to verify homomorphic\nevaluations. This paper presents efficient attacks that exploit the homomorphic\nproperties of encryption schemes. The one strategy is to retrieve the secret\ninformation in encrypted state from the input ciphertexts and then leverage it\nto modify the resulting ciphertext without being detected by the verification\nalgorithm. The other is to exploit the secret embedding structure for\nmodification of the evaluation function $f$ into $f'$ which works well on input\nvalues for verification purpose. Our forgery attack on vADDG achieves a success\nprobability of $70.2\\%$ under the suggested 80-bit security parameter. Our\nattack on REP and PE achieves a probability 1 attack with linear time\ncomplexity when using fully homomorphic encryption.\n","authors":["Jung Hee Cheon","Daehyun Jang"],"pdf_url":"https://arxiv.org/pdf/2502.12628v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2410.03413v2","updated":"2025-02-18T07:58:17Z","published":"2024-10-04T13:24:03Z","title":"A Simple Framework for Secure Key Leasing","summary":"  Secure key leasing (a.k.a. key-revocable cryptography) enables us to lease a\ncryptographic key as a quantum state in such a way that the key can be later\nrevoked in a verifiable manner. We propose a simple framework for constructing\ncryptographic primitives with secure key leasing via the certified deletion\nproperty of BB84 states. Based on our framework, we obtain the following\nschemes.\n  - A public key encryption scheme with secure key leasing that has classical\nrevocation based on any IND-CPA secure public key encryption scheme. Prior\nworks rely on either quantum revocation or stronger assumptions such as the\nquantum hardness of the learning with errors (LWE) problem.\n  - A pseudorandom function with secure key leasing that has classical\nrevocation based on one-way functions. Prior works rely on stronger assumptions\nsuch as the quantum hardness of the LWE problem.\n  - A digital signature scheme with secure key leasing that has classical\nrevocation based on the quantum hardness of the short integer solution (SIS)\nproblem. Our construction has static signing keys, i.e., the state of a signing\nkey almost does not change before and after signing. Prior constructions either\nrely on non-static signing keys or indistinguishability obfuscation to achieve\na stronger goal of copy-protection.\n  In addition, all of our schemes remain secure even if a verification key for\nrevocation is leaked after the adversary submits a valid certificate of\ndeletion. To our knowledge, all prior constructions are totally broken in this\nsetting. Moreover, in our view, our security proofs are much simpler than those\nfor existing schemes.\n","authors":["Fuyuki Kitagawa","Tomoyuki Morimae","Takashi Yamakawa"],"pdf_url":"https://arxiv.org/pdf/2410.03413v2.pdf","comment":"56 pages"},{"id":"http://arxiv.org/abs/2502.12575v1","updated":"2025-02-18T06:26:15Z","published":"2025-02-18T06:26:15Z","title":"DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on\n  LLM-based Agent","summary":"  As LLM-based agents become increasingly prevalent, backdoors can be implanted\ninto agents through user queries or environment feedback, raising critical\nconcerns regarding safety vulnerabilities. However, backdoor attacks are\ntypically detectable by safety audits that analyze the reasoning process of\nagents. To this end, we propose a novel backdoor implantation strategy called\n\\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}.\nSpecifically, we introduce dynamic encryption, which maps the backdoor into\nbenign content, effectively circumventing safety audits. To enhance\nstealthiness, we further decompose the backdoor into multiple sub-backdoor\nfragments. Based on these advancements, backdoors are allowed to bypass safety\naudits significantly. Additionally, we present AgentBackdoorEval, a dataset\ndesigned for the comprehensive evaluation of agent backdoor attacks.\nExperimental results across multiple datasets demonstrate that our method\nachieves an attack success rate nearing 100\\% while maintaining a detection\nrate of 0\\%, illustrating its effectiveness in evading safety audits. Our\nfindings highlight the limitations of existing safety mechanisms in detecting\nadvanced attacks, underscoring the urgent need for more robust defenses against\nbackdoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent.\n","authors":["Pengyu Zhu","Zhenhong Zhou","Yuanhe Zhang","Shilinlu Yan","Kun Wang","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2502.12575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13879v3","updated":"2025-02-18T06:08:19Z","published":"2024-12-18T14:19:23Z","title":"Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under\n  Black-box Settings","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks yet still are vulnerable to external threats, particularly LLM\nDenial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to\nexhaust computational resources and block services. However, existing studies\npredominantly focus on white-box attacks, leaving black-box scenarios\nunderexplored. In this paper, we introduce Auto-Generation for LLM-DoS\n(AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS\nconstructs the DoS Attack Tree and expands the node coverage to achieve\neffectiveness under black-box conditions. By transferability-driven iterative\noptimization, AutoDoS could work across different models in one prompt.\nFurthermore, we reveal that embedding the Length Trojan allows AutoDoS to\nbypass existing defenses more effectively. Experimental results show that\nAutoDoS significantly amplifies service response latency by over\n250$\\times\\uparrow$, leading to severe resource consumption in terms of GPU\nutilization and memory usage. Our work provides a new perspective on LLM-DoS\nattacks and security defenses. Our code is available at\nhttps://github.com/shuita2333/AutoDoS.\n","authors":["Yuanhe Zhang","Zhenhong Zhou","Wei Zhang","Xinyue Wang","Xiaojun Jia","Yang Liu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2412.13879v3.pdf","comment":"22 pages, 8 figures, 11 tables"},{"id":"http://arxiv.org/abs/2502.12562v1","updated":"2025-02-18T05:57:35Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02220v4","updated":"2025-02-18T05:47:09Z","published":"2024-10-03T05:24:38Z","title":"Data to Defense: The Role of Curation in Customizing LLMs Against\n  Jailbreaking Attacks","summary":"  Large language models (LLMs) are widely adapted for downstream applications\nthrough fine-tuning, a process named customization. However, recent studies\nhave identified a vulnerability during this process, where malicious samples\ncan compromise the robustness of LLMs and amplify harmful behaviors-an attack\ncommonly referred to as jailbreaking. To address this challenge, we propose an\nadaptive data curation approach allowing any text to be curated to enhance its\neffectiveness in counteracting harmful samples during customization. To avoid\nthe need for additional defensive modules, we further introduce a comprehensive\nmitigation framework spanning the lifecycle of the customization process:\nbefore customization to immunize LLMs against future jailbreak attempts, during\ncustomization to neutralize risks, and after customization to restore\ncompromised models. Experimental results demonstrate a significant reduction in\njailbreaking effects, achieving up to a 100% success rate in generating safe\nresponses. By combining adaptive data curation with lifecycle-based mitigation\nstrategies, this work represents a solid step forward in mitigating\njailbreaking risks and ensuring the secure adaptation of LLMs.\n","authors":["Xiaoqun Liu","Jiacheng Liang","Luoxi Tang","Muchao Ye","Weicheng Ma","Zhaohan Xi"],"pdf_url":"https://arxiv.org/pdf/2410.02220v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00077v5","updated":"2025-02-18T05:19:16Z","published":"2024-06-22T15:32:53Z","title":"Differentially Private Graph Diffusion with Applications in Personalized\n  PageRanks","summary":"  Graph diffusion, which iteratively propagates real-valued substances among\nthe graph, is used in numerous graph/network-involved applications. However,\nreleasing diffusion vectors may reveal sensitive linking information in the\ndata such as transaction information in financial network data. However,\nprotecting the privacy of graph data is challenging due to its interconnected\nnature. This work proposes a novel graph diffusion framework with edge-level\ndifferential privacy guarantees by using noisy diffusion iterates. The\nalgorithm injects Laplace noise per diffusion iteration and adopts a\ndegree-based thresholding function to mitigate the high sensitivity induced by\nlow-degree nodes. Our privacy loss analysis is based on Privacy Amplification\nby Iteration (PABI), which to our best knowledge, is the first effort that\nanalyzes PABI with Laplace noise and provides relevant applications. We also\nintroduce a novel Infinity-Wasserstein distance tracking method, which tightens\nthe analysis of privacy leakage and makes PABI more applicable in practice. We\nevaluate this framework by applying it to Personalized Pagerank computation for\nranking tasks. Experiments on real-world network data demonstrate the\nsuperiority of our method under stringent privacy conditions.\n","authors":["Rongzhe Wei","Eli Chien","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2407.00077v5.pdf","comment":"Github Code Available"},{"id":"http://arxiv.org/abs/2502.12544v1","updated":"2025-02-18T05:08:45Z","published":"2025-02-18T05:08:45Z","title":"Mixing Algorithm for Extending the Tiers of the Unapparent Information\n  Send through the Audio Streams","summary":"  Usage of the fast development of real-life digital applications in modern\ntechnology should guarantee novel and efficient way-outs of their protection.\nEncryption facilitates the data hiding. With the express development of\ntechnology, people tend to figure out a method that is capable of hiding a\nmessage and the survival of the message. Secrecy and efficiency can be obtained\nthrough steganographic involvement, a novel approach, along with multipurpose\naudio streams. Generally, steganography advantages are not used among industry\nand learners even though it is extensively discussed in the present information\nworld. Information hiding in audio files is exclusively inspiring due to the\ncompassion of the Human Auditory System (HAS). The proposed resolution supports\nAdvance Encryption Standard (AES)256 key encryption and tolerates all existing\naudio file types as the container. This paper analyzes and proposes a way out\naccording to the performance based on robustness, security, and hiding\ncapacity. Furthermore, a survey of audio steganography applications, as well as\na proposed resolution, is discussed in this paper.\n","authors":["Sachith Dassanayaka"],"pdf_url":"https://arxiv.org/pdf/2502.12544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11054v2","updated":"2025-02-18T04:47:19Z","published":"2025-02-16T09:27:44Z","title":"Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models","summary":"  Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.\n","authors":["Zonghao Ying","Deyue Zhang","Zonglei Jing","Yisong Xiao","Quanchen Zou","Aishan Liu","Siyuan Liang","Xiangzheng Zhang","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.11054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12497v1","updated":"2025-02-18T03:22:38Z","published":"2025-02-18T03:22:38Z","title":"SoK: Understanding Vulnerabilities in the Large Language Model Supply\n  Chain","summary":"  Large Language Models (LLMs) transform artificial intelligence, driving\nadvancements in natural language understanding, text generation, and autonomous\nsystems. The increasing complexity of their development and deployment\nintroduces significant security challenges, particularly within the LLM supply\nchain. However, existing research primarily focuses on content safety, such as\nadversarial attacks, jailbreaking, and backdoor attacks, while overlooking\nsecurity vulnerabilities in the underlying software systems. To address this\ngap, this study systematically analyzes 529 vulnerabilities reported across 75\nprominent projects spanning 13 lifecycle stages. The findings show that\nvulnerabilities are concentrated in the application (50.3%) and model (42.7%)\nlayers, with improper resource control (45.7%) and improper neutralization\n(25.1%) identified as the leading root causes. Additionally, while 56.7% of the\nvulnerabilities have available fixes, 8% of these patches are ineffective,\nresulting in recurring vulnerabilities. This study underscores the challenges\nof securing the LLM ecosystem and provides actionable insights to guide future\nresearch and mitigation strategies.\n","authors":["Shenao Wang","Yanjie Zhao","Zhao Liu","Quanchen Zou","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12491v1","updated":"2025-02-18T03:19:54Z","published":"2025-02-18T03:19:54Z","title":"PKE and ABE with Collusion-Resistant Secure Key Leasing","summary":"  Secure key leasing (SKL) is an advanced encryption functionality that allows\na secret key holder to generate a quantum decryption key and securely lease it\nto a user. Once the user returns the quantum decryption key (or provides a\nclassical certificate confirming its deletion), they lose their decryption\ncapability. Previous works on public key encryption with SKL (PKE-SKL) have\nonly considered the single-key security model, where the adversary receives at\nmost one quantum decryption key. However, this model does not accurately\nreflect real-world applications of PKE-SKL. To address this limitation, we\nintroduce collusion-resistant security for PKE-SKL (denoted as PKE-CR-SKL). In\nthis model, the adversary can adaptively obtain multiple quantum decryption\nkeys and access a verification oracle which validates the correctness of\nqueried quantum decryption keys. Importantly, the size of the public key and\nciphertexts must remain independent of the total number of generated quantum\ndecryption keys. We present the following constructions:\n  - A PKE-CR-SKL scheme based on the learning with errors (LWE) assumption.\n  - An attribute-based encryption scheme with collusion-resistant SKL\n(ABE-CR-SKL), also based on the LWE assumption.\n  - An ABE-CR-SKL scheme with classical certificates, relying on multi-input\nABE with polynomial arity.\n","authors":["Fuyuki Kitagawa","Ryo Nishimaki","Nikhil Pappu"],"pdf_url":"https://arxiv.org/pdf/2502.12491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12460v1","updated":"2025-02-18T02:45:46Z","published":"2025-02-18T02:45:46Z","title":"LMN: A Tool for Generating Machine Enforceable Policies from Natural\n  Language Access Control Rules using LLMs","summary":"  Organizations often lay down rules or guidelines called Natural Language\nAccess Control Policies (NLACPs) for specifying who gets access to which\ninformation and when. However, these cannot be directly used in a target access\ncontrol model like Attribute-based Access Control (ABAC). Manually translating\nthe NLACP rules into Machine Enforceable Security Policies (MESPs) is both time\nconsuming and resource intensive, rendering it infeasible especially for large\norganizations. Automated machine translation workflows, on the other hand,\nrequire information security officers to be adept at using such processes. To\neffectively address this problem, we have developed a free web-based publicly\naccessible tool called LMN (LLMs for generating MESPs from NLACPs) that takes\nan NLACP as input and converts it into a corresponding MESP. Internally, LMN\nuses the GPT 3.5 API calls and an appropriately chosen prompt. Extensive\nexperiments with different prompts and performance metrics firmly establish the\nusefulness of LMN.\n","authors":["Pratik Sonune","Ritwik Rai","Shamik Sural","Vijayalakshmi Atluri","Ashish Kundu"],"pdf_url":"https://arxiv.org/pdf/2502.12460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12441v1","updated":"2025-02-18T02:23:38Z","published":"2025-02-18T02:23:38Z","title":"Choosing Coordinate Forms for Solving ECDLP Using Shor's Algorithm","summary":"  Shor's algorithm is well-known for its capability to address the elliptic\ncurve discrete logarithm problem (ECDLP) in polynomial time. The enhancement of\nits quantum resources continues to be a crucial focus of research.\nNevertheless, the application of projective coordinates for quantum resource\noptimization remains an unresolved issue, mainly because the representation of\nprojective coordinates lacks uniqueness without employing modular division\noperations. Our study reveals that projective coordinates do not provide the\nsame advantages as affine coordinates when utilizing Shor's method to tackle\nthe ECDLP.\n","authors":["Yan Huang","Fangguo Zhang","Fei Gao","Zijian Zhou","Longjiang Qu"],"pdf_url":"https://arxiv.org/pdf/2502.12441v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.08754v3","updated":"2025-02-18T02:14:46Z","published":"2024-06-13T02:24:08Z","title":"StructuralSleight: Automated Jailbreak Attacks on Large Language Models\n  Utilizing Uncommon Text-Organization Structures","summary":"  Large Language Models (LLMs) are widely used in natural language processing\nbut face the risk of jailbreak attacks that maliciously induce them to generate\nharmful content. Existing jailbreak attacks, including character-level and\ncontext-level attacks, mainly focus on the prompt of plain text without\nspecifically exploring the significant influence of its structure. In this\npaper, we focus on studying how the prompt structure contributes to the\njailbreak attack. We introduce a novel structure-level attack method based on\nlong-tailed structures, which we refer to as Uncommon Text-Organization\nStructures (UTOS). We extensively study 12 UTOS templates and 6 obfuscation\nmethods to build an effective automated jailbreak tool named StructuralSleight\nthat contains three escalating attack strategies: Structural Attack, Structural\nand Character/Context Obfuscation Attack, and Fully Obfuscated Structural\nAttack. Extensive experiments on existing LLMs show that StructuralSleight\nsignificantly outperforms the baseline methods. In particular, the attack\nsuccess rate reaches 94.62\\% on GPT-4o, which has not been addressed by\nstate-of-the-art techniques.\n","authors":["Bangxin Li","Hengrui Xing","Cong Tian","Chao Huang","Jin Qian","Huangqing Xiao","Linfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2406.08754v3.pdf","comment":"15 pages, 7 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.13146v1","updated":"2025-02-18T18:59:57Z","published":"2025-02-18T18:59:57Z","title":"Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization","summary":"  The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align.\n","authors":["Shuo Xing","Yuping Wang","Peiran Li","Ruizheng Bai","Yueqi Wang","Chengxuan Qian","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2502.13146v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.13141v1","updated":"2025-02-18T18:59:00Z","published":"2025-02-18T18:59:00Z","title":"UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models","summary":"  Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.\n","authors":["Huawei Lin","Yingjie Lao","Tong Geng","Tan Yu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.13141v1.pdf","comment":"18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks"},{"id":"http://arxiv.org/abs/2502.13140v1","updated":"2025-02-18T18:58:35Z","published":"2025-02-18T18:58:35Z","title":"Towards Quantum Tensor Decomposition in Biomedical Applications","summary":"  Tensor decomposition has emerged as a powerful framework for feature\nextraction in multi-modal biomedical data. In this review, we present a\ncomprehensive analysis of tensor decomposition methods such as Tucker,\nCANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse\napplications across biomedical domains such as imaging, multi-omics, and\nspatial transcriptomics. To systematically investigate the literature, we\napplied a topic modeling-based approach that identifies and groups distinct\nthematic sub-areas in biomedicine where tensor decomposition has been used,\nthereby revealing key trends and research directions. We evaluated challenges\nrelated to the scalability of latent spaces along with obtaining the optimal\nrank of the tensor, which often hinder the extraction of meaningful features\nfrom increasingly large and complex datasets. Additionally, we discuss recent\nadvances in quantum algorithms for tensor decomposition, exploring how quantum\ncomputing can be leveraged to address these challenges. Our study includes a\npreliminary resource estimation analysis for quantum computing platforms and\nexamines the feasibility of implementing quantum-enhanced tensor decomposition\nmethods on near-term quantum devices. Collectively, this review not only\nsynthesizes current applications and challenges of tensor decomposition in\nbiomedical analyses but also outlines promising quantum computing strategies to\nenhance its impact on deriving actionable insights from complex biomedical\ndata.\n","authors":["Myson Burch","Jiasen Zhang","Gideon Idumah","Hakan Doga","Richard Lartey","Lamis Yehia","Mingrui Yang","Murat Yildirim","Mihriban Karaayvaz","Omar Shehab","Weihong Guo","Ying Ni","Laxmi Parida","Xiaojuan Li","Aritra Bose"],"pdf_url":"https://arxiv.org/pdf/2502.13140v1.pdf","comment":"31 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.24210v3","updated":"2025-02-18T18:58:14Z","published":"2024-10-31T17:58:41Z","title":"TabM: Advancing Tabular Deep Learning with Parameter-Efficient\n  Ensembling","summary":"  Deep learning architectures for supervised learning on tabular data range\nfrom simple multilayer perceptrons (MLP) to sophisticated Transformers and\nretrieval-augmented methods. This study highlights a major, yet so far\noverlooked opportunity for designing substantially better MLP-based tabular\narchitectures. Namely, our new model TabM relies on efficient ensembling, where\none TabM efficiently imitates an ensemble of MLPs and produces multiple\npredictions per object. Compared to a traditional deep ensemble, in TabM, the\nunderlying implicit MLPs are trained simultaneously, and (by default) share\nmost of their parameters, which results in significantly better performance and\nefficiency. Using TabM as a new baseline, we perform a large-scale evaluation\nof tabular DL architectures on public benchmarks in terms of both task\nperformance and efficiency, which renders the landscape of tabular DL in a new\nlight. Generally, we show that MLPs, including TabM, form a line of stronger\nand more practical models compared to attention- and retrieval-based\narchitectures. In particular, we find that TabM demonstrates the best\nperformance among tabular DL models. Then, we conduct an empirical analysis on\nthe ensemble-like nature of TabM. We observe that the multiple predictions of\nTabM are weak individually, but powerful collectively. Overall, our work brings\nan impactful technique to tabular DL and advances the performance-efficiency\ntrade-off with TabM -- a simple and powerful baseline for researchers and\npractitioners.\n","authors":["Yury Gorishniy","Akim Kotelnikov","Artem Babenko"],"pdf_url":"https://arxiv.org/pdf/2410.24210v3.pdf","comment":"ICLR 2025. Code: https://github.com/yandex-research/tabm"},{"id":"http://arxiv.org/abs/2502.13138v1","updated":"2025-02-18T18:57:21Z","published":"2025-02-18T18:57:21Z","title":"AIDE: AI-Driven Exploration in the Space of Code","summary":"  Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.\n","authors":["Zhengyao Jiang","Dominik Schmidt","Dhruv Srikanth","Dixing Xu","Ian Kaplan","Deniss Jacenko","Yuxiang Wu"],"pdf_url":"https://arxiv.org/pdf/2502.13138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13135v1","updated":"2025-02-18T18:56:44Z","published":"2025-02-18T18:56:44Z","title":"Sleepless Nights, Sugary Days: Creating Synthetic Users with Health\n  Conditions for Realistic Coaching Agent Interactions","summary":"  We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.\n","authors":["Taedong Yun","Eric Yang","Mustafa Safdari","Jong Ha Lee","Vaishnavi Vinod Kumar","S. Sara Mahdavi","Jonathan Amar","Derek Peyton","Reut Aharony","Andreas Michaelides","Logan Schneider","Isaac Galatzer-Levy","Yugang Jia","John Canny","Arthur Gretton","Maja Matari"],"pdf_url":"https://arxiv.org/pdf/2502.13135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13134v1","updated":"2025-02-18T18:56:41Z","published":"2025-02-18T18:56:41Z","title":"RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human\n  Demonstrations","summary":"  Humanoid robots have shown success in locomotion and manipulation. Despite\nthese basic abilities, humanoids are still required to quickly understand human\ninstructions and react based on human interaction signals to become valuable\nassistants in human daily life. Unfortunately, most existing works only focus\non multi-stage interactions, treating each task separately, and neglecting\nreal-time feedback. In this work, we aim to empower humanoid robots with\nreal-time reaction abilities to achieve various tasks, allowing human to\ninterrupt robots at any time, and making robots respond to humans immediately.\nTo support such abilities, we propose a general humanoid-human-object\ninteraction framework, named RHINO, i.e., Real-time Humanoid-human Interaction\nand Object manipulation. RHINO provides a unified view of reactive motion,\ninstruction-based manipulation, and safety concerns, over multiple human signal\nmodalities, such as languages, images, and motions. RHINO is a hierarchical\nlearning framework, enabling humanoids to learn reaction skills from\nhuman-human-object demonstrations and teleoperation data. In particular, it\ndecouples the interaction process into two levels: 1) a high-level planner\ninferring human intentions from real-time human behaviors; and 2) a low-level\ncontroller achieving reactive motion behaviors and object manipulation skills\nbased on the predicted intentions. We evaluate the proposed framework on a real\nhumanoid robot and demonstrate its effectiveness, flexibility, and safety in\nvarious scenarios.\n","authors":["Jingxiao Chen","Xinyao Li","Jiahang Cao","Zhengbang Zhu","Wentao Dong","Minghuan Liu","Ying Wen","Yong Yu","Liqing Zhang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.13134v1.pdf","comment":"Project website: https://humanoid-interaction.github.io/"},{"id":"http://arxiv.org/abs/2502.13132v1","updated":"2025-02-18T18:55:53Z","published":"2025-02-18T18:55:53Z","title":"Learning to Defer for Causal Discovery with Imperfect Experts","summary":"  Integrating expert knowledge, e.g. from large language models, into causal\ndiscovery algorithms can be challenging when the knowledge is not guaranteed to\nbe correct. Expert recommendations may contradict data-driven results, and\ntheir reliability can vary significantly depending on the domain or specific\nquery. Existing methods based on soft constraints or inconsistencies in\npredicted causal relationships fail to account for these variations in\nexpertise. To remedy this, we propose L2D-CD, a method for gauging the\ncorrectness of expert recommendations and optimally combining them with\ndata-driven causal discovery results. By adapting learning-to-defer (L2D)\nalgorithms for pairwise causal discovery (CD), we learn a deferral function\nthat selects whether to rely on classical causal discovery methods using\nnumerical data or expert recommendations based on textual meta-data. We\nevaluate L2D-CD on the canonical T\\\"ubingen pairs dataset and demonstrate its\nsuperior performance compared to both the causal discovery method and the\nexpert used in isolation. Moreover, our approach identifies domains where the\nexpert's performance is strong or weak. Finally, we outline a strategy for\ngeneralizing this approach to causal discovery on graphs with more than two\nvariables, paving the way for further research in this area.\n","authors":["Oscar Clivio","Divyat Mahajan","Perouz Taslakian","Sara Magliacane","Ioannis Mitliagkas","Valentina Zantedeschi","Alexandre Drouin"],"pdf_url":"https://arxiv.org/pdf/2502.13132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11687v2","updated":"2025-02-18T18:55:39Z","published":"2024-10-15T15:22:38Z","title":"State-space models can learn in-context by gradient descent","summary":"  Deep state-space models (Deep SSMs) are becoming popular as effective\napproaches to model sequence data. They have also been shown to be capable of\nin-context learning, much like transformers. However, a complete picture of how\nSSMs might be able to do in-context learning has been missing. In this study,\nwe provide a direct and explicit construction to show that state-space models\ncan perform gradient-based learning and use it for in-context learning in much\nthe same way as transformers. Specifically, we prove that a single structured\nstate-space model layer, augmented with multiplicative input and output gating,\ncan reproduce the outputs of an implicit linear model with least squares loss\nafter one step of gradient descent. We then show a straightforward extension to\nmulti-step linear and non-linear regression tasks. We validate our construction\nby training randomly initialized augmented SSMs on linear and non-linear\nregression tasks. The empirically obtained parameters through optimization\nmatch the ones predicted analytically by the theoretical construction. Overall,\nwe elucidate the role of input- and output-gating in recurrent architectures as\nthe key inductive biases for enabling the expressive power typical of\nfoundation models. We also provide novel insights into the relationship between\nstate-space models and linear self-attention, and their ability to learn\nin-context.\n","authors":["Neeraj Mohan Sushma","Yudou Tian","Harshvardhan Mestha","Nicolo Colombo","David Kappel","Anand Subramoney"],"pdf_url":"https://arxiv.org/pdf/2410.11687v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.13130v1","updated":"2025-02-18T18:55:21Z","published":"2025-02-18T18:55:21Z","title":"Magma: A Foundation Model for Multimodal AI Agents","summary":"  We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.\n","authors":["Jianwei Yang","Reuben Tan","Qianhui Wu","Ruijie Zheng","Baolin Peng","Yongyuan Liang","Yu Gu","Mu Cai","Seonghyeon Ye","Joel Jang","Yuquan Deng","Lars Liden","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.13130v1.pdf","comment":"29 pages, 16 figures, technical report from MSR"},{"id":"http://arxiv.org/abs/2502.12118v2","updated":"2025-02-18T18:54:12Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02673v2","updated":"2025-02-18T18:39:05Z","published":"2025-01-05T22:03:46Z","title":"Exploring the Impact of Dataset Statistical Effect Size on Model\n  Performance and Data Sample Size Sufficiency","summary":"  Having a sufficient quantity of quality data is a critical enabler of\ntraining effective machine learning models. Being able to effectively determine\nthe adequacy of a dataset prior to training and evaluating a model's\nperformance would be an essential tool for anyone engaged in experimental\ndesign or data collection. However, despite the need for it, the ability to\nprospectively assess data sufficiency remains an elusive capability. We report\nhere on two experiments undertaken in an attempt to better ascertain whether or\nnot basic descriptive statistical measures can be indicative of how effective a\ndataset will be at training a resulting model. Leveraging the effect size of\nour features, this work first explores whether or not a correlation exists\nbetween effect size, and resulting model performance (theorizing that the\nmagnitude of the distinction between classes could correlate to a classifier's\nresulting success). We then explore whether or not the magnitude of the effect\nsize will impact the rate of convergence of our learning rate, (theorizing\nagain that a greater effect size may indicate that the model will converge more\nrapidly, and with a smaller sample size needed). Our results appear to indicate\nthat this is not an effective heuristic for determining adequate sample size or\nprojecting model performance, and therefore that additional work is still\nneeded to better prospectively assess adequacy of data.\n","authors":["Arya Hatamian","Lionel Levine","Haniyeh Ehsani Oskouie","Majid Sarrafzadeh"],"pdf_url":"https://arxiv.org/pdf/2501.02673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13115v1","updated":"2025-02-18T18:35:24Z","published":"2025-02-18T18:35:24Z","title":"Near-Optimal Private Learning in Linear Contextual Bandits","summary":"  We analyze the problem of private learning in generalized linear contextual\nbandits. Our approach is based on a novel method of re-weighted regression,\nyielding an efficient algorithm with regret of order\n$\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ in the joint and local model\nof $\\alpha$-privacy, respectively. Further, we provide near-optimal private\nprocedures that achieve dimension-independent rates in private linear models\nand linear contextual bandits. In particular, our results imply that joint\nprivacy is almost \"for free\" in all the settings we consider, partially\naddressing the open problem posed by Azize and Basu (2024).\n","authors":["Fan Chen","Jiachun Li","Alexander Rakhlin","David Simchi-Levi"],"pdf_url":"https://arxiv.org/pdf/2502.13115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13112v1","updated":"2025-02-18T18:26:20Z","published":"2025-02-18T18:26:20Z","title":"Constrained Online Convex Optimization with Polyak Feasibility Steps","summary":"  In this work, we study online convex optimization with a fixed constraint\nfunction $g : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Prior work on this problem\nhas shown $O(\\sqrt{T})$ regret and cumulative constraint satisfaction\n$\\sum_{t=1}^{T} g(x_t) \\leq 0$, while only accessing the constraint value and\nsubgradient at the played actions $g(x_t), \\partial g(x_t)$. Using the same\nconstraint information, we show a stronger guarantee of anytime constraint\nsatisfaction $g(x_t) \\leq 0 \\ \\forall t \\in [T]$, and matching $O(\\sqrt{T})$\nregret guarantees. These contributions are thanks to our approach of using\nPolyak feasibility steps to ensure constraint satisfaction, without sacrificing\nregret. Specifically, after each step of online gradient descent, our algorithm\napplies a subgradient descent step on the constraint function where the\nstep-size is chosen according to the celebrated Polyak step-size. We further\nvalidate this approach with numerical experiments.\n","authors":["Spencer Hutchinson","Mahnoosh Alizadeh"],"pdf_url":"https://arxiv.org/pdf/2502.13112v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.13110v1","updated":"2025-02-18T18:23:33Z","published":"2025-02-18T18:23:33Z","title":"MLPs at the EOC: Dynamics of Feature Learning","summary":"  Since infinitely wide neural networks in the kernel regime are random feature\nmodels, the success of contemporary deep learning lies in the rich regime,\nwhere a satisfying theory should explain not only the convergence of gradient\ndescent but the learning of features along the way. Such a theory should also\ncover phenomena observed by practicioners including the Edge of Stability (EOS)\nand the catapult mechanism. For a practically relevant theory in the limit,\nneural network parameterizations have to efficiently reproduce limiting\nbehavior as width and depth are scaled up. While widthwise scaling is mostly\nsettled, depthwise scaling is solved only at initialization by the Edge of\nChaos (EOC). During training, scaling up depth is either done by inversely\nscaling the learning rate or adding residual connections. We propose $(1)$ the\nNormalized Update Parameterization ($\\nu$P) to solve this issue by growing\nhidden layer sizes depthwise inducing the regularized evolution of\npreactivations, $(2)$ a hypothetical explanation for feature learning via the\ncosine of new and cumulative parameter updates and $(3)$ a geometry-aware\nlearning rate schedule that is able to prolong the catapult phase indefinitely.\nWe support our hypotheses and demonstrate the usefulness of $\\nu$P and the\nlearning rate schedule by empirical evidence.\n","authors":["Dvid Terjk"],"pdf_url":"https://arxiv.org/pdf/2502.13110v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.13108v1","updated":"2025-02-18T18:20:37Z","published":"2025-02-18T18:20:37Z","title":"Improving Clinical Question Answering with Multi-Task Learning: A Joint\n  Approach for Answer Extraction and Medical Categorization","summary":"  Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.\n","authors":["Priyaranjan Pattnayak","Hitesh Laxmichand Patel","Amit Agarwal","Bhargava Kumar","Srikant Panda","Tejaswini Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.13108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07186v2","updated":"2025-02-18T18:20:08Z","published":"2025-01-13T10:31:36Z","title":"Generalizable Graph Neural Networks for Robust Power Grid Topology\n  Control","summary":"  The energy transition necessitates new congestion management methods. One\nsuch method is controlling the grid topology with machine learning (ML). This\napproach has gained popularity following the Learning to Run a Power Network\n(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models\nthat reflect graph structure in their computation, which makes them suitable\nfor power grid modeling. Various GNN approaches for topology control have thus\nbeen proposed. We propose the first GNN model for grid topology control that\nuses only GNN layers. Additionally, we identify the busbar information\nasymmetry problem that the popular homogeneous graph representation suffers\nfrom, and propose a heterogeneous graph representation to resolve it. We train\nboth homogeneous and heterogeneous GNNs and fully connected neural networks\n(FCNN) baselines on an imitation learning task. We evaluate the models\naccording to their classification accuracy and grid operation ability. We find\nthat the heterogeneous GNNs perform best on in-distribution networks, followed\nby the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN\ntypes generalize better to out-of-distribution networks than FCNNs.\n","authors":["Matthijs de Jong","Jan Viebahn","Yuliya Shapovalova"],"pdf_url":"https://arxiv.org/pdf/2501.07186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13107v1","updated":"2025-02-18T18:19:36Z","published":"2025-02-18T18:19:36Z","title":"MatterChat: A Multi-Modal LLM for Material Science","summary":"  Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.\n","authors":["Yingheng Tang","Wenbin Xu","Jie Cao","Jianzhu Ma","Weilu Gao","Steve Farrell","Benjamin Erichson","Michael W. Mahoney","Andy Nonaka","Zhi Yao"],"pdf_url":"https://arxiv.org/pdf/2502.13107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14700v3","updated":"2025-02-18T18:19:07Z","published":"2025-01-24T18:22:37Z","title":"An Attentive Graph Agent for Topology-Adaptive Cyber Defence","summary":"  As cyber threats grow increasingly sophisticated, reinforcement learning (RL)\nis emerging as a promising technique to create intelligent and adaptive cyber\ndefense systems. However, most existing autonomous defensive agents have\noverlooked the inherent graph structure of computer networks subject to cyber\nattacks, potentially missing critical information and constraining their\nadaptability. To overcome these limitations, we developed a custom version of\nthe Cyber Operations Research Gym (CybORG) environment, encoding network state\nas a directed graph with realistic low-level features. We employ a Graph\nAttention Network (GAT) architecture to process node, edge, and global\nfeatures, and adapt its output to be compatible with policy gradient methods in\nRL. Our GAT-based approach offers key advantages over flattened alternatives:\npolicies that demonstrate resilience to certain types of unexpected dynamic\nnetwork topology changes, reasonable generalisation to networks of varying\nsizes within the same structural distribution, and interpretable defensive\nactions grounded in tangible network properties. We demonstrate that GAT\ndefensive policies can be trained using our low-level directed graph\nobservations, even when unexpected connections arise during simulation.\nEvaluations across networks of different sizes, but consistent subnetwork\nstructure, show our policies achieve comparable performance to policies trained\nspecifically for each network configuration. Our study contributes to the\ndevelopment of robust cyber defence systems that can better adapt to real-world\nnetwork security challenges.\n","authors":["Ilya Orson Sandoval","Isaac Symes Thompson","Vasilios Mavroudis","Chris Hicks"],"pdf_url":"https://arxiv.org/pdf/2501.14700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13105v1","updated":"2025-02-18T18:17:49Z","published":"2025-02-18T18:17:49Z","title":"Enhanced uncertainty quantification variational autoencoders for the\n  solution of Bayesian inverse problems","summary":"  Among other uses, neural networks are a powerful tool for solving\ndeterministic and Bayesian inverse problems in real-time. In the Bayesian\nframework, variational autoencoders, a specialized type of neural network,\nenable the estimation of model parameters and their distribution based on\nobservational data allowing to perform real-time inverse uncertainty\nquantification. In this work, we build upon existing research [Goh, H. et al.,\nProceedings of Machine Learning Research, 2022] by proposing a novel loss\nfunction to train variational autoencoders for Bayesian inverse problems. When\nthe forward map is affine, we provide a theoretical proof of the convergence of\nthe latent states of variational autoencoders to the posterior distribution of\nthe model parameters. We validate this theoretical result through numerical\ntests and we compare the proposed variational autoencoder with the existing one\nin the literature. Finally, we test the proposed variational autoencoder on the\nLaplace equation.\n","authors":["Andrea Tonini","Luca Dede'"],"pdf_url":"https://arxiv.org/pdf/2502.13105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13095v1","updated":"2025-02-18T18:06:48Z","published":"2025-02-18T18:06:48Z","title":"Understanding and Rectifying Safety Perception Distortion in VLMs","summary":"  Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.\n","authors":["Xiaohan Zou","Jian Kang","George Kesidis","Lu Lin"],"pdf_url":"https://arxiv.org/pdf/2502.13095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13090v1","updated":"2025-02-18T17:57:29Z","published":"2025-02-18T17:57:29Z","title":"tn4ml: Tensor Network Training and Customization for Machine Learning","summary":"  Tensor Networks have emerged as a prominent alternative to neural networks\nfor addressing Machine Learning challenges in foundational sciences, paving the\nway for their applications to real-life problems. This paper introduces tn4ml,\na novel library designed to seamlessly integrate Tensor Networks into\noptimization pipelines for Machine Learning tasks. Inspired by existing Machine\nLearning frameworks, the library offers a user-friendly structure with modules\nfor data embedding, objective function definition, and model training using\ndiverse optimization strategies. We demonstrate its versatility through two\nexamples: supervised learning on tabular data and unsupervised learning on an\nimage dataset. Additionally, we analyze how customizing the parts of the\nMachine Learning pipeline for Tensor Networks influences performance metrics.\n","authors":["Ema Puljak","Sergio Sanchez-Ramirez","Sergi Masot-Llima","Jofre Valls-Muns","Artur Garcia-Saez","Maurizio Pierini"],"pdf_url":"https://arxiv.org/pdf/2502.13090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01077v2","updated":"2025-02-18T17:57:26Z","published":"2024-11-01T23:18:32Z","title":"Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection","summary":"  Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards.\n","authors":["Zhipeng Wei","Yuqi Liu","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2411.01077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13085v1","updated":"2025-02-18T17:48:25Z","published":"2025-02-18T17:48:25Z","title":"A Neural Difference-of-Entropies Estimator for Mutual Information","summary":"  Estimating Mutual Information (MI), a key measure of dependence of random\nquantities without specific modelling assumptions, is a challenging problem in\nhigh dimensions. We propose a novel mutual information estimator based on\nparametrizing conditional densities using normalizing flows, a deep generative\nmodel that has gained popularity in recent years. This estimator leverages a\nblock autoregressive structure to achieve improved bias-variance trade-offs on\nstandard benchmark tasks.\n","authors":["Haoran Ni","Martin Lotz"],"pdf_url":"https://arxiv.org/pdf/2502.13085v1.pdf","comment":"23 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.02251v2","updated":"2025-02-18T17:42:49Z","published":"2024-12-03T08:28:47Z","title":"Selective Reviews of Bandit Problems in AI via a Statistical View","summary":"  Reinforcement Learning (RL) is a widely researched area in artificial\nintelligence that focuses on teaching agents decision-making through\ninteractions with their environment. A key subset includes stochastic\nmulti-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which\nmodel sequential decision-making under uncertainty. This review outlines the\nfoundational models and assumptions of bandit problems, explores non-asymptotic\ntheoretical tools like concentration inequalities and minimax regret bounds,\nand compares frequentist and Bayesian algorithms for managing\nexploration-exploitation trade-offs. Additionally, we explore K-armed\ncontextual bandits and SCAB, focusing on their methodologies and regret\nanalyses. We also examine the connections between SCAB problems and functional\ndata analysis. Finally, we highlight recent advances and ongoing challenges in\nthe field.\n","authors":["Pengjie Zhou","Haoyu Wei","Huiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.02251v2.pdf","comment":"52 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.13080v1","updated":"2025-02-18T17:33:41Z","published":"2025-02-18T17:33:41Z","title":"BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression\n  Classification","summary":"  Gene expression classification is a pivotal yet challenging task in\nbioinformatics, primarily due to the high dimensionality of genomic data and\nthe risk of overfitting. To bridge this gap, we propose BOLIMES, a novel\nfeature selection algorithm designed to enhance gene expression classification\nby systematically refining the feature subset. Unlike conventional methods that\nrely solely on statistical ranking or classifier-specific selection, we\nintegrate the robustness of Boruta with the interpretability of LIME, ensuring\nthat only the most relevant and influential genes are retained. BOLIMES first\nemploys Boruta to filter out non-informative genes by comparing each feature\nagainst its randomized counterpart, thus preserving valuable information. It\nthen uses LIME to rank the remaining genes based on their local importance to\nthe classifier. Finally, an iterative classification evaluation determines the\noptimal feature subset by selecting the number of genes that maximizes\npredictive accuracy. By combining exhaustive feature selection with\ninterpretability-driven refinement, our solution effectively balances\ndimensionality reduction with high classification performance, offering a\npowerful solution for high-dimensional gene expression analysis.\n","authors":["Bich-Chung Phan","Thanh Ma","Huu-Hoa Nguyen","and Thanh-Nghi Do"],"pdf_url":"https://arxiv.org/pdf/2502.13080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05241v3","updated":"2025-02-18T17:20:48Z","published":"2024-05-08T17:37:57Z","title":"BenthicNet: A global compilation of seafloor images for deep learning\n  applications","summary":"  Advances in underwater imaging enable collection of extensive seafloor image\ndatasets necessary for monitoring important benthic ecosystems. The ability to\ncollect seafloor imagery has outpaced our capacity to analyze it, hindering\nmobilization of this crucial environmental information. Machine learning\napproaches provide opportunities to increase the efficiency with which seafloor\nimagery is analyzed, yet large and consistent datasets to support development\nof such approaches are scarce. Here we present BenthicNet: a global compilation\nof seafloor imagery designed to support the training and evaluation of\nlarge-scale image recognition models. An initial set of over 11.4 million\nimages was collected and curated to represent a diversity of seafloor\nenvironments using a representative subset of 1.3 million images. These are\naccompanied by 3.1 million annotations translated to the CATAMI scheme, which\nspan 190,000 of the images. A large deep learning model was trained on this\ncompilation and preliminary results suggest it has utility for automating large\nand small-scale image analysis tasks. The compilation and model are made openly\navailable for reuse at https://doi.org/10.20383/103.0614.\n","authors":["Scott C. Lowe","Benjamin Misiuk","Isaac Xu","Shakhboz Abdulazizov","Amit R. Baroi","Alex C. Bastos","Merlin Best","Vicki Ferrini","Ariell Friedman","Deborah Hart","Ove Hoegh-Guldberg","Daniel Ierodiaconou","Julia Mackin-McLaughlin","Kathryn Markey","Pedro S. Menandro","Jacquomo Monk","Shreya Nemani","John O'Brien","Elizabeth Oh","Luba Y. Reshitnyk","Katleen Robert","Chris M. Roelfsema","Jessica A. Sameoto","Alexandre C. G. Schimel","Jordan A. Thomson","Brittany R. Wilson","Melisa C. Wong","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2405.05241v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13399v3","updated":"2025-02-18T17:16:55Z","published":"2024-07-18T11:08:40Z","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overoptimization via Chi-Squared Preference Optimization","summary":"  Language model alignment methods such as reinforcement learning from human\nfeedback (RLHF) have led to impressive advances in language model capabilities,\nbut are limited by a widely observed phenomenon known as overoptimization,\nwhere the quality of the language model degrades over the course of the\nalignment process. As the model optimizes performance with respect to an\noffline reward model, it overfits to inaccuracies and drifts away from\npreferred responses covered by the data. To discourage such distribution shift,\nKL-regularization is widely employed in existing offline alignment methods, but\noveroptimization continues to harm performance. Lending theoretical insight\ninto the source of these empirical observations, we first show that the\nKL-regularization is too weak to prevent overfitting, then raise the following\nquestion: is it possible to design an efficient algorithm that is provably\nrobust to overoptimization?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","authors":["Audrey Huang","Wenhao Zhan","Tengyang Xie","Jason D. Lee","Wen Sun","Akshay Krishnamurthy","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2407.13399v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06254v2","updated":"2025-02-18T17:10:39Z","published":"2025-01-09T02:54:19Z","title":"Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words","summary":"  Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.\n","authors":["Gouki Minegishi","Hiroki Furuta","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2501.06254v2.pdf","comment":"Published at ICLR2025"},{"id":"http://arxiv.org/abs/2502.13063v1","updated":"2025-02-18T17:08:45Z","published":"2025-02-18T17:08:45Z","title":"Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity","summary":"  A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.\n","authors":["Yuri Kuratov","Mikhail Arkhipov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2502.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13061v1","updated":"2025-02-18T17:07:29Z","published":"2025-02-18T17:07:29Z","title":"Improved Fine-Tuning of Large Multimodal Models for Hateful Meme\n  Detection","summary":"  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While large multimodal models\nhave shown strong generalization across various tasks, they exhibit poor\ngeneralization to hateful meme detection due to the dynamic nature of memes\ntied to emerging social trends and breaking news. Recent work further\nhighlights the limitations of conventional supervised fine-tuning for large\nmultimodal models in this context. To address these challenges, we propose\nLarge Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a\nnovel two-stage fine-tuning framework designed to improve both in-domain\naccuracy and cross-domain generalization. Experimental results on six widely\nused meme classification datasets demonstrate that LMM-RGCL achieves\nstate-of-the-art performance, outperforming agent-based systems such as\nVPD-PALI-X-55B. Furthermore, our method effectively generalizes to\nout-of-domain memes under low-resource settings, surpassing models like GPT-4o.\n","authors":["Jingbiao Mei","Jinghong Chen","Guangyu Yang","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2502.13061v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.13056v1","updated":"2025-02-18T17:02:41Z","published":"2025-02-18T17:02:41Z","title":"Benchmarking MedMNIST dataset on real quantum hardware","summary":"  Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present first comprehensive QML study by\nbenchmarking the MedMNIST-a diverse collection of medical imaging datasets on a\n127-qubit real IBM quantum hardware, to evaluate the feasibility and\nperformance of quantum models (without any classical neural networks) in\npractical applications. This study explore recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression and\nmitigation for medical image classification. Our methodology comprised of three\nstages: preprocessing, generation of noise-resilient and hardware-efficient\nquantum circuits, optimizing/training of quantum circuits on classical\nhardware, and inference on real IBM quantum hardware. Firstly, we process all\ninput images in the preprocessing stage to reduce the spatial dimension due to\nthe quantum hardware limitations. We generate hardware-efficient quantum\ncircuits using backend properties expressible to learn complex patterns for\nmedical image classification. After classical optimization of QML models, we\nperform the inference on real quantum hardware. We also incorporates advanced\nerror suppression and mitigation techniques in our QML workflow including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establishes a benchmark for future\nadvancements in QML applied to healthcare.\n","authors":["Gurinder Singh","Hongni Jin","Kenneth M. Merz Jr"],"pdf_url":"https://arxiv.org/pdf/2502.13056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13055v1","updated":"2025-02-18T17:01:37Z","published":"2025-02-18T17:01:37Z","title":"LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs","summary":"  The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.\n","authors":["Xingzhi Qian","Xinran Zheng","Yiling He","Shuo Yang","Lorenzo Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2502.13055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13049v1","updated":"2025-02-18T16:59:51Z","published":"2025-02-18T16:59:51Z","title":"$k$-Graph: A Graph Embedding for Interpretable Time Series Clustering","summary":"  Time series clustering poses a significant challenge with diverse\napplications across domains. A prominent drawback of existing solutions lies in\ntheir limited interpretability, often confined to presenting users with\ncentroids. In addressing this gap, our work presents $k$-Graph, an unsupervised\nmethod explicitly crafted to augment interpretability in time series\nclustering. Leveraging a graph representation of time series subsequences,\n$k$-Graph constructs multiple graph representations based on different\nsubsequence lengths. This feature accommodates variable-length time series\nwithout requiring users to predetermine subsequence lengths. Our experimental\nresults reveal that $k$-Graph outperforms current state-of-the-art time series\nclustering algorithms in accuracy, while providing users with meaningful\nexplanations and interpretations of the clustering outcomes.\n","authors":["Paul Boniol","Donato Tiano","Angela Bonifati","Themis Palpanas"],"pdf_url":"https://arxiv.org/pdf/2502.13049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10322v3","updated":"2025-02-18T16:52:29Z","published":"2024-06-14T17:41:55Z","title":"LieRE: Generalizing Rotary Position Encodings","summary":"  Transformer architectures rely on position encodings to capture token\ndependencies. Rotary Position Encoding (RoPE) has emerged as a popular choice\nin language models due to its efficient encoding of relative position\ninformation through key-query rotations. However, RoPE faces significant\nlimitations beyond language processing: it is constrained to one-dimensional\nsequence data and, even with learnable phases, offers limited representational\ncapacity. We address these challenges with Lie Relative Encodings (LieRE),\nwhich replaces RoPE's block-2D rotation matrix with a learned, dense,\nhigh-dimensional rotation matrix of variable sparsity. Through extensive\nevaluation on three image datasets across 2D and 3D classification tasks, LieRE\nachieves 2\\% relative improvement over state-of-the-art baselines on 2D tasks\nand 1.5\\% on 3D tasks, while demonstrating superior generalization to higher\nresolutions. Our implementation is computationally efficient, with results\nreproducible on 4 A100 GPUs in 30 minutes on CIFAR100, and we release our code\nto facilitate further research.\n","authors":["Sophie Ostmeier","Brian Axelrod","Michael E. Moseley","Akshay Chaudhari","Curtis Langlotz"],"pdf_url":"https://arxiv.org/pdf/2406.10322v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13034v1","updated":"2025-02-18T16:48:18Z","published":"2025-02-18T16:48:18Z","title":"Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions","summary":"  The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.\n","authors":["Aditya K Surikuchi","Raquel Fernndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2502.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13030v1","updated":"2025-02-18T16:46:44Z","published":"2025-02-18T16:46:44Z","title":"Likelihood-Ratio Regularized Quantile Regression: Adapting Conformal\n  Prediction to High-Dimensional Covariate Shifts","summary":"  We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, and an image classification task\nfrom the WILDS repository.\n","authors":["Sunay Joshi","Shayan Kiyani","George Pappas","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2502.13030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13025v1","updated":"2025-02-18T16:44:42Z","published":"2025-02-18T16:44:42Z","title":"Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks","summary":"  We present an agentic, autonomous graph expansion framework that iteratively\nstructures and refines knowledge in situ. Unlike conventional knowledge graph\nconstruction methods relying on static extraction or single-pass learning, our\napproach couples a reasoning-native large language model with a continually\nupdated graph representation. At each step, the system actively generates new\nconcepts and relationships, merges them into a global graph, and formulates\nsubsequent prompts based on its evolving structure. Through this\nfeedback-driven loop, the model organizes information into a scale-free network\ncharacterized by hub formation, stable modularity, and bridging nodes that link\ndisparate knowledge clusters. Over hundreds of iterations, new nodes and edges\ncontinue to appear without saturating, while centrality measures and shortest\npath distributions evolve to yield increasingly distributed connectivity. Our\nanalysis reveals emergent patterns, such as the rise of highly connected 'hub'\nconcepts and the shifting influence of 'bridge' nodes, indicating that agentic,\nself-reinforcing graph construction can yield open-ended, coherent knowledge\nstructures. Applied to materials design problems, we present compositional\nreasoning experiments by extracting node-specific and synergy-level principles\nto foster genuinely novel knowledge synthesis, yielding cross-domain ideas that\ntranscend rote summarization and strengthen the framework's potential for\nopen-ended scientific discovery. We discuss other applications in scientific\ndiscovery and outline future directions for enhancing scalability and\ninterpretability.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2502.13025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13024v1","updated":"2025-02-18T16:44:03Z","published":"2025-02-18T16:44:03Z","title":"Fragility-aware Classification for Understanding Risk and Improving\n  Generalization","summary":"  Classification models play a critical role in data-driven decision-making\napplications such as medical diagnosis, user profiling, recommendation systems,\nand default detection. Traditional performance metrics, such as accuracy, focus\non overall error rates but fail to account for the confidence of incorrect\npredictions, thereby overlooking the risk of confident misjudgments. This risk\nis particularly significant in cost-sensitive and safety-critical domains like\nmedical diagnosis and autonomous driving, where overconfident false predictions\nmay cause severe consequences. To address this issue, we introduce the\nFragility Index (FI), a novel metric that evaluates classification performance\nfrom a risk-averse perspective by explicitly capturing the tail risk of\nconfident misjudgments. To enhance generalizability, we define FI within the\nrobust satisficing (RS) framework, incorporating data uncertainty. We further\ndevelop a model training approach that optimizes FI while maintaining\ntractability for common loss functions. Specifically, we derive exact\nreformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and\nextend the approach to deep learning models. Through synthetic experiments and\nreal-world medical diagnosis tasks, we demonstrate that FI effectively\nidentifies misjudgment risk and FI-based training improves model robustness and\ngeneralizability. Finally, we extend our framework to deep neural network\ntraining, further validating its effectiveness in enhancing deep learning\nmodels.\n","authors":["Chen Yang","Zheng Cui","Daniel Zhuoyu Long","Jin Qi","Ruohan Zhan"],"pdf_url":"https://arxiv.org/pdf/2502.13024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13023v1","updated":"2025-02-18T16:43:11Z","published":"2025-02-18T16:43:11Z","title":"Detection and Geographic Localization of Natural Objects in the Wild: A\n  Case Study on Palms","summary":"  Palms are ecologically and economically indicators of tropical forest health,\nbiodiversity, and human impact that support local economies and global forest\nproduct supply chains. While palm detection in plantations is well-studied,\nefforts to map naturally occurring palms in dense forests remain limited by\noverlapping crowns, uneven shading, and heterogeneous landscapes. We develop\nPRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline\nfor detecting and localizing palms in dense tropical forests using large\northomosaic images. Orthomosaics are created from thousands of aerial images\nand spanning several to hundreds of gigabytes. Our contributions are threefold.\nFirst, we construct a large UAV-derived orthomosaic dataset collected across 21\necologically diverse sites in western Ecuador, annotated with 8,830 bounding\nboxes and 5,026 palm center points. Second, we evaluate multiple\nstate-of-the-art object detectors based on efficiency and performance,\nintegrating zero-shot SAM 2 as the segmentation backbone, and refining the\nresults for precise geographic mapping. Third, we apply calibration methods to\nalign confidence scores with IoU and explore saliency maps for feature\nexplainability. Though optimized for palms, PRISM is adaptable for identifying\nother natural objects, such as eastern white pines. Future work will explore\ntransfer learning for lower-resolution datasets (0.5 to 1m).\n","authors":["Kangning Cui","Rongkun Zhu","Manqi Wang","Wei Tang","Gregory D. Larsen","Victor P. Pauca","Sarra Alqahtani","Fan Yang","David Segurado","David Lutz","Jean-Michel Morel","Miles R. Silman"],"pdf_url":"https://arxiv.org/pdf/2502.13023v1.pdf","comment":"15 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.13022v1","updated":"2025-02-18T16:42:24Z","published":"2025-02-18T16:42:24Z","title":"Efficient and Sharp Off-Policy Learning under Unobserved Confounding","summary":"  We develop a novel method for personalized off-policy learning in scenarios\nwith unobserved confounding. Thereby, we address a key limitation of standard\npolicy learning: standard policy learning assumes unconfoundedness, meaning\nthat no unobserved factors influence both treatment assignment and outcomes.\nHowever, this assumption is often violated, because of which standard policy\nlearning produces biased estimates and thus leads to policies that can be\nharmful. To address this limitation, we employ causal sensitivity analysis and\nderive a statistically efficient estimator for a sharp bound on the value\nfunction under unobserved confounding. Our estimator has three advantages: (1)\nUnlike existing works, our estimator avoids unstable minimax optimization based\non inverse propensity weighted outcomes. (2) Our estimator is statistically\nefficient. (3) We prove that our estimator leads to the optimal\nconfounding-robust policy. Finally, we extend our theory to the related task of\npolicy improvement under unobserved confounding, i.e., when a baseline policy\nsuch as the standard of care is available. We show in experiments with\nsynthetic and real-world data that our method outperforms simple plug-in\napproaches and existing baselines. Our method is highly relevant for\ndecision-making where unobserved confounding can be problematic, such as in\nhealthcare and public policy.\n","authors":["Konstantin Hess","Dennis Frauen","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2502.13022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08330v3","updated":"2025-02-18T16:39:54Z","published":"2025-01-14T18:59:09Z","title":"Gradient Equilibrium in Online Learning: Theory and Applications","summary":"  We present a new perspective on online learning that we refer to as gradient\nequilibrium: a sequence of iterates achieves gradient equilibrium if the\naverage of gradients of losses along the sequence converges to zero. In\ngeneral, this condition is not implied by, nor implies, sublinear regret. It\nturns out that gradient equilibrium is achievable by standard online learning\nmethods such as gradient descent and mirror descent with constant step sizes\n(rather than decaying step sizes, as is usually required for no regret).\nFurther, as we show through examples, gradient equilibrium translates into an\ninterpretable and meaningful property in online prediction problems spanning\nregression, classification, quantile estimation, and others. Notably, we show\nthat the gradient equilibrium framework can be used to develop a debiasing\nscheme for black-box predictions under arbitrary distribution shift, based on\nsimple post hoc online descent updates. We also show that post hoc gradient\nupdates can be used to calibrate predicted quantiles under distribution shift,\nand that the framework leads to unbiased Elo scores for pairwise preference\nprediction.\n","authors":["Anastasios N. Angelopoulos","Michael I. Jordan","Ryan J. Tibshirani"],"pdf_url":"https://arxiv.org/pdf/2501.08330v3.pdf","comment":"Code available at\n  https://github.com/aangelopoulos/gradient-equilibrium/"},{"id":"http://arxiv.org/abs/2411.14904v2","updated":"2025-02-18T16:36:23Z","published":"2024-11-22T13:01:36Z","title":"Exploring Kolmogorov-Arnold Networks for Interpretable Time Series\n  Classification","summary":"  Time series classification is a relevant step supporting decision-making\nprocesses in various domains, and deep neural models have shown promising\nperformance.\n  Despite significant advancements in deep learning, the theoretical\nunderstanding of how and why complex architectures function remains limited,\nprompting the need for more interpretable models. Recently, the\nKolmogorov-Arnold Networks (KANs) have been proposed as a more interpretable\nalternative. While KAN-related research is significantly rising, to date, the\nstudy of KAN architectures for time series classification has been limited.\n  In this paper, we aim to conduct a comprehensive and robust exploration of\nthe KAN architecture for time series classification on the UCR benchmark. More\nspecifically, we look at a) how reference architectures for forecasting\ntransfer to classification, at the b) hyperparameter and implementation\ninfluence on the classification performance in view of finding the one that\nperforms best on the selected benchmark, the c) complexity trade-offs and d)\ninterpretability advantages. Our results show that (1) Efficient KAN\noutperforms MLP in performance and computational efficiency, showcasing its\nsuitability for tasks classification tasks. (2) Efficient KAN is more stable\nthan KAN across grid sizes, depths, and layer configurations, particularly with\nlower learning rates. (3) KAN maintains competitive accuracy compared to\nstate-of-the-art models like HIVE-COTE2, with smaller architectures and faster\ntraining times, supporting its balance of performance and transparency. (4) The\ninterpretability of the KAN model aligns with findings from SHAP analysis,\nreinforcing its capacity for transparent decision-making.\n","authors":["Irina Barain","Bla Bertalani","Mihael Mohori","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2411.14904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13931v3","updated":"2025-02-18T16:27:26Z","published":"2024-09-20T22:34:37Z","title":"On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists","summary":"  On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs.\n","authors":["Dongyang Fan","Bettina Messmer","Nikita Doikov","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2409.13931v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09962v2","updated":"2025-02-18T16:24:59Z","published":"2024-04-15T17:39:44Z","title":"Invariant Subspace Decomposition","summary":"  We consider the task of predicting a response Y from a set of covariates X in\nsettings where the conditional distribution of Y given X changes over time. For\nthis to be feasible, assumptions on how the conditional distribution changes\nover time are required. Existing approaches assume, for example, that changes\noccur smoothly over time so that short-term prediction using only the recent\npast becomes feasible. To additionally exploit observations further in the\npast, we propose a novel invariance-based framework for linear conditionals,\ncalled Invariant Subspace Decomposition (ISD), that splits the conditional\ndistribution into a time-invariant and a residual time-dependent component. As\nwe show, this decomposition can be utilized both for zero-shot and\ntime-adaptation prediction tasks, that is, settings where either no or a small\namount of training data is available at the time points we want to predict Y\nat, respectively. We propose a practical estimation procedure, which\nautomatically infers the decomposition using tools from approximate joint\nmatrix diagonalization. Furthermore, we provide finite sample guarantees for\nthe proposed estimator and demonstrate empirically that it indeed improves on\napproaches that do not use the additional invariant structure.\n","authors":["Margherita Lazzaretto","Jonas Peters","Niklas Pfister"],"pdf_url":"https://arxiv.org/pdf/2404.09962v2.pdf","comment":"60 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.13000v1","updated":"2025-02-18T16:20:50Z","published":"2025-02-18T16:20:50Z","title":"Edge-Colored Clustering in Hypergraphs: Beyond Minimizing Unsatisfied\n  Edges","summary":"  We consider a framework for clustering edge-colored hypergraphs, where the\ngoal is to cluster (equivalently, to color) objects based on the primary type\nof multiway interactions they participate in. One well-studied objective is to\ncolor nodes to minimize the number of unsatisfied hyperedges -- those\ncontaining one or more nodes whose color does not match the hyperedge color. We\nmotivate and present advances for several directions that extend beyond this\nminimization problem. We first provide new algorithms for maximizing satisfied\nedges, which is the same at optimality but is much more challenging to\napproximate, with all prior work restricted to graphs. We develop the first\napproximation algorithm for hypergraphs, and then refine it to improve the\nbest-known approximation factor for graphs. We then introduce new objective\nfunctions that incorporate notions of balance and fairness, and provide new\nhardness results, approximations, and fixed-parameter tractability results.\n","authors":["Alex Crane","Thomas Stanley","Blair D. Sullivan","Nate Veldt"],"pdf_url":"https://arxiv.org/pdf/2502.13000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12999v1","updated":"2025-02-18T16:19:28Z","published":"2025-02-18T16:19:28Z","title":"Asymptotic Optimism of Random-Design Linear and Kernel Regression Models","summary":"  We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.\n","authors":["Hengrui Luo","Yunzhang Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.12999v1.pdf","comment":"56 pages;"},{"id":"http://arxiv.org/abs/2502.12998v1","updated":"2025-02-18T16:19:08Z","published":"2025-02-18T16:19:08Z","title":"Personalized Top-k Set Queries Over Predicted Scores","summary":"  This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications.\n","authors":["Sohrab Namazi Nia","Subhodeep Ghosh","Senjuti Basu Roy","Sihem Amer-Yahia"],"pdf_url":"https://arxiv.org/pdf/2502.12998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01068v2","updated":"2025-02-18T16:17:43Z","published":"2024-10-01T20:52:08Z","title":"Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness","summary":"  We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD\nalgorithms over a bounded domain. Standard privacy analysis for Noisy-SGD\nassumes all internal states are revealed, which leads to a divergent R'enyi DP\nbound with respect to the number of iterations. Ye & Shokri (2022) and\nAltschuler & Talwar (2022) proved convergent bounds for smooth (strongly)\nconvex losses, and raise open questions about whether these assumptions can be\nrelaxed. We provide positive answers by proving convergent R'enyi DP bound for\nnon-convex non-smooth losses, where we show that requiring losses to have\nH\\\"older continuous gradient is sufficient. We also provide a strictly better\nprivacy bound compared to state-of-the-art results for smooth strongly convex\nlosses. Our analysis relies on the improvement of shifted divergence analysis\nin multiple aspects, including forward Wasserstein distance tracking,\nidentifying the optimal shifts allocation, and the H\"older reduction lemma. Our\nresults further elucidate the benefit of hidden-state analysis for DP and its\napplicability.\n","authors":["Eli Chien","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2410.01068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12993v1","updated":"2025-02-18T16:13:46Z","published":"2025-02-18T16:13:46Z","title":"Approximate Tree Completion and Learning-Augmented Algorithms for Metric\n  Minimum Spanning Trees","summary":"  Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric\nspace is a fundamental primitive for hierarchical clustering and many other ML\ntasks, but this takes $\\Omega(n^2)$ time to even approximate. We introduce a\nframework for metric MSTs that first (1) finds a forest of disconnected\ncomponents using practical heuristics, and then (2) finds a small weight set of\nedges to connect disjoint components of the forest into a spanning tree. We\nprove that optimally solving the second step still takes $\\Omega(n^2)$ time,\nbut we provide a subquadratic 2.62-approximation algorithm. In the spirit of\nlearning-augmented algorithms, we then show that if the forest found in step\n(1) overlaps with an optimal MST, we can approximate the original MST problem\nin subquadratic time, where the approximation factor depends on a measure of\noverlap. In practice, we find nearly optimal spanning trees for a wide range of\nmetrics, while being orders of magnitude faster than exact algorithms.\n","authors":["Nate Veldt","Thomas Stanley","Benjamin W. Priest","Trevor Steil","Keita Iwabuchi","T. S. Jayram","Geoffrey Sanders"],"pdf_url":"https://arxiv.org/pdf/2502.12993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12987v1","updated":"2025-02-18T16:11:05Z","published":"2025-02-18T16:11:05Z","title":"Ensemble Kalman filter in latent space using a variational autoencoder\n  pair","summary":"  Popular (ensemble) Kalman filter data assimilation (DA) approaches assume\nthat the errors in both the a priori estimate of the state and those in the\nobservations are Gaussian. For constrained variables, e.g. sea ice\nconcentration or stress, such an assumption does not hold. The variational\nautoencoder (VAE) is a machine learning (ML) technique that allows to map an\narbitrary distribution to/from a latent space in which the distribution is\nsupposedly closer to a Gaussian. We propose a novel hybrid DA-ML approach in\nwhich VAEs are incorporated in the DA procedure. Specifically, we introduce a\nvariant of the popular ensemble transform Kalman filter (ETKF) in which the\nanalysis is applied in the latent space of a single VAE or a pair of VAEs. In\ntwin experiments with a simple circular model, whereby the circle represents an\nunderlying submanifold to be respected, we find that the use of a VAE ensures\nthat a posteri ensemble members lie close to the manifold containing the truth.\nFurthermore, online updating of the VAE is necessary and achievable when this\nmanifold varies in time, i.e. when it is non-stationary. We demonstrate that\nintroducing an additional second latent space for the observational innovations\nimproves robustness against detrimental effects of non-Gaussianity and bias in\nthe observational errors but it slightly lessens the performance if\nobservational errors are strictly Gaussian.\n","authors":["Ivo Pasmans","Yumeng Chen","Tobias Sebastian Finn","Marc Bocquet","Alberto Carrassi"],"pdf_url":"https://arxiv.org/pdf/2502.12987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09992v2","updated":"2025-02-18T16:08:59Z","published":"2025-02-14T08:23:51Z","title":"Large Language Diffusion Models","summary":"  Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.\n","authors":["Shen Nie","Fengqi Zhu","Zebin You","Xiaolu Zhang","Jingyang Ou","Jun Hu","Jun Zhou","Yankai Lin","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12982v1","updated":"2025-02-18T16:04:57Z","published":"2025-02-18T16:04:57Z","title":"Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs","summary":"  Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.\n","authors":["Longxu Dou","Qian Liu","Fan Zhou","Changyu Chen","Zili Wang","Ziqi Jin","Zichen Liu","Tongyao Zhu","Cunxiao Du","Penghui Yang","Haonan Wang","Jiaheng Liu","Yongchi Zhao","Xiachong Feng","Xin Mao","Man Tsung Yeung","Kunat Pipatanakul","Fajri Koto","Min Si Thu","Hynek Kydlek","Zeyi Liu","Qunshu Lin","Sittipong Sripaisarnmongkol","Kridtaphad Sae-Khow","Nirattisai Thongchim","Taechawat Konkaew","Narong Borijindargoon","Anh Dao","Matichon Maneegard","Phakphum Artkaew","Zheng-Xin Yong","Quan Nguyen","Wannaphong Phatthiyaphaibun","Hoang H. Tran","Mike Zhang","Shiqi Chen","Tianyu Pang","Chao Du","Xinyi Wan","Wei Lu","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2502.12982v1.pdf","comment":"49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/"},{"id":"http://arxiv.org/abs/2502.12981v1","updated":"2025-02-18T16:02:10Z","published":"2025-02-18T16:02:10Z","title":"Towards Variational Flow Matching on General Geometries","summary":"  We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an\nextension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian\ndistributions for generative modeling on structured manifolds. We derive a\nvariational objective for probability flows on manifolds with closed-form\ngeodesics, making RG-VFM comparable - though fundamentally different to\nRiemannian Flow Matching (RFM) in this geometric setting. Experiments on a\ncheckerboard dataset wrapped on the sphere demonstrate that RG-VFM captures\ngeometric structure more effectively than Euclidean VFM and baseline methods,\nestablishing it as a robust framework for manifold-aware generative modeling.\n","authors":["Olga Zaghen","Floor Eijkelboom","Alison Pouplin","Erik J. Bekkers"],"pdf_url":"https://arxiv.org/pdf/2502.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12979v1","updated":"2025-02-18T16:01:17Z","published":"2025-02-18T16:01:17Z","title":"Electron flow matching for generative reaction mechanism prediction\n  obeying conservation laws","summary":"  Central to our understanding of chemical reactivity is the principle of mass\nconservation, which is fundamental for ensuring physical consistency, balancing\nequations, and guiding reaction design. However, data-driven computational\nmodels for tasks such as reaction product prediction rarely abide by this most\nbasic constraint. In this work, we recast the problem of reaction prediction as\na problem of electron redistribution using the modern deep generative framework\nof flow matching. Our model, FlowER, overcomes limitations inherent in previous\napproaches by enforcing exact mass conservation, thereby resolving\nhallucinatory failure modes, recovering mechanistic reaction sequences for\nunseen substrate scaffolds, and generalizing effectively to out-of-domain\nreaction classes with extremely data-efficient fine-tuning. FlowER additionally\nenables estimation of thermodynamic or kinetic feasibility and manifests a\ndegree of chemical intuition in reaction prediction tasks. This inherently\ninterpretable framework represents a significant step in bridging the gap\nbetween predictive accuracy and mechanistic understanding in data-driven\nreaction outcome prediction.\n","authors":["Joonyoung F. Joung","Mun Hong Fong","Nicholas Casetti","Jordan P. Liles","Ne S. Dassanayake","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2502.12979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12949v1","updated":"2025-02-18T16:00:10Z","published":"2025-02-18T16:00:10Z","title":"Efficient Learning Under Density Shift in Incremental Settings Using\n  Cramr-Rao-Based Regularization","summary":"  The continuous surge in data volume and velocity is often dealt with using\ndata orchestration and distributed processing approaches, abstracting away the\nmachine learning challenges that exist at the algorithmic level. With growing\ninterest in automating the learning loop, training with data that arrive in a\nsequence rather than in the classical in-memory training data form will face a\nmachine learning challenge because of evolving feature distributions across\nbatches of training data biasing the cross-validation step\n(\\cite{sugiyama2012machine}). This work takes a distributed density estimation\nangle to the problem where data are temporally distributed. It processes data\nin batches and allows a neural network to treat a batch as training data. The\nmethod accumulates knowledge about the data density via posterior probability\nabsorption using the Fisher Information Matrix, which contains information\nabout the local optimization gradients for the batch. This is then used as a\nregularizer for the loss in the following batch, and therefore the density\nestimate for the entire dataset constructively gets more robust to the non-iid\ndistribution shift. This needs the presence of a pair of batches in memory at a\ntime, so the space cost is not a function of the size of the complete,\ndistributed dataset. We proposed a novel regularization-based approach\nCovariate Shift Correction $C^{2}A$ that leverages Fisher information and\nKullback-Leibler divergence to adapt to both natural and sequential covariate\nshift caused by dataset fragmentation. $C^{2}A$ achieves $19\\%$ accuracy at\nmaximum against state-of-the-art methods.\n","authors":["Behraj Khan","Behroz Mirza","Nouman Durrani","Tahir Syed"],"pdf_url":"https://arxiv.org/pdf/2502.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12978v1","updated":"2025-02-18T15:58:58Z","published":"2025-02-18T15:58:58Z","title":"Statistically Significant $k$NNAD by Selective Inference","summary":"  In this paper, we investigate the problem of unsupervised anomaly detection\nusing the k-Nearest Neighbor method. The k-Nearest Neighbor Anomaly Detection\n(kNNAD) is a simple yet effective approach for identifying anomalies across\nvarious domains and fields. A critical challenge in anomaly detection,\nincluding kNNAD, is appropriately quantifying the reliability of detected\nanomalies. To address this, we formulate kNNAD as a statistical hypothesis test\nand quantify the probability of false detection using $p$-values. The main\ntechnical challenge lies in performing both anomaly detection and statistical\ntesting on the same data, which hinders correct $p$-value calculation within\nthe conventional statistical testing framework. To resolve this issue, we\nintroduce a statistical hypothesis testing framework called Selective Inference\n(SI) and propose a method named Statistically Significant NNAD (Stat-kNNAD). By\nleveraging SI, the Stat-kNNAD method ensures that detected anomalies are\nstatistically significant with theoretical guarantees. The proposed Stat-kNNAD\nmethod is applicable to anomaly detection in both the original feature space\nand latent feature spaces derived from deep learning models. Through numerical\nexperiments on synthetic data and applications to industrial product anomaly\ndetection, we demonstrate the validity and effectiveness of the Stat-kNNAD\nmethod.\n","authors":["Mizuki Niihori","Teruyuki Katsuoka","Tomohiro Shiraishi","Shuichi Nishino","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2502.12978v1.pdf","comment":"40 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.12976v1","updated":"2025-02-18T15:56:52Z","published":"2025-02-18T15:56:52Z","title":"Does Training with Synthetic Data Truly Protect Privacy?","summary":"  As synthetic data becomes increasingly popular in machine learning tasks,\nnumerous methods--without formal differential privacy guarantees--use synthetic\ndata for training. These methods often claim, either explicitly or implicitly,\nto protect the privacy of the original training data. In this work, we explore\nfour different training paradigms: coreset selection, dataset distillation,\ndata-free knowledge distillation, and synthetic data generated from diffusion\nmodels. While all these methods utilize synthetic data for training, they lead\nto vastly different conclusions regarding privacy preservation. We caution that\nempirical approaches to preserving data privacy require careful and rigorous\nevaluation; otherwise, they risk providing a false sense of privacy.\n","authors":["Yunpeng Zhao","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12976v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2409.17513v2","updated":"2025-02-18T15:49:04Z","published":"2024-09-26T03:48:47Z","title":"Comparing Unidirectional, Bidirectional, and Word2vec Models for\n  Discovering Vulnerabilities in Compiled Lifted Code","summary":"  Ransomware and other forms of malware cause significant financial and\noperational damage to organizations by exploiting long-standing and often\ndifficult-to-detect software vulnerabilities. To detect vulnerabilities such as\nbuffer overflows in compiled code, this research investigates the application\nof unidirectional transformer-based embeddings, specifically GPT-2. Using a\ndataset of LLVM functions, we trained a GPT-2 model to generate embeddings,\nwhich were subsequently used to build LSTM neural networks to differentiate\nbetween vulnerable and non-vulnerable code. Our study reveals that embeddings\nfrom the GPT-2 model significantly outperform those from bidirectional models\nof BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.\nLSTM neural networks were developed with both frozen and unfrozen embedding\nmodel layers. The model with the highest performance was achieved when the\nembedding layers were unfrozen. Further, the research finds that, in exploring\nthe impact of different optimizers within this domain, the SGD optimizer\ndemonstrates superior performance over Adam. Overall, these findings reveal\nimportant insights into the potential of unidirectional transformer-based\napproaches in enhancing cybersecurity defenses.\n","authors":["Gary A. McCully","John D. Hastings","Shengjie Xu","Adam Fortier"],"pdf_url":"https://arxiv.org/pdf/2409.17513v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.12965v1","updated":"2025-02-18T15:46:54Z","published":"2025-02-18T15:46:54Z","title":"A Survey of Text Classification Under Class Distribution Shift","summary":"  The basic underlying assumption of machine learning (ML) models is that the\ntraining and test data are sampled from the same distribution. However, in\ndaily practice, this assumption is often broken, i.e.~the distribution of the\ntest data changes over time, which hinders the application of conventional ML\nmodels. One domain where the distribution shift naturally occurs is text\nclassification, since people always find new topics to discuss. To this end, we\nsurvey research articles studying open-set text classification and related\ntasks. We divide the methods in this area based on the constraints that define\nthe kind of distribution shift and the corresponding problem formulation,\ni.e.~learning with the Universum, zero-shot learning, and open-set learning. We\nnext discuss the predominant mitigation approaches for each problem setup.\nFinally, we identify several future work directions, aiming to push the\nboundaries beyond the state of the art. Interestingly, we find that continual\nlearning can solve many of the issues caused by the shifting class\ndistribution. We maintain a list of relevant papers at\nhttps://github.com/Eduard6421/Open-Set-Survey.\n","authors":["Adriana Valentina Costache","Silviu Florin Gheorghe","Eduard Gabriel Poesina","Paul Irofti","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01002v2","updated":"2025-02-18T15:46:29Z","published":"2023-02-02T10:40:06Z","title":"Over-parameterised Shallow Neural Networks with Asymmetrical Node\n  Scaling: Global Convergence Guarantees and Feature Learning","summary":"  We consider gradient-based optimisation of wide, shallow neural networks,\nwhere the output of each hidden node is scaled by a positive parameter. The\nscaling parameters are non-identical, differing from the classical Neural\nTangent Kernel (NTK) parameterisation. We prove that for large such neural\nnetworks, with high probability, gradient flow and gradient descent converge to\na global minimum and can learn features in some sense, unlike in the NTK\nparameterisation. We perform experiments illustrating our theoretical results\nand discuss the benefits of such scaling in terms of prunability and transfer\nlearning.\n","authors":["Francois Caron","Fadhel Ayed","Paul Jung","Hoil Lee","Juho Lee","Hongseok Yang"],"pdf_url":"https://arxiv.org/pdf/2302.01002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12958v1","updated":"2025-02-18T15:43:14Z","published":"2025-02-18T15:43:14Z","title":"Preventing the Popular Item Embedding Based Attack in Federated\n  Recommendations","summary":"  Privacy concerns have led to the rise of federated recommender systems (FRS),\nwhich can create personalized models across distributed clients. However, FRS\nis vulnerable to poisoning attacks, where malicious users manipulate gradients\nto promote their target items intentionally. Existing attacks against FRS have\nlimitations, as they depend on specific models and prior knowledge, restricting\ntheir real-world applicability. In our exploration of practical FRS\nvulnerabilities, we devise a model-agnostic and prior-knowledge-free attack,\nnamed PIECK (Popular Item Embedding based Attack). The core module of PIECK is\npopular item mining, which leverages embedding changes during FRS training to\neffectively identify the popular items. Built upon the core module, PIECK\nbranches into two diverse solutions: The PIECKIPE solution employs an item\npopularity enhancement module, which aligns the embeddings of targeted items\nwith the mined popular items to increase item exposure. The PIECKUEA further\nenhances the robustness of the attack by using a user embedding approximation\nmodule, which approximates private user embeddings using mined popular items.\nUpon identifying PIECK, we evaluate existing federated defense methods and find\nthem ineffective against PIECK, as poisonous gradients inevitably overwhelm the\ncold target items. We then propose a novel defense method by introducing two\nregularization terms during user training, which constrain item popularity\nenhancement and user embedding approximation while preserving FRS performance.\nWe evaluate PIECK and its defense across two base models, three real datasets,\nfour top-tier attacks, and six general defense methods, affirming the efficacy\nof both PIECK and its defense.\n","authors":["Jun Zhang","Huan Li","Dazhong Rong","Yan Zhao","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2502.12958v1.pdf","comment":"Accepted at ICDE 2024, Extension"},{"id":"http://arxiv.org/abs/2410.07708v2","updated":"2025-02-18T15:41:56Z","published":"2024-10-10T08:20:57Z","title":"Learning Tree Pattern Transformations","summary":"  Explaining why and how a tree $t$ structurally differs from another tree\n$t^\\star$ is a question that is encountered throughout computer science,\nincluding in understanding tree-structured data such as XML or JSON data. In\nthis article, we explore how to learn explanations for structural differences\nbetween pairs of trees from sample data: suppose we are given a set $\\{(t_1,\nt_1^\\star),\\dots, (t_n, t_n^\\star)\\}$ of pairs of labelled, ordered trees; is\nthere a small set of rules that explains the structural differences between all\npairs $(t_i, t_i^\\star)$? This raises two research questions: (i) what is a\ngood notion of \"rule\" in this context?; and (ii) how can sets of rules\nexplaining a data set be learned algorithmically?\n  We explore these questions from the perspective of database theory by (1)\nintroducing a pattern-based specification language for tree transformations;\n(2) exploring the computational complexity of variants of the above algorithmic\nproblem, e.g. showing NP-hardness for very restricted variants; and (3)\ndiscussing how to solve the problem for data from CS education research using\nSAT solvers.\n","authors":["Daniel Neider","Leif Sabellek","Johannes Schmidt","Fabian Vehlken","Thomas Zeume"],"pdf_url":"https://arxiv.org/pdf/2410.07708v2.pdf","comment":"Full version of the ICDT 2025 paper"},{"id":"http://arxiv.org/abs/2502.12953v1","updated":"2025-02-18T15:36:16Z","published":"2025-02-18T15:36:16Z","title":"Task-Informed Anti-Curriculum by Masking Improves Downstream Performance\n  on Text","summary":"  Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.\n","authors":["Andrei Jarca","Florinel Alin Croitoru","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2502.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12951v1","updated":"2025-02-18T15:33:09Z","published":"2025-02-18T15:33:09Z","title":"Guaranteed Conditional Diffusion: 3D Block-based Models for Scientific\n  Data Compression","summary":"  This paper proposes a new compression paradigm -- Guaranteed Conditional\nDiffusion with Tensor Correction (GCDTC) -- for lossy scientific data\ncompression. The framework is based on recent conditional diffusion (CD)\ngenerative models, and it consists of a conditional diffusion model, tensor\ncorrection, and error guarantee. Our diffusion model is a mixture of 3D\nconditioning and 2D denoising U-Net. The approach leverages a 3D block-based\ncompressing module to address spatiotemporal correlations in structured\nscientific data. Then, the reverse diffusion process for 2D spatial data is\nconditioned on the ``slices'' of content latent variables produced by the\ncompressing module. After training, the denoising decoder reconstructs the data\nwith zero noise and content latent variables, and thus it is entirely\ndeterministic. The reconstructed outputs of the CD model are further\npost-processed by our tensor correction and error guarantee steps to control\nand ensure a maximum error distortion, which is an inevitable requirement in\nlossy scientific data compression. Our experiments involving two datasets\ngenerated by climate and chemical combustion simulations show that our\nframework outperforms standard convolutional autoencoders and yields\ncompetitive compression quality with an existing scientific data compression\nalgorithm.\n","authors":["Jaemoon Lee","Xiao Li","Liangji Zhu","Sanjay Ranka","Anand Rangarajan"],"pdf_url":"https://arxiv.org/pdf/2502.12951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12947v1","updated":"2025-02-18T15:30:34Z","published":"2025-02-18T15:30:34Z","title":"Every Expert Matters: Towards Effective Knowledge Distillation for\n  Mixture-of-Experts Language Models","summary":"  With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.\n","authors":["Gyeongman Kim","Gyouk Chu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12944v1","updated":"2025-02-18T15:28:02Z","published":"2025-02-18T15:28:02Z","title":"Performance of Zero-Shot Time Series Foundation Models on Cloud Data","summary":"  Time series foundation models (FMs) have emerged as a popular paradigm for\nzero-shot multi-domain forecasting. FMs are trained on numerous diverse\ndatasets and claim to be effective forecasters across multiple different time\nseries domains, including cloud data. In this work we investigate this claim,\nexploring the effectiveness of FMs on cloud data. We demonstrate that many\nwell-known FMs fail to generate meaningful or accurate zero-shot forecasts in\nthis setting. We support this claim empirically, showing that FMs are\noutperformed consistently by simple linear baselines. We also illustrate a\nnumber of interesting pathologies, including instances where FMs suddenly\noutput seemingly erratic, random-looking forecasts. Our results suggest a\nwidespread failure of FMs to model cloud data.\n","authors":["William Toner","Thomas L. Lee","Artjom Joosen","Rajkarn Singh","Martin Asenov"],"pdf_url":"https://arxiv.org/pdf/2502.12944v1.pdf","comment":"5 pages, Preprint"},{"id":"http://arxiv.org/abs/2404.12917v3","updated":"2025-02-18T15:17:38Z","published":"2024-04-19T14:42:42Z","title":"R3L: Relative Representations for Reinforcement Learning","summary":"  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.\n","authors":["Antonio Pio Ricciardi","Valentino Maiorca","Luca Moschella","Riccardo Marin","Emanuele Rodol"],"pdf_url":"https://arxiv.org/pdf/2404.12917v3.pdf","comment":"12 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.12937v1","updated":"2025-02-18T15:16:23Z","published":"2025-02-18T15:16:23Z","title":"Tuning Algorithmic and Architectural Hyperparameters in Graph-Based\n  Semi-Supervised Learning with Provable Guarantees","summary":"  Graph-based semi-supervised learning is a powerful paradigm in machine\nlearning for modeling and exploiting the underlying graph structure that\ncaptures the relationship between labeled and unlabeled data. A large number of\nclassical as well as modern deep learning based algorithms have been proposed\nfor this problem, often having tunable hyperparameters. We initiate a formal\nstudy of tuning algorithm hyperparameters from parameterized algorithm families\nfor this problem. We obtain novel $O(\\log n)$ pseudo-dimension upper bounds for\nhyperparameter selection in three classical label propagation-based algorithm\nfamilies, where $n$ is the number of nodes, implying bounds on the amount of\ndata needed for learning provably good parameters. We further provide matching\n$\\Omega(\\log n)$ pseudo-dimension lower bounds, thus asymptotically\ncharacterizing the learning-theoretic complexity of the parameter tuning\nproblem. We extend our study to selecting architectural hyperparameters in\nmodern graph neural networks. We bound the Rademacher complexity for tuning the\nself-loop weighting in recently proposed Simplified Graph Convolution (SGC)\nnetworks. We further propose a tunable architecture that interpolates graph\nconvolutional neural networks (GCN) and graph attention networks (GAT) in every\nlayer, and provide Rademacher complexity bounds for tuning the interpolation\ncoefficient.\n","authors":["Ally Yalei Du","Eric Huang","Dravyansh Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.12937v1.pdf","comment":"31 pages (11 pages main body), 2 figures"},{"id":"http://arxiv.org/abs/2410.01706v3","updated":"2025-02-18T15:15:20Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to\nachieve computationally efficient processing of multi-agent observations with\nlong context memory for temporal reasoning. Through extensive evaluations\nacross six diverse environments, we demonstrate how Sable is able to\nsignificantly outperform existing state-of-the-art methods in a large number of\ndiverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance\nas we scale the number of agents, handling environments with more than a\nthousand agents while exhibiting a linear increase in memory usage. Finally, we\nconduct ablation studies to isolate the source of Sable's performance gains and\nconfirm its efficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12930v1","updated":"2025-02-18T15:12:02Z","published":"2025-02-18T15:12:02Z","title":"Universal Embedding Function for Traffic Classification via QUIC Domain\n  Recognition Pretraining: A Transfer Learning Success","summary":"  Encrypted traffic classification (TC) methods must adapt to new protocols and\nextensions as well as to advancements in other machine learning fields. In this\npaper, we follow a transfer learning setup best known from computer vision. We\nfirst pretrain an embedding model on a complex task with a large number of\nclasses and then transfer it to five well-known TC datasets. The pretraining\ntask is recognition of SNI domains in encrypted QUIC traffic, which in itself\nis a problem for network monitoring due to the growing adoption of TLS\nEncrypted Client Hello. Our training pipeline -- featuring a disjoint class\nsetup, ArcFace loss function, and a modern deep learning architecture -- aims\nto produce universal embeddings applicable across tasks. The proposed solution,\nbased on nearest neighbors search in the embedding space, surpasses SOTA\nperformance on four of the five TC datasets. A comparison with a baseline\nmethod utilizing raw packet sequences revealed unexpected findings with\npotential implications for the broader TC field. We published the model\narchitecture, trained weights, and transfer learning experiments.\n","authors":["Jan Luxemburk","Karel Hynek","Richard Pln","Tom ejka"],"pdf_url":"https://arxiv.org/pdf/2502.12930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12929v1","updated":"2025-02-18T15:11:46Z","published":"2025-02-18T15:11:46Z","title":"Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options","summary":"  We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.\n","authors":["Lakshmi Nair","Ian Trase","Mark Kim"],"pdf_url":"https://arxiv.org/pdf/2502.12929v1.pdf","comment":"Github code: https://github.com/flagshippioneering/Flow-of-Options"},{"id":"http://arxiv.org/abs/2307.11127v4","updated":"2025-02-18T15:10:12Z","published":"2023-07-20T15:52:22Z","title":"Asymptotically Unbiased Synthetic Control Methods by Density Matching","summary":"  Synthetic Control Methods (SCMs) have become a fundamental tool for\ncomparative case studies. The core idea behind SCMs is to estimate treatment\neffects by predicting counterfactual outcomes for a treated unit using a\nweighted combination of observed outcomes from untreated units. The accuracy of\nthese predictions is crucial for evaluating the treatment effect of a policy\nintervention. Subsequent research has therefore focused on estimating SC\nweights. In this study, we highlight a key endogeneity issue in existing\nSCMs-namely, the correlation between the outcomes of untreated units and the\nerror term of the synthetic control, which leads to bias in both counterfactual\noutcome prediction and treatment effect estimation. To address this issue, we\npropose a novel SCM based on density matching, assuming that the outcome\ndensity of the treated unit can be approximated by a weighted mixture of the\njoint density of untreated units. Under this assumption, we estimate SC weights\nby matching the moments of the treated outcomes with the weighted sum of the\nmoments of the untreated outcomes. Our method offers three advantages: first,\nunder the mixture model assumption, our estimator is asymptotically unbiased;\nsecond, this asymptotic unbiasedness reduces the mean squared error in\ncounterfactual predictions; and third, our method provides full densities of\nthe treatment effect rather than just expected values, thereby broadening the\napplicability of SCMs. Finally, we present experimental results that\ndemonstrate the effectiveness of our approach.\n","authors":["Masahiro Kato","Akari Ohda"],"pdf_url":"https://arxiv.org/pdf/2307.11127v4.pdf","comment":"This study was presented at the Workshop on Counterfactuals in Minds\n  and Machines at the International Conference on Machine Learning in July 2023\n  and at the International Conference on Econometrics and Statistics in August\n  2023"},{"id":"http://arxiv.org/abs/2502.12920v1","updated":"2025-02-18T15:01:02Z","published":"2025-02-18T15:01:02Z","title":"Lightweight Online Adaption for Time Series Foundation Model Forecasts","summary":"  Foundation models (FMs) have emerged as a promising approach for time series\nforecasting. While effective, FMs typically remain fixed during deployment due\nto the high computational costs of learning them online. Consequently, deployed\nFMs fail to adapt their forecasts to current data characteristics, despite the\navailability of online feedback from newly arriving data. This raises the\nquestion of whether FM performance can be enhanced by the efficient usage of\nthis feedback. We propose AdapTS to answer this question.\n  AdapTS is a lightweight mechanism for the online adaption of FM forecasts in\nresponse to online feedback. AdapTS consists of two parts: a) the\nAdapTS-Forecaster which is used to learn the current data distribution; and b)\nthe AdapTS-Weighter which is used to combine the forecasts of the FM and the\nAdapTS-Forecaster. We evaluate the performance of AdapTS in conjunction with\nseveral recent FMs across a suite of standard time series datasets. In all of\nour experiments we find that using AdapTS improves performance. This work\ndemonstrates how efficient usage of online feedback can be used to improve FM\nforecasts.\n","authors":["Thomas L. Lee","William Toner","Rajkarn Singh","Artjom Joosem","Martin Asenov"],"pdf_url":"https://arxiv.org/pdf/2502.12920v1.pdf","comment":"8 pages, Preprint"},{"id":"http://arxiv.org/abs/2502.12919v1","updated":"2025-02-18T14:59:54Z","published":"2025-02-18T14:59:54Z","title":"A Smooth Transition Between Induction and Deduction: Fast Abductive\n  Learning Based on Probabilistic Symbol Perception","summary":"  Abductive learning (ABL) that integrates strengths of machine learning and\nlogical reasoning to improve the learning generalization, has been recently\nshown effective. However, its efficiency is affected by the transition between\nnumerical induction and symbolical deduction, leading to high computational\ncosts in the worst-case scenario. Efforts on this issue remain to be limited.\nIn this paper, we identified three reasons why previous optimization algorithms\nfor ABL were not effective: insufficient utilization of prediction, symbol\nrelationships, and accumulated experience in successful abductive processes,\nresulting in redundant calculations to the knowledge base. To address these\nchallenges, we introduce an optimization algorithm named as Probabilistic\nSymbol Perception (PSP), which makes a smooth transition between induction and\ndeduction and keeps the correctness of ABL unchanged. We leverage probability\nas a bridge and present an efficient data structure, achieving the transfer\nfrom a continuous probability sequence to discrete Boolean sequences with low\ncomputational complexity. Experiments demonstrate the promising results.\n","authors":["Lin-Han Jia","Si-Yu Han","Lan-Zhe Guo","Zhi Zhou","Zhao-Long Li","Yu-Feng Li","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.12919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12913v1","updated":"2025-02-18T14:54:55Z","published":"2025-02-18T14:54:55Z","title":"GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning","summary":"  Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.\n","authors":["Sifan Zhou","Shuo Wang","Zhihang Yuan","Mingjia Shi","Yuzhang Shang","Dawei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12912v1","updated":"2025-02-18T14:53:56Z","published":"2025-02-18T14:53:56Z","title":"A Simplified and Numerically Stable Approach to the BG/NBD Churn\n  Prediction model","summary":"  This study extends the BG/NBD churn probability model, addressing its\nlimitations in industries where customer behaviour is often influenced by\nseasonal events and possibly high purchase counts. We propose a modified\ndefinition of churn, considering a customer to have churned if they make no\npurchases within M days. Our contribution is twofold: First, we simplify the\ngeneral equation for the specific case of zero purchases within M days. Second,\nwe derive an alternative expression using numerical techniques to mitigate\nnumerical overflow or underflow issues. This approach provides a more practical\nand robust method for predicting customer churn in industries with irregular\npurchase patterns.\n","authors":["Dylan Zammit","Christopher Zerafa"],"pdf_url":"https://arxiv.org/pdf/2502.12912v1.pdf","comment":"4 pages, numerically stable BG/NBD"},{"id":"http://arxiv.org/abs/2405.20324v2","updated":"2025-02-18T14:43:42Z","published":"2024-05-30T17:57:26Z","title":"Don't drop your samples! Coherence-aware training benefits Conditional\n  diffusion","summary":"  Conditional diffusion models are powerful generative models that can leverage\nvarious types of conditional information, such as class labels, segmentation\nmasks, or text captions. However, in many real-world scenarios, conditional\ninformation may be noisy or unreliable due to human annotation errors or weak\nalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a\nnovel method that integrates coherence in conditional information into\ndiffusion models, allowing them to learn from noisy annotations without\ndiscarding data. We assume that each data point has an associated coherence\nscore that reflects the quality of the conditional information. We then\ncondition the diffusion model on both the conditional information and the\ncoherence score. In this way, the model learns to ignore or discount the\nconditioning when the coherence is low. We show that CAD is theoretically sound\nand empirically effective on various conditional generation tasks. Moreover, we\nshow that leveraging coherence generates realistic and diverse samples that\nrespect conditional information better than models trained on cleaned datasets\nwhere samples with low coherence have been discarded.\n","authors":["Nicolas Dufour","Victor Besnier","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2405.20324v2.pdf","comment":"Accepted at CVPR 2024 as a Highlight. Project page:\n  https://nicolas-dufour.github.io/cad.html"},{"id":"http://arxiv.org/abs/2502.12902v1","updated":"2025-02-18T14:42:11Z","published":"2025-02-18T14:42:11Z","title":"Probabilistic neural operators for functional uncertainty quantification","summary":"  Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.\n","authors":["Christopher Blte","Philipp Scholl","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2502.12902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11734v2","updated":"2025-02-18T14:42:02Z","published":"2024-03-18T12:42:53Z","title":"Learning More Expressive General Policies for Classical Planning Domains","summary":"  GNN-based approaches for learning general policies across planning domains\nare limited by the expressive power of $C_2$, namely; first-order logic with\ntwo variables and counting. This limitation can be overcame by transitioning to\n$k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet\nembeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-\nand $2$-GNNs that are confined to $C_2$, they require quartic time for message\nexchange and cubic space to store embeddings, rendering them infeasible in\npractice. In this work, we introduce a parameterized version R-GNN[$t$] (with\nparameter $t$) of Relational GNNs. Unlike GNNs, that are designed to perform\ncomputation on graphs, Relational GNNs are designed to do computation on\nrelational structures. When $t=\\infty$, R-GNN[$t$] approximates $3$-GNNs over\ngraphs, but using only quadratic space for embeddings. For lower values of $t$,\nsuch as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by\nexchanging fewer messages, yet interestingly, often yield the expressivity\nrequired in several planning domains. Furthermore, the new R-GNN[$t$]\narchitecture is the original R-GNN architecture with a suitable transformation\napplied to the inputs only. Experimental results illustrate the clear\nperformance gains of R-GNN[$1$] over the plain R-GNNs, and also over Edge\nTransformers that also approximate $3$-GNNs.\n","authors":["Simon Sthlberg","Blai Bonet","Hector Geffner"],"pdf_url":"https://arxiv.org/pdf/2403.11734v2.pdf","comment":"Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2410.19546v2","updated":"2025-02-18T14:38:12Z","published":"2024-10-25T13:19:26Z","title":"Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?","summary":"  Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's o1,\nhave emerged, seemingly demonstrating advanced reasoning capabilities across\ntext and image modalities. However, the depth of these advances in\nlanguage-guided perception and abstract reasoning remains underexplored, and it\nis unclear whether these models can truly live up to their ambitious promises.\nTo assess the progress and identify shortcomings, we enter the wonderland of\nBongard problems, a set of classic visual reasoning puzzles that require\nhuman-like abilities of pattern recognition and abstract reasoning. With our\nextensive evaluation setup, we show that while VLMs occasionally succeed in\nidentifying discriminative concepts and solving some of the problems, they\nfrequently falter. Surprisingly, even elementary concepts that may seem trivial\nto humans, such as simple spirals, pose significant challenges. Moreover, when\nexplicitly asked to recognize ground truth concepts, they continue to falter,\nsuggesting not only a lack of understanding of these elementary visual concepts\nbut also an inability to generalize to unseen concepts. We compare the results\nof VLMs to human performance and observe that a significant gap remains between\nhuman visual reasoning capabilities and machine cognition.\n","authors":["Antonia Wst","Tim Tobiasch","Lukas Helff","Inga Ibs","Wolfgang Stammer","Devendra S. Dhami","Constantin A. Rothkopf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12898v1","updated":"2025-02-18T14:34:22Z","published":"2025-02-18T14:34:22Z","title":"The Relationship Between Head Injury and Alzheimer's Disease: A Causal\n  Analysis with Bayesian Networks","summary":"  This study examines the potential causal relationship between head injury and\nthe risk of developing Alzheimer's disease (AD) using Bayesian networks and\nregression models. Using a dataset of 2,149 patients, we analyze key medical\nhistory variables, including head injury history, memory complaints,\ncardiovascular disease, and diabetes. Logistic regression results suggest an\nodds ratio of 0.88 for head injury, indicating a potential but statistically\ninsignificant protective effect against AD. In contrast, memory complaints\nexhibit a strong association with AD, with an odds ratio of 4.59. Linear\nregression analysis further confirms the lack of statistical significance for\nhead injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive\nimportance of memory complaints. These findings highlight the complex interplay\nof medical history factors in AD risk assessment and underscore the need for\nfurther research utilizing larger datasets and advanced causal modeling\ntechniques.\n","authors":["Andrei Lixandru"],"pdf_url":"https://arxiv.org/pdf/2502.12898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10635v2","updated":"2025-02-18T14:16:06Z","published":"2025-02-15T02:25:27Z","title":"Privacy Preservation through Practical Machine Unlearning","summary":"  Machine Learning models thrive on vast datasets, continuously adapting to\nprovide accurate predictions and recommendations. However, in an era dominated\nby privacy concerns, Machine Unlearning emerges as a transformative approach,\nenabling the selective removal of data from trained models. This paper examines\nmethods such as Naive Retraining and Exact Unlearning via the SISA framework,\nevaluating their Computational Costs, Consistency, and feasibility using the\n$\\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning\nprinciples into Positive Unlabeled (PU) Learning to address challenges posed by\npartially labeled datasets. Our findings highlight the promise of unlearning\nframeworks like $\\textit{DaRE}$ for ensuring privacy compliance while\nmaintaining model performance, albeit with significant computational\ntrade-offs. This study underscores the importance of Machine Unlearning in\nachieving ethical AI and fostering trust in data-driven systems.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2502.10635v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.12877v1","updated":"2025-02-18T14:06:20Z","published":"2025-02-18T14:06:20Z","title":"Pushing the Limits of the Reactive Affine Shaker Algorithm to Higher\n  Dimensions","summary":"  Bayesian Optimization (BO) for the minimization of expensive functions of\ncontinuous variables uses all the knowledge acquired from previous samples\n(${\\boldsymbol x}_i$ and $f({\\boldsymbol x}_i)$ values) to build a surrogate\nmodel based on Gaussian processes. The surrogate is then exploited to define\nthe next point to sample, through a careful balance of exploration and\nexploitation. Initially intended for low-dimensional spaces, BO has recently\nbeen modified and used also for very large-dimensional spaces (up to about one\nthousand dimensions).\n  In this paper we consider a much simpler algorithm, called \"Reactive Affine\nShaker\" (RAS). The next sample is always generated with a uniform probability\ndistribution inside a parallelepiped (the \"box\"). At each iteration, the form\nof the box is adapted during the search through an affine transformation, based\nonly on the point $\\boldsymbol x$ position and on the success or failure in\nimproving the function. The function values are therefore not used directly to\nmodify the search area and to generate the next sample. The entire\ndimensionality is kept (no active subspaces).\n  Despite its extreme simplicity and its use of only stochastic local search,\nsurprisingly the produced results are comparable to and not too far from the\nstate-of-the-art results of high-dimensional versions of BO, although with some\nmore function evaluations.\n  An ablation study and an analysis of probability distribution of directions\n(improving steps and prevailing box orientation) in very large-dimensional\nspaces are conducted to understand more about the behavior of RAS and to assess\nthe relative importance of the algorithmic building blocks for the final\nresults.\n","authors":["Roberto Battiti","Mauro Brunato"],"pdf_url":"https://arxiv.org/pdf/2502.12877v1.pdf","comment":"Submitted to: the 19th Learning and Intelligent Optimization\n  Conference (LION19), June 15-19 2025, Prague, Czech Republic\n  (https://lion19.org/)"},{"id":"http://arxiv.org/abs/2502.12874v1","updated":"2025-02-18T14:05:04Z","published":"2025-02-18T14:05:04Z","title":"Testing for Causal Fairness","summary":"  Causality is widely used in fairness analysis to prevent discrimination on\nsensitive attributes, such as genders in career recruitment and races in crime\nprediction. However, the current data-based Potential Outcomes Framework (POF)\noften leads to untrustworthy fairness analysis results when handling\nhigh-dimensional data. To address this, we introduce a distribution-based POF\nthat transform fairness analysis into Distributional Closeness Testing (DCT) by\nintervening on sensitive attributes. We define counterfactual closeness\nfairness as the null hypothesis of DCT, where a sensitive attribute is\nconsidered fair if its factual and counterfactual potential outcome\ndistributions are sufficiently close. We introduce the Norm-Adaptive Maximum\nMean Discrepancy Treatment Effect (N-TE) as a statistic for measuring\ndistributional closeness and apply DCT using the empirical estimator of NTE,\nreferred to Counterfactual Fairness-CLOseness Testing ($\\textrm{CF-CLOT}$). To\nensure the trustworthiness of testing results, we establish the testing\nconsistency of N-TE through rigorous theoretical analysis. $\\textrm{CF-CLOT}$\ndemonstrates sensitivity in fairness analysis through the flexibility of the\ncloseness parameter $\\epsilon$. Unfair sensitive attributes have been\nsuccessfully tested by $\\textrm{CF-CLOT}$ in extensive experiments across\nvarious real-world scenarios, which validate the consistency of the testing.\n","authors":["Jiarun Fu","LiZhong Ding","Pengqi Li","Qiuning Wei","Yurong Cheng","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17002v3","updated":"2025-02-18T14:02:43Z","published":"2024-06-24T14:37:17Z","title":"A Closer Look at Mortality Risk Prediction from Electrocardiograms","summary":"  Several recent studies combine large private ECG databases with AI to predict\npatient mortality. These studies typically use a few, highly variable, modeling\napproaches. While benchmarking these approaches has historically been limited\nby a lack of public ECG datasets, this changed with the 2023 release of\nMIMIC-IV, containing 795,546 ECGs from a U.S. hospital system, and the 2020\nrelease of Code-15, containing 345,779 ECGs collected during routine care in\nBrazil. We benchmark over 500 AI-ECG survival models predicting all-cause\nmortality on Code-15 and MIMIC-IV with 2 neural architectures, 4\nDeep-Survival-Analysis approaches, and classifiers predicting mortality at 4\ntime horizons. We extend the highest-performing approach to a dataset from\nBoston Children's Hospital (BCH, 225,379 ECGs). Models train with and without\ndemographics (age/sex) and evaluate across datasets. The best performing\nDeep-Survival-Analysis models trained with ECG and demographics yield good\nmedian Concordance Indices (Code-15: 0.82, MIMIC-IV: 0.78, BCH: 0.76) and AUPRC\nscores (median 1-yr/5-yr, Code-15: 0.07/0.15; MIMIC-IV: 0.45/0.55; BCH:\n0.04/0.13) considering the percentage of ECGs linked to mortality (1-yr/5-yr,\nCode-15: 1.2%/3.4%; MIMIC-IV: 14.8%/24.5%; BCH: 0.9%/4.8%). Contrasting with\nDeep-Survival-Analysis models, classifier-based AI-ECG models exhibit\nsignificant, site-dependent sensitivity to the choice of time horizon (median\nPearson's R, Code-15: 0.69, p<1E-5; MIMIC-IV: -0.80 p<1E-5). Demographic-only\nmodels perform surprisingly well on Code-15. Concordance drops 0.03-0.24 on\nexternal validation. We recommend Deep-Survival-Analysis over Classifier-Cox\napproaches and the inclusion of demographic covariates in ECG survival\nmodeling. Comparisons to demographic-only and baseline models is crucial.\nExternal evaluations support fine-tuning models on site-specific data.\n","authors":["Platon Lukyanenko","Joshua Mayourian","Mingxuan Liu","John K. Triedman","Sunil J. Ghelani","William G. La Cava"],"pdf_url":"https://arxiv.org/pdf/2406.17002v3.pdf","comment":"13 pages plus references and appendix, 2 figures"},{"id":"http://arxiv.org/abs/2410.03514v3","updated":"2025-02-18T13:56:08Z","published":"2024-10-04T15:29:11Z","title":"Stabilized Neural Prediction of Potential Outcomes in Continuous Time","summary":"  Patient trajectories from electronic health records are widely used to\nestimate conditional average potential outcomes (CAPOs) of treatments over\ntime, which then allows to personalize care. Yet, existing neural methods for\nthis purpose have a key limitation: while some adjust for time-varying\nconfounding, these methods assume that the time series are recorded in discrete\ntime. In other words, they are constrained to settings where measurements and\ntreatments are conducted at fixed time steps, even though this is unrealistic\nin medical practice. In this work, we aim to estimate CAPOs in continuous time.\nThe latter is of direct practical relevance because it allows for modeling\npatient trajectories where measurements and treatments take place at arbitrary,\nirregular timestamps. We thus propose a new method called stabilized continuous\ntime inverse propensity network (SCIP-Net). For this, we further derive\nstabilized inverse propensity weights for robust estimation of the CAPOs. To\nthe best of our knowledge, our SCIP-Net is the first neural method that\nperforms proper adjustments for time-varying confounding in continuous time.\n","authors":["Konstantin Hess","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2410.03514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07460v2","updated":"2025-02-18T13:55:04Z","published":"2025-02-11T11:11:05Z","title":"Logarithmic Regret for Online KL-Regularized Reinforcement Learning","summary":"  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.\n","authors":["Heyang Zhao","Chenlu Ye","Wei Xiong","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12863v1","updated":"2025-02-18T13:51:56Z","published":"2025-02-18T13:51:56Z","title":"Malware Detection based on API calls","summary":"  Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.\n","authors":["Christofer Fellicious","Manuel Bischof","Kevin Mayer","Dorian Eikenberg","Stefan Hausotte","Hans P. Reiser","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2502.12863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15173v4","updated":"2025-02-18T13:45:50Z","published":"2024-02-23T08:11:55Z","title":"Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed\n  Zeroth-Order Optimizer","summary":"  Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8.\n","authors":["Yanjun Zhao","Sizhe Dang","Haishan Ye","Guang Dai","Yi Qian","Ivor W. Tsang"],"pdf_url":"https://arxiv.org/pdf/2402.15173v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12855v1","updated":"2025-02-18T13:43:06Z","published":"2025-02-18T13:43:06Z","title":"Integrating Arithmetic Learning Improves Mathematical Reasoning in\n  Smaller Models","summary":"  While large models pre-trained on high-quality data exhibit excellent\nperformance across various reasoning tasks, including mathematical reasoning\n(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical\nreasoning remains a challenging problem. Common approaches to address this\nchallenge include knowledge distillation, where smaller student models learn\nfrom large pre-trained teacher models, and data augmentation, such as\nrephrasing questions. Despite these efforts, smaller models struggle with\narithmetic computations, leading to errors in mathematical reasoning. In this\nwork, we focus on leveraging a programmatically generated arithmetic dataset to\nenhance the reasoning capabilities of smaller models. We investigate two key\napproaches to incorporate this dataset -- (1) intermediate fine-tuning, where a\nmodel is fine-tuned on the arithmetic dataset before being trained on a\nreasoning dataset, and (2) integrating the arithmetic dataset into the\ninstruction-tuning mixture, allowing the model to learn arithmetic skills\nalongside general instruction-following abilities. Our experiments on multiple\nreasoning benchmarks demonstrate that incorporating an arithmetic dataset,\nwhether through targeted fine-tuning or within the instruction-tuning mixture,\nenhances the models' arithmetic capabilities, which in turn improves their\nmathematical reasoning performance.\n","authors":["Neeraj Gangwar","Suma P Bhat","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2502.12855v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12853v1","updated":"2025-02-18T13:40:22Z","published":"2025-02-18T13:40:22Z","title":"S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning","summary":"  Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.\n","authors":["Ruotian Ma","Peisong Wang","Cheng Liu","Xingyan Liu","Jiaqi Chen","Bang Zhang","Xin Zhou","Nan Du","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2502.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12849v1","updated":"2025-02-18T13:38:19Z","published":"2025-02-18T13:38:19Z","title":"Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection","summary":"  In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.\n","authors":["Gianluca Guglielmo","Marc Masana"],"pdf_url":"https://arxiv.org/pdf/2502.12849v1.pdf","comment":"Code is available at https://github.com/gigug/LIR"},{"id":"http://arxiv.org/abs/2406.09207v2","updated":"2025-02-18T13:36:43Z","published":"2024-06-13T15:08:44Z","title":"Investigating potential causes of Sepsis with Bayesian network structure\n  learning","summary":"  Sepsis is a life-threatening and serious global health issue. This study\ncombines knowledge with available hospital data to investigate the potential\ncauses of Sepsis that can be affected by policy decisions. We investigate the\nunderlying causal structure of this problem by combining clinical expertise\nwith score-based, constraint-based, and hybrid structure learning algorithms. A\nnovel approach to model averaging and knowledge-based constraints was\nimplemented to arrive at a consensus structure for causal inference. The\nstructure learning process highlighted the importance of exploring data-driven\napproaches alongside clinical expertise. This includes discovering unexpected,\nalthough reasonable, relationships from a clinical perspective. Hypothetical\ninterventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and\nDiabetes suggest that the presence of any of these risk factors in patients\nincreases the likelihood of Sepsis. This finding, alongside measuring the\neffect of these risk factors on Sepsis, has potential policy implications.\nRecognising the importance of prediction in improving health outcomes related\nto Sepsis, the model is also assessed in its ability to predict Sepsis by\nevaluating accuracy, sensitivity, and specificity. These three indicators all\nhad results around 70%, and the AUC was 80%, which means the causal structure\nof the model is reasonably accurate given that the models were trained on data\navailable for commissioning purposes only.\n","authors":["Bruno Petrungaro","Neville K. Kitson","Anthony C. Constantinou"],"pdf_url":"https://arxiv.org/pdf/2406.09207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12845v1","updated":"2025-02-18T13:25:00Z","published":"2025-02-18T13:25:00Z","title":"MOLLM: Multi-Objective Large Language Model for Molecular Design --\n  Optimizing with Experts","summary":"  Molecular design plays a critical role in advancing fields such as drug\ndiscovery, materials science, and chemical engineering. This work introduces\nthe Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel\nframework that combines domain-specific knowledge with the adaptability of\nLarge Language Models to optimize molecular properties across multiple\nobjectives. Leveraging in-context learning and multi-objective optimization,\nMOLLM achieves superior efficiency, innovation, and performance, significantly\nsurpassing state-of-the-art (SOTA) methods. Recognizing the substantial impact\nof initial populations on evolutionary algorithms, we categorize them into\nthree types: best initial, worst initial, and random initial, to ensure the\ninitial molecules are the same for each method across experiments. Our results\ndemonstrate that MOLLM consistently outperforms SOTA models in all of our\nexperiments. We also provide extensive ablation studies to evaluate the\nsuperiority of our components.\n","authors":["Nian Ran","Yue Wang","Richard Allmendinger"],"pdf_url":"https://arxiv.org/pdf/2502.12845v1.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2410.08524v2","updated":"2025-02-18T13:08:33Z","published":"2024-10-11T04:55:08Z","title":"IGNN-Solver: A Graph Neural Solver for Implicit Graph Neural Networks","summary":"  Implicit graph neural networks (IGNNs), which exhibit strong expressive power\nwith a single layer, have recently demonstrated remarkable performance in\ncapturing long-range dependencies (LRD) in underlying graphs while effectively\nmitigating the over-smoothing problem. However, IGNNs rely on computationally\nexpensive fixed-point iterations, which lead to significant speed and\nscalability limitations, hindering their application to large-scale graphs. To\nachieve fast fixed-point solving for IGNNs, we propose a novel graph neural\nsolver, IGNN-Solver, which leverages the generalized Anderson Acceleration\nmethod, parameterized by a tiny GNN, and learns iterative updates as a\ngraph-dependent temporal process. To improve effectiveness on large-scale graph\ntasks, we further integrate sparsification and storage compression methods,\nspecifically tailored for the IGNN-Solver, into its design. Extensive\nexperiments demonstrate that the IGNN-Solver significantly accelerates\ninference on both small- and large-scale tasks, achieving a $1.5\\times$ to\n$8\\times$ speedup without sacrificing accuracy. This advantage becomes more\npronounced as the graph scale grows, facilitating its large-scale deployment in\nreal-world applications. The code to reproduce our results is available at\nhttps://github.com/landrarwolf/IGNN-Solver.\n","authors":["Junchao Lin","Zenan Ling","Zhanbo Feng","Jingwen Xu","Minxuan Liao","Feng Zhou","Tianqi Hou","Zhenyu Liao","Robert C. Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.08524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18885v2","updated":"2025-02-18T13:01:07Z","published":"2024-09-27T16:20:51Z","title":"HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting","summary":"  The application of large deep learning models in weather forecasting has led\nto significant advancements in the field, including higher-resolution\nforecasting and extended prediction periods exemplified by models such as Pangu\nand Fuxi. Despite these successes, previous research has largely been\ncharacterized by the neglect of extreme weather events, and the availability of\ndatasets specifically curated for such events remains limited. Given the\ncritical importance of accurately forecasting extreme weather, this study\nintroduces a comprehensive dataset that incorporates high-resolution extreme\nweather cases derived from the High-Resolution Rapid Refresh (HRRR) data, a\n3-km real-time dataset provided by NOAA.\n  We also evaluate the current state-of-the-art deep learning models and\nNumerical Weather Prediction (NWP) systems on HR-Extreme, and provide a\nimproved baseline deep learning model called HR-Heim which has superior\nperformance on both general loss and HR-Extreme compared to others. Our results\nreveal that the errors of extreme weather cases are significantly larger than\noverall forecast error, highlighting them as an crucial source of loss in\nweather prediction. These findings underscore the necessity for future research\nto focus on improving the accuracy of extreme weather forecasts to enhance\ntheir practical utility.\n","authors":["Nian Ran","Peng Xiao","Yue Wang","Wesley Shi","Jianxin Lin","Qi Meng","Richard Allmendinger"],"pdf_url":"https://arxiv.org/pdf/2409.18885v2.pdf","comment":"Accepted at the International Conference on Learning Representations\n  (ICLR) 2025. Supplementary matrials link:\n  https://openreview.net/forum?id=5AtlfHYCPa"},{"id":"http://arxiv.org/abs/2502.12834v1","updated":"2025-02-18T13:00:52Z","published":"2025-02-18T13:00:52Z","title":"NTP-INT: Network Traffic Prediction-Driven In-band Network Telemetry for\n  High-load Switches","summary":"  In-band network telemetry (INT) is essential to network management due to its\nreal-time visibility. However, because of the rapid increase in network devices\nand services, it has become crucial to have targeted access to detailed network\ninformation in a dynamic network environment. This paper proposes an\nintelligent network telemetry system called NTP-INT to obtain more fine-grained\nnetwork information on high-load switches. Specifically, NTP-INT consists of\nthree modules: network traffic prediction module, network pruning module, and\nprobe path planning module. Firstly, the network traffic prediction module\nadopts a Multi-Temporal Graph Neural Network (MTGNN) to predict future network\ntraffic and identify high-load switches. Then, we design the network pruning\nalgorithm to generate a subnetwork covering all high-load switches to reduce\nthe complexity of probe path planning. Finally, the probe path planning module\nuses an attention-mechanism-based deep reinforcement learning (DEL) model to\nplan efficient probe paths in the network slice. The experimental results\ndemonstrate that NTP-INT can acquire more precise network information on\nhigh-load switches while decreasing the control overhead by 50\\%.\n","authors":["Penghui Zhang","Hua Zhang","Yuqi Dai","Cheng Zeng","Jingyu Wang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2502.12834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01114v3","updated":"2025-02-18T13:00:46Z","published":"2024-05-02T09:22:54Z","title":"Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling","summary":"  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n","authors":["Sharmita Dey","Benjamin Paassen","Sarath Ravindran Nair","Sabri Boughorbel","Arndt F. Schilling"],"pdf_url":"https://arxiv.org/pdf/2405.01114v3.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR) 2025"},{"id":"http://arxiv.org/abs/2407.20891v5","updated":"2025-02-18T12:44:35Z","published":"2024-07-30T15:07:13Z","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian\n  Neural Networks","summary":"  Computational complexity of Bayesian learning is impeding its adoption in\npractical, large-scale tasks. Despite demonstrations of significant merits such\nas improved robustness and resilience to unseen or out-of-distribution inputs\nover their non- Bayesian counterparts, their practical use has faded to near\ninsignificance. In this study, we introduce an innovative framework to mitigate\nthe computational burden of Bayesian neural networks (BNNs). Our approach\nfollows the principle of Bayesian techniques based on deep ensembles, but\nsignificantly reduces their cost via multiple low-rank perturbations of\nparameters arising from a pre-trained neural network. Both vanilla version of\nensembles as well as more sophisticated schemes such as Bayesian learning with\nStein Variational Gradient Descent (SVGD), previously deemed impractical for\nlarge models, can be seamlessly implemented within the proposed framework,\ncalled Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a\ndramatic reduction in the number of trainable parameters required to\napproximate a Bayesian posterior; and ii) it not only maintains, but in some\ninstances, surpasses the performance of conventional Bayesian learning methods\nand non-Bayesian baselines. Our results with large-scale tasks such as\nImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the\neffectiveness and versatility of Bella in building highly scalable and\npractical Bayesian deep models for real-world applications.\n","authors":["Bao Gia Doan","Afshar Shamsi","Xiao-Yu Guo","Arash Mohammadi","Hamid Alinejad-Rokny","Dino Sejdinovic","Damien Teney","Damith C. Ranasinghe","Ehsan Abbasnejad"],"pdf_url":"https://arxiv.org/pdf/2407.20891v5.pdf","comment":"This paper is accepted in AAAI'25\", and the code is available at\n  https://bnn-bella.github.io/BNN-Bella/"}]}}